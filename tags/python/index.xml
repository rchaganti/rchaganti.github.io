<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Python on Ravikanth Chaganti</title>
    <link>https://ravikanthchaganti.com/tags/python/</link>
    <description>Recent content in Python on Ravikanth Chaganti</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 06 Apr 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://ravikanthchaganti.com/tags/python/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Changes in Autogen release 0.5.1</title>
      <link>https://ravikanthchaganti.com/blog/changes-in-autogen-release-0_5_1/</link>
      <pubDate>Sun, 06 Apr 2025 00:00:00 +0000</pubDate>
      <guid>https://ravikanthchaganti.com/blog/changes-in-autogen-release-0_5_1/</guid>
      <description>&lt;p&gt;Microsoft released [version 0.5.1 of the AutoGen](&lt;a href=&#34;https://github.com/microsoft/autogen/releases/tag/python-v0.5.1&#34;&gt;Release python-v0.5.1 · microsoft/autogen&lt;/a&gt;) stable build last week, and I quickly reviewed the release notes to verify if there were any breaking changes to what I had been writing so far using AutoGen. There is code refactoring to change the structure of base types. This release provides enhanced support for structured output. When working directly with model clients, you can set to a Pydantic model. Here is an example from the release notes.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Building a Model Context Protocol server for Azure</title>
      <link>https://ravikanthchaganti.com/blog/building-a-model-context-protocol-server-for-azure/</link>
      <pubDate>Tue, 25 Feb 2025 00:00:00 +0000</pubDate>
      <guid>https://ravikanthchaganti.com/blog/building-a-model-context-protocol-server-for-azure/</guid>
      <description>&lt;p&gt;The &lt;a href=&#34;https://ravichaganti.com/blog/anthropic-model-context-protocol/&#34;&gt;earlier article&lt;/a&gt; in this &lt;a href=&#34;https://ravichaganti.com/series/mcp/&#34;&gt;series&lt;/a&gt; introduced Anthropic&amp;rsquo;s Model Context Protocol. It presented an example of building a simple MCP server for use with the Claude desktop application. The &lt;a href=&#34;https://github.com/rchaganti/mcp-servers/tree/main/hello-world&#34;&gt;hello-world example&lt;/a&gt; was a very basic implementation of an MCP server. In today&amp;rsquo;s article, we shall extend our knowledge of creating MCP servers to achieve more practical applications. We will build an MCP server to interact with Microsoft Azure resources.&lt;/p&gt;&#xA;&lt;p&gt;Anthropic made bootstrap MCP server development easy by providing the &lt;code&gt;create-mcp-server&lt;/code&gt; package. To get started, you need to install this locally as a tool.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Understanding Agents in Microsoft AutoGen framework</title>
      <link>https://ravikanthchaganti.com/blog/understanding-agents-in-autogen-framework/</link>
      <pubDate>Mon, 10 Feb 2025 00:00:00 +0000</pubDate>
      <guid>https://ravikanthchaganti.com/blog/understanding-agents-in-autogen-framework/</guid>
      <description>&lt;p&gt;In the previous parts of this series on the &lt;a href=&#34;https://ravichaganti.com/series/autogen/&#34;&gt;Microsoft AutoGen framework&lt;/a&gt;, we looked at how to get started with the AutoGen framework. We examined the process of building a simple agent using the prebuilt &lt;code&gt;AssistantAgent&lt;/code&gt; in AutoGen and explored the development of a multi-agent team.&lt;/p&gt;&#xA;&lt;p&gt;However, what is an agent in the context of AutoGen? How do agents communicate with each other? I needed answers to these questions before proceeding to the next step in using AutoGen to build larger, multi-agent applications. So, I started exploring the &lt;code&gt;Core&lt;/code&gt; package upon which the &lt;code&gt;autogen-chat&lt;/code&gt; is built. This article is a result of that exploration.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Teams of agents in AutoGen</title>
      <link>https://ravikanthchaganti.com/blog/teams-of-agents-in-autogen/</link>
      <pubDate>Wed, 05 Feb 2025 00:00:00 +0000</pubDate>
      <guid>https://ravikanthchaganti.com/blog/teams-of-agents-in-autogen/</guid>
      <description>&lt;p&gt;In the previous article in this series on Microsoft AutoGen, we looked at &lt;a href=&#34;https://ravichaganti.com/blog/getting-started-with-autogen-framework-for-building-ai-agents-and-applications/&#34;&gt;how to get started with Microsoft AutoGen&lt;/a&gt; and create a simple agent. We built a weather agent capable of retrieving real-time weather data. Through this demonstration, we gained an understanding of the basics of implementing an agent using the AutoGen framework. In this article, we will examine the process of building a team of agents or a multi-agent team to achieve a slightly more complex goal. We will start this exploration by building a single-agent team. Yes, you read that correctly. AutoGen allows you to create a single-agent team. This is useful in running an agent in a loop until a goal is achieved.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Model Context Protocol by Anthropic for connecting AI models to data</title>
      <link>https://ravikanthchaganti.com/blog/anthropic-model-context-protocol/</link>
      <pubDate>Sat, 01 Feb 2025 00:00:00 +0000</pubDate>
      <guid>https://ravikanthchaganti.com/blog/anthropic-model-context-protocol/</guid>
      <description>&lt;p&gt;A couple of months ago, Anthropic introduced and open-sourced the &lt;a href=&#34;https://www.anthropic.com/news/model-context-protocol&#34;&gt;Model Context Protocol&lt;/a&gt; (MCP). MCP is the new standard for connecting AI models to external data sources and APIs more easily and consistently. With the advances in AI, models are becoming increasingly powerful in reasoning and quality. However, as text-completion machines, these models still lack access to real-time data. AI providers have worked around this using Retrieval Augmented Generation (RAG) and tool calling. Every data source requires custom implementation, and every provider has a way of integrating tools with AI models. MCP addresses these silos by providing a universal, open standard for connecting AI systems with data sources.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Getting started with AutoGen framework for building AI agents and applications</title>
      <link>https://ravikanthchaganti.com/blog/getting-started-with-autogen-framework-for-building-ai-agents-and-applications/</link>
      <pubDate>Thu, 30 Jan 2025 00:00:00 +0000</pubDate>
      <guid>https://ravikanthchaganti.com/blog/getting-started-with-autogen-framework-for-building-ai-agents-and-applications/</guid>
      <description>&lt;p&gt;I have been closely following developments in the Agentic AI space. &lt;a href=&#34;https://microsoft.github.io/autogen&#34;&gt;AutoGen&lt;/a&gt; from Microsoft Research is emerging as a powerful yet easy-to-use framework for building AI agents and applications. With their &lt;a href=&#34;https://www.microsoft.com/en-us/research/blog/autogen-v0-4-reimagining-the-foundation-of-agentic-ai-for-scale-extensibility-and-robustness/&#34;&gt;re-architected release&lt;/a&gt; (0.4.0), they have re-imagined building advanced AI applications. This release introduces an asynchronous, event-driven architecture.&lt;/p&gt;&#xA;&lt;p&gt;&lt;figure class=&#34;figure &#34; style=&#34;max-width: 100%; margin: 1.5rem auto;&#34;&gt;&#xA;  &lt;a href=&#34;https://ravikanthchaganti.com/images/ai/autogen-arch.jpg&#34; class=&#34;glightbox&#34; data-gallery=&#34;post-images&#34; data-glightbox=&#34;description: &#34;&gt;&#xA;    &lt;img src=&#34;https://ravikanthchaganti.com/images/ai/autogen-arch.jpg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; style=&#34;width: 100%; height: auto; border-radius: 0.5rem; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);&#34;&gt;&#xA;  &lt;/a&gt;&lt;/figure&gt;&#xA; &#xA; &lt;link rel=&#34;stylesheet&#34; href=&#34;https://cdn.jsdelivr.net/npm/glightbox@3.3.0/dist/css/glightbox.min.css&#34;&gt;&#xA; &#xA; &#xA; &lt;script src=&#34;https://cdn.jsdelivr.net/npm/glightbox@3.3.0/dist/js/glightbox.min.js&#34;&gt;&lt;/script&gt;&#xA; &#xA; &#xA; &lt;script&gt;&#xA;   document.addEventListener(&#39;DOMContentLoaded&#39;, function() {&#xA;     const lightbox = GLightbox({&#xA;       selector: &#39;.glightbox&#39;,&#xA;       touchNavigation: true,&#xA;       loop: true,&#xA;       autoplayVideos: true,&#xA;       zoomable: true,&#xA;       draggable: true,&#xA;       closeButton: true,&#xA;       moreLength: 0&#xA;     });&#xA;   });&#xA; &lt;/script&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Predicted outputs in Azure OpenAI</title>
      <link>https://ravikanthchaganti.com/blog/predicted-outputs-azure-openai/</link>
      <pubDate>Fri, 10 Jan 2025 00:00:00 +0000</pubDate>
      <guid>https://ravikanthchaganti.com/blog/predicted-outputs-azure-openai/</guid>
      <description>&lt;p&gt;Sometimes, you may want the LLM to perform only minimal changes to what is provided as a prompt. For example, you have a couple of paragraphs of text that you want the LLM to modify to ensure no spelling mistakes. You do not want the LLM to change the overall content, but make sure the misspellings are corrected. This usually helps in reducing the LLM response latency. This is useful in scenarios where you already know a large portion of the expected response and is well-suited for code completion and error detection scenarios. In this part of the &lt;a href=&#34;https://ravichaganti.com/series/azure-openai/&#34;&gt;series of articles on Azure OpenAI&lt;/a&gt;, we will use predicted outputs with Azure OpenAI to build AI applications.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Parallel tool calling in Azure OpenAI</title>
      <link>https://ravikanthchaganti.com/blog/parallel-tool-calling-azure-openai/</link>
      <pubDate>Tue, 20 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://ravikanthchaganti.com/blog/parallel-tool-calling-azure-openai/</guid>
      <description>&lt;p&gt;We have learned to perform single- and multi-tool calling with the Azure OpenAI API for chat completions. This part of the &lt;a href=&#34;https://ravichaganti.com/series/azure-openai/&#34;&gt;series&lt;/a&gt; on Azure OpenAI will describe the parallel tool calling feature and how to implement it. Parallel tool calling allows you to perform multiple calls together. This enables parallel execution, result retrieval, and fewer calls to the LLM. Parallelizing tool calls improves overall performance.&lt;/p&gt;&#xA;&lt;p&gt;In a previous example on retrieving weather at a given location, we examined how to iterate over the LLM response for tool calls and append it to the conversation history before making the next LLM API call.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Structured output in Azure OpenAI</title>
      <link>https://ravikanthchaganti.com/blog/structured-output-azure-openai/</link>
      <pubDate>Sun, 18 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://ravikanthchaganti.com/blog/structured-output-azure-openai/</guid>
      <description>&lt;p&gt;In this &lt;a href=&#34;https://ravichaganti.com/series/azure-openai/&#34;&gt;series&lt;/a&gt;, we have examined the basics of Azure Open AI, using the chat completions API, streaming responses, and finally, single and multi-tool calling. In today&amp;rsquo;s article, we will examine how to return structured output from the LLM response. We will first examine structured output without function calling and then update the earlier multi-function calling example to output JSON instead of text.&lt;/p&gt;&#xA;&lt;p&gt;Structured outputs tell an LLM to follow the schema represented by the &lt;code&gt;response_format&lt;/code&gt; parameter of a request to the LLM. We can use &lt;a href=&#34;https://docs.pydantic.dev/latest/&#34;&gt;Pydantic&lt;/a&gt; to build the schema.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Implementing multiple tool/function calling when using Azure OpenAI</title>
      <link>https://ravikanthchaganti.com/blog/azure-openai-function-calling-with-multiple-tools/</link>
      <pubDate>Fri, 16 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://ravikanthchaganti.com/blog/azure-openai-function-calling-with-multiple-tools/</guid>
      <description>&lt;p&gt;In the &lt;a href=&#34;https://ravichaganti.com/blog/azure-openai-function-calling/&#34;&gt;last article&lt;/a&gt; of &lt;a href=&#34;https://ravichaganti.com/series/azure-openai/&#34;&gt;this series&lt;/a&gt;, we learned about function/tool calling. Based on the prompt, the LLM indicates that we must call the &lt;code&gt;get_weather&lt;/code&gt; tool. The LLM finally returns the answer to our prompt using the tool response. However, let us try to add a few more variables to our prompt. The updated prompt will be &amp;ldquo;What&amp;rsquo;s the weather like in Bengaluru next week?&amp;rdquo;.&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;&#xA;&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;&#xA;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1&#xA;&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2&#xA;&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3&#xA;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&#xA;&lt;td class=&#34;lntd&#34;&gt;&#xA;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ python .\05_function_calling.py&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;get_current_time called with location: Bengaluru&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;The current temperature in Bengaluru is approximately 28.2°C. However, for next week&amp;#39;s weather prediction, you&amp;#39;d need a forecast service as I currently provide only current weather information.&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&#xA;&lt;/div&gt;&#xA;&lt;/div&gt;&lt;p&gt;LLM uses the &lt;code&gt;get_weather&lt;/code&gt; tool to determine the current weather but fails to determine next week&amp;rsquo;s weather. This is because we have not provided any tool for the LLM to determine what next week means. Determining the meaning of next week requires the knowledge of the current date and time. This article will demonstrate how to add multiple tool-calling capabilities to our program. With the updated script, you can receive the weather information for a specific date.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Implementing tool/function calling when using Azure OpenAI</title>
      <link>https://ravikanthchaganti.com/blog/azure-openai-function-calling/</link>
      <pubDate>Mon, 12 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://ravikanthchaganti.com/blog/azure-openai-function-calling/</guid>
      <description>&lt;p&gt;In the last article of &lt;a href=&#34;https://ravichaganti.com/series/azure-openai/&#34;&gt;this series&lt;/a&gt;, we learned how to use the chat completion API. Towards the end, we learned that the LLMs have a knowledge cut-off date and no real-time access to information. However, this can be bridged using the tool/function calling feature of Azure OpenAI service. In this article, we shall learn how to implement tool calling.&lt;/p&gt;&#xA;&lt;p&gt;Azure OpenAI&amp;rsquo;s function calling capability lets you connect your language models to external tools and APIs, enabling them to perform a wider range of tasks and access real-time information. This opens up a world of possibilities, allowing your models to interact with the real world in ways never imagined.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Using chat completion API in Azure OpenAI</title>
      <link>https://ravikanthchaganti.com/blog/using-chat-completion-api-azure-openai/</link>
      <pubDate>Thu, 08 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://ravikanthchaganti.com/blog/using-chat-completion-api-azure-openai/</guid>
      <description>&lt;p&gt;So far in &lt;a href=&#34;https://ravichaganti.com/series/azure-openai/&#34;&gt;this series&lt;/a&gt;, we have looked at the Azure OpenAI completion API, which generates a response for a given prompt. This is a legacy API, and using the chat completion API is recommended. We can build conversational chatbots and similar applications with the chat completion API. This article will examine how to use the Azure OpenAI chat completion API. In the earlier articles, we used the &lt;code&gt;client.completions.create()&lt;/code&gt; function to generate a response. We need to use the &lt;code&gt;client.chat.completions.create()&lt;/code&gt; in the &lt;code&gt;openai&lt;/code&gt; library to build a conversation with the LLM.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Streaming model responses when using Azure OpenAI</title>
      <link>https://ravikanthchaganti.com/blog/streaming-completions-azure-openai/</link>
      <pubDate>Sun, 04 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://ravikanthchaganti.com/blog/streaming-completions-azure-openai/</guid>
      <description>&lt;p&gt;Responses are streamed to the user interface as they are generated using ChatGPT and similar tools. This eliminates the need for the user to wait until the complete response is generated. In today&amp;rsquo;s article, we shall look at streaming LLM-generated responses when using Azure OpenAI API in Python.  In the earlier part of &lt;a href=&#34;https://ravichaganti.com/series/azure-openai/&#34;&gt;this series&lt;/a&gt;, we learned about the &lt;code&gt;client.completions.create()&lt;/code&gt; function used to send a prompt to the LLM and retrieve one or more responses. This function supports a parameter called &lt;code&gt;stream&lt;/code&gt; when set to &lt;code&gt;True&lt;/code&gt;, asks LLM to stream the response as it gets generated. The way to handle this response is a bit different from a standard completion response.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Getting started with Azure OpenAI</title>
      <link>https://ravikanthchaganti.com/blog/getting-started-with-azure-openai/</link>
      <pubDate>Thu, 01 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://ravikanthchaganti.com/blog/getting-started-with-azure-openai/</guid>
      <description>&lt;p&gt;Generative AI and &lt;a href=&#34;https://openai.com/&#34;&gt;OpenAI&lt;/a&gt; should not be alien anymore. Several startups are already riding this new wave and creating stunning applications that solve several important use cases. I use GenAI regularly to learn and become more efficient in coding. GitHub Copilot has been a good friend. I experimented with creating Large Language Model (LLM) applications using different providers (&lt;a href=&#34;https://platform.openai.com/docs/overview&#34;&gt;OpenAI&lt;/a&gt; and &lt;a href=&#34;https://ai.google.dev/&#34;&gt;Google Gemini&lt;/a&gt;) and in different programming languages. OpenAI provides client libraries that can be used with any provider that offers an OpenAI-compatible API. For example, we can use the &lt;a href=&#34;https://pypi.org/project/openai/&#34;&gt;OpenAI Python library&lt;/a&gt; to work with OpenAI and Azure OpenAI services.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
