[{"categories":["Agents"],"content":"We are in the middle of a fundamental shift in how software gets built. LLMs are no longer used as passive autocomplete machines. AI agents reason, plan, retrieve information, and act. An agent is only as good as the knowledge it can access. An LLM\u0026rsquo;s training data is frozen in time. The Kubernetes API changed last week. Your company shipped a new SDK version this morning. The compliance rules were updated yesterday. If an agent can\u0026rsquo;t reach this knowledge at inference time, it hallucinates, generates deprecated code, or simply gives up.\nThe solution? Build documentation that agents can consume. Not as an afterthought, but as a deliberate, first-class engineering practice.\nIn today\u0026rsquo;s article, we will look at the landscape of agent-consumable documentation. This includes the standards, patterns, and tools that are emerging to bridge the gap between human-written knowledge and machine-readable context.\nThe Documentation Stack There are various existing and evolving methods to provide knowledge to agents. I like to visualize these options in a stack, the documentation stack.\nThe layers of this stack represent different ways to provide knowledge to the agents. To understand these options, we will explore a sample use case where a developer needs to implement a feature in an existing project.\nThe task A backend developer at a mid-sized e-commerce company has been asked to add Stripe payment processing to the company\u0026rsquo;s checkout flow. This company uses Google Cloud Run for deployment, follows strict PCI compliance policies documented in an internal wiki, and has a TypeScript/Node.js monorepo with specific architectural conventions.\nThe developer opens the IDE and asks the AI coding agent:\nAdd Stripe payment processing to our checkout service. Use Payment Intents and support credit cards and Apple Pay. Deploy to our staging Cloud Run environment when done.\nThis is a realistic, multi-layered task. The agent needs to:\nDiscover what documentation exists for Stripe and Google Cloud. Understand the company\u0026rsquo;s project conventions before writing any code. Read the actual Stripe and Cloud Run documentation efficiently. Understand the Stripe API contract precisely enough to generate correct API calls. Query live documentation services for the latest API details. Follow a multi-step deployment procedure. Check the company\u0026rsquo;s internal compliance requirements. Each of these needs maps to a different layer of the documentation stack shown earlier. Let\u0026rsquo;s walk through them.\nllms.txt Imagine walking into a massive library for the first time. You don\u0026rsquo;t start pulling books off random shelves. You go to the information desk and ask: \u0026ldquo;I need to add payment processing. Where should I start?\u0026rdquo; The librarian hands you a curated reading list.\nThat\u0026rsquo;s llms.txt. It is the librarian\u0026rsquo;s reading list for an AI agent. If robots.txt tells search engine crawlers what not to index, and sitemap.xml tells them what to index, then llms.txt tells AI agents what to read. It is a plain-text, Markdown-formatted file placed at a website\u0026rsquo;s root (e.g., https://docs.stripe.com/llms.txt) that provides a curated overview of a site\u0026rsquo;s most important content. The agent knows it needs Stripe documentation, so it fetches https://docs.stripe.com/llms.txt:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 # Stripe Documentation \u0026gt; Stripe is a suite of APIs for online payment processing. ## Getting Started - [Quick Start Guide](https://docs.stripe.com/quickstart): Set up Stripe in 5 minutes - [API Keys](https://docs.stripe.com/keys): Authentication and key management ## Payments - [Payment Intents](https://docs.stripe.com/payments/payment-intents): Create and manage payments - [Apple Pay](https://docs.stripe.com/apple-pay): Accept Apple Pay in your app - [Webhooks](https://docs.stripe.com/webhooks): Receive event notifications ## SDKs - [Node.js SDK](https://docs.stripe.com/sdks/node): Server-side Node.js integration In seconds, the agent curates a map of exactly where to find information about payment Intents and Apple Pay. It didn\u0026rsquo;t have to crawl the entire Stripe docs site, parse HTML navigation menus, or guess which pages are relevant.\nA valid llms.txt file contains a single H1 heading (the project name), a blockquote summary, and H2 sections with curated link lists. Each link uses the format - [Title](URL): Description. The file lives at the site root and requires zero infrastructure. Stripe, Anthropic, Cloudflare, and dozens of other companies now host llms.txt files. The standard is community-driven and growing as companies recognize AI-discoverability as a competitive advantage.\nAGENTS.md When a new developer joins a team, they don\u0026rsquo;t just start writing code. They go through onboarding — learn the build system, understand the architecture decisions, and absorb the team\u0026rsquo;s style preferences. AGENTS.md is the onboarding document for your AI teammate. AGENTS.md is a project-level Markdown file. It is now a part of the Linux Foundation\u0026rsquo;s Agentic AI Foundation (AAIF). This gives AI coding agents specific instructions for working within a repository. It captures the tribal knowledge that would normally be passed down in Slack threads and code reviews.\nBefore writing a single line of code, the agent reads AGENTS.md at the root of the company\u0026rsquo;s monorepo:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # AGENTS.md ## Build \u0026amp; Run - Build: `npm run build` - Dev server: `npm run dev` - Tests: `npm test` (Vitest) - Lint: `npm run lint` (ESLint + Prettier) ## Architecture - Framework: Express.js on Node.js 20 - Language: TypeScript (strict mode) - API layer: All routes in `src/routes/`, one file per resource - Services: Business logic in `src/services/`, never in route handlers - External integrations: Wrapper clients in `src/clients/` ## Conventions - All environment variables loaded via `src/config/env.ts` — never use `process.env` directly - API keys and secrets: Always use Google Secret Manager, never `.env` files in production - Error handling: Use custom `AppError` class from `src/utils/errors.ts` - Logging: Use the `logger` instance from `src/utils/logger.ts` ## Payment Processing (IMPORTANT) - All payment code goes in `src/services/payment/` - Stripe keys must be loaded from Secret Manager via `src/config/secrets.ts` - All payment amounts are in cents (integer), never floating point - Must emit `payment.created` and `payment.completed` events via `src/events/` Now the agent knows that the Stripe client should go in src/clients/stripe.ts, the payment logic should go in src/services/payment/, environment variables should go through env.ts, and amounts should be integers in cents. Without this, the agent might have placed files in the wrong directories, used process.env.STRIPE_SECRET_KEY directly, or represented amounts as floating-point numbers, all valid code, but every line violates the team\u0026rsquo;s conventions.\nFor large monorepos, AGENTS.md files can be nested. A monorepo might look like:\n1 2 3 4 5 6 7 8 9 10 company/ ├── AGENTS.md ← Global: TypeScript, ESLint, monorepo nav ├── services/ │ ├── checkout/ │ │ └── AGENTS.md ← Checkout-specific: payment patterns, PCI notes │ └── inventory/ │ └── AGENTS.md ← Inventory-specific: DB conventions └── packages/ └── ui/ └── AGENTS.md ← Frontend: React patterns, CSS approach The closest AGENTS.md in the directory tree takes precedence. This is similar to how .gitignore scoping works. As of early 2026, AGENTS.md has been adopted by over 60,000 open-source projects and is supported by virtually every major AI coding agent.\nMarkdown-First Documentation HTML documentation is like reading a technical manual with full-color illustrations, glossy covers, and marketing inserts. Markdown documentation is the same content typed up in plain text. Both carry identical information, but the email is much easier for a machine to process and much cheaper to process. Several companies now serve their documentation in plain Markdown format, alongside the traditional HTML rendering. Stripe pioneered this: append .md to any Stripe docs URL to get a clean, token-efficient Markdown version of the page.\nUsing the URLs discovered via llms.txt, the agent fetches the Payment Intents guide in Markdown:\nHuman URL: https://docs.stripe.com/payments/payment-intents Agent URL: https://docs.stripe.com/payments/payment-intents.md The HTML version includes a navigation sidebar, a header bar, a search widget, breadcrumbs, syntax-highlighted code blocks with framework tabs, and a footer with legal links. Rendered to text, it\u0026rsquo;s thousands of tokens of noise around the actual content.\nThe Markdown version? Pure signal:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # Payment Intents API Use the Payment Intents API to build a payment integration that can handle complex payment flows with a status that changes over the PaymentIntent\u0026#39;s lifecycle. ## Create a PaymentIntent To create a PaymentIntent, make a POST request to `/v1/payment_intents` with at least an `amount` and `currency`: ```javascript const paymentIntent = await stripe.paymentIntents.create({ amount: 2000, currency: \u0026#39;usd\u0026#39;, payment_method_types: [\u0026#39;card\u0026#39;, \u0026#39;apple_pay\u0026#39;], }); ``` ## Confirm the PaymentIntent ... The agent gets the same knowledge at a fraction of the token cost, in a format it can parse natively. Markdown strips away the signal-to-noise ratio with HTML, delivering pure content. This pattern works with any docs-as-code pipeline. If you build documentation with Hugo, Docusaurus, or MkDocs, the Markdown source files already exist. You just need to make them accessible via HTTP. Documentation platforms like Mintlify, ReadMe, and GitBook are also beginning to offer Markdown endpoints.\nOpenAPI Specifications If llms.txt is the library index and Markdown docs are the books, then an OpenAPI spec is the operating manual for a specific machine. It doesn\u0026rsquo;t explain philosophy. It tells you exactly which buttons to press, in what order, and what will happen when you do. The OpenAPI Specification (OAS) defines every endpoint, parameter, request body, response format, authentication method, and error code in a structured, machine-readable YAML or JSON document. Where Markdown docs explain concepts, OpenAPI specs define contracts.\nWith this, the agent reads the conceptual docs and understands Payment Intents. Now it needs to write actual API calls. It fetches Stripe\u0026rsquo;s OpenAPI spec and finds the precise contract for creating a Payment Intent:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 paths: /v1/payment_intents: post: operationId: CreatePaymentIntent summary: Creates a PaymentIntent object. description: \u0026gt; Creates a PaymentIntent object. After it is created, attach a payment method and confirm to continue the payment. requestBody: required: true content: application/x-www-form-urlencoded: schema: type: object required: - amount - currency properties: amount: type: integer description: Amount intended to be collected, in cents. currency: type: string description: Three-letter ISO currency code. payment_method_types: type: array items: type: string description: List of payment method types to accept. responses: \u0026#39;200\u0026#39;: description: Successful response content: application/json: schema: $ref: \u0026#39;#/components/schemas/PaymentIntent\u0026#39; \u0026#39;400\u0026#39;: description: Invalid request — check parameters With this spec, the agent generates type-safe TypeScript code with confidence. It knows the amount is an integer (not a float — consistent with what AGENTS.md also specified), currency is a string, and payment_method_types is an array. No more guessing. Modern agent frameworks (LangChain, Google Agent Development Kit, etc.) can automatically convert OpenAPI specs into callable tools. The agent doesn\u0026rsquo;t just read the spec — it uses it to act.\nMCP Servers Static files (llms.txt, AGENTS.md) are like textbooks. You read them once and reference them. MCP servers are like having a reference librarian on speed dial. You describe what you need in natural language, and they hand you the exact page. Always current, always relevant to your specific question. The Model Context Protocol (MCP) is an open standard, and it is now part of the Linux Foundation\u0026rsquo;s Agentic AI Foundation. MCP provides a standardized interface for LLMs to query external data sources and tools. MCP servers expose three primitives: Resources (data to read), Tools (functions to call), and Prompts (templated interactions). MCP creates a uniform interface to heterogeneous knowledge sources. The agent queries Google Cloud, Microsoft Azure, and Stripe docs using the exact same protocol. No more bespoke integrations per vendor. As more documentation providers expose MCP servers, the agent\u0026rsquo;s reach expands without any code changes.\nFor the task at hand, the agent has the core Stripe integration working. Now it needs to deploy to Google Cloud Run. Rather than relying on possibly stale training data, it queries the Google Developer Knowledge MCP server directly:\n1 2 3 4 5 6 7 8 9 10 Agent → MCP Tool Call: search_documents query: \u0026#34;Deploy a Node.js service to Cloud Run with Secret Manager integration\u0026#34; MCP Server → Response: [1] \u0026#34;Deploying to Cloud Run\u0026#34; (cloud.google.com/run/docs/deploying) \u0026#34;Use gcloud run deploy to deploy a container image to Cloud Run...\u0026#34; [2] \u0026#34;Using secrets from Secret Manager\u0026#34; (cloud.google.com/run/docs/configuring/secrets) \u0026#34;Cloud Run integrates with Secret Manager to securely provide secrets...\u0026#34; [3] \u0026#34;Configuring service accounts\u0026#34; (cloud.google.com/run/docs/securing/service-identity) ... The agent then fetches the full content of the most relevant documents:\n1 2 3 4 5 6 7 8 9 10 11 12 Agent → MCP Tool Call: get_document name: \u0026#34;documents/cloud.google.com/run/docs/configuring/secrets\u0026#34; MCP Server → Response: # Using secrets You can make a secret available from Secret Manager to your Cloud Run service by mounting the secret as a volume or exposing it as an environment variable... gcloud run deploy checkout-service \\ --image gcr.io/company/checkout:latest \\ --set-secrets STRIPE_KEY=stripe-secret-key:latest The agent now has the exact, current Cloud Run deployment syntax and not what was in its training data from months ago, but what\u0026rsquo;s live in Google\u0026rsquo;s docs right now. It does the same with the Microsoft Learn MCP server when it needs to verify Azure-specific patterns.\nAgent Skills Files If documentation is a medical textbook that describes what diseases exist, their symptoms, and their mechanisms, then a skill file is a surgical checklist (step 1: verify patient identity; step 2: confirm procedure site; step 3: administer anesthesia). One provides understanding; the other provides an executable procedure. Agent skills files are portable packages of instructions, stored as SKILL.md files, that encode procedural knowledge: step-by-step workflows, not just reference information. Unlike documentation that describes what something is, skills describe how to do something.\nFor the task the developer prompted, the agent has written the Stripe integration code and knows how to deploy it to Cloud Run. But the company has a specific, multi-step deployment process, including staging validation, smoke tests, and rollback procedures, that isn\u0026rsquo;t documented externally. This is internal operational knowledge. The monorepo has a skills directory. At startup, the agent loaded only lightweight metadata:\n1 2 3 4 Available skills: - deploy-to-staging: \u0026#34;Deploy a service to the Cloud Run staging environment\u0026#34; - run-load-tests: \u0026#34;Run load tests against a staging service\u0026#34; - database-migration: \u0026#34;Apply and verify database schema migrations\u0026#34; During stage 1 (Discovery), the agent sees that deploy-to-staging matches the developer\u0026rsquo;s request. It activates the skill. It is hardly 20 tokens.\nIn the second stage, activation, the agent reads the full SKILL.md:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 --- name: deploy-to-staging description: Deploy a service to the Cloud Run staging environment --- ## Prerequisites - Code must pass `npm test` and `npm run lint` with zero errors - Docker image must be built and pushed to `gcr.io/company-staging/` - PCI compliance check must pass (run `scripts/pci-check.sh`) ## Steps 1. Build the Docker image: `docker build -t gcr.io/company-staging/checkout:$(git rev-parse --short HEAD) .` 2. Push to Container Registry: `docker push gcr.io/company-staging/checkout:$(git rev-parse --short HEAD)` 3. Deploy to Cloud Run staging: `gcloud run deploy checkout-service --project company-staging \\ --image gcr.io/company-staging/checkout:$(git rev-parse --short HEAD) \\ --region us-central1 \\ --set-secrets STRIPE_KEY=stripe-staging-key:latest` 4. Run smoke tests: `scripts/smoke-test.sh staging` 5. Verify in Datadog dashboard: Check for error rate \u0026lt; 0.1% ## Rollback If smoke tests fail: `gcloud run services update-traffic checkout-service \\ --to-revisions=LATEST=0,\u0026lt;previous-revision\u0026gt;=100` In the final stage, as needed, the agent checks scripts/pci-check.sh to understand what the compliance check does and opens scripts/smoke-test.sh to see which endpoints it validates. This layered loading is critical. If the agent loaded the full contents of all 15 skills at startup, it would consume thousands of tokens of irrelevant context, degrading its performance on the actual task. Progressive disclosure works like how humans process information. You scan chapter titles (Layer 1), open the relevant chapter (Layer 2), and check the appendix only if you need a deep dive (Layer 3). The agent does the same. Skills are used by Claude Code, Gemini CLI, Kiro, and various agent frameworks. The open-standard structure (Markdown + optional scripts/resources) makes them portable.\nRAG Pipelines If MCP is a reference librarian, RAG is an entire research department. You don\u0026rsquo;t just get directed to the right book. A team of researchers reads through thousands of internal documents, extracts the relevant passages, and synthesizes a briefing tailored to your exact question. Retrieval-Augmented Generation (RAG) ingests documents, converts them into vector embeddings, stores them in a vector database, and retrieves relevant chunks at query time to augment the agent\u0026rsquo;s context. It\u0026rsquo;s the catch-all for knowledge that doesn\u0026rsquo;t fit into static files or public APIs.\nFor the task at hand, the agent has written the code, run tests, and loaded the deployment skill. But before deploying, the agent remembers that AGENTS.md mentioned PCI compliance and that the skill\u0026rsquo;s prerequisites include running a PCI check. The agent needs to understand the company\u0026rsquo;s specific PCI requirements.\nThis knowledge lives in an internal Confluence wiki. Not on a public website, not in the repo, and not available through any MCP server. But the company\u0026rsquo;s platform team has set up a RAG pipeline over their internal documentation.\nThe agent queries the RAG system:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 Agent → RAG Query: \u0026#34;PCI compliance requirements for payment processing services\u0026#34; RAG → Retrieved Chunks: [Chunk 1 — PCI Policy v3.2, Section 4.1] \u0026#34;All services that process, store, or transmit cardholder data must: - Never log full card numbers (mask to last 4 digits) - Use TLS 1.2+ for all Stripe API communications - Store Stripe API keys exclusively in Google Secret Manager - Implement idempotency keys for all payment creation requests\u0026#34; [Chunk 2 — Incident Report IR-2025-044] \u0026#34;Root cause: Payment service was logging full PaymentIntent responses including card fingerprints. Fix: Added response sanitization in the logging middleware. Policy updated to require all payment-related log entries pass through sanitize() before logging.\u0026#34; [Chunk 3 — Architecture Decision Record ADR-031] \u0026#34;Decision: All payment amounts stored as integers in cents. Context: We experienced rounding errors with floating-point USD amounts. Consequences: All APIs accept/return amounts in cents; frontend handles display formatting.\u0026#34; This is gold. The agent now adds a sanitize() call to its logging middleware, implements idempotency keys for payment creation (something it might not have done otherwise), and double-confirms that it\u0026rsquo;s using integer cents, with the organizational context for why.\nThe most powerful pattern is Agentic RAG, where the agent dynamically decides when and from which sources to retrieve. The agent in our example demonstrated this naturally. It consulted llms.txt, AGENTS.md, Markdown docs, an OpenAPI spec, MCP servers, a skill file, and the RAG pipeline, orchestrating across all seven layers within a single task.\nAs you see in this scenario, no single layer was sufficient. Each contributed a different type of knowledge. However, you may not always need all seven layers. So, how do you choose?\nHere\u0026rsquo;s a decision framework:\nYour Need Start With Why Help external AI agents discover your site llms.txt Zero infrastructure, widely supported Guide coding agents in your repo AGENTS.md Project-scoped, version-controlled Serve docs content to agents via HTTP Markdown endpoints (.md) Token-efficient, no protocol needed Enable agents to call your API correctly OpenAPI spec Precise, machine-actionable Provide a live, queryable docs interface MCP Server Standardized, searchable, always current Encode multi-step internal workflows Skills (SKILL.md) Procedural, portable, progressive disclosure Make internal knowledge agent-accessible RAG pipeline Flexible, private, any content type What\u0026rsquo;s clear is that the era of documentation-as-an-afterthought is ending. In the age of agents, your documentation is not just a reference for human developers. It is the knowledge infrastructure that powers autonomous systems.\n","date":"2026-02-10T00:00:00Z","excerpt":"Learn what you need to make your knowledge consumable by AI — from static files to live APIs.","permalink":"https://ravichaganti.com/blog/documentation-stack-for-ai-agents/","series":null,"title":"Documentation stack for AI agents"},{"categories":["Agent Skills","MCP"],"content":"Anthropic introduced Agent Skills in October 2025, just a year after they introduced Model Context Protocol (MCP). Today, both Agent Skills and MCP are open standards. The fact that a single company produced two standards that seem to be competing has fueled heated debate on social media, Hacker News, and developer blogs. Some claim that MCP is dead, while others argue that the comparison itself is moot. The truth is more nuanced than that. You can build powerful agents with either or both. For anyone building enterprise-grade agent-powered systems, understanding where these standards genuinely overlap, where they serve fundamentally different purposes, and where each falls short is essential.\nLet us start with a quick overview of each standard.\nModel Context Protocol (MCP) Every AI application that wants to talk to an external service or access external data needs its own integration. This is an N x M integration problem. With MCP, this is reduced to N + M. You build one MCP server per external service, one MCP client per AI application, and they all interoperate through a universal JSON-RPC 2.0 protocol. Anthropic calls it \u0026ldquo;USB-C for AI applications\u0026rdquo;.\nThe MCP architecture has three components. MCP hosts are AI applications such as VS Code, Claude Code, Goose, and ChatGPT. Each MCP host contains one or more MCP Clients that maintain a 1:1 connection to an MCP server. An MCP server exposes tools, resources, and prompts. These MCP servers can be run locally via stdio transport or remotely via streamable HTTP. When a client connects to an MCP server, it calls tools/list to discover available tools. The LLM then decides when to call tools/call to invoke specific tools based on the tool descriptions loaded into its context window.\nMCP saw extraordinary adoption since its announcement in November 2024. The open specification underwent multiple revisions, adding or improving security requirements and introducing new features. The MCP registry is the official centralized metadata repository for publicly accessible MCP servers. The SDK availability for different programming languages enabled developers to quickly adopt and implement MCP servers and clients. Every major AI company has come out in support of MCP. OpenAI integrated MCP into ChatGPT and the Agents SDK. Google confirmed support in Gemini, ADK, and Cloud databases. Microsoft built it into VS Code, Copilot, and Microsoft Agent Framework. AWS embedded it in Bedrock and AgentCore. You throw a stone, and it literally falls on an MCP server in a public MCP registry. There is one for almost every popular service on the public internet.\nAgent Skills Anthropic introduced Agent Skills in October 2025 to give Claude repeatable, domain-specific expertise. In December 2025, the company released the Agent Skills specification as an open standard.\nAt its core, a Skill is a directory containing a SKILL.md file with YAML frontmatter, optional scripts, templates, reference documents, and other assets such as images. The SKILL.md contains the name, description, and detailed instructions that teach an agent how to perform a specific task: a company\u0026rsquo;s code review checklist, a financial modeling methodology, a brand guidelines enforcement workflow, or a legal contract review procedure. The standard defines packaging and metadata but leaves execution semantics to each platform. Skills run in whatever execution environment the host agent provides. The agent reads the SKILL.md like a human reading a procedure manual, then follows the instructions using whatever tools it has available (bash, file system, code execution).\nAt the heart of Skills is the principle of progressive disclosure. Instead of loading all skill content into the context window upfront, the system works in three phases. In the discovery phase, only the skill name and description load into the system prompt. In the activation phase, when a user\u0026rsquo;s request matches a skill\u0026rsquo;s domain, the full SKILL.md instructions load into context. Finally, when in the execution phase, the agent reads additional files (scripts, references, templates) from the skill directory only as needed, using file system operations rather than context window space. This means the amount of knowledge bundled into a skill is effectively unbounded, because the agent navigates it like a reference manual rather than consuming it all at once.\nMany agent platforms, including Claude Code, OpenAI Codex, Cursor, Gemini CLI, VS Code, GitHub Copilot, Amp, Goose, OpenCode, and Letta, have adopted the Agent Skills standard. Vercel introduced a Skills ecosystem platform.\nMCP vs Skills To decide whether MCP vs skills or both, we need to first understand what each cannot do.\nMCP provides an agent or an AI application with a set of tools to work with external services and data. But it cannot teach the agent the procedural knowledge. Consider an example. You can give your agent git tools to work with your project repository. This does not give the agent knowledge of your organization\u0026rsquo;s branching strategy, code review checklist, pull request templates, or issue templates. This is the procedural knowledge buried in your organization\u0026rsquo;s document libraries or in your team as tribal knowledge. This procedural knowledge is what Skills provide.\nGiving an agent Skills does not automagically give it the ability to perform actions. It is like giving someone a recipe for baking a cake without any utensils. They will know the procedural knowledge to bake a cake, but won\u0026rsquo;t actually be able to bake one. Agent skills cannot physically perform any actions. Skills are inert knowledge that depends on the agent having other capabilities to actually take action. MCP is the other capability that Skills need.\nThis narrative highlights the complementary relationship between Skills and MCP. To decide whether to leverage this relationship or choose between them, we need to examine a few key tenets.\nSecurity This is the most important tenet for enterprise adoption.\nSkills are text files with options scripts that live (and execute) in-process as the agent\u0026rsquo;s execution environment. There is no process isolation or a new network boundary. This poses a risk of prompt injection through malicious skill content. A malicious skill could instruct an agent to execute some arbitrary code, read sensitive files, or exfiltrate data through the agent\u0026rsquo;s responses. The skills specification doesn\u0026rsquo;t prescribe how credentials should be handled, how skills should be isolated from one another, or how code execution should be sandboxed. This design choice makes the spec flexible but delegates security decisions entirely to implementers. Different implementations can make vastly different security choices.\nAnthropic\u0026rsquo;s documentation cautions against using skills from untrusted sources without a thorough audit of their contents. You must treat Skills installation with the same rigor as installing software on production systems.\nSkills do not expand an agent\u0026rsquo;s attack surface beyond what it already has. If your agent already has bash access and can read files, a Skill doesn\u0026rsquo;t add new capabilities. Instead, it just provides instructions for using existing ones.\nMCP, on the other hand, runs as a separate process with its own runtime, filesystem access, and credential scope. This architectural separation means each server can be sandboxed, containerized, or run under a dedicated OS user, receiving only the secrets and permissions needed for its specific integration. This is a fundamentally stronger isolation model than Agent Skills provides. This stronger isolation does not mean that MCP is more secure. authzed.com has documented a timeline of MCP security breaches. The MCP spec recommends human-in-the-loop oversight using SHOULD rather than MUST, making it advisory rather than mandatory. Gartner has explicitly warned that \u0026ldquo;MCP introduces new security, stability, and governance risks.\u0026rdquo;\nAgent Skills are optimized for rapid capability expansion, allowing you to control all content. MCP is optimized for production deployments with strict security requirements around external service access.\nAuthentication and Authorization As Skills are in progress, they inherit whatever auth context the agent already has. If the agent has access to environment variables with API keys, or if it\u0026rsquo;s running in a context with OAuth tokens already available, Skills can reference those credentials through the tools the agent invokes. There\u0026rsquo;s no auth protocol to implement. Skills are inert knowledge, and any auth-dependent actions happen through whatever tools the agent has access to (which might themselves be MCP tools). Enterprises need to handle Skills governance through platform admin controls. Administrators can centrally provision which Skills are available, control access, and monitor usage. Claude and OpenAI\u0026rsquo;s Codex offer such controls. There\u0026rsquo;s no OAuth flow to manage, no token exchange to implement, no per-skill authorization boundary. This is both a strength and a limitation.\nMCP\u0026rsquo;s authentication story is anything but perfect. The first version of the MCP specification shipped without mandatory authentication. This led to major security exposures. The next version mandated OAuth 2.1 with PKCE and mandated every MCP server to implement a full OAuth authorization server, which is an unrealistic requirement for enterprises. Later versions removed the need for an authorization server, classified MCP servers as OAuth resource servers, mandated protected resource metadata and resource indicators, and replaced Dynamic Client Registration with Client ID Metadata Documents. Each MCP server connection requires its own OAuth flow, which is operationally burdensome at scale. The community argues that this is still unnecessary complexity for enterprise environments.\nFor enterprise IT operations teams, the auth story creates a stark asymmetry: Skills are trivial to govern through existing IT controls (file access, git permissions, admin provisioning), while MCP requires dedicated auth infrastructure (OAuth servers, token management, gateway proxying) that adds significant operational complexity. This is precisely why the MCP Gateway market has emerged so rapidly.\nContext window and token cost Agent Skills uses progressive disclosure to minimize context impact. As described above, only skill metadata loads at startup (50–100 tokens per skill), full instructions load only when activated (typically under 5,000 tokens), and additional reference files are accessed through the filesystem rather than the context window. Even with a large library of 100 skills, the total always-present context overhead is roughly 5,000–10,000 tokens.\nMCP\u0026rsquo;s context window impact is its most documented operational problem. MCP clients typically load all tool definitions from all connected servers upfront via the tools/list call. Each tool definition might consume 50–1,000+ tokens depending on schema complexity. Anthropic has published mitigation strategies: a meta-tool pattern that uses two gateway tools (one for discovery, one for execution) to achieve 85–95% token reduction, and a code-execution pattern in which the agent writes code to interact with MCP servers in a sandbox, achieving 80–99% token reduction. These are architectural workarounds that add complexity and are not the default behavior of MCP clients.\nAgent Skills\u0026rsquo; progressive disclosure directly addresses MCP\u0026rsquo;s biggest practical pain point. Skills represent Anthropic learning from MCP\u0026rsquo;s deployment challenges and building a knowledge-loading system that treats context budget as a first-class design constraint. Skills have a low token-cost profile compared with the linear scaling of token cost with the number of connected MCP servers. For enterprise deployments with dozens of integrations, the cost differential is substantial. An IT operations platform that connects to 15 external services via MCP could easily consume 50,000–100,000 tokens per session for tool definitions alone. The same operational knowledge encoded as Skills would cost a fraction of what it would in context usage. Skills alone wouldn\u0026rsquo;t provide external connectivity. You\u0026rsquo;d still need something (MCP, direct API calls, or CLI tools) to actually reach the external services.\nEase of deployment Skills are lightweight folders that you can version-control and deploy with well-known tools in the IT operations space. The barrier to entry is as low as writing a Markdown file. The maintenance burden is near-zero compared to running software.\nBuilding an MCP server means implementing the JSON-RPC 2.0 protocol, defining tool schemas with JSON Schema, handling transport (stdio or Streamable HTTP), managing error handling, and implementing OAuth 2.1 authentication. SDKs for many programming languages, and building a simple MCP server with an SDK takes minutes. But production-grade servers with proper error handling, authentication, rate limiting, and observability require substantial engineering investment.\nThe operational cost of developing, deploying, and maintaining an MCP server is significantly higher than writing a text file with instructions.\nAdoption blockers The Agent Skills specification is intentionally minimal, defining only the structure and format of a Skill. This leaves the execution semantics, progressive disclosure mechanisms, and other capabilities to the agent\u0026rsquo;s runtime. This leads to different implementations across different platforms. This forces enterprises to evaluate each platform\u0026rsquo;s implementation independently. Unlike MCP, there is no centralized official registry for the discovery and distribution of Skills. Skill effectiveness varies by model. There is no standard testing framework, no CI/CD integration for skill validation, and no industry-standard metrics for skill reliability.\nMCP\u0026rsquo;s security track record isn\u0026rsquo;t particularly encouraging for enterprise adoption. The authentication complexity and the emergence of MCP gateways as essential infrastructure reflect the gap between the spec\u0026rsquo;s auth design and enterprise operational reality. For enterprise adoption, the ability to observe and debug is most critical. The MCP specification includes no standardized distributed tracing, correlation IDs, cost attribution, or structured logging. Debugging MCP tool failures requires understanding the JSON-RPC protocol, inspecting transport-level messages, and correlating across process boundaries. Gateway vendors are filling this gap, but observability is not built into the protocol.\nWhen to use each Use Agent Skills when you need to encode organizational expertise, workflows, and best practices into a reusable, portable format. Skills are the right choice for procedural consistency — your team\u0026rsquo;s code review process, your company\u0026rsquo;s financial modeling approach, your legal department\u0026rsquo;s contract review checklist. They\u0026rsquo;re also appropriate when you want to share domain knowledge across multiple agents and platforms without vendor lock-in, and when token efficiency is a priority.\nUse MCP when agents need to interact with external systems such as databases, APIs, SaaS tools, file systems, and cloud services. When the task requires reading or writing data to or from systems outside the agent\u0026rsquo;s native environment, MCP provides a standardized connectivity layer. MCP is also the right choice when you want process-level isolation between the agent and external tools, and when you want to share tool capabilities across multiple AI platforms (one MCP server serves all MCP clients).\nUse both together when agents need not only access to external systems but also access in accordance with your organization\u0026rsquo;s specific methods. This is the most powerful pattern: MCP connections for tool access combined with Skills that teach the agent how to use those tools effectively in your context. An MCP server grants Claude access to your Jira instance, and a Skill teaches Claude your team\u0026rsquo;s sprint-planning methodology, backlog grooming criteria, and ticket template conventions.\nConsider Skills + direct CLI/API calls (without MCP) when you\u0026rsquo;re working at an individual developer scale. The overhead of configuring an MCP server outweighs the benefit, and a Skill documenting how to use curl or a CLI tool directly is more practical. This is the workflow driving much of the \u0026ldquo;Skills replace MCP\u0026rdquo; sentiment online, and it\u0026rsquo;s a valid architectural choice for individual developers or small teams where MCP\u0026rsquo;s universality isn\u0026rsquo;t needed.\nConclusion The Agent Skills vs MCP debate is not about which standard wins. It is about recognizing that Anthropic deliberately built two standards because it identified two distinct, unsolved problems in the agentic stack, and that solving one does not eliminate the other. Skills deliver token efficiency, ease of authoring, portability of organizational knowledge, and minimal operational complexity, but they cannot independently reach the outside world. MCP delivers universal external connectivity, process isolation, and ecosystem breadth, but carries significant overhead in security management, authentication complexity, context window consumption, and operational infrastructure.\nAnthropic built Skills after MCP, and Skills\u0026rsquo; progressive disclosure design directly addresses MCP\u0026rsquo;s biggest practical weakness (context window bloat). The two standards don\u0026rsquo;t compete. They represent Anthropic learning from its first standard and the construction of a complementary layer to address a problem that MCP doesn\u0026rsquo;t solve.\nFor enterprise IT operations, the actionable takeaway is: invest in Skills to encode your operational playbooks, runbooks, and institutional expertise into portable, token-efficient packages. Deploy MCP selectively for external integrations that genuinely require standardized connectivity, and invest in MCP gateway infrastructure (auth proxying, observability, rate limiting) before going to production. Treat context window budget as a first-class architectural constraint. And use both together where the combination makes workflows both connected and intelligent.\nI love how Block\u0026rsquo;s goose team concluded: \u0026ldquo;The existence of both is a good sign. It means the ecosystem is maturing. We\u0026rsquo;re no longer arguing about whether agents should have tools or instructions. We\u0026rsquo;re building systems that assume you need both. That\u0026rsquo;s progress, not replacement.\u0026rdquo;\nWhat is your take on this?\nIn the next article, I will share my thoughts on context graphs and how Skills and MCP shape and populate the graph with reality.\n","date":"2026-02-06T00:00:00Z","excerpt":"Agent Skills and MCP are a hot topic. Understanding these standards genuinely overlap, where they serve fundamentally different purposes, and where each falls short, is essential for anyone building agent-powered systems.","permalink":"https://ravichaganti.com/blog/agent-skills-vs-model-context-protocol-how-do-you-choose/","series":"Agents","title":"Agent skills vs Model Context Protocol - [How] do you choose?"},{"categories":["Microsoft","Agent Framework","Agents","A2A"],"content":"In the previous article, we explored how to build multi-agent systems using Google\u0026rsquo;s Agent Development Kit (ADK) and the A2A (Agent-to-Agent) protocol. We built a currency conversion agent and exposed it via A2A, then created a travel assistant that consumed it—all within the ADK ecosystem.\nBut here\u0026rsquo;s where A2A truly shines: framework interoperability. The A2A protocol isn\u0026rsquo;t tied to any specific agent framework. Any A2A-compliant agent can communicate with any other, regardless of whether it was built with Google ADK, Microsoft Agent Framework, LangChain, or a custom implementation.\nIn today\u0026rsquo;s article, I\u0026rsquo;ll demonstrate this by building a Microsoft Agent Framework (MAF) agent that consumes the ADK currency agent we created earlier. This proves the real value of A2A—true framework-agnostic agent communication.\nComplete code for this article is available at rchaganti/a2a-samples\nPrerequisites Before diving in, make sure you have:\nThe ADK currency agent from the previous article is running on port 8001 Microsoft Agent Framework installed: 1 pip install agent-framework --pre httpx a2a When a MAF agent wants to communicate with an A2A server (our ADK agent), it needs to:\nDiscover the agent by fetching its agent card from the well-known endpoint. Create a task by sending a message to the agent. Receive the response, which may include the agent\u0026rsquo;s reply or artifacts. The A2A protocol handles all the complexity of message formatting, task management, and response handling through a standard HTTP/JSON interface.\nThe A2A Client Components in MAF Microsoft Agent Framework provides first-class support for A2A through two key components:\n1. A2ACardResolver The A2ACardResolver class from the a2a.client package handles agent discovery:\n1 2 3 4 5 6 7 8 9 10 from a2a.client import A2ACardResolver import httpx async with httpx.AsyncClient(timeout=60.0) as http_client: resolver = A2ACardResolver( httpx_client=http_client, base_url=\u0026#34;http://localhost:8001\u0026#34; ) agent_card = await resolver.get_agent_card() This fetches the Agent Card from http://localhost:8001/.well-known/agent.json, which contains:\nAgent name and description Supported capabilities and skills Authentication requirements Protocol version information 2. A2AAgent The A2AAgent class wraps a remote A2A agent as a local MAF agent:\n1 2 3 4 5 6 7 8 from agent_framework.a2a import A2AAgent currency_agent = A2AAgent( name=\u0026#34;currency_converter\u0026#34;, description=\u0026#34;Remote agent for currency conversions\u0026#34;, agent_card=agent_card, url=\u0026#34;http://localhost:8001\u0026#34;, ) Once wrapped, you can interact with the remote agent using the standard MAF interface:\n1 2 3 response = await currency_agent.run(\u0026#34;Convert 100 USD to EUR\u0026#34;) for message in response.messages: print(message.text) Building a MAF Shopping Assistant Let\u0026rsquo;s build a practical example: a shopping assistant that uses local tools for product management and delegates currency conversions to the remote ADK agent.\nLocal Shopping Tools First, we define our local capabilities:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 PRODUCTS = { \u0026#34;laptop\u0026#34;: {\u0026#34;price\u0026#34;: 1299.99, \u0026#34;category\u0026#34;: \u0026#34;Electronics\u0026#34;}, \u0026#34;headphones\u0026#34;: {\u0026#34;price\u0026#34;: 199.99, \u0026#34;category\u0026#34;: \u0026#34;Electronics\u0026#34;}, \u0026#34;keyboard\u0026#34;: {\u0026#34;price\u0026#34;: 89.99, \u0026#34;category\u0026#34;: \u0026#34;Electronics\u0026#34;}, \u0026#34;backpack\u0026#34;: {\u0026#34;price\u0026#34;: 79.99, \u0026#34;category\u0026#34;: \u0026#34;Accessories\u0026#34;}, } def get_product_price(product_name: str) -\u0026gt; dict: \u0026#34;\u0026#34;\u0026#34;Get the price of a product in USD.\u0026#34;\u0026#34;\u0026#34; product_lower = product_name.lower().strip() if product_lower in PRODUCTS: product = PRODUCTS[product_lower] return { \u0026#34;success\u0026#34;: True, \u0026#34;product\u0026#34;: product_name.title(), \u0026#34;price_usd\u0026#34;: product[\u0026#34;price\u0026#34;], \u0026#34;category\u0026#34;: product[\u0026#34;category\u0026#34;], \u0026#34;message\u0026#34;: f\u0026#34;{product_name.title()}: ${product[\u0026#39;price\u0026#39;]:.2f} USD\u0026#34; } return { \u0026#34;success\u0026#34;: False, \u0026#34;message\u0026#34;: f\u0026#34;Product \u0026#39;{product_name}\u0026#39; not found.\u0026#34; } def calculate_cart_total(items: list[dict]) -\u0026gt; dict: \u0026#34;\u0026#34;\u0026#34;Calculate the total price for a shopping cart.\u0026#34;\u0026#34;\u0026#34; subtotal = 0.0 cart_items = [] for item in items: product_name = item.get(\u0026#34;product\u0026#34;, \u0026#34;\u0026#34;).lower().strip() quantity = item.get(\u0026#34;quantity\u0026#34;, 1) if product_name in PRODUCTS: product = PRODUCTS[product_name] item_total = product[\u0026#34;price\u0026#34;] * quantity subtotal += item_total cart_items.append({ \u0026#34;product\u0026#34;: product_name.title(), \u0026#34;unit_price\u0026#34;: product[\u0026#34;price\u0026#34;], \u0026#34;quantity\u0026#34;: quantity, \u0026#34;item_total\u0026#34;: round(item_total, 2) }) # Apply tax (8.5%) tax = round(subtotal * 0.085, 2) total = round(subtotal + tax, 2) return { \u0026#34;success\u0026#34;: True, \u0026#34;items\u0026#34;: cart_items, \u0026#34;subtotal_usd\u0026#34;: round(subtotal, 2), \u0026#34;tax_usd\u0026#34;: tax, \u0026#34;total_usd\u0026#34;: total, \u0026#34;message\u0026#34;: f\u0026#34;Cart total: ${total:.2f} USD (including ${tax:.2f} tax)\u0026#34; } Connecting to the Remote ADK Agent Now we connect to the ADK Currency Agent:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import asyncio import httpx from a2a.client import A2ACardResolver from agent_framework.a2a import A2AAgent ADK_CURRENCY_AGENT_URL = \u0026#34;http://localhost:8001\u0026#34; async def create_currency_agent(): \u0026#34;\u0026#34;\u0026#34;Create an A2A agent wrapper for the remote ADK currency agent.\u0026#34;\u0026#34;\u0026#34; async with httpx.AsyncClient(timeout=60.0) as http_client: resolver = A2ACardResolver( httpx_client=http_client, base_url=ADK_CURRENCY_AGENT_URL ) agent_card = await resolver.get_agent_card() print(f\u0026#34;✅ Connected to: {agent_card.name}\u0026#34;) print(f\u0026#34; Description: {agent_card.description}\u0026#34;) return A2AAgent( name=\u0026#34;currency_converter\u0026#34;, description=\u0026#34;\u0026#34;\u0026#34;Remote agent for currency conversions. Use for: converting amounts, getting exchange rates, listing supported currencies.\u0026#34;\u0026#34;\u0026#34;, agent_card=agent_card, url=ADK_CURRENCY_AGENT_URL, ) We can now orchestrate between local tools and the remote A2A agent:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 async def shopping_with_currency_conversion(): \u0026#34;\u0026#34;\u0026#34;Demonstrate local tools + remote A2A agent working together.\u0026#34;\u0026#34;\u0026#34; async with httpx.AsyncClient(timeout=60.0) as http_client: # Connect to the remote ADK agent resolver = A2ACardResolver( httpx_client=http_client, base_url=ADK_CURRENCY_AGENT_URL ) agent_card = await resolver.get_agent_card() currency_agent = A2AAgent( name=\u0026#34;currency_converter\u0026#34;, description=\u0026#34;Remote currency conversion agent\u0026#34;, agent_card=agent_card, url=ADK_CURRENCY_AGENT_URL, ) # Step 1: Use LOCAL tools to calculate cart print(\u0026#34;📦 Using LOCAL shopping tools...\u0026#34;) cart_result = calculate_cart_total([ {\u0026#34;product\u0026#34;: \u0026#34;laptop\u0026#34;, \u0026#34;quantity\u0026#34;: 1}, {\u0026#34;product\u0026#34;: \u0026#34;headphones\u0026#34;, \u0026#34;quantity\u0026#34;: 2} ]) print(f\u0026#34; {cart_result[\u0026#39;message\u0026#39;]}\u0026#34;) total_usd = cart_result[\u0026#34;total_usd\u0026#34;] # Step 2: Use REMOTE A2A agent for currency conversion print(\u0026#34;\\n🌐 Using REMOTE A2A agent for currency conversion...\u0026#34;) response = await currency_agent.run(f\u0026#34;Convert {total_usd} USD to EUR\u0026#34;) for message in response.messages: print(f\u0026#34; {message.text}\u0026#34;) Running this produces:\n1 2 3 4 5 📦 Using LOCAL shopping tools... Cart total: $1838.46 USD (including $143.50 tax) 🌐 Using REMOTE A2A agent for currency conversion... 1838.46 USD = 1691.38 EUR Let\u0026rsquo;s trace through exactly what happens when MAF calls the ADK agent:\nAgent Discovery When A2ACardResolver.get_agent_card() is called, it sends:\n1 2 GET /.well-known/agent.json HTTP/1.1 Host: localhost:8001 The ADK agent responds with its Agent Card:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 { \u0026#34;name\u0026#34;: \u0026#34;currency_agent\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;A currency conversion agent that can convert amounts...\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;http://localhost:8001\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1.0.0\u0026#34;, \u0026#34;capabilities\u0026#34;: { \u0026#34;streaming\u0026#34;: false, \u0026#34;pushNotifications\u0026#34;: false }, \u0026#34;defaultInputModes\u0026#34;: [\u0026#34;text\u0026#34;], \u0026#34;defaultOutputModes\u0026#34;: [\u0026#34;text\u0026#34;], \u0026#34;skills\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;convert_currency\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Currency Conversion\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Convert amounts between different currencies\u0026#34; } ] } Task Creation When currency_agent.run(\u0026quot;Convert 100 USD to EUR\u0026quot;) is called, MAF sends:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 POST /tasks/send HTTP/1.1 Host: localhost:8001 Content-Type: application/json { \u0026#34;jsonrpc\u0026#34;: \u0026#34;2.0\u0026#34;, \u0026#34;method\u0026#34;: \u0026#34;tasks/send\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;unique-request-id\u0026#34;, \u0026#34;params\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;task-uuid\u0026#34;, \u0026#34;message\u0026#34;: { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;parts\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Convert 100 USD to EUR\u0026#34; } ] } } } Response Handling The ADK agent processes the request, calls its tools, and returns:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 { \u0026#34;jsonrpc\u0026#34;: \u0026#34;2.0\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;unique-request-id\u0026#34;, \u0026#34;result\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;task-uuid\u0026#34;, \u0026#34;status\u0026#34;: { \u0026#34;state\u0026#34;: \u0026#34;completed\u0026#34; }, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;agent\u0026#34;, \u0026#34;parts\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;100 USD = 92 EUR\u0026#34; } ] } ] } } MAF\u0026rsquo;s A2AAgent handles all this serialization and deserialization, presenting a clean interface.\nThe A2A protocol represents a significant step toward a future in which AI agents can collaborate regardless of their underlying frameworks. By standardizing the communication layer, we can build best-of-breed systems that leverage specialized agents from different ecosystems. A2A enables true interoperability, allowing agents developed in different frameworks to communicate seamlessly. The well-known endpoint (/.well-known/agent.json) provides all the information needed for agent discovery.\nIn future articles, we\u0026rsquo;ll explore:\nExposing MAF agents via A2A for ADK or other frameworks to consume. Multi-agent orchestration with agents from different frameworks. Authentication and security in A2A communications. Streaming responses for long-running agent tasks. ","date":"2026-01-28T00:00:00Z","excerpt":"Microsoft Agent Protocol is an open framework for building enterprise-grade agents. It offers support for using the A2A protocol for cross-agent communication.","permalink":"https://ravichaganti.com/blog/cross-framework-agent-communication-microsoft-agent-framework-meets-google-adk-via-a2a/","series":"Microsoft Agent Framework","title":"Cross-Framework Agent Communication: Microsoft Agent Framework meets Google ADK via A2A"},{"categories":["Google","Agent Development Kit","Agents","A2A"],"content":"The landscape of AI agents is evolving rapidly, and one of the most significant developments is the emergence of standardized protocols that enable agents to communicate with one another. Google\u0026rsquo;s Agent Development Kit (ADK) natively supports the Agent-to-Agent (A2A) protocol, enabling developers to build sophisticated multi-agent systems where specialized agents can collaborate across network boundaries. In this post, we\u0026rsquo;ll explore what A2A is, why it matters, and walk through a complete implementation that demonstrates how to expose an agent as a remote service and consume it from another agent.\nTraditional agent architectures tend to be monolithic. You build an agent, give it tools and capabilities, and it operates as a self-contained unit. This works well for simple use cases, but as AI applications grow more complex, we encounter significant limitations. Consider a scenario where you need an agent that can handle travel planning, currency conversions, flight bookings, hotel reservations, and local recommendations. Building all of this into a single agent becomes unwieldy—the instruction set grows massive, the tools become numerous, and the agent struggles to maintain context across such diverse domains.\nThe natural solution is decomposition: break the monolith into specialized agents, each focused on a specific domain. A currency agent handles conversions, a booking agent manages reservations, and a travel advisor provides recommendations. But this raises a new challenge: how do these agents communicate? If they\u0026rsquo;re all running in the same process, you can simply wire them together as sub-agents. But what if you want to deploy them as independent services? What if one team builds the currency agent and another team builds the travel planner? What if you want to use a third-party currency agent?\nThis is where the A2A protocol comes into play. It provides a standardized way for agents to discover each other, understand each other\u0026rsquo;s capabilities, and exchange messages—regardless of where they\u0026rsquo;re deployed or who built them.\nCode sample for this article is available at a2a-samples/01-basics at main · rchaganti/a2a-samples\nUnderstanding A2A protocol Almost a year ago, Google announced the release of an open protocol called agent-to-agent. This is an open standard designed to enable interoperability between AI agents. At its core, A2A defines three key concepts: agent cards, message formats, and communication patterns.\nAn agent card is a JSON document that describes an agent\u0026rsquo;s identity and capabilities. Think of it as a machine-readable resume that tells other agents what this agent can do. The agent card includes the agent\u0026rsquo;s name, a description of its purpose, the skills it offers, and the URL where it can be reached. When one agent wants to use another, it first fetches the agent card to understand what services are available. The agent card is served at a well-known URL path (/.well-known/agent-card.json), making agent discovery straightforward and predictable.\nThe message format standardizes how agents exchange information. A2A uses a request-response model built on HTTP and JSON, making it compatible with existing web infrastructure. Messages contain the user\u0026rsquo;s input, conversation context, and request metadata. Responses include the agent\u0026rsquo;s output along with any artifacts or structured data it produces. This standardization means that an agent built with Google ADK can communicate with an agent built using a completely different framework, as long as both implement the A2A protocol.\nCommunication patterns in A2A support both synchronous request-response interactions and streaming responses. For simple queries like \u0026ldquo;what\u0026rsquo;s the exchange rate?\u0026rdquo;, a synchronous call works perfectly. For more complex interactions that might take time or produce incremental results, streaming allows the consuming agent to receive partial responses as they\u0026rsquo;re generated.\nLet us understand this in-depth with an example.\nA travel agent system To demonstrate these concepts concretely, we will look at a travel assistant system with two distinct agents. The first is a currency conversion agent that runs as a standalone A2A server. It knows nothing about travel. Its sole expertise is converting amounts between currencies and providing exchange rate information. The second is a travel assistant agent that operates locally and provides destination information, budget calculations, and trip-planning advice. When the travel assistant needs currency conversion capabilities, it doesn\u0026rsquo;t implement them itself; instead, it delegates to the remote currency agent via A2A.\nThis architecture mirrors real-world scenarios where specialized services are deployed independently. The currency agent could be maintained by an external fintech team, updated with real-time exchange rates from market feeds, and scaled independently based on demand. The travel assistant could be developed by a completely separate team, consuming the currency service without needing to understand its internal implementation. The A2A protocol serves as the contract between them, ensuring they can communicate reliably even as their internal implementations evolve.\nThe currency agent Let\u0026rsquo;s examine how the currency agent is built and exposed via A2A. The agent itself is a standard Google ADK agent with tools for currency operations. The key difference from a typical agent is the final step: wrapping it with the to_a2a() function to make it A2A-compatible.\nThe currency agent defines three tools that encapsulate its domain expertise. The convert_currency function takes an amount, a source currency, and a target currency, then returns the converted amount along with the exchange rate used. The get_exchange_rate function returns just the rate between two currencies without performing a conversion. The list_supported_currencies function returns all currency codes the agent can work with. Each tool returns a structured dictionary with a success flag, the requested data, and a human-readable message. This structured approach makes it easy for the consuming agent to parse the results programmatically while also providing natural-language output for the user.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 def convert_currency(amount: float, from_currency: str, to_currency: str) -\u0026gt; dict: \u0026#34;\u0026#34;\u0026#34;Convert an amount from one currency to another.\u0026#34;\u0026#34;\u0026#34; from_currency = from_currency.upper() to_currency = to_currency.upper() if from_currency == to_currency: return { \u0026#34;success\u0026#34;: True, \u0026#34;original_amount\u0026#34;: amount, \u0026#34;from_currency\u0026#34;: from_currency, \u0026#34;to_currency\u0026#34;: to_currency, \u0026#34;converted_amount\u0026#34;: amount, \u0026#34;exchange_rate\u0026#34;: 1.0, \u0026#34;message\u0026#34;: f\u0026#34;{amount} {from_currency} = {amount} {to_currency}\u0026#34; } rate_key = (from_currency, to_currency) if rate_key in EXCHANGE_RATES: rate = EXCHANGE_RATES[rate_key] converted = round(amount * rate, 2) return { \u0026#34;success\u0026#34;: True, \u0026#34;original_amount\u0026#34;: amount, \u0026#34;from_currency\u0026#34;: from_currency, \u0026#34;to_currency\u0026#34;: to_currency, \u0026#34;converted_amount\u0026#34;: converted, \u0026#34;exchange_rate\u0026#34;: rate, \u0026#34;message\u0026#34;: f\u0026#34;{amount} {from_currency} = {converted} {to_currency}\u0026#34; } # Handle conversion through USD as intermediate when direct rate unavailable # ... implementation continues The agent definition follows the standard ADK pattern. We specify the model (Gemini 2.0 Flash in this case), give the agent a name and description, provide detailed instructions about its capabilities, and attach the tools it can use. The description is particularly important in an A2A context because it becomes part of the agent card and helps consuming agents understand when to delegate tasks to this agent.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 root_agent = Agent( model=\u0026#34;gemini-2.0-flash\u0026#34;, name=\u0026#34;currency_agent\u0026#34;, description=\u0026#34;A currency conversion agent that can convert amounts between \u0026#34; \u0026#34;different currencies and provide exchange rate information.\u0026#34;, instruction=\u0026#34;\u0026#34;\u0026#34;You are a helpful currency conversion assistant. Your capabilities: 1. Convert amounts from one currency to another using the convert_currency tool 2. Get exchange rates between currencies using the get_exchange_rate tool 3. List all supported currencies using the list_supported_currencies tool When users ask about currency conversions: - Always confirm the currencies and amount before converting - Provide clear, formatted results - If a currency is not supported, let the user know and suggest alternatives \u0026#34;\u0026#34;\u0026#34;, tools=[convert_currency, get_exchange_rate, list_supported_currencies], ) The single line of code below transforms this standard ADK agent into an A2A-compatible service:\n1 2 3 from google.adk.a2a.utils.agent_to_a2a import to_a2a a2a_app = to_a2a(root_agent, port=8001) The to_a2a() function performs several important tasks behind the scenes. First, it creates a FastAPI application that implements the A2A protocol endpoints. Second, it analyzes the agent\u0026rsquo;s configuration—its name, description, instructions, and tools—and automatically generates an agent card that describes these capabilities in the A2A format. Third, it sets up the message handling pipeline that receives A2A requests, translates them into ADK\u0026rsquo;s internal format, runs the agent, and translates the responses back to A2A format.\nThe resulting a2a_app is a standard ASGI application that can be served using any ASGI server. We use uvicorn in our example:\n1 uvicorn \u0026#34;07-a2a-protocol.remote_a2a.currency_agent.agent:a2a_app\u0026#34; --host localhost --port 8001 Once running, the agent card becomes available at http://localhost:8001/.well-known/agent-card.json. This auto-generated card includes the agent\u0026rsquo;s name, description, supported input and output modes, and a skills array derived from the agent\u0026rsquo;s tools and instructions. Other agents can fetch this card to discover what the currency agent can do.\nConsuming the remote agent On the consuming side, we have the travel assistant agent that needs to use the currency agent\u0026rsquo;s capabilities. The ADK provides the RemoteA2aAgent class specifically for this purpose. Unlike a regular sub-agent that runs in the same process, a RemoteA2aAgent represents a remote service that communicates over the network.\n1 2 3 4 5 6 7 8 9 10 11 12 from google.adk.agents.remote_a2a_agent import RemoteA2aAgent, AGENT_CARD_WELL_KNOWN_PATH currency_agent = RemoteA2aAgent( name=\u0026#34;currency_agent\u0026#34;, description=\u0026#34;\u0026#34;\u0026#34;Remote agent specialized in currency conversions and exchange rates. Use this agent when you need to: - Convert amounts from one currency to another - Get current exchange rates between currencies - List supported currencies for conversion \u0026#34;\u0026#34;\u0026#34;, agent_card=f\u0026#34;http://localhost:8001{AGENT_CARD_WELL_KNOWN_PATH}\u0026#34;, ) The RemoteA2aAgent requires three pieces of information: a name to identify it within the local agent system, a description that helps the root agent understand when to delegate to it, and the URL of the agent card. The AGENT_CARD_WELL_KNOWN_PATH constant provides the standard path (/.well-known/agent-card.json), ensuring consistency with the A2A specification.\nWhen the travel assistant is initialized, it doesn\u0026rsquo;t connect to the remote agent immediately. Instead, it stores the agent card URL and fetches the card lazily when needed. This design allows the local agent to start even if the remote service is temporarily unavailable, and ensures the agent card information is always up to date.\nThe travel assistant also has a local sub-agent for travel information. This demonstrates the hybrid nature of real-world systems—some capabilities are local, some are remote, and the root agent orchestrates between them:\n1 2 3 4 5 6 7 8 9 10 11 12 13 travel_info_agent = Agent( model=\u0026#34;gemini-2.0-flash\u0026#34;, name=\u0026#34;travel_info_agent\u0026#34;, description=\u0026#34;Local agent that provides travel destination information and budget calculations.\u0026#34;, instruction=\u0026#34;\u0026#34;\u0026#34;You are a travel information specialist. Your capabilities: 1. Get detailed information about travel destinations using get_destination_info 2. Calculate trip budgets using calculate_trip_budget 3. List all available destinations using list_destinations \u0026#34;\u0026#34;\u0026#34;, tools=[get_destination_info, calculate_trip_budget, list_destinations], ) The root agent brings everything together. Its instruction set explains the available sub-agents and provides guidance on when to use each one. The sub_agents array includes both the local travel_info_agent and the remote currency_agent, and from the root agent\u0026rsquo;s perspective, they\u0026rsquo;re used in exactly the same way:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 root_agent = Agent( model=\u0026#34;gemini-2.0-flash\u0026#34;, name=\u0026#34;travel_assistant\u0026#34;, description=\u0026#34;A comprehensive travel assistant that can provide destination \u0026#34; \u0026#34;information and handle currency conversions.\u0026#34;, instruction=\u0026#34;\u0026#34;\u0026#34;You are a helpful travel assistant that helps users plan their trips. You have access to two specialized agents: 1. **travel_info_agent** (local): For destination information, trip budgets, and travel tips 2. **currency_agent** (remote via A2A): For currency conversions and exchange rates When a user asks about: - Destination information, attractions, best time to visit → delegate to travel_info_agent - Trip budget estimates → delegate to travel_info_agent, then optionally convert to local currency via currency_agent - Currency conversions or exchange rates → delegate to currency_agent - Combined queries (e.g., \u0026#34;How much is the Paris trip in Euros?\u0026#34;) → coordinate between both agents Workflow for budget with currency conversion: 1. First get the budget in USD from travel_info_agent 2. Then convert the USD amount to the destination\u0026#39;s local currency using currency_agent 3. Present both values to the user \u0026#34;\u0026#34;\u0026#34;, sub_agents=[travel_info_agent, currency_agent], ) When a user asks, \u0026ldquo;What\u0026rsquo;s the exchange rate from USD to EUR?\u0026rdquo;, the root agent recognizes this as a currency-related query and delegates to the currency_agent. Behind the scenes, the ADK framework handles A2A communication: it formats the request per the A2A protocol, sends it to the remote server, waits for the response, and returns the result via the root agent. The user sees a seamless interaction, unaware that part of the processing happened on a different server.\nCommunication flow Let\u0026rsquo;s trace through what happens when a user asks the travel assistant to \u0026ldquo;Calculate a 5-day budget for Tokyo and convert it to Japanese Yen.\u0026rdquo; This query requires coordination between both agents and illustrates the full power of the A2A architecture.\nThroughout this process, the user experiences a single, unified conversation with the travel assistant. The complexity of multi-agent coordination and network communication is completely hidden.\nError handling Production systems must handle failures gracefully, and A2A introduces new failure modes that don\u0026rsquo;t exist in monolithic agents. The remote service might be unavailable, the network might be slow, or the remote agent might return an error. The ADK framework and A2A protocol include mechanisms to handle these scenarios.\nWhen a RemoteA2aAgent cannot reach its target service, the framework raises an exception that the root agent can handle. In our travel assistant, this might manifest as the root agent telling the user: \u0026ldquo;I\u0026rsquo;m currently unable to perform currency conversions. I can still help you with destination information and budget estimates in USD.\u0026rdquo; The key is that a failure in one agent doesn\u0026rsquo;t crash the entire system, making the local capabilities remain available.\nTimeout handling is also important for remote services. A2A requests include timeout parameters, and the ADK framework respects these when making remote calls. If a currency conversion takes too long (perhaps because the remote agent is overloaded), the request times out, and the root agent can decide how to proceed. It may retry, skip the conversion, or ask the user to try again later.\nThe agent card mechanism also provides resilience benefits. Because the consuming agent fetches the agent card dynamically, the remote agent can update its capabilities without requiring changes to the consumer. If the currency agent adds support for cryptocurrency conversions, this new capability appears on the agent card, and sophisticated consuming agents could automatically discover and use it.\nDeploying A2A agents Deploying A2A agents in production requires thinking about several architectural concerns. First, consider where each agent runs. The currency agent in our example runs on localhost, but in production it might run on a cloud server, in a Kubernetes cluster, or as a serverless function. The A2A protocol\u0026rsquo;s HTTP-based design allows agents to be deployed on any standard web infrastructure.\nAuthentication and authorization become important when agents communicate over networks. The A2A protocol supports various authentication mechanisms, and the ADK\u0026rsquo;s to_a2a() function can be configured with authentication middleware. You might need API keys to access your currency agent, or use OAuth tokens for more sophisticated access control.\nObservability is another crucial concern. When a request traverses multiple agents, you need distributed tracing to understand the full request flow. The ADK integrates with standard observability tools, and A2A requests propagate trace context headers, allowing you to correlate logs and traces across agent boundaries.\nScaling considerations differ between agent types. A computationally lightweight agent that primarily orchestrates other agents (such as our travel assistant) might require different scaling characteristics than an agent that performs heavy processing. The A2A architecture naturally supports independent scaling. You can run multiple instances of the currency agent behind a load balancer without changing anything about how the travel assistant consumes it.\nBeyond basics Our travel assistant example demonstrates the fundamental A2A patterns, but the protocol supports more sophisticated scenarios. Streaming responses allow remote agents to send partial results as they\u0026rsquo;re generated, which is valuable for long-running operations or when you want to provide progressive feedback to users. The ADK supports streaming through A2A, and configuring it requires only minor changes to the agent setup.\nAgent discovery services can maintain registries of available agents and their capabilities. Instead of hardcoding the currency agent\u0026rsquo;s URL, a production system might query a discovery service to find agents capable of performing currency conversions. This enables dynamic agent selection based on capabilities, availability, or other criteria.\nMulti-hop agent chains are also possible, where Agent A calls Agent B, who then calls Agent C. The A2A protocol handles this naturally—each agent simply sees requests from its immediate caller, regardless of how deep the chain goes. However, designing such systems requires careful attention to latency accumulation and error propagation.\nSummary The A2A protocol represents a significant step forward in building modular, scalable AI agent systems. By providing a standardized way for agents to communicate across network boundaries, it enables architectures that were previously difficult or impossible to build. Specialized agents can be developed, deployed, and maintained independently, then composed into larger systems that leverage their combined capabilities.\nGoogle ADK\u0026rsquo;s support for A2A makes implementing these patterns straightforward. The to_a2a() function handles the complexity of exposing an agent as an A2A service, automatically generating agent cards and managing the protocol details. The RemoteA2aAgent class provides a clean abstraction for consuming remote agents, making them appear as natural sub-agents within the local agent hierarchy.\nAs AI agents become more prevalent in production systems, protocols like A2A will become essential infrastructure. The ability to build, share, and compose specialized agents—treating them as interoperable services rather than monolithic applications—unlocks new possibilities for AI system architecture. The example we\u0026rsquo;ve built here is just the beginning; the patterns and principles apply to systems of any scale and complexity.\n","date":"2026-01-27T00:00:00Z","excerpt":"Google Agent Development Kit (ADK) is a flexible and modular framework for developing and deploying AI agents. It offers native support to interact with other agents via the agent-to-agent protocol.","permalink":"https://ravichaganti.com/blog/building-multi-agent-systems-with-google-adk-and-the-a2a-protocol/","series":"Google ADK","title":"Building multi-agent systems with Google ADK and the A2A protocol"},{"categories":["Google","Agent Development Kit","Agents"],"content":"Google\u0026rsquo;s Agent Development Kit (ADK) provides workflow agents to enable more deterministic agent execution. These workflows can be combined to create more complex agentic workflows.\nIn this post, I\u0026rsquo;ll walk you through how I created an AI-powered content pipeline that:\nResearches trending topics using Google Search Writes engaging newsletter drafts Edits and quality-checks the content Publishes the final version to a local file Let\u0026rsquo;s dive in!\nThe Big Picture: A Team of Specialized Agents Instead of building one massive AI that does everything (and probably does nothing well), I went with a multi-agent architecture. Think of it like assembling a content team:\nAgent Role Job Description WebResearchAgent Research Analyst Finds trending topics from the web WriterAgent Content Creator Transforms research into engaging prose EditorAgent Quality Control Reviews drafts and provides feedback PublisherAgent Publisher Saves the final approved content The cool part? These agents talk to each other, pass work around, and even have a built-in feedback loop. Let me show you how it all works.\nSetting Up the Foundation First, let\u0026rsquo;s look at the imports and setup:\n1 2 3 4 5 6 7 8 9 10 import asyncio from google.adk.agents import LlmAgent, SequentialAgent, LoopAgent from google.adk.runners import Runner from google.adk.sessions import InMemorySessionService from google.adk.tools import google_search from google.genai import types from utils.agent_utils import load_instruction, save_to_local_file from dotenv import load_dotenv load_dotenv() The key imports here are:\nLlmAgent – Your basic AI agent that can think and respond SequentialAgent – Runs agents one after another (like a pipeline) LoopAgent – Keeps running until a condition is met (perfect for edit cycles!) google_search – Built-in tool for web searches The Web Research Agent This agent is our digital bloodhound. It sniffs out the latest trends using Google Search.\n1 2 3 4 5 6 7 web_research_agent = LlmAgent( name=\u0026#34;WebResearchAgent\u0026#34;, model=\u0026#34;gemini-2.0-flash\u0026#34;, instruction=load_instruction(\u0026#34;web-search-agent.txt\u0026#34;), tools=[google_search], output_key=\u0026#34;research_output\u0026#34;, ) Notice the output_key=\u0026quot;research_output\u0026quot; – this is crucial! It stores the agent\u0026rsquo;s output in a shared state that other agents can access later. Think of it as the agent putting its work in a shared folder.\nThe Research Prompt Here\u0026rsquo;s what the research agent\u0026rsquo;s instructions look like:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 Role: Strategic Research Analyst Task: Identify and summarize the top 5 trending items for a provided set of topics from the last 7 days. Instructions (Chain-of-Thought): For each topic provided, follow these steps internally: - Deconstruct: Break down the topic into 2-3 specific search queries - Execute: Use the Google Search tool for each query - Evaluate: Select the 5 most \u0026#34;impactful\u0026#34; items - Synthesize: Write a high-density summary for each item Required Output Format per Topic: Topic: [Topic Name] - [Headline]: [1-sentence summary] - Insight: [Why this matters to professionals] - Reference: [Source URL] The Chain-of-Thought (CoT) approach forces the agent to think systematically rather than just vomiting out whatever comes to mind first.\nThe Writer Agent Now we need someone to turn that raw research into something people actually want to read.\n1 2 3 4 5 6 writer_agent = LlmAgent( name=\u0026#34;WriterAgent\u0026#34;, model=\u0026#34;gemini-2.0-flash\u0026#34;, instruction=load_instruction(\u0026#34;writer-agent.txt\u0026#34;), output_key=\u0026#34;writer_draft\u0026#34; ) The Writer\u0026rsquo;s Prompt 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 Role: Creative Content Creator Instruction: You are a professional Copywriter specializing in LinkedIn Newsletters. Research Output: {research_output} Your workflow: - Draft a LinkedIn Newsletter in Markdown format - Structure the draft with: - Catchy Title: Use a hook that targets professionals - Introduction: Briefly set the stage for the week\u0026#39;s trends - Body Paragraphs: One section per topic (narrative style, not just a list!) - Always use citations and link to the sources - Takeaways: A \u0026#34;So What?\u0026#34; section for the reader - Call to Action (CTA): Encourage comments or shares Feedback Integration: If the editor responds with APPROVED, call the exit_loop tool. Otherwise, use editor feedback provided below and revise the draft. Editor Feedback (if any): {editor_feedback:} See that {research_output} placeholder? That\u0026rsquo;s pulling in the WebResearchAgent\u0026rsquo;s work automatically. And {editor_feedback} lets the writer know what needs fixing. Magic!\nThe Editor Agent Every writer needs an editor (even AI writers). This agent plays the bad cop and ensures quality standards are met.\n1 2 3 4 5 6 editor_agent = LlmAgent( name=\u0026#34;EditorAgent\u0026#34;, model=\u0026#34;gemini-2.0-flash\u0026#34;, instruction=load_instruction(\u0026#34;editor-agent.txt\u0026#34;), output_key=\u0026#34;editor_feedback\u0026#34; ) The Editor\u0026rsquo;s Prompt 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 Role: Quality Control \u0026amp; Compliance Instruction: You are a meticulous Senior Editor. Writer\u0026#39;s Draft to Review: {writer_draft} Review Checklist: - Accuracy: Does it reflect the 5 trending items? - Trust Worthy: Are citations added and linked to sources? - Formatting: Is it valid Markdown with proper headers? - Tone: Is it appropriate for a LinkedIn professional audience? - Clarity: Any grammatical errors or awkward phrasings? - Engagement: Is the hook strong enough to stop the scroll? Response Logic: - If the draft passes all checks: Respond only with \u0026#34;APPROVED\u0026#34; - If the draft fails: Provide bulleted feedback for the Writer Agent (Do NOT rewrite the draft yourself!) The key here is the Response Logic – if everything looks good, the editor just says \u0026ldquo;APPROVED\u0026rdquo;, and we move on. If not, it sends feedback, and the loop continues.\nThe Writer-Editor Loop This is where things get interesting. We wrap the writer and editor in a LoopAgent:\n1 2 3 4 5 6 7 8 writer_editor_loop = LoopAgent( name=\u0026#34;ReviewCycle\u0026#34;, sub_agents=[ writer_agent, editor_agent ], max_iterations=3 ) What happens here:\nWriter creates a draft Editor reviews it If \u0026ldquo;APPROVED\u0026rdquo; → Exit the loop If not → Writer revises based on feedback Repeat until approved or max iterations (3) is reached This mimics real-world content workflows! No more publishing first drafts.\nThe Publisher Agent Once the editor gives the green light, we need to save the final product.\n1 2 3 4 5 6 publisher_agent = LlmAgent( name=\u0026#34;PublisherAgent\u0026#34;, model=\u0026#34;gemini-2.0-flash\u0026#34;, instruction=load_instruction(\u0026#34;publisher-agent.txt\u0026#34;), tools=[save_to_local_file] ) Custom tool for saving the newsletter content The publisher uses a custom tool to save files:\n1 2 3 4 5 6 7 8 9 10 11 12 def save_to_local_file(content: str, filename: str = None) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Saves the combined multi-topic newsletter to a local file.\u0026#34;\u0026#34;\u0026#34; try: if filename is None: date_str = datetime.now().strftime(\u0026#34;%Y-%m-%d\u0026#34;) filename = f\u0026#34;newsletter_issue_{date_str}.md\u0026#34; with open(filename, \u0026#34;w\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: f.write(content) return f\u0026#34;SUCCESS: Multi-topic newsletter saved to {filename}\u0026#34; except Exception as e: return f\u0026#34;FAILURE: {str(e)}\u0026#34; Always return meaningful success/failure messages from your tools. The agent needs to know what happened!\nPutting It All Together: The Sequential Pipeline Now we chain everything into a master workflow:\n1 2 3 4 5 6 7 8 newsletter_system = SequentialAgent( name=\u0026#34;LinkedInNewsletterSystem\u0026#34;, sub_agents=[ web_research_agent, # Step 1: Research writer_editor_loop, # Step 2: Write \u0026amp; Review (loops) publisher_agent # Step 3: Publish ] ) This creates a pipeline that flows like:\n1 Research → Write → Edit → (Loop if needed) → Publish Running the Whole Thing Here\u0026rsquo;s the main() function that kicks everything off:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 async def main(): topics = [\u0026#34;AI data center infrastructure\u0026#34;, \u0026#34;Networking for AI\u0026#34;, \u0026#34;Storage for AI\u0026#34;] APP_NAME = \u0026#34;linkedin_newsletter_assistant\u0026#34; USER_ID = \u0026#34;user_admin\u0026#34; SESSION_ID = \u0026#34;multi_topic_001\u0026#34; # Create a session service to manage state session_service = InMemorySessionService() await session_service.create_session( app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID, ) # Create the runner runner = Runner( agent=newsletter_system, app_name=APP_NAME, session_service=session_service, ) print(f\u0026#34;🚀 Launching Multi-Topic System for: {\u0026#39;, \u0026#39;.join(topics)}\u0026#34;) # Send the initial message new_message = types.Content( role=\u0026#34;user\u0026#34;, parts=[types.Part(text=f\u0026#34;Process these topics: {\u0026#39;, \u0026#39;.join(topics)}\u0026#34;)], ) # Run and stream results for event in runner.run( new_message=new_message, user_id=USER_ID, session_id=SESSION_ID, ): if event.is_final_response() and event.content and event.content.parts: print(\u0026#34;\\n--- Process Report ---\u0026#34;) print(event.content.parts[0].text) if __name__ == \u0026#34;__main__\u0026#34;: asyncio.run(main()) The load_instruction helper function loads the agent\u0026rsquo;s prompt from local text files. This helps keep the code clean.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 def load_instruction(file_name: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34; Finds the \u0026#39;prompts\u0026#39; directory relative to the project root and returns the content of the specified text file. \u0026#34;\u0026#34;\u0026#34; current_dir = os.path.dirname(os.path.abspath(__file__)) project_root = os.path.dirname(current_dir) file_path = os.path.join(project_root, \u0026#34;prompts\u0026#34;, file_name) try: with open(file_path, \u0026#39;r\u0026#39;, encoding=\u0026#34;utf-8\u0026#34;) as file: return file.read().strip() except FileNotFoundError: return f\u0026#34;Error: The file {file_name} was not found in the prompts directory.\u0026#34; Keeping prompts in separate text files is a game-changer for:\nVersion control – Track changes to prompts separately Non-technical editing – Let others tweak prompts without touching code Experimentation – A/B test different prompt strategies easily The Google ADK makes building these multi-agent systems surprisingly straightforward. An example of the output from this multi-agent system is available as a LinkedIn article I published last week.\nGive it a try and let me know what you build!\n","date":"2026-01-26T00:00:00Z","excerpt":"Automating content creation with a team of AI agents that research, write, edit, and publish","permalink":"https://ravichaganti.com/blog/building-a-multi-agent-linkedin-newsletter-system-with-google-adk/","series":"Google ADK","title":"Building a Multi-Agent LinkedIn Newsletter System with Google ADK"},{"categories":["teaching","courses","learning"],"content":"The beginning of 2026 also marks the start of my 26th year in the IT industry. Being an 80s kid and having joined the IT industry in the early 2000s, I have seen and experienced different waves of computing, from bare-metal systems to virtualization, to containers and Kubernetes, to generative AI. I was fortunate to participate in and contribute to each of these waves of computing. My work at Wipro and Dell Technologies enabled me in this journey. This was a great learning experience, and applying those skills at work is the most satisfying part. Outside my role at these companies, I benefited immensely from online and offline technical communities. I have learned from many experts in several areas. At the same time, I started giving back to the community. I started PowerShell Magazine back in 2011, and while it was at the top of the game, it provided the most in-depth technical content to help the PowerShell community learn. I moved on from PowerShell as a tool for automation and started focusing on Linux, containers, Kubernetes, and eventually on Artificial Intelligence.\nOver the years, I have not been very active in technical communities, but my interest in sharing my knowledge with them never died down. I authored a few books and wrote sparingly on this blog. But one idea always stuck with me. I have always wanted to create a free, open, and self-paced learning platform. I wanted to publish bite-sized learning modules across multiple programming languages and technologies that I have been learning and using. For hands-on learning, I have used and benefited from platforms like The Best DevOps Online Training Courses | KodeKloud, Killercoda Interactive Environments, and Ivan\u0026rsquo;s iximiuz labs. These innovative platforms offer learners a great way to practice various technologies. While I don\u0026rsquo;t want to create a commercial platform like one of these, I wanted to build a local playground for practicing technologies such as containers and Kubernetes. I attempted to make these ideas a reality. I built different content-sharing platforms using the Hugo static site generator. I worked on creating Hugo themes for the same. I always believed that I didn\u0026rsquo;t need a fancy hosted platform and a mix of complex technologies to deliver this learning platform. This was the prep work. For the local playground, I learnt Go and experimented with creating CLIs and TUIs. I wrote a book on developing CLIs and TUIs in Go. With generative AI, I have learnt a bunch of frameworks, SDKs, and built prototype applications. The evolving landscape of coding agents has certainly played a role in accelerating my work towards today\u0026rsquo;s announcement.\nWhat I am announcing today is the result of years of work across these different technologies. I am happy to announce an open, free, and self-paced learning platform \u0026ndash; learn.ravichaganti.com. I have started building the courses on this platform with Python Fundamentals as the first fully available course. The remaining courses will become available in the next few weeks.\nYouTube Video This video requires your consent to load content from YouTube, which may set tracking cookies. Enable YouTube Video ID: xVdZ_ctFAYI To complement this reading, I am also working on a local playground available as a command-line tool. It is called LearnCtl. LearnCtl provides an interactive local playground where learners are given hands-on tasks of varying difficulty. This interactive playground also comes integrated with an AI tutor. This tutor can help you learn more about the task and guide you toward completing it, or help you learn more about the topic. Your progress gets tracked locally, and you own your learning journey. There will be timed learning tasks to help you prepare for exams like CKA, CKAD, and CKS. LearnCtl embeds deliberate practice and spaced repetition at its core.\nYouTube Video This video requires your consent to load content from YouTube, which may set tracking cookies. Enable YouTube Video ID: EGj5NTavxm4 Both learn.ravichaganti.com and LearnCtl go hand in hand. For technology tracks such as Docker, Podman, Kubernetes, Terraform, and Bicep, the online learning platform will provide the reading material, and LearnCtl will provide hands-on exercises.\nlearn.ravichaganti.com is online and ready for you to start your learning journey. I always welcome your feedback on how to improve this platform. LearnCtl is still cooking and should be ready for a preview in a couple of weeks. In the meantime, I look forward to your feedback and suggestions. If you want to contribute course modules, please reach out to me.\n","date":"2026-01-19T00:00:00Z","excerpt":"An open and free learning platform for the technical community.","permalink":"https://ravichaganti.com/blog/announcing-a-self-paced-learning-platform-and-preview-of-learnctl/","series":null,"title":"Announcing a self-paced learning platform and preview of LearnCTL"},{"categories":["Google","Agent Development Kit","Agents"],"content":"Consider you are building an agent that needs to interact with a company\u0026rsquo;s database, pull data from a third-party API, and read files from a shared drive. Without a standard protocol, you would end up writing custom integration code for each backend. Every new tool means a new adapter. Every new agent that needs the same tool means duplicating that adapter. This approach does not scale. The Model Context Protocol (MCP) solves this by providing a universal standard for how agents connect to external tools and data sources. With MCP, a tool is built once, and any MCP-compatible agent can discover and use it. Google ADK has first-class support for MCP, enabling your agents to both consume tools from MCP servers and expose their own tools as MCP servers.\nWhat is MCP? MCP is an open standard initiated by Anthropic that defines how LLMs communicate with external applications, data sources, and tools. It follows a client-server architecture. An MCP server exposes capabilities such as data (resources), interaction templates (prompts), and executable functions (tools). An MCP client, which in our case is the ADK agent, connects to the server, discovers what it offers, and uses those capabilities as if they were native.\nThe key benefit is standardization. Instead of building one-off integrations for every external system, you write an MCP server once, and any MCP client can talk to it. Conversely, your ADK agent, acting as a client, can connect to any MCP server without needing to know the implementation details behind it.\nADK supports two primary integration patterns with MCP.\nConsuming MCP servers: Your ADK agent acts as an MCP client, using the tools provided by an external MCP server. Exposing ADK tools via an MCP server: You wrap your existing ADK tools inside an MCP server, making them accessible to any MCP-compatible client, not just ADK agents. Consuming MCP servers This is the most common pattern. You have an existing MCP server, one from the community, a cloud-managed service, or one you built for another application, and you want your ADK agent to use its tools.\nMcpToolset McpToolset is the class at the heart of ADK\u0026rsquo;s MCP integration. When you add an McpToolset instance to your agent\u0026rsquo;s tools list, it handles the entire lifecycle behind the scenes.\nResponsibility What happens Connection management Establishes and maintains the connection to the MCP server, and tears it down on shutdown Tool discovery Queries the server for its available tools via the MCP list_tools method Schema adaptation Converts each discovered MCP tool schema into an ADK-compatible BaseTool instance Transparent proxying When the LLM invokes a discovered tool, McpToolset forwards the call via the MCP call_tool method Filtering Selectively exposes only the tools you want, instead of all the tools the server advertises The connection to an MCP server can use one of three transport mechanisms.\nTransport Class Use case Standard I/O (Stdio) StdioConnectionParams The MCP server runs as a local subprocess; communication via stdin/stdout Server-Sent Events (SSE) SseConnectionParams The MCP server is a remote HTTP service using the legacy SSE transport Streamable HTTP StreamableHTTPConnectionParams The MCP server is a remote HTTP service using the newer single-endpoint transport Connecting via Stdio Stdio is the simplest transport. The MCP server runs as a child process on the same machine, and ADK communicates with it through standard input and output pipes. Many community MCP servers distributed as Node.js packages use this model.\nHere is an example that connects an ADK agent to the community filesystem MCP server. This server provides tools for listing directories, reading files, and performing other file operations.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 import os from google.adk.agents import LlmAgent from google.adk.tools.mcp_tool import McpToolset from google.adk.tools.mcp_tool.mcp_session_manager import StdioConnectionParams from mcp import StdioServerParameters from dotenv import load_dotenv load_dotenv() ACCESSIBLE_FOLDER = \u0026#34;C:/GitHub/google-adk-101\u0026#34; root_agent = LlmAgent( model=\u0026#34;gemini-2.0-flash\u0026#34;, name=\u0026#34;file_assistant\u0026#34;, instruction=f\u0026#34;\u0026#34;\u0026#34;Help the user manage files. You can list directories, read files, and search for content. You have access to the directory: {ACCESSIBLE_FOLDER} Always use the full absolute path when calling filesystem tools. Use list_directory first to explore, then read specific files.\u0026#34;\u0026#34;\u0026#34;, tools=[ McpToolset( connection_params=StdioConnectionParams( server_params=StdioServerParameters( command=\u0026#34;npx.cmd\u0026#34;, args=[ \u0026#34;-y\u0026#34;, \u0026#34;@modelcontextprotocol/server-filesystem\u0026#34;, ACCESSIBLE_FOLDER, ], ), timeout=60, ), ) ], ) When this agent starts, McpToolset spawns npx @modelcontextprotocol/server-filesystem as a subprocess, queries it for tools, and makes them available to the LLM. The agent can now respond to prompts such as \u0026ldquo;list all Python files in my project directory\u0026rdquo; or \u0026ldquo;show me the contents of config.yaml\u0026rdquo; without any custom tool code.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 \u0026gt; adk run .\\mcp_agent\\ Running agent file_assistant, type exit to exit. [user]: list files in mcp_agent folder [file_assistant]: Okay, I see the following files and directories in the `mcp_agent` folder: * `.adk` (directory) * `agent.py` (file) * `__init__.py` (file) * `__pycache__` (directory) Is there anything specific you\u0026#39;d like me to do with these files? For example, I can read the content of `agent.py` or any other file. [user]: Connecting via SSE When the MCP server is running remotely as a standalone HTTP service, you use SseConnectionParams. SSE (Server-Sent Events) was the original network transport for MCP. It uses two separate HTTP endpoints, one for the client to send requests (POST) and another where the client establishes a persistent SSE connection (GET) to receive streaming responses.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from google.adk.agents import LlmAgent from google.adk.tools.mcp_tool import McpToolset from google.adk.tools.mcp_tool.mcp_session_manager import SseConnectionParams root_agent = LlmAgent( model=\u0026#34;gemini-2.0-flash\u0026#34;, name=\u0026#34;wiki_agent\u0026#34;, instruction=\u0026#34;Help users retrieve and summarize Wikipedia articles.\u0026#34;, tools=[ McpToolset( connection_params=SseConnectionParams( url=\u0026#34;http://localhost:8001/sse\u0026#34;, headers={\u0026#34;Authorization\u0026#34;: \u0026#34;Bearer my-api-token\u0026#34;}, timeout=30, ), ) ], ) The SseConnectionParams accept url, optional headers for authentication, a timeout for the initial connection, and an sse_read_timeout for the streaming read.\nConnecting via Streamable HTTP In March 2025, the MCP specification introduced Streamable HTTP as the successor to SSE. Instead of two separate endpoints, Streamable HTTP uses a single HTTP endpoint for both sending requests and receiving responses. This simplifies deployment and works better with standard HTTP infrastructure, such as load balancers and reverse proxies.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import os from google.adk.agents import LlmAgent from google.adk.tools.mcp_tool import MCPToolset, StreamableHTTPConnectionParams root_agent = LlmAgent( model=\u0026#34;gemini-2.0-flash\u0026#34;, name=\u0026#34;github_agent\u0026#34;, instruction=\u0026#34;Help users work with GitHub repositories, issues, and pull requests.\u0026#34;, tools=[ MCPToolset( connection_params=StreamableHTTPConnectionParams( url=\u0026#34;https://api.githubcopilot.com/mcp/\u0026#34;, headers={ \u0026#34;Authorization\u0026#34;: \u0026#34;Bearer \u0026#34; + os.environ[\u0026#34;GITHUB_TOKEN\u0026#34;], }, ), ) ], ) For new MCP server integrations, prefer Streamable HTTP over SSE when the server supports it.\n1 2 3 4 5 6 7 8 \u0026gt; adk run .\\mcp_agent\\ Running agent github_agent, type exit to exit. [user]: get rchaganti/dscresources info [github_agent]: The repository name is DSCResources, full name is rchaganti/DSCResources, description is Custom DSC resource modules by PowerShell Magazine, the language is PowerShell, it has 62 stars, 26 forks, and 13 open issues. It was created on 2014-10-13T07:06:17Z and updated on 2024-10-24T03:19:14Z. It is a public repository and the default branch is master. [user]: Filtering tools An MCP server might expose dozens of tools, but your agent may only need a handful. Exposing unnecessary tools to the LLM increases token consumption and can confuse the model\u0026rsquo;s tool selection. The tool_filter parameter on McpToolset lets you whitelist only the tools you need.\n1 2 3 4 5 6 7 8 9 McpToolset( connection_params=StdioConnectionParams( server_params=StdioServerParameters( command=\u0026#34;npx\u0026#34;, args=[\u0026#34;-y\u0026#34;, \u0026#34;@modelcontextprotocol/server-filesystem\u0026#34;, \u0026#34;/home/user/data\u0026#34;], ), ), tool_filter=[\u0026#34;read_file\u0026#34;, \u0026#34;list_directory\u0026#34;], ) With this filter, even if the filesystem server exposes tools for writing, moving, or deleting files, only read_file and list_directory will be available to the agent. This is a good practice for both security and performance.\nUsing McpToolset outside adk web When you are running your own application loop instead of using adk web, the setup requires a bit more care around async initialization and connection cleanup. The McpToolset must be created asynchronously, and you need to manage the connection lifecycle properly.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 import asyncio from google.adk.agents import LlmAgent from google.adk.sessions import InMemorySessionService from google.adk.runners import Runner from google.adk.tools.mcp_tool import McpToolset from google.adk.tools.mcp_tool.mcp_session_manager import StdioConnectionParams from mcp import StdioServerParameters from google.genai.types import Content, Part async def main(): toolset = McpToolset( connection_params=StdioConnectionParams( server_params=StdioServerParameters( command=\u0026#34;npx\u0026#34;, args=[\u0026#34;-y\u0026#34;, \u0026#34;@modelcontextprotocol/server-filesystem\u0026#34;, \u0026#34;C:\\\\github\\\\google-adk-101\u0026#34;], ), ), tool_filter=[\u0026#34;read_file\u0026#34;, \u0026#34;list_directory\u0026#34;], ) agent = LlmAgent( model=\u0026#34;gemini-2.0-flash\u0026#34;, name=\u0026#34;file_reader\u0026#34;, instruction=\u0026#34;You help users explore files.\u0026#34;, tools=[toolset], ) session_service = InMemorySessionService() runner = Runner( agent=agent, app_name=\u0026#34;mcp_demo\u0026#34;, session_service=session_service, ) session = await session_service.create_session( app_name=\u0026#34;mcp_demo\u0026#34;, user_id=\u0026#34;user1\u0026#34; ) user_msg = Content( parts=[ Part( text=\u0026#34;What files are in mcp_agent folder?\u0026#34; ) ] ,role=\u0026#34;user\u0026#34; ) async for event in runner.run_async( user_id=\u0026#34;user1\u0026#34;, session_id=session.id, new_message=user_msg ): if event.is_final_response() and event.content and event.content.parts: print(f\u0026#34;Agent: {event.content.parts[0].text}\u0026#34;) asyncio.run(main()) When using McpToolset directly in adk web, the framework handles connection setup and teardown automatically. In standalone applications like the one above, the Runner manages the lifecycle of the toolset.\n1 2 3 \u0026gt; python.exe .\\mcp_agent\\agent.py Agent: Okay, the files in the mcp_agent folder are: .adk (directory), agent.py, __init__.py, and __pycache__ (directory). Connecting to multiple MCP servers An agent can consume tools from several MCP servers simultaneously. Each McpToolset manages its own independent connection. You simply list multiple toolsets in the agent\u0026rsquo;s tools parameter.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 import os from google.adk.agents import LlmAgent from google.adk.tools.mcp_tool import McpToolset, MCPToolset, StreamableHTTPConnectionParams from google.adk.tools.mcp_tool.mcp_session_manager import StdioConnectionParams from mcp import StdioServerParameters root_agent = LlmAgent( model=\u0026#34;gemini-2.0-flash\u0026#34;, name=\u0026#34;research_assistant\u0026#34;, instruction=\u0026#34;\u0026#34;\u0026#34;You are a research assistant. Use the filesystem tools to read local notes, and the GitHub tools to look up repository information.\u0026#34;\u0026#34;\u0026#34;, tools=[ # Local filesystem server McpToolset( connection_params=StdioConnectionParams( server_params=StdioServerParameters( command=\u0026#34;npx\u0026#34;, args=[\u0026#34;-y\u0026#34;, \u0026#34;@modelcontextprotocol/server-filesystem\u0026#34;, \u0026#34;C:\\\\Github\\\\google-adk-101\u0026#34;], ), ), tool_filter=[\u0026#34;read_file\u0026#34;, \u0026#34;list_directory\u0026#34;], ), # Remote GitHub server MCPToolset( connection_params=StreamableHTTPConnectionParams( url=\u0026#34;https://api.githubcopilot.com/mcp/\u0026#34;, headers={\u0026#34;Authorization\u0026#34;: \u0026#34;Bearer \u0026#34; + os.environ[\u0026#34;GITHUB_TOKEN\u0026#34;]}, ), ), ], ) The LLM sees all the tools from both servers in a single flat list and can combine them freely within a single conversation.\nExposing ADK tools as an MCP server The second integration pattern is the reverse direction. You have tool functions built with ADK, and you want to make them available to any MCP client, not just ADK agents. This is useful when other teams or frameworks in your organization need access to the same tools.\nADK provides a conversion utility, adk_to_mcp_tool_type, that translates ADK tool definitions into MCP-compatible schemas. You then wrap these inside a standard MCP server built with the mcp Python library.\nBuilding the MCP server Here is a complete example. We will create a simple ADK tool that checks the health of a set of service endpoints, then expose it through an MCP server.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 # health_check_mcp_server.py import asyncio import json from mcp import types as mcp_types from mcp.server.lowlevel import Server, NotificationOptions from mcp.server.models import InitializationOptions import mcp.server.stdio from google.adk.tools.function_tool import FunctionTool from google.adk.tools.mcp_tool.conversion_utils import adk_to_mcp_tool_type def check_service_health(service_name: str) -\u0026gt; dict: \u0026#34;\u0026#34;\u0026#34;Checks the health status of a named service.\u0026#34;\u0026#34;\u0026#34; # In a real application, this would ping the actual service health_data = { \u0026#34;payments\u0026#34;: {\u0026#34;status\u0026#34;: \u0026#34;healthy\u0026#34;, \u0026#34;latency_ms\u0026#34;: 45, \u0026#34;uptime\u0026#34;: \u0026#34;99.97%\u0026#34;}, \u0026#34;inventory\u0026#34;: {\u0026#34;status\u0026#34;: \u0026#34;degraded\u0026#34;, \u0026#34;latency_ms\u0026#34;: 320, \u0026#34;uptime\u0026#34;: \u0026#34;98.5%\u0026#34;}, \u0026#34;auth\u0026#34;: {\u0026#34;status\u0026#34;: \u0026#34;healthy\u0026#34;, \u0026#34;latency_ms\u0026#34;: 12, \u0026#34;uptime\u0026#34;: \u0026#34;99.99%\u0026#34;}, } result = health_data.get( service_name.lower(), {\u0026#34;status\u0026#34;: \u0026#34;unknown\u0026#34;, \u0026#34;error\u0026#34;: f\u0026#34;No service found with name \u0026#39;{service_name}\u0026#39;\u0026#34;} ) result[\u0026#34;service\u0026#34;] = service_name return result # Wrap the Python function as an ADK FunctionTool health_tool = FunctionTool(check_service_health) # Create the MCP server app = Server(\u0026#34;health-check-server\u0026#34;) @app.list_tools() async def list_tools() -\u0026gt; list[mcp_types.Tool]: \u0026#34;\u0026#34;\u0026#34;Advertise our ADK tool in MCP format.\u0026#34;\u0026#34;\u0026#34; return [adk_to_mcp_tool_type(health_tool)] @app.call_tool() async def call_tool(name: str, arguments: dict) -\u0026gt; list[mcp_types.TextContent]: \u0026#34;\u0026#34;\u0026#34;Route incoming MCP tool calls to the ADK tool.\u0026#34;\u0026#34;\u0026#34; if name == health_tool.name: result = await health_tool.run_async(args=arguments, tool_context=None) return [mcp_types.TextContent(type=\u0026#34;text\u0026#34;, text=json.dumps(result, indent=2))] return [mcp_types.TextContent( type=\u0026#34;text\u0026#34;, text=json.dumps({\u0026#34;error\u0026#34;: f\u0026#34;Unknown tool: {name}\u0026#34;}) )] async def run_server(): async with mcp.server.stdio.stdio_server() as (reader, writer): await app.run( reader, writer, InitializationOptions( server_name=app.name, server_version=\u0026#34;0.1.0\u0026#34;, capabilities=app.get_capabilities( notification_options=NotificationOptions(), experimental_capabilities={}, ), ), ) if __name__ == \u0026#34;__main__\u0026#34;: asyncio.run(run_server()) The adk_to_mcp_tool_type(health_tool) call reads the function signature, docstring, and type hints from the ADK FunctionTool and converts them into the MCP Tool schema that clients expect. The call_tool handler receives calls from any MCP client, executes the underlying ADK tool, and returns the result in MCP format.\nNote that tool_context is None when running outside of a full ADK Runner invocation. If your tool relies on features from ToolContext, such as session state or artifacts, you will need additional handling to provide that context.\nTesting with an ADK agent Now we can test this server by connecting an ADK agent to it as a client. The agent treats our custom MCP server exactly like any other MCP server.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 # agent.py import os from google.adk.agents import LlmAgent from google.adk.tools.mcp_tool import McpToolset from google.adk.tools.mcp_tool.mcp_session_manager import StdioConnectionParams from mcp import StdioServerParameters # Get path relative to this agent.py file _AGENT_DIR = os.path.dirname(os.path.abspath(__file__)) MCP_SERVER_PATH = os.path.join(_AGENT_DIR, \u0026#34;health_check_mcp_server.py\u0026#34;) root_agent = LlmAgent( model=\u0026#34;gemini-2.0-flash\u0026#34;, name=\u0026#34;ops_assistant\u0026#34;, instruction=\u0026#34;You are an operations assistant. Check the health of services when asked.\u0026#34;, tools=[ McpToolset( connection_params=StdioConnectionParams( server_params=StdioServerParameters( command=\u0026#34;python\u0026#34;, args=[MCP_SERVER_PATH], ), timeout=60, ), ) ], ) When this agent runs and a user asks \u0026ldquo;How is the payments service doing?\u0026rdquo;, the LLM will invoke the check_service_health tool through the MCP protocol, and the server will execute the ADK tool function and return the health data.\n1 2 3 4 5 \u0026gt; adk run .\\mcp_agent\\ Running agent ops_assistant, type exit to exit. [user]: How is the payments service doing? [ops_assistant]: The payments service is healthy with a latency of 45ms and uptime of 99.97%. Using FastMCP for simpler server creation For quick server development, you can use FastMCP, which ADK leverages internally. With FastMCP, you decorate your tool functions directly and skip the boilerplate of manually implementing list_tools and call_tool handlers.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # fast_mcp_server.py from mcp.server.fastmcp import FastMCP mcp = FastMCP(\u0026#34;weather-service\u0026#34;) @mcp.tool() def get_current_temperature(city: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Gets the current temperature for a given city.\u0026#34;\u0026#34;\u0026#34; # Simulated data temps = {\u0026#34;london\u0026#34;: \u0026#34;14°C\u0026#34;, \u0026#34;tokyo\u0026#34;: \u0026#34;22°C\u0026#34;, \u0026#34;new york\u0026#34;: \u0026#34;18°C\u0026#34;, \u0026#34;mumbai\u0026#34;: \u0026#34;32°C\u0026#34;} return temps.get(city.lower(), f\u0026#34;No data available for {city}\u0026#34;) @mcp.tool() def get_weather_forecast(city: str, days: int = 3) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Gets a weather forecast for a city for the specified number of days.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Forecast for {city} over {days} days: Mostly sunny with occasional clouds.\u0026#34; if __name__ == \u0026#34;__main__\u0026#34;: mcp.run(transport=\u0026#34;stdio\u0026#34;) This FastMCP server can be consumed by any ADK agent using StdioConnectionParams pointing to python3 fast_mcp_server.py. For remote deployment, you can switch the transport to Streamable HTTP and run the server behind an ASGI server such as Uvicorn.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # remote_fast_mcp_server.py import uvicorn from mcp.server.fastmcp import FastMCP mcp = FastMCP(\u0026#34;weather-service\u0026#34;) @mcp.tool() def get_current_temperature(city: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Gets the current temperature for a given city.\u0026#34;\u0026#34;\u0026#34; # Simulated data temps = {\u0026#34;london\u0026#34;: \u0026#34;14°C\u0026#34;, \u0026#34;tokyo\u0026#34;: \u0026#34;22°C\u0026#34;, \u0026#34;new york\u0026#34;: \u0026#34;18°C\u0026#34;, \u0026#34;mumbai\u0026#34;: \u0026#34;32°C\u0026#34;} return temps.get(city.lower(), f\u0026#34;No data available for {city}\u0026#34;) @mcp.tool() def get_weather_forecast(city: str, days: int = 3) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Gets a weather forecast for a city for the specified number of days.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Forecast for {city} over {days} days: Mostly sunny with occasional clouds.\u0026#34; app = mcp.streamable_http_app() if __name__ == \u0026#34;__main__\u0026#34;: uvicorn.run(app, host=\u0026#34;0.0.0.0\u0026#34;, port=3000) An ADK agent then connects using StreamableHTTPConnectionParams.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from google.adk.agents import LlmAgent from google.adk.tools.mcp_tool import MCPToolset, StreamableHTTPConnectionParams root_agent = LlmAgent( model=\u0026#34;gemini-2.0-flash\u0026#34;, name=\u0026#34;weather_agent\u0026#34;, instruction=\u0026#34;Provide weather information when asked.\u0026#34;, tools=[ MCPToolset( connection_params=StreamableHTTPConnectionParams( url=\u0026#34;http://localhost:3000/mcp\u0026#34;, ), ) ], ) 1 2 3 4 5 6 7 8 9 10 \u0026gt; adk run .\\mcp_agent\\ Running agent weather_agent, type exit to exit. [user]: how is weather in Bengaluru? [weather_agent]: I am sorry, I cannot fulfill this request. I do not have the weather information for Bengaluru. [user]: how is weather in london? [weather_agent]: The current temperature in London is 14°C. Key considerations There are several important points to keep in mind when working with MCP tools in ADK.\nStatefulness MCP establishes stateful, persistent connections between client and server. This is different from typical stateless REST APIs and has implications for scaling and deployment. When deploying remote MCP servers, you need to think about session affinity, load balancing, and connection pooling. The McpToolset manages this connection lifecycle for you within the agent process.\nAsync nature Both ADK and the MCP Python library are built on Python\u0026rsquo;s asyncio. Tool implementations and server handlers should generally be async functions. The McpToolset handles the async communication transparently, but if you are building custom MCP servers, your tool handlers should follow async patterns.\nDeployment considerations When deploying ADK agents with MCP tools to production environments like Cloud Run or Vertex AI Agent Engine, the agent and McpToolset must be defined synchronously in your agent.py file. While adk web allows for asynchronous agent creation, deployment environments require synchronous instantiation.\nFor Stdio-based MCP servers, the server binary or script must be included in the deployment container since the agent spawns it as a subprocess. For remote servers using SSE or Streamable HTTP, you deploy the MCP server independently and configure your agent with the server\u0026rsquo;s URL.\nA production deployment checklist for agents using MCP tools includes:\nEnsuring MCP server binaries are bundled in the container image for Stdio transports Configuring health checks for remote MCP server connections Setting appropriate timeouts on SseConnectionParams or StreamableHTTPConnectionParams Implementing authentication headers for remote servers, and monitoring connection stability. Pre-built MCP tools ADK offers a growing catalog of pre-built MCP tool integrations for popular services. These include integrations with Atlassian (Jira and Confluence), Google Cloud services via the API Registry, and generative media services such as Imagen and Veo. You can find the full list in the ADK documentation under Tools and Integrations. Using a prebuilt integration is as simple as adding the appropriate McpToolset with the correct connection parameters, as we demonstrated earlier with the filesystem and Google Maps examples.\nPutting it all together Here is a complete example that combines consuming an MCP server with tool filtering and session state tracking. The agent uses a local filesystem MCP server to read project notes, while also maintaining conversation context through ADK\u0026rsquo;s session state.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 import asyncio import os from google.adk.agents import LlmAgent from google.adk.sessions import InMemorySessionService from google.adk.runners import Runner from google.adk.tools import ToolContext from google.adk.tools.mcp_tool import McpToolset from google.adk.tools.mcp_tool.mcp_session_manager import StdioConnectionParams from mcp import StdioServerParameters from google.genai.types import Content, Part def summarize_findings(summary: str, tool_context: ToolContext) -\u0026gt; dict: \u0026#34;\u0026#34;\u0026#34;Records a summary of findings from the file research.\u0026#34;\u0026#34;\u0026#34; summaries = tool_context.state.get(\u0026#34;research_summaries\u0026#34;, []) summaries.append(summary) tool_context.state[\u0026#34;research_summaries\u0026#34;] = summaries tool_context.state[\u0026#34;total_summaries\u0026#34;] = len(summaries) return {\u0026#34;status\u0026#34;: \u0026#34;recorded\u0026#34;, \u0026#34;total_summaries\u0026#34;: len(summaries)} NOTES_DIR = os.path.expanduser(\u0026#34;~/project-notes\u0026#34;) research_agent = LlmAgent( model=\u0026#34;gemini-2.0-flash\u0026#34;, name=\u0026#34;ResearchAgent\u0026#34;, instruction=\u0026#34;\u0026#34;\u0026#34;You are a research assistant. Use file tools to read notes from the project directory. After reading relevant files, use the summarize_findings tool to record your analysis. Total summaries recorded so far: {total_summaries?}\u0026#34;\u0026#34;\u0026#34;, tools=[ McpToolset( connection_params=StdioConnectionParams( server_params=StdioServerParameters( command=\u0026#34;npx\u0026#34;, args=[ \u0026#34;-y\u0026#34;, \u0026#34;@modelcontextprotocol/server-filesystem\u0026#34;, os.path.abspath(NOTES_DIR), ], ), ), tool_filter=[\u0026#34;read_file\u0026#34;, \u0026#34;list_directory\u0026#34;, \u0026#34;search_files\u0026#34;], ), summarize_findings, ], output_key=\u0026#34;last_response\u0026#34;, ) async def main(): session_service = InMemorySessionService() runner = Runner( agent=research_agent, app_name=\u0026#34;research_app\u0026#34;, session_service=session_service, ) session = await session_service.create_session( app_name=\u0026#34;research_app\u0026#34;, user_id=\u0026#34;researcher_1\u0026#34;, state={\u0026#34;total_summaries\u0026#34;: 0}, ) # Turn 1: Explore available files msg1 = Content( parts=[Part(text=\u0026#34;What project notes do we have available?\u0026#34;)], role=\u0026#34;user\u0026#34;, ) async for event in runner.run_async( user_id=\u0026#34;researcher_1\u0026#34;, session_id=session.id, new_message=msg1 ): if event.is_final_response() and event.content: print(f\u0026#34;Agent: {event.content.parts[0].text}\u0026#34;) # Turn 2: Read and summarize a specific file msg2 = Content( parts=[Part(text=\u0026#34;Read the architecture.md file and summarize the key decisions.\u0026#34;)], role=\u0026#34;user\u0026#34;, ) async for event in runner.run_async( user_id=\u0026#34;researcher_1\u0026#34;, session_id=session.id, new_message=msg2 ): if event.is_final_response() and event.content: print(f\u0026#34;Agent: {event.content.parts[0].text}\u0026#34;) # Check state s = await session_service.get_session( app_name=\u0026#34;research_app\u0026#34;, user_id=\u0026#34;researcher_1\u0026#34;, session_id=session.id ) print(f\u0026#34;State: total_summaries={s.state.get(\u0026#39;total_summaries\u0026#39;)}\u0026#34;) print(f\u0026#34;State: research_summaries={s.state.get(\u0026#39;research_summaries\u0026#39;)}\u0026#34;) asyncio.run(main()) This example demonstrates a common pattern in real applications:\nCombining external tools from an MCP server with custom ADK tools that maintain state. The filesystem tools handle file operations The summarize_findings function records analysis in the session state using ToolContext The agent\u0026rsquo;s instruction template dynamically reflects the current summary count. Understanding how to consume and expose MCP tools with ADK opens up a broad ecosystem of reusable capabilities for your agents. The McpToolset class makes it straightforward to connect to any MCP-compliant server, regardless of the transport mechanism, while ADK\u0026rsquo;s conversion utilities let you share your tools with the wider MCP ecosystem. I recommend starting with a community MCP server, such as the Filesystem or GitHub examples, to get comfortable with the pattern, then build your own servers for custom capabilities.\n","date":"2025-12-20T00:00:00Z","excerpt":"An agent that can only use the tools you hard-code into it has a ceiling. The Model Context Protocol (MCP) shatters that ceiling by giving your ADK agents a standardized way to discover and use tools hosted anywhere — local processes, remote servers, or cloud services.","permalink":"https://ravichaganti.com/blog/google-adk-mcp-tools/","series":"Google ADK","title":"Google ADK - MCP tools"},{"categories":["Google","Agent Development Kit","Agents"],"content":"You have defined your agents, wired up tools, set up callbacks, and configured sessions with scoped state. But when a user sends a message, what actually happens? Who calls the agent? Who persists in the state changes? Who decides when the conversation turn is over? The answer to all of these is the Runner. The Runner is the central orchestrator of the ADK runtime. It receives a user\u0026rsquo;s query, starts the agent, processes every event the agent emits, commits state changes via the SessionService, and forwards events to the caller. Without the Runner, your agents, tools, and callbacks are just definitions sitting idle. The Runner is the engine that brings them to life.\nThe event loop An event loop is at the heart of the ADK runtime and facilitates the communication between the Runner component and the agent execution. When a user prompt arrives, the Runner hands it over to the agent for processing. The agent runs until it has something to yield, at which point it emits an event. The Runner receives the event, processes any associated actions, calls the session service to append the event to the current state, and forwards the event. After the Runner completes event processing, the agent resumes from where it was paused and continues this loop until it has no more events to yield. The Runner component is the central orchestrator of this event loop.\nSeveral components work together within the ADK runtime. Understanding their roles clarifies how the event loop functions.\nThe Runner The Runner serves as the central coordinator for a single-user invocation. Its responsibilities in the loop are:\nInitiation: Receives the user\u0026rsquo;s query (new_message) and appends it to the session history via the SessionService. Kick-off: Starts event generation by calling the main agent\u0026rsquo;s execution method (agent.run_async(...)). Receive and process: Waits for the agent logic to yield an Event. Upon receiving one, it uses configured services (SessionService, ArtifactService, MemoryService) to commit changes indicated in event.actions (such as state_delta and artifact_delta). Yield upstream: Forwards the processed event onwards to the calling application or UI for rendering. Iterate: Signals the agent logic that processing is complete, allowing it to resume and generate the next event. The execution logic Your code within agents, tools, and callbacks is responsible for the actual computation and decision-making. Its interaction with the loop follows a specific pattern:\nExecute: Runs its logic based on the current InvocationContext, including the session state as it was when execution resumed. Yield: When the logic needs to communicate, it constructs an Event containing the relevant content and actions, then yields it back to the Runner. Pause: The agent\u0026rsquo;s execution pauses immediately after the yield. It waits for the Runner to complete processing and committing. Resume: Only after the Runner has processed the yielded event does the agent logic resume from the statement immediately following the yield. See updated state: Upon resumption, the agent logic can now reliably access the session state reflecting the changes that were committed by the Runner. This cooperative yield/pause/resume cycle between the Runner and your execution logic, mediated by Event objects, forms the core of the ADK runtime. When the Runner starts processing a user query, it creates an InvocationContext. This is the runtime\u0026rsquo;s \u0026ldquo;traveling notebook\u0026rdquo; that accompanies the interaction from start to finish, collecting information, tracking progress, and providing context to every component along the way. You do not typically create or manage this object directly. The ADK framework creates it when an invocation starts via runner.run_async and passes the relevant contextual information to your agent code, callbacks, and tools. When you implement custom agents, you receive them as the ctx parameter in _run_async_impl.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from google.adk.agents import BaseAgent from google.adk.agents.invocation_context import InvocationContext from google.adk.events import Event from typing import AsyncGenerator class MyAgent(BaseAgent): async def _run_async_impl( self, ctx: InvocationContext ) -\u0026gt; AsyncGenerator[Event, None]: # Direct access to runtime information agent_name = ctx.agent.name session_id = ctx.session.id current_state = ctx.session.state # Use ctx.end_invocation = True to terminate early ... It is important to understand the hierarchy of concepts within an invocation. An invocation starts with a user message and ends with a final response. It can contain one or multiple agent calls, for example, when using agent transfer or AgentTool. Each agent call is handled by agent.run_async(). An LLM agent call can contain one or multiple steps. Each step calls the LLM once and yields its response. If the LLM requests tool calls, those are executed within the same step.\nState variables prefixed with temp: are strictly scoped to a single invocation and discarded afterwards. When a parent agent calls a sub-agent, it passes its InvocationContext to the sub-agent. This means the entire chain of agent calls shares the same invocation ID and the same temp: state.\nCreating and using a Runner To create a Runner, you need an agent and a SessionService. Optionally, you can provide an ArtifactService and a MemoryService.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from google.adk.agents import LlmAgent from google.adk.sessions import InMemorySessionService from google.adk.runners import Runner # Define the agent agent = LlmAgent( name=\u0026#34;MyAgent\u0026#34;, model=\u0026#34;gemini-2.0-flash\u0026#34;, instruction=\u0026#34;You are a helpful assistant.\u0026#34; ) # Create the session service session_service = InMemorySessionService() # Create the runner runner = Runner( agent=agent, app_name=\u0026#34;my_app\u0026#34;, session_service=session_service ) Once you have a runner, you interact with it using one of its run methods.\nrun_async This is the primary method for executing agent invocations. It returns an async generator of events. The ADK runtime is fundamentally built on asynchronous patterns using Python\u0026rsquo;s asyncio to handle concurrent operations like waiting for LLM responses or tool executions efficiently without blocking.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from google.genai.types import Content, Part session = await session_service.create_session( app_name=\u0026#34;my_app\u0026#34;, user_id=\u0026#34;user_1\u0026#34; ) user_msg = Content(parts=[Part(text=\u0026#34;Hello!\u0026#34;)], role=\u0026#34;user\u0026#34;) async for event in runner.run_async( user_id=\u0026#34;user_1\u0026#34;, session_id=session.id, new_message=user_msg ): if event.is_final_response() and event.content and event.content.parts: print(event.content.parts[0].text) run (synchronous) A synchronous Runner.run method exists for convenience in simple scripts or testing environments. Internally, it calls Runner.run_async and manages the async event loop execution for you.\n1 2 3 4 5 6 7 8 9 user_msg = Content(parts=[Part(text=\u0026#34;Hello!\u0026#34;)], role=\u0026#34;user\u0026#34;) for event in runner.run( user_id=\u0026#34;user_1\u0026#34;, session_id=session.id, new_message=user_msg ): if event.is_final_response() and event.content and event.content.parts: print(event.content.parts[0].text) For production applications, especially web servers, we should design applications to be asynchronous using run_async for best performance.\nrun_live For bidirectional streaming scenarios, such as voice conversations, the Runner provides run_live. This method uses a LiveRequestQueue for sending messages and returns an async generator of events. Unlike run_async, which handles a single request-response cycle, run_live maintains a persistent streaming connection to the LLM.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 from google.adk.agents.run_config import RunConfig from google.adk.runners import Runner from google.adk.sessions import InMemorySessionService runner = Runner( agent=agent, app_name=\u0026#34;my_app\u0026#34;, session_service=InMemorySessionService() ) session = await session_service.create_session( app_name=\u0026#34;my_app\u0026#34;, user_id=\u0026#34;user_1\u0026#34; ) live_request_queue = LiveRequestQueue() run_config = RunConfig(response_modalities=[\u0026#34;TEXT\u0026#34;]) async for event in runner.run_live( session=session, live_request_queue=live_request_queue, run_config=run_config ): if event.content and event.content.parts: print(event.content.parts[0].text) One InvocationContext corresponds to one run_live() loop. It is created when you call run_live() and persists for the entire streaming session.\nRunConfig The RunConfig class defines runtime behavior and options for agents. It controls streaming settings, function calling, artifact saving, and LLM call limits. You pass a RunConfig to customize how the runner executes your agent.\n1 2 3 4 5 6 7 8 from google.adk.agents.run_config import RunConfig, StreamingMode run_config = RunConfig( streaming_mode=StreamingMode.NONE, max_llm_calls=500, save_input_blobs_as_artifacts=False, support_cfc=False ) Some of the key properties of the RunConfig class are:\nProperty Type Default Purpose streaming_mode StreamingMode StreamingMode.NONE Controls output delivery: NONE, SSE, or BIDI max_llm_calls int 500 Safety limit on total LLM calls per invocation save_input_blobs_as_artifacts bool False Whether to save input binary data as artifacts support_cfc bool False Enables Compositional Function Calling speech_config SpeechConfig None Voice configuration for live/audio agents response_modalities list[str] None Controls output format: [\u0026quot;TEXT\u0026quot;] or [\u0026quot;AUDIO\u0026quot;] StreamingMode The streaming_mode setting determines how the agent\u0026rsquo;s responses are delivered.\nStreamingMode.NONE is the default. The LLM generates its entire response before delivering it. The Runner receives a single non-partial event for the response. StreamingMode.SSE (Server-Sent Events) uses HTTP streaming. The LLM generates its response in chunks. The Runner yields multiple events with partial=True for progressive display, followed by a final non-partial event. StreamingMode.BIDI enables full bidirectional streaming via WebSocket, used with run_live() for real-time voice and multimodal interactions. max_llm_calls The max_llm_calls parameter acts as a safety limit to prevent runaway agent loops. If an agent enters an infinite tool-calling cycle, this limit ensures the invocation terminates after a set number of LLM calls. The default of 500 is generous for most use cases.\nCompositional Function Calling Setting support_cfc=True enables Compositional Function Calling. This allows the model to orchestrate multiple tools in sophisticated patterns, calling tools in parallel, chaining outputs as inputs to other tools, or conditionally executing tools based on intermediate results.\nUnderstanding a few key aspects of how the ADK runtime handles state and streaming is crucial for building predictable agents.\nPutting it all together Here is a complete example that demonstrates the Runner orchestrating a multi-turn conversation with a tool-calling agent. This builds on the sessions and state concepts from the earlier article in this series.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 import asyncio from google.adk.agents import LlmAgent from google.adk.sessions import InMemorySessionService from google.adk.runners import Runner from google.adk.tools import ToolContext from google.genai.types import Content, Part # --- Tools that modify state --- def lookup_weather( city: str, tool_context: ToolContext ) -\u0026gt; dict: \u0026#34;\u0026#34;\u0026#34;Looks up the current weather for a city.\u0026#34;\u0026#34;\u0026#34; weather_data = { \u0026#34;Paris\u0026#34;: {\u0026#34;temp\u0026#34;: 18, \u0026#34;condition\u0026#34;: \u0026#34;Partly cloudy\u0026#34;}, \u0026#34;London\u0026#34;: {\u0026#34;temp\u0026#34;: 14, \u0026#34;condition\u0026#34;: \u0026#34;Rainy\u0026#34;}, \u0026#34;Tokyo\u0026#34;: {\u0026#34;temp\u0026#34;: 22, \u0026#34;condition\u0026#34;: \u0026#34;Sunny\u0026#34;}, } result = weather_data.get(city, {\u0026#34;temp\u0026#34;: 0, \u0026#34;condition\u0026#34;: \u0026#34;Unknown\u0026#34;}) # Track the last city queried in session state tool_context.state[\u0026#34;last_city\u0026#34;] = city tool_context.state[\u0026#34;user:query_count\u0026#34;] = ( tool_context.state.get(\u0026#34;user:query_count\u0026#34;, 0) + 1 ) return result # --- Agent --- weather_agent = LlmAgent( name=\u0026#34;WeatherAgent\u0026#34;, model=\u0026#34;gemini-2.0-flash\u0026#34;, instruction=\u0026#34;\u0026#34;\u0026#34;You are a weather assistant for {user:name}. They have made {user:query_count?} queries so far. Last city checked: {last_city?} Provide weather information when asked.\u0026#34;\u0026#34;\u0026#34;, tools=[lookup_weather], output_key=\u0026#34;last_response\u0026#34; ) # --- Runner and interaction --- async def main(): session_service = InMemorySessionService() runner = Runner( agent=weather_agent, app_name=\u0026#34;weather_app\u0026#34;, session_service=session_service ) session = await session_service.create_session( app_name=\u0026#34;weather_app\u0026#34;, user_id=\u0026#34;user1\u0026#34;, state={\u0026#34;user:name\u0026#34;: \u0026#34;Ravi\u0026#34;, \u0026#34;user:query_count\u0026#34;: 0} ) # Turn 1 msg1 = Content(parts=[Part(text=\u0026#34;What\u0026#39;s the weather in Paris?\u0026#34;)], role=\u0026#34;user\u0026#34;) async for event in runner.run_async( user_id=\u0026#34;user1\u0026#34;, session_id=session.id, new_message=msg1 ): if event.is_final_response() and event.content: print(f\u0026#34;Agent: {event.content.parts[0].text}\u0026#34;) # Inspect state after Turn 1 s = await session_service.get_session( app_name=\u0026#34;weather_app\u0026#34;, user_id=\u0026#34;user1\u0026#34;, session_id=session.id ) print(f\u0026#34;State: last_city={s.state.get(\u0026#39;last_city\u0026#39;)}\u0026#34;) print(f\u0026#34;State: user:query_count={s.state.get(\u0026#39;user:query_count\u0026#39;)}\u0026#34;) print(f\u0026#34;Events so far: {len(s.events)}\u0026#34;) # Turn 2 msg2 = Content(parts=[Part(text=\u0026#34;How about Tokyo?\u0026#34;)], role=\u0026#34;user\u0026#34;) async for event in runner.run_async( user_id=\u0026#34;user1\u0026#34;, session_id=session.id, new_message=msg2 ): if event.is_final_response() and event.content: print(f\u0026#34;Agent: {event.content.parts[0].text}\u0026#34;) # Inspect state after Turn 2 s = await session_service.get_session( app_name=\u0026#34;weather_app\u0026#34;, user_id=\u0026#34;user1\u0026#34;, session_id=session.id ) print(f\u0026#34;State: last_city={s.state.get(\u0026#39;last_city\u0026#39;)}\u0026#34;) print(f\u0026#34;State: user:query_count={s.state.get(\u0026#39;user:query_count\u0026#39;)}\u0026#34;) asyncio.run(main()) When you run this, you can observe the Runner orchestrating the full event loop across multiple turns, with the session service persisting state between them.\n1 2 3 4 5 6 7 8 9 10 11 \u0026gt; python.exe .\\runner-weather.py Agent: The weather in Paris is partly cloudy with a temperature of 18 degrees. State: last_city=Paris State: user:query_count=1 Events so far: 4 Agent: The weather in Tokyo is sunny with a temperature of 22 degrees. State: last_city=Tokyo State: user:query_count=2 Notice how the Runner drives the entire lifecycle. It receives each user message, passes it to the agent, processes every event the agent generates, including tool call events where state is modified via ToolContext, commits the state_delta through the SessionService, and yields the final response events back to our code. The user:query_count counter increases across turns because the user-scoped state is persisted by the session service between invocations. The agent\u0026rsquo;s instruction template {user:query_count?} is resolved by the framework on each turn using the latest committed state.\nUnderstanding the Runner and its execution architecture is essential for building reliable agent applications with Google ADK. The Runner is the piece that connects everything we have covered so far in this series, from sessions and state to callbacks and tools. Every state change flows through events, every event flows through the Runner, and the Runner ensures consistency through the yield/pause/process/resume cycle. I recommend experimenting with different RunConfig settings and tracing the events your agents emit to build a deeper understanding of this architecture.\n","date":"2025-12-18T00:00:00Z","excerpt":"You have agents, tools, callbacks, and sessions. But what actually runs them? The Runner is the central orchestrator that powers every agent interaction in Google ADK, driving the event loop that connects all these pieces together.","permalink":"https://ravichaganti.com/blog/google-adk-runner-and-execution-architecture/","series":"Google ADK","title":"Google ADK - Runner and execution architecture"},{"categories":["Google","Agent Development Kit","Agents"],"content":"Imagine you built a customer support agent. It works well, but you then discover that the LLM occasionally leaks email addresses in its responses. You need to sanitize output before the user sees it. Or consider a flight booking agent where only admin users should be allowed to cancel reservations. You need to check permissions before a tool executes. Or maybe the same question is asked a hundred times a day, and each time you are paying for an LLM API call. You need to return cached responses and skip the model entirely. All of these share a common pattern. You need to run your code at specific points in the agent\u0026rsquo;s execution pipeline, before or after the model call, before or after a tool runs, before or after the agent itself. This is where the callbacks feature comes into play.\nCallbacks Callbacks are user-defined functions that hook into an agent\u0026rsquo;s execution pipeline at predefined checkpoints. You define them, attach them to an agent at creation time, and ADK calls them automatically at key stages. They let you observe, customize, and control behavior without modifying any ADK framework code. Think of them as checkpoints during the agent\u0026rsquo;s process.\nThe core mental model is a pipeline with six interception points grouped into three pairs.\nThe agent lifecycle pair (before_agent_callback and after_agent_callback) wraps the entire agent execution. The model interaction pair (before_model_callback and after_model_callback) wraps the LLM API call. The tool execution pair (before_tool_callback and after_tool_callback) wraps each tool invocation. Every before_* callback acts as a gatekeeper. These callbacks return None to proceed to the next step, or return a specific object to skip the step and use that object as the output instead.\nEvery after_* callback acts as a post-processor. Return None to pass through the original result, or return a specific object to replace it.\nThese agent lifecycle callbacks are available on all agent types (BaseAgent, LlmAgent, SequentialAgent, ParallelAgent, LoopAgent). The model and tool callbacks are specific to LlmAgent. These callbacks are normal Python functions with specific parameters and return types.\nAgent lifecycle callbacks The agent lifecycle callbacks are available before the agent starts work and after it finishes.\nThe before_agent_callback is useful for setting up the environment needed for the agent\u0026rsquo;s run. You can use this callback validate session state or even modify the invocation context before the agent\u0026rsquo;s core logic starts.\n1 2 def my_before_agent(callback_context: CallbackContext) -\u0026gt; Optional[types.Content]: ... The Runner invokes this function and provides a CallbackContext object as input. This object contains information about the agent\u0026rsquo;s current execution state, session state, and other references such as artifacts and memory.\nThe after_agent_callback is useful for cleaning up after the agent run, performing post-execution checks, or modifying or augmenting the agent\u0026rsquo;s final output.\n1 2 def my_after_agent(callback_context: CallbackContext) -\u0026gt; Optional[types.Content]: ... Model interaction callbacks The model interaction callbacks are specific to LlmAgent and wrap the LLM API calls.\nThe before_model_callback is useful when you want to inspect or modify the request to the LLM API. For example, you can implement use cases such as adding a few-shot examples to the prompt sent to the LLM, implementing input guardrails, etc.\n1 2 3 4 5 def my_before_model( callback_context: CallbackContext, llm_request: LlmRequest ) -\u0026gt; Optional[LlmResponse]: ... The LlmRequest parameter is mutable. You can modify it in-place to change what the LLM receives.\nThe after_model_callback is useful when you want to inspect or modify LLM generated response. With this callback, you can implement response reformatting, such as removing sensitive information.\n1 2 3 4 5 def my_after_model( callback_context: CallbackContext, llm_response: LlmResponse ) -\u0026gt; Optional[LlmResponse]: ... The LlmResponse object in the after_model_callback contains LLM generated response. Both agent lifecycle and model interaction callback pairs receive CallbackContext as input.\nTool execution callbacks The tool_execution_callback is also specific to LlmAgent and wraps each individual tool invocation that the LLM might request.\nThe before_tool_callback is useful when you want to determine whether tool execution should be allowed, inspect or modify tool arguments, or implement tool-level caching.\n1 2 3 4 5 6 def my_before_tool( tool: BaseTool, args: Dict[str, Any], tool_context: ToolContext ) -\u0026gt; Optional[Dict]: ... The args parameter is mutable. We can modify it in-place to change what the tool receives, then return None to invoke the tool with the modified arguments.\nThe after_tool_callback is useful for scenarios such as inspecting or modifying tool output before sending it to the LLM, and for logging tool results.\n1 2 3 4 5 6 7 def my_after_tool( tool: BaseTool, args: Dict[str, Any], tool_context: ToolContext, tool_response: Dict ) -\u0026gt; Optional[Dict]: ... As seen so far, every callback receives a context object that provides access to the agent\u0026rsquo;s session state, artifacts, and invocation metadata. There are two types of context, and understanding them is essential.\nCallbackContext CallbackContext is used in agent lifecycle and model interaction callbacks. It provides access to the agent name, the invocation ID, and the session state.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 from google.adk.agents.callback_context import CallbackContext # Properties available: callback_context.agent_name # Name of the current agent callback_context.invocation_id # Unique ID for this invocation callback_context.state # Session state (read/write) # State modifications are automatically tracked callback_context.state[\u0026#34;my_key\u0026#34;] = \u0026#34;my_value\u0026#34; current = callback_context.state.get(\u0026#34;my_key\u0026#34;, \u0026#34;default\u0026#34;) # Artifact operations: version = callback_context.save_artifact(\u0026#34;filename.txt\u0026#34;, types.Part(text=\u0026#34;content\u0026#34;)) part = callback_context.load_artifact(\u0026#34;filename.txt\u0026#34;) When you write callback_context.state[\u0026quot;key\u0026quot;] = value, the framework automatically captures this into EventActions.state_delta, which the SessionService processes via append_event. This is the same mechanism described in the article on sessions, state, and memory. You never need to manually construct events or call append_event inside callbacks.\nToolContext ToolContext is used in tool callbacks. It extends CallbackContext with tool-specific capabilities.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from google.adk.tools.tool_context import ToolContext # Everything from CallbackContext PLUS: tool_context.function_call_id # ID linking to the LLM\u0026#39;s function call tool_context.actions # Direct access to EventActions # Authentication: tool_context.request_credential(auth_config) # Trigger OAuth flow tool_context.get_auth_response(auth_config) # Retrieve credentials # Memory \u0026amp; Artifacts: results = tool_context.search_memory(\u0026#34;query\u0026#34;) # Search memory service artifacts = tool_context.list_artifacts() # List available artifacts # Summarization control: tool_context.actions.skip_summarization = True # Skip LLM summarization of tool result State prefixes in callbacks State keys in callbacks use the same prefix scoping system described in the sessions, state, and memory article.\n1 2 3 4 5 6 7 8 9 10 11 # Application-wide state (shared across all users/sessions) callback_context.state[\u0026#34;app:global_config\u0026#34;] = {\u0026#34;max_retries\u0026#34;: 3} # User-specific state (shared across sessions for this user) callback_context.state[\u0026#34;user:preferences\u0026#34;] = {\u0026#34;language\u0026#34;: \u0026#34;es\u0026#34;} # Temporary state (cleared between invocations) callback_context.state[\u0026#34;temp:current_step\u0026#34;] = \u0026#34;processing\u0026#34; # No prefix = session-scoped (default, persists within session) callback_context.state[\u0026#34;conversation_topic\u0026#34;] = \u0026#34;weather\u0026#34; Registering callbacks You register callbacks by passing them as parameters when creating an agent. Here is an example that registers all six callbacks on an LlmAgent.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 from google.adk.agents import LlmAgent agent = LlmAgent( name=\u0026#34;MyAgent\u0026#34;, model=\u0026#34;gemini-2.0-flash\u0026#34;, instruction=\u0026#34;You are a helpful assistant.\u0026#34;, tools=[my_tool], before_agent_callback=my_before_agent, after_agent_callback=my_after_agent, before_model_callback=my_before_model, after_model_callback=my_after_model, before_tool_callback=my_before_tool, after_tool_callback=my_after_tool, ) You can also pass a list of callbacks. When a list is provided, the callbacks execute sequentially. The first callback that returns a non-None value stops the chain, and subsequent callbacks in the list do not execute.\n1 2 3 4 5 6 7 8 9 10 11 agent = LlmAgent( name=\u0026#34;ProductionAgent\u0026#34;, model=\u0026#34;gemini-2.0-flash\u0026#34;, instruction=\u0026#34;You are helpful.\u0026#34;, before_model_callback=[ check_cache, # Returns LlmResponse if cache hit content_guardrail, # Returns LlmResponse if policy violation inject_context, # Modifies request, returns None log_request, # Logs, returns None ], ) In this example, if check_cache finds a cache hit, it returns an LlmResponse, and the remaining three callbacks are skipped entirely. Callbacks can be either synchronous or asynchronous functions.\nFor workflow agents (SequentialAgent, ParallelAgent, LoopAgent), only agent lifecycle callbacks (before_agent_callback and after_agent_callback) are available, since these agents do not interact directly with an LLM or tools.\nWith the basics out of the way, let us look at some sample use cases.\nObservability logging The simplest use of callbacks is logging. Every callback returns None, so the agent\u0026rsquo;s behavior is unaffected. This provides a complete picture of the agent\u0026rsquo;s execution for debugging and monitoring.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 import logging from google.adk.agents import LlmAgent from google.adk.agents.callback_context import CallbackContext from google.adk.models import LlmResponse, LlmRequest from google.adk.tools import BaseTool, ToolContext from typing import Optional, Dict, Any from google.genai import types logger = logging.getLogger(__name__) def log_before_agent(callback_context: CallbackContext) -\u0026gt; Optional[types.Content]: logger.info(f\u0026#34;[{callback_context.agent_name}] Agent starting - invocation: {callback_context.invocation_id}\u0026#34;) return None def log_after_agent(callback_context: CallbackContext) -\u0026gt; Optional[types.Content]: logger.info(f\u0026#34;[{callback_context.agent_name}] Agent completed\u0026#34;) return None def log_before_model(callback_context: CallbackContext, llm_request: LlmRequest) -\u0026gt; Optional[LlmResponse]: msg_count = len(llm_request.contents) logger.info(f\u0026#34;[{callback_context.agent_name}] Sending {msg_count} messages to LLM\u0026#34;) return None def log_after_model(callback_context: CallbackContext, llm_response: LlmResponse) -\u0026gt; Optional[LlmResponse]: has_tool_calls = any( part.function_call for part in (llm_response.content.parts or []) ) logger.info(f\u0026#34;[{callback_context.agent_name}] LLM responded (tool_calls={has_tool_calls})\u0026#34;) return None def log_before_tool(tool: BaseTool, args: Dict[str, Any], tool_context: ToolContext) -\u0026gt; Optional[Dict]: logger.info(f\u0026#34;[Tool: {tool.name}] Executing with args: {args}\u0026#34;) return None def log_after_tool(tool: BaseTool, args: Dict[str, Any], tool_context: ToolContext, tool_response: Dict) -\u0026gt; Optional[Dict]: logger.info(f\u0026#34;[Tool: {tool.name}] Returned: {tool_response}\u0026#34;) return None agent = LlmAgent( name=\u0026#34;ObservableAgent\u0026#34;, model=\u0026#34;gemini-2.0-flash\u0026#34;, instruction=\u0026#34;You are a helpful assistant.\u0026#34;, before_agent_callback=log_before_agent, after_agent_callback=log_after_agent, before_model_callback=log_before_model, after_model_callback=log_after_model, before_tool_callback=log_before_tool, after_tool_callback=log_after_tool, ) Every callback returns None, so the agent operates exactly as it would without callbacks. The logs give you a complete trace of what happened and when.\nInput guardrails A common requirement is to block prompts containing forbidden content before they reach the LLM. The before_model_callback is the natural place for this, as it fires just before the LLM API call.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 from google.adk.agents import LlmAgent from google.adk.agents.callback_context import CallbackContext from google.adk.models import LlmResponse, LlmRequest from typing import Optional from google.genai import types BLOCKED_KEYWORDS = [\u0026#34;hack\u0026#34;, \u0026#34;exploit\u0026#34;, \u0026#34;bypass security\u0026#34;, \u0026#34;illegal\u0026#34;] def content_guardrail( callback_context: CallbackContext, llm_request: LlmRequest ) -\u0026gt; Optional[LlmResponse]: \u0026#34;\u0026#34;\u0026#34;Block requests containing forbidden keywords.\u0026#34;\u0026#34;\u0026#34; all_text = \u0026#34;\u0026#34; for content in llm_request.contents: for part in content.parts: if part.text: all_text += part.text.lower() + \u0026#34; \u0026#34; for keyword in BLOCKED_KEYWORDS: if keyword in all_text: callback_context.state[\u0026#34;temp:last_violation\u0026#34;] = keyword callback_context.state[\u0026#34;user:violation_count\u0026#34;] = ( callback_context.state.get(\u0026#34;user:violation_count\u0026#34;, 0) + 1 ) return LlmResponse( content=types.Content( role=\u0026#34;model\u0026#34;, parts=[types.Part( text=\u0026#34;I\u0026#39;m unable to help with that request. \u0026#34; \u0026#34;It appears to involve content that violates our usage policy.\u0026#34; )] ) ) return None agent = LlmAgent( name=\u0026#34;GuardedAgent\u0026#34;, model=\u0026#34;gemini-2.0-flash\u0026#34;, instruction=\u0026#34;You are a helpful assistant.\u0026#34;, before_model_callback=content_guardrail, ) When a blocked keyword is detected, the callback returns an LlmResponse directly. This skips the LLM call entirely; the user sees the policy message, and you pay no API call cost. The violation is tracked using the temp: prefix for the current invocation and the user: prefix for persistent tracking across sessions.\nOutput sanitization After the LLM responds, you may need to clean its output before the user sees it. The after_model_callback lets you inspect and modify the LLM\u0026rsquo;s response.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 import re from copy import deepcopy from google.adk.agents.callback_context import CallbackContext from google.adk.models import LlmResponse from typing import Optional EMAIL_PATTERN = re.compile(r\u0026#39;\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b\u0026#39;) PHONE_PATTERN = re.compile(r\u0026#39;\\b(\\+?1[-.\\s]?)?\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}\\b\u0026#39;) def sanitize_llm_output( callback_context: CallbackContext, llm_response: LlmResponse ) -\u0026gt; Optional[LlmResponse]: \u0026#34;\u0026#34;\u0026#34;Redact PII from model responses.\u0026#34;\u0026#34;\u0026#34; if not llm_response.content or not llm_response.content.parts: return None modified = False new_parts = [] for part in llm_response.content.parts: if part.function_call: new_parts.append(part) continue if part.text: new_text = EMAIL_PATTERN.sub(\u0026#34;[EMAIL REDACTED]\u0026#34;, part.text) new_text = PHONE_PATTERN.sub(\u0026#34;[PHONE REDACTED]\u0026#34;, new_text) if new_text != part.text: modified = True new_parts.append(deepcopy(part)) new_parts[-1].text = new_text else: new_parts.append(part) if modified: new_response = deepcopy(llm_response) new_response.content.parts = new_parts return new_response return None There is an important detail here. The callback checks for part.function_call and skips those parts. If your after_model_callback modifies function call responses, it breaks tool execution. Always check for function calls first and pass them through unchanged. Also note the use of deepcopy when building a modified LlmResponse to avoid corrupting the original if other callbacks need it.\nTool authorization The before_tool_callback lets you enforce access control before a tool executes. This is important for tools that perform destructive operations, such as deleting records or cancelling bookings.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 from google.adk.tools import BaseTool, ToolContext from typing import Dict, Any, Optional TOOL_PERMISSIONS = { \u0026#34;delete_record\u0026#34;: [\u0026#34;admin\u0026#34;], \u0026#34;cancel_booking\u0026#34;: [\u0026#34;admin\u0026#34;, \u0026#34;manager\u0026#34;], \u0026#34;search_flights\u0026#34;: [\u0026#34;admin\u0026#34;, \u0026#34;manager\u0026#34;, \u0026#34;user\u0026#34;], } def authorize_tool_usage( tool: BaseTool, args: Dict[str, Any], tool_context: ToolContext ) -\u0026gt; Optional[Dict]: \u0026#34;\u0026#34;\u0026#34;Check user role before allowing tool execution.\u0026#34;\u0026#34;\u0026#34; user_role = tool_context.state.get(\u0026#34;user:role\u0026#34;, \u0026#34;user\u0026#34;) allowed_roles = TOOL_PERMISSIONS.get(tool.name, [\u0026#34;admin\u0026#34;]) if user_role not in allowed_roles: return { \u0026#34;error\u0026#34;: f\u0026#34;Access denied. Tool \u0026#39;{tool.name}\u0026#39; requires one of {allowed_roles}. \u0026#34; f\u0026#34;Your role: \u0026#39;{user_role}\u0026#39;.\u0026#34; } return None For this to work, the user:role must be set in the session state when the session is created. Because the key uses the user: prefix, it persists across sessions for that user within the same application.\nResponse caching Response caching is a powerful pattern that uses a before_model_callback and after_model_callback pair working together. The before callback checks the cache and returns a stored response if found (skipping the LLM call). The after callback stores new responses in the cache.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 import hashlib from copy import deepcopy from google.adk.agents.callback_context import CallbackContext from google.adk.models import LlmResponse, LlmRequest from typing import Optional def _cache_key(llm_request: LlmRequest) -\u0026gt; str: last_msg = \u0026#34;\u0026#34; if llm_request.contents and llm_request.contents[-1].parts: last_msg = llm_request.contents[-1].parts[0].text or \u0026#34;\u0026#34; return f\u0026#34;temp:cache:{hashlib.md5(last_msg.encode()).hexdigest()}\u0026#34; def check_cache_before_model( callback_context: CallbackContext, llm_request: LlmRequest ) -\u0026gt; Optional[LlmResponse]: \u0026#34;\u0026#34;\u0026#34;Return cached response if available.\u0026#34;\u0026#34;\u0026#34; key = _cache_key(llm_request) cached = callback_context.state.get(key) if cached: return LlmResponse( content=types.Content(role=\u0026#34;model\u0026#34;, parts=[types.Part(text=cached)]) ) return None def store_cache_after_model( callback_context: CallbackContext, llm_response: LlmResponse ) -\u0026gt; Optional[LlmResponse]: \u0026#34;\u0026#34;\u0026#34;Cache the response for future use.\u0026#34;\u0026#34;\u0026#34; if llm_response.content and llm_response.content.parts: text = llm_response.content.parts[0].text if text: key = f\u0026#34;temp:cache:{hashlib.md5(text.encode()).hexdigest()}\u0026#34; callback_context.state[key] = text return None The cache key uses the temp: prefix because cached responses are only relevant within the current invocation. If you want responses to persist across invocations, use a session-scoped key (no prefix) instead.\nDynamic instruction injection One of the most practical callback patterns is dynamically modifying the agent\u0026rsquo;s system instructions based on state. The before_model_callback can modify the llm_request in-place to inject context that the LLM would not otherwise have.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 from google.adk.agents.callback_context import CallbackContext from google.adk.models import LlmResponse, LlmRequest from typing import Optional from google.genai import types def inject_user_context( callback_context: CallbackContext, llm_request: LlmRequest ) -\u0026gt; Optional[LlmResponse]: \u0026#34;\u0026#34;\u0026#34;Dynamically inject user context into the system instruction.\u0026#34;\u0026#34;\u0026#34; user_lang = callback_context.state.get(\u0026#34;user:language\u0026#34;, \u0026#34;English\u0026#34;) user_tier = callback_context.state.get(\u0026#34;user:tier\u0026#34;, \u0026#34;free\u0026#34;) conversation_count = callback_context.state.get(\u0026#34;user:conversation_count\u0026#34;, 0) additions = [] if user_lang != \u0026#34;English\u0026#34;: additions.append(f\u0026#34;Respond in {user_lang}.\u0026#34;) if user_tier == \u0026#34;premium\u0026#34;: additions.append(\u0026#34;This is a premium user. Provide detailed, comprehensive answers.\u0026#34;) else: additions.append(\u0026#34;This is a free-tier user. Keep responses concise.\u0026#34;) if conversation_count == 0: additions.append(\u0026#34;This is the user\u0026#39;s first conversation. Be welcoming.\u0026#34;) context_text = \u0026#34; \u0026#34;.join(additions) current_instruction = llm_request.config.system_instruction if current_instruction and current_instruction.parts: original_text = current_instruction.parts[0].text or \u0026#34;\u0026#34; current_instruction.parts[0].text = f\u0026#34;{original_text}\\n\\n{context_text}\u0026#34; else: llm_request.config.system_instruction = types.Content( role=\u0026#34;system\u0026#34;, parts=[types.Part(text=context_text)] ) callback_context.state[\u0026#34;user:conversation_count\u0026#34;] = conversation_count + 1 return None This callback modifies the llm_request in-place and returns None, so the LLM call proceeds normally but with enriched instructions. Note that ADK also provides simpler alternatives for instruction injection using {key} template syntax directly in agent instructions, as described in the sessions, state, and memory article. Use the callback approach when you need conditional logic that goes beyond simple value substitution.\nArtifact handling in callbacks Callbacks can save and load artifacts, which are files or data blobs stored alongside the session. This is useful for persisting generated content, such as reports or processed results.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 from google.adk.agents.callback_context import CallbackContext from google.adk.models import LlmResponse from google.genai import types from typing import Optional import json def save_report_artifact( callback_context: CallbackContext, llm_response: LlmResponse ) -\u0026gt; Optional[LlmResponse]: \u0026#34;\u0026#34;\u0026#34;Save long model responses as downloadable artifacts.\u0026#34;\u0026#34;\u0026#34; if not llm_response.content or not llm_response.content.parts: return None text = llm_response.content.parts[0].text or \u0026#34;\u0026#34; if len(text) \u0026gt; 500: callback_context.save_artifact( \u0026#34;generated_report.md\u0026#34;, types.Part(text=text) ) metadata = { \u0026#34;artifact\u0026#34;: \u0026#34;generated_report.md\u0026#34;, \u0026#34;length\u0026#34;: len(text), \u0026#34;agent\u0026#34;: callback_context.agent_name } callback_context.save_artifact( \u0026#34;report_metadata.json\u0026#34;, types.Part(text=json.dumps(metadata)) ) return None Putting it all together Here is a complete example that combines multiple callback patterns into a production-ready agent. This agent uses guardrails, caching, tool authorization, and logging.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 import asyncio import logging import hashlib from google.adk.agents import LlmAgent from google.adk.agents.callback_context import CallbackContext from google.adk.models import LlmResponse, LlmRequest from google.adk.tools import BaseTool, ToolContext from google.adk.sessions import InMemorySessionService from google.adk.runners import Runner from google.genai import types from typing import Optional, Dict, Any logger = logging.getLogger(__name__) BLOCKED_KEYWORDS = [\u0026#34;hack\u0026#34;, \u0026#34;exploit\u0026#34;] # --- Callbacks --- def guardrail(callback_context: CallbackContext, llm_request: LlmRequest) -\u0026gt; Optional[LlmResponse]: all_text = \u0026#34; \u0026#34;.join( part.text.lower() for content in llm_request.contents for part in content.parts if part.text ) for kw in BLOCKED_KEYWORDS: if kw in all_text: callback_context.state[\u0026#34;user:violations\u0026#34;] = callback_context.state.get(\u0026#34;user:violations\u0026#34;, 0) + 1 return LlmResponse( content=types.Content( role=\u0026#34;model\u0026#34;, parts=[types.Part(text=\u0026#34;Request blocked by policy.\u0026#34;)] ) ) return None def log_model_call(callback_context: CallbackContext, llm_request: LlmRequest) -\u0026gt; Optional[LlmResponse]: logger.info(f\u0026#34;[{callback_context.agent_name}] LLM call with {len(llm_request.contents)} messages\u0026#34;) return None def authorize_tool(tool: BaseTool, args: Dict[str, Any], tool_context: ToolContext) -\u0026gt; Optional[Dict]: role = tool_context.state.get(\u0026#34;user:role\u0026#34;, \u0026#34;user\u0026#34;) if tool.name == \u0026#34;delete_booking\u0026#34; and role != \u0026#34;admin\u0026#34;: return {\u0026#34;error\u0026#34;: \u0026#34;Admin access required for deletion\u0026#34;} return None def log_tool_call(tool: BaseTool, args: Dict[str, Any], tool_context: ToolContext) -\u0026gt; Optional[Dict]: logger.info(f\u0026#34;[Tool: {tool.name}] args={args}\u0026#34;) return None # --- Tools --- def search_bookings(query: str, tool_context: ToolContext) -\u0026gt; dict: \u0026#34;\u0026#34;\u0026#34;Search for existing bookings.\u0026#34;\u0026#34;\u0026#34; tool_context.state[\u0026#34;last_search\u0026#34;] = query return {\u0026#34;bookings\u0026#34;: [{\u0026#34;id\u0026#34;: \u0026#34;BK001\u0026#34;, \u0026#34;flight\u0026#34;: \u0026#34;AA101\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;confirmed\u0026#34;}]} def delete_booking(booking_id: str, tool_context: ToolContext) -\u0026gt; dict: \u0026#34;\u0026#34;\u0026#34;Delete a booking by ID. Admin only.\u0026#34;\u0026#34;\u0026#34; return {\u0026#34;status\u0026#34;: \u0026#34;deleted\u0026#34;, \u0026#34;booking_id\u0026#34;: booking_id} # --- Agent --- support_agent = LlmAgent( name=\u0026#34;SupportAgent\u0026#34;, model=\u0026#34;gemini-2.0-flash\u0026#34;, instruction=\u0026#34;You are a booking support agent. Help users search and manage bookings.\u0026#34;, tools=[search_bookings, delete_booking], before_model_callback=[guardrail, log_model_call], before_tool_callback=[authorize_tool, log_tool_call], output_key=\u0026#34;last_response\u0026#34; ) # --- Run --- async def main(): session_service = InMemorySessionService() runner = Runner( agent=support_agent, app_name=\u0026#34;support_app\u0026#34;, session_service=session_service ) session = await session_service.create_session( app_name=\u0026#34;support_app\u0026#34;, user_id=\u0026#34;user1\u0026#34;, state={\u0026#34;user:name\u0026#34;: \u0026#34;Ravi\u0026#34;, \u0026#34;user:role\u0026#34;: \u0026#34;user\u0026#34;} ) # Turn 1: Normal request msg1 = types.Content(parts=[types.Part(text=\u0026#34;Find my bookings\u0026#34;)], role=\u0026#34;user\u0026#34;) async for event in runner.run_async( user_id=\u0026#34;user1\u0026#34;, session_id=session.id, new_message=msg1 ): if event.is_final_response() and event.content: print(f\u0026#34;Agent: {event.content.parts[0].text}\u0026#34;) # Turn 2: Attempt deletion without admin role msg2 = types.Content(parts=[types.Part(text=\u0026#34;Delete booking BK001\u0026#34;)], role=\u0026#34;user\u0026#34;) async for event in runner.run_async( user_id=\u0026#34;user1\u0026#34;, session_id=session.id, new_message=msg2 ): if event.is_final_response() and event.content: print(f\u0026#34;Agent: {event.content.parts[0].text}\u0026#34;) # Turn 3: Blocked request msg3 = types.Content(parts=[types.Part(text=\u0026#34;Help me hack the system\u0026#34;)], role=\u0026#34;user\u0026#34;) async for event in runner.run_async( user_id=\u0026#34;user1\u0026#34;, session_id=session.id, new_message=msg3 ): if event.is_final_response() and event.content: print(f\u0026#34;Agent: {event.content.parts[0].text}\u0026#34;) # Check state s = await session_service.get_session( app_name=\u0026#34;support_app\u0026#34;, user_id=\u0026#34;user1\u0026#34;, session_id=session.id ) print(f\u0026#34;Violations: {s.state.get(\u0026#39;user:violations\u0026#39;, 0)}\u0026#34;) asyncio.run(main()) When you run this, you can observe how the callback chain works in practice.\n1 2 3 4 5 6 \u0026gt; python.exe .\\callbacks-demo.py Agent: OK. I found one booking with ID BK001 for flight AA101. Is that correct? Agent: I am sorry, I do not have the permission to delete this booking. Agent: Request blocked by policy. Violations: 1 The first request flows through normally. The guardrail finds no blocked keywords, logging fires, and the search_bookings tool executes. The second request attempts to use the delete_booking tool, but the authorize_tool callback blocks it because the user\u0026rsquo;s role is user, not admin. The third request is blocked at the model level by the guardrail before the LLM is even called.\nWhen writing callbacks, keep a few guidelines in mind. Each callback should do one thing. Compose multiple concerns using callback lists rather than building monolithic functions. Always wrap callback logic in try/except. A crashing callback can break the entire agent invocation. When in doubt, fail open (return None) and log the error.\n1 2 3 4 5 6 7 def safe_callback(ctx: CallbackContext) -\u0026gt; Optional[types.Content]: try: # ... your logic ... return None except Exception as e: logger.error(f\u0026#34;Callback error: {e}\u0026#34;) return None Callbacks execute synchronously in the agent\u0026rsquo;s processing loop. Avoid blocking I/O or heavy computation in sync callbacks. Use async callbacks when making network calls. Use descriptive function names that convey purpose — authorize_tool_usage is better than before_tool_cb.\nFor state management within callbacks, use specific keys with appropriate prefixes (app:, user:, temp:). Changes are visible immediately within the current invocation and persisted at event processing time. The full mechanics of state scoping and persistence are described in the sessions, state, and memory article.\nThere are also a few common pitfalls to be aware of. If your observation-only callback accidentally returns a value, it will override the step\u0026rsquo;s behavior. When building a modified LlmResponse in after_model_callback, use deepcopy to avoid corrupting the original. If your after_model_callback modifies text responses, always check for part.function_call first and skip those — modifying function call responses breaks tool execution. And be careful with state key collisions when multiple callbacks write to the same key without coordination; use namespaced keys.\nUnderstanding how callbacks integrate with the agent execution pipeline is essential for building production-ready agents. This article explored the six callback types, their signatures and return behavior, and demonstrated practical patterns from simple logging through input guardrails, output sanitization, tool authorization, response caching, and dynamic instruction injection. These patterns, combined with the session state management covered in the sessions, state, and memory article, provide the building blocks for creating robust, observable, and customizable agent workflows. I recommend experimenting with these patterns in your own agents.\n","date":"2025-12-12T00:00:00Z","excerpt":"Callbacks are user-defined functions that hook into an agent's execution pipeline at predefined checkpoints. They let you observe, customize, and control agent behavior without modifying the ADK framework code.","permalink":"https://ravichaganti.com/blog/google-adk-callbacks/","series":"Google ADK","title":"Google ADK - Callbacks"},{"categories":["Google","Agent Development Kit","Agents"],"content":"Imagine you built a travel-booking agent. A user says, \u0026ldquo;Find flights to Paris, and the agent returns options. Then the user says, \u0026ldquo;Book the second one.\u0026rdquo; Without any memory of the previous turn, the agent has no idea what \u0026ldquo;the second one\u0026rdquo; refers to. The conversation is dead. This is the fundamental problem with LLMs. LLMs are stateless. Every API call to an LLM is independent. The model does not inherently remember what was said before. Yet meaningful conversations are inherently multi-turn, contextual, and stateful. This is where agent memory comes into play. Agent memory is the system built around the LLM to allow it to retain information, learn from past interactions, and maintain continuity within and across conversations.\nAgent memory At a high-level, there are three types of memory.\nShort-term or working memory is what\u0026rsquo;s currently visible within the model\u0026rsquo;s context window. This is needed for continuity in the current conversation. It includes the current prompt, the immediate conversation history, and a scratch pad where the agent reasons through a problem. This memory is volatile. The information in this memory will be lost once the conversation ends or the context limit is reached. There are methods to compact the context and continue the conversation. Long-term memory is usually stored externally, mostly in a vector database, and retrieved when needed using techniques such as RAG. This is used to store events \u0026amp; experiences (episodic memory) and facts \u0026amp; knowledge (semantic memory). This allows an agent to recall user preferences, past decisions, and specific interaction history. Procedural memory is the implicit knowledge of how to perform tasks. This is usually represented by the available tools and instructions for using them. When developing agents with Google ADK, sessions and memory enable the agent to retain information, learn from past interactions, and maintain continuity within and across conversations. Let us dive into these concepts.\nSessions A session in Google ADK is a single, ongoing interaction between the user and your agent. Think of it as one chat thread. It contains the chronological history of everything that happened (as Events) and a scratchpad of data relevant to this conversation (State). A Session object is the container for one conversation thread. Some of the key properties of the Session object are:\nProperty Type Purpose id str Unique identifier for this specific conversation app_name str Which agent application does this belong to user_id str Which user owns this conversation state dict Key-value scratchpad events list[Event] Chronological history of all interactions last_update_time float Timestamp of the most recent event To create and manage sessions, we use the SessionService.\n1 2 3 4 5 6 7 8 9 10 from google.adk.sessions import InMemorySessionService session_service = InMemorySessionService() # Create a new session session = await session_service.create_session( app_name=\u0026#34;travel_agent\u0026#34;, user_id=\u0026#34;user_123\u0026#34;, state={\u0026#34;user:name\u0026#34;: \u0026#34;Ravi\u0026#34;} ) You can list all sessions for a user using the list_sessions method.\n1 2 3 4 5 6 7 sessions = await session_service.list_sessions( app_name=\u0026#34;my_app\u0026#34;, user_id=\u0026#34;user_42\u0026#34; ) for s in sessions.sessions: print(f\u0026#34;Session {s.id} — last updated: {s.last_update_time}\u0026#34;) To get an existing session, we use the get_session method.\n1 2 3 4 5 6 7 8 session = await session_service.get_session( app_name=\u0026#34;my_app\u0026#34;, user_id=\u0026#34;user_42\u0026#34;, session_id=\u0026#34;custom_id_optional\u0026#34; ) print(session.state) print(len(session.events)) We can use the delete_session method to delete a session.\n1 2 3 4 5 await session_service.delete_session( app_name=\u0026#34;my_app\u0026#34;, user_id=\u0026#34;user_42\u0026#34;, session_id=\u0026#34;custom_id_optional\u0026#34; ) InMemorySessionService stores all session data directly in the application\u0026rsquo;s memory; therefore, all conversation history will be lost if the application restarts. The DatabaseSessionService and VertextAiSessionService provide session persistence. The InMemorySessionService service is a good fit for quick development and local testing.\nAs discussed, a Session object is a container for a conversation thread. This object contains state and events.\nState The state is the agent\u0026rsquo;s working memory for the current conversation, and it is a dictionary holding key-value pairs. This is the agent\u0026rsquo;s scratchpad, which is updated during a conversation. This scratchpad can be used to track and recall information such as user preferences and task execution progress, and to store any information that will be useful during an agent\u0026rsquo;s execution. For the state, the keys must be strings. The values associated with these must be serializable.\nYou can organize session state into multiple scopes. This is determined using the prefix on the state keys. Scope determines who can see the state and how long it lives.\nSession Scope When the state key has no prefix, it becomes a part of the current session scope. This is useful for tracking progress on the current task or for temporary flags.\nsession.state['current_step'] = 'payment' sets the current_step key to payment in the current session.\nUser Scope When the state keys has user: prefix, it becomes a user state. It gets tied to user_id and is shared across all sessions for that user within the same application (app_name). This scoped state is best for storing user preferences and task-specific details for the user.\nsession.state['user:preferred_payment_mode'] = 'card' sets and tracks user\u0026rsquo;s payment mode preference.\nApp Scope You can use the app scope when you need state that is shared across all users and sessions for an application. This is useful for global settings that apply to all users and all sessions.\nsession.state['app:payment_api'] = 'http://payments.yourbank.com' sets app-wide the payments API endpoint.\nTemporary Scope Finally, if you want to track the state within the current invocation only, use the temp prefix. For example, state variables such as tool call results, which are relevant only to the current invocation, can be set as a temp scope.\nsession.state['temp:raw_api_response'] = {...} sets raw_api_response as a state variable in the current invocation.\nExcept for the temporary scoped state, other scoped states can be persisted when using the database or Vertex AI services as a session service.\nYour agent code sees a single, flat session.state dictionary. The SessionService handles the magic of merging state from different scopes behind the scenes. When you read session.state['user:name'], the service fetches it from the user-level store. When you read session.state['cart_total'], it comes from the session-level store. It\u0026rsquo;s all transparent.\nEvents Everything that happens in a session is recorded as an event. Events are the fundamental units of information flow in Google ADK agents. They carry user messages, agent responses, tool calls, tool results, state changes, and control signals. An event loop is at the heart of the ADK runtime and facilitates the communication between the Runner component and the agent execution. When a user prompt arrives, the Runner hands it over to the agent for processing. The agent runs until it has something to yield, at which point it emits an event. The Runner receives the event, processes any associated actions, calls the session service to append the event to the current state, and forwards the event. After the Runner completes event processing, the agent resumes from where it was paused and continues this loop until it has no more events to yield. The Runner component is the central orchestrator of this event loop. The agent or execution logic running within the event loop yields or emits events. An event has the following structure.\nWe will dive into Runner and its architecture in a later part of this series.\nField Purpose id Unique identifier for this specific event invocation_id Groups all events from one user-request-to-final-response cycle author 'user' or agent name (e.g., 'WeatherAgent') content The payload (text, function calls, or function responses) actions Side effects (state_delta, artifact_delta, transfer_to_agent, escalate) timestamp When this event was created partial True if this is a streaming chunk, not yet complete Besides the content, the actions field contains an important piece of information that decides the next step in the event loop. The actions field is where state changes and control flow signals live. State is never updated in-place. Every state change flows through an Event\u0026rsquo;s state_delta. When the SessionService processes append_event(session, event), it reads event.actions.state_delta and applies those changes to the persisted state. This is the only reliable path for state updates.\nIn an earlier article on building a multi-agent workflow using Google ADK, we saw how event.actions.escalate was used to signal the workflow\u0026rsquo;s termination.\nReading and writing state You can read state from a Session object, from CallbackContext, or from ToolContext:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 # From a session object (read-only recommended) session = await session_service.get_session(app_name=APP, user_id=UID, session_id=SID) language = session.state.get(\u0026#34;user:preferred_payment_mode\u0026#34;, \u0026#34;card\u0026#34;) step = session.state.get(\u0026#34;booking_step\u0026#34;, \u0026#34;start\u0026#34;) # From a callback context def my_callback(callback_context, llm_request): name = callback_context.state.get(\u0026#34;user:name\u0026#34;, \u0026#34;Guest\u0026#34;) return None # From a tool context def my_tool(query: str, tool_context: ToolContext) -\u0026gt; dict: api_url = tool_context.state.get(\u0026#34;app:api_endpoint\u0026#34;) return {\u0026#34;result\u0026#34;: \u0026#34;...\u0026#34;} There are three ways to write to the session state.\noutput_key is the simplest, for saving the agent\u0026rsquo;s final text response. EventActions.state_delta is used for manual and complex updates. CallbackContext.state / ToolContext.state is recommended for callbacks and tools. We should never modify the session.state directly on a session object retrieved by the SessionService. This bypasses event tracking, breaks persistence, and is not thread-safe.\noutput_key We have seen in earlier articles in this series that using output_key stores the agent\u0026rsquo;s response in the session state. This is the simplest method and is useful when we need to capture the agent\u0026rsquo;s complete response so that other agents and tools can refer to it.\n1 2 3 4 5 6 7 8 from google.adk.agents import LlmAgent greeting_agent = LlmAgent( name=\u0026#34;Greeter\u0026#34;, model=\u0026#34;gemini-2.0-flash\u0026#34;, instruction=\u0026#34;Generate a short, friendly greeting.\u0026#34;, output_key=\u0026#34;last_greeting\u0026#34; # Agent\u0026#39;s response → state[\u0026#39;last_greeting\u0026#39;] ) After the agent runs the session.state['last_greeting'] will contain something like \u0026quot;Hello there! How can I help you today?\u0026quot;. This method only captures text, cannot store structured data, multiple keys, or scoped keys like user:.\nEventActions.state_delta We can use the state_delta event action for fine-grained control over what gets stored and when. Using this method, we can manually construct the state changes, wrap them in an Event, and append it.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from google.adk.events import Event, EventActions import time # Define multiple state changes across scopes state_changes = { \u0026#34;task_status\u0026#34;: \u0026#34;active\u0026#34;, # session scope \u0026#34;user:login_count\u0026#34;: 5, # user scope \u0026#34;user:last_login\u0026#34;: time.time(), # user scope \u0026#34;app:maintenance_mode\u0026#34;: False, # app scope \u0026#34;temp:validation_result\u0026#34;: {\u0026#34;valid\u0026#34;: True} # invocation scope (discarded after) } event = Event( invocation_id=\u0026#34;inv_system_update\u0026#34;, author=\u0026#34;system\u0026#34;, actions=EventActions(state_delta=state_changes), timestamp=time.time() ) await session_service.append_event(session, event) This method is useful for system-level updates, multi-key updates, updates to scoped state keys, and other scenarios not tied to the agent\u0026rsquo;s direct text response.\nCallbackContext and ToolContext For modifying state inside callbacks and tool functions, using CallbackContext and ToolContext is the recommended approach.\nInside a callback function, you can use the CallbackContext.state to retrieve or update the state. We will learn about callbacks in a future article in this series.\nIn a tool function, you can use ToolContext.state to retrieve or update the state.\n1 2 3 4 5 6 7 8 9 from google.adk.tools import ToolContext def add_to_cart(item: str, tool_context: ToolContext) -\u0026gt; dict: \u0026#34;\u0026#34;\u0026#34;Adds an item to the shopping cart.\u0026#34;\u0026#34;\u0026#34; cart = tool_context.state.get(\u0026#34;cart_items\u0026#34;, []) cart.append(item) tool_context.state[\u0026#34;cart_items\u0026#34;] = cart tool_context.state[\u0026#34;cart_count\u0026#34;] = len(cart) return {\u0026#34;status\u0026#34;: \u0026#34;added\u0026#34;, \u0026#34;item\u0026#34;: item, \u0026#34;cart_size\u0026#34;: len(cart)} In both CallbackContext and ToolContext, you write clean, natural code, context.state[\u0026quot;key\u0026quot;] = value, and the framework handles creating EventActions, populating state_delta, calling append_event, and ensuring persistence. There is no boilerplate code required.\nInjecting state into agent instructions One of ADK\u0026rsquo;s most powerful features: you can embed state values directly into agent instructions using {key} template syntax.\n1 2 3 4 5 6 7 8 9 10 from google.adk.agents import LlmAgent agent = LlmAgent( name=\u0026#34;PersonalizedAgent\u0026#34;, model=\u0026#34;gemini-2.0-flash\u0026#34;, instruction=\u0026#34;\u0026#34;\u0026#34;You are a helpful assistant for {user:name}. They prefer responses in {user:language}. Their current task is: {current_task}. Their membership tier is: {user:tier}.\u0026#34;\u0026#34;\u0026#34; ) If session.state contains:\n1 2 3 4 5 6 { \u0026#34;user:name\u0026#34;: \u0026#34;Ravi\u0026#34;, \u0026#34;user:language\u0026#34;: \u0026#34;English\u0026#34;, \u0026#34;current_task\u0026#34;: \u0026#34;booking a hotel\u0026#34;, \u0026#34;user:tier\u0026#34;: \u0026#34;Gold\u0026#34; } The LLM receives:\n1 2 3 4 You are a helpful assistant for Ravi. They prefer responses in English. Their current task is: booking a hotel. Their membership tier is: Gold. For this template syntax to work, the key must exist in the state, or the agent will throw an error. We can use {key?} for keys that may be absent. Keys suffixed with a question mark resolve to an empty string if they are missing. The values should be strings or easily convertible to strings. If you need literal curly braces in the agent instructions, you must use InstructionProvider function as shown below.\n1 2 3 4 5 6 7 8 9 10 11 12 from google.adk.agents import LlmAgent from google.adk.agents.readonly_context import ReadonlyContext def my_instruction(context: ReadonlyContext) -\u0026gt; str: # This will NOT have {key} replaced — passed as-is to the LLM return \u0026#34;Format your response as: {{name: string, age: number}}\u0026#34; agent = LlmAgent( model=\u0026#34;gemini-2.0-flash\u0026#34;, name=\u0026#34;JsonHelper\u0026#34;, instruction=my_instruction # Function instead of string ) This method is useful when we need full control over instruction generation, including bypassing the {key} templating. For example, when your instruction contains literal curly braces for JSON examples, or you need completely dynamic instruction generation based on state.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 def role_based_instruction(context: ReadonlyContext) -\u0026gt; str: tier = context.state.get(\u0026#34;user:tier\u0026#34;, \u0026#34;free\u0026#34;) if tier == \u0026#34;premium\u0026#34;: return \u0026#34;You are a premium concierge. Provide detailed, proactive assistance.\u0026#34; elif tier == \u0026#34;enterprise\u0026#34;: return \u0026#34;You are an enterprise support specialist. Be thorough and reference documentation.\u0026#34; else: return \u0026#34;You are a helpful assistant. Answer questions clearly and concisely.\u0026#34; agent = LlmAgent( model=\u0026#34;gemini-2.0-flash\u0026#34;, name=\u0026#34;AdaptiveAgent\u0026#34;, instruction=role_based_instruction ) If you need to inject state via {key} templating but need to preserve the literal braces, you must use inject_session_state() method.\n1 2 3 4 5 6 7 8 9 10 11 12 13 from google.adk.utils import instructions_utils async def dynamic_instruction(context: ReadonlyContext) -\u0026gt; str: template = \u0026#34;\u0026#34;\u0026#34;You are helping {user:name}. Format your output as: {{result: string, confidence: number}}\u0026#34;\u0026#34;\u0026#34; return await instructions_utils.inject_session_state(template, context) agent = LlmAgent( model=\u0026#34;gemini-2.0-flash\u0026#34;, name=\u0026#34;HybridAgent\u0026#34;, instruction=dynamic_instruction ) The instructions_utils.inject_session_state(template, context) injects the {user:name} state variable but preserves the literal braces.\nMemory A session tracks the history and temporary data for a single, ongoing conversation. To enable history and state persistence across conversations and enable agents to recall the information from past interactions, we must use the memory service. There are two types of memory service.\nThe InMemoryMemoryService is used for quick prototyping and local testing of agents, and it is good for basic keyword search. Using this requires no setup, stores session data in application memory, and does not provide persistence. The VertexAiMemoryBankService connects to the Vertex AI memory bank service and provides LLM-based extraction of meaningful information from sessions. This service persists data in Google Cloud and is production-ready. Each of these memory services offers two operations.\nIngest: memory_service.add_session_to_memory(session) takes a completed session and adds its information to the long-term store Search: memory_service.search_memory(app_name, user_id, query) returns relevant snippets from past sessions These memory services also offer tools that agents can use in their work.\nPreloadMemoryTool automatically retrieves relevant memories at the start of every turn (like a callback that always fires) LoadMemoryTool (or load_memory) lets the agent decide when to search memory (on-demand) Here is an example of working with InMemoryMemoryService.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 from google.adk.agents import LlmAgent from google.adk.sessions import InMemorySessionService from google.adk.memory import InMemoryMemoryService from google.adk.runners import Runner from google.adk.tools import load_memory from google.genai.types import Content, Part import asyncio # --- Setup services (shared across runners) --- session_service = InMemorySessionService() memory_service = InMemoryMemoryService() # --- Step 1: Agent captures information --- capture_agent = LlmAgent( model=\u0026#34;gemini-2.0-flash\u0026#34;, name=\u0026#34;CaptureAgent\u0026#34;, instruction=\u0026#34;Acknowledge what the user tells you.\u0026#34; ) runner1 = Runner( agent=capture_agent, app_name=\u0026#34;memory_app\u0026#34;, session_service=session_service, memory_service=memory_service ) async def main(): session1 = await session_service.create_session( app_name=\u0026#34;memory_app\u0026#34;, user_id=\u0026#34;user1\u0026#34;, session_id=\u0026#34;session_capture\u0026#34; ) # Run the agent with user input user_msg = Content( parts=[ Part( text=\u0026#34;My favorite color is blue and I love hiking.\u0026#34; ) ], role=\u0026#34;user\u0026#34; ) async for event in runner1.run_async(user_id=\u0026#34;user1\u0026#34;, session_id=\u0026#34;session_capture\u0026#34;, new_message=user_msg): pass # Agent processes and responds # --- Step 2: Ingest session into memory --- completed_session = await session_service.get_session( app_name=\u0026#34;memory_app\u0026#34;, user_id=\u0026#34;user1\u0026#34;, session_id=\u0026#34;session_capture\u0026#34; ) await memory_service.add_session_to_memory(completed_session) # --- Step 3: New session, recall from memory --- recall_agent = LlmAgent( model=\u0026#34;gemini-2.0-flash\u0026#34;, name=\u0026#34;RecallAgent\u0026#34;, instruction=\u0026#34;Answer questions using past conversation context. \u0026#34; \u0026#34;Use the load_memory tool to search past conversations.\u0026#34;, tools=[load_memory] ) runner2 = Runner( agent=recall_agent, app_name=\u0026#34;memory_app\u0026#34;, session_service=session_service, memory_service=memory_service ) session2 = await session_service.create_session( app_name=\u0026#34;memory_app\u0026#34;, user_id=\u0026#34;user1\u0026#34;, session_id=\u0026#34;session_recall\u0026#34; ) recall_msg = Content(parts=[Part(text=\u0026#34;What\u0026#39;s my favorite color?\u0026#34;)], role=\u0026#34;user\u0026#34;) async for event in runner2.run_async(user_id=\u0026#34;user1\u0026#34;, session_id=\u0026#34;session_recall\u0026#34;, new_message=recall_msg): if event.is_final_response() and event.content and event.content.parts: print(event.content.parts[0].text) # Expected: Something like \u0026#34;Your favorite color is blue!\u0026#34; if __name__ == \u0026#34;__main__\u0026#34;: asyncio.run(main()) When you run this, the first runner stores the user preference in memory. This is done using add_session_to_memory method. The second runner then uses the basic keyword match provided by the memory service to generate the response. The load_memory tool provided to the recall_agent automatically retrieves the memory for search.\n1 2 3 PS\u0026gt; python.exe .\\inmemory-memory.py .... Your favorite color is blue. The process of saving memory can be automated using agent callbacks. We will learn about this in a later article in the series.\nHere is another example where we use InMemoryMemoryService and help agent track the conversation progress through state.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 import asyncio from google.adk.agents import LlmAgent from google.adk.sessions import InMemorySessionService from google.adk.runners import Runner from google.adk.tools import ToolContext from google.genai.types import Content, Part # --- Tool that modifies state --- def search_flights( origin: str, destination: str, tool_context: ToolContext ) -\u0026gt; dict: \u0026#34;\u0026#34;\u0026#34;Searches for flights between two cities.\u0026#34;\u0026#34;\u0026#34; results = [ {\u0026#34;flight\u0026#34;: \u0026#34;AA101\u0026#34;, \u0026#34;price\u0026#34;: 450, \u0026#34;time\u0026#34;: \u0026#34;8:00 AM\u0026#34;}, {\u0026#34;flight\u0026#34;: \u0026#34;UA202\u0026#34;, \u0026#34;price\u0026#34;: 380, \u0026#34;time\u0026#34;: \u0026#34;11:30 AM\u0026#34;}, {\u0026#34;flight\u0026#34;: \u0026#34;DL303\u0026#34;, \u0026#34;price\u0026#34;: 520, \u0026#34;time\u0026#34;: \u0026#34;3:00 PM\u0026#34;}, ] # Save search context to state for next turn tool_context.state[\u0026#34;search_results\u0026#34;] = results tool_context.state[\u0026#34;origin\u0026#34;] = origin tool_context.state[\u0026#34;destination\u0026#34;] = destination tool_context.state[\u0026#34;booking_step\u0026#34;] = \u0026#34;select_flight\u0026#34; return {\u0026#34;flights\u0026#34;: results} def book_flight( flight_number: str, tool_context: ToolContext ) -\u0026gt; dict: \u0026#34;\u0026#34;\u0026#34;Books a specific flight.\u0026#34;\u0026#34;\u0026#34; results = tool_context.state.get(\u0026#34;search_results\u0026#34;, []) selected = next((f for f in results if f[\u0026#34;flight\u0026#34;] == flight_number), None) if not selected: return {\u0026#34;error\u0026#34;: f\u0026#34;Flight {flight_number} not found in search results\u0026#34;} tool_context.state[\u0026#34;booked_flight\u0026#34;] = selected tool_context.state[\u0026#34;booking_step\u0026#34;] = \u0026#34;confirmed\u0026#34; tool_context.state[\u0026#34;user:total_bookings\u0026#34;] = ( tool_context.state.get(\u0026#34;user:total_bookings\u0026#34;, 0) + 1 ) return {\u0026#34;status\u0026#34;: \u0026#34;confirmed\u0026#34;, \u0026#34;flight\u0026#34;: selected} # --- Agent --- booking_agent = LlmAgent( name=\u0026#34;BookingAgent\u0026#34;, model=\u0026#34;gemini-2.0-flash\u0026#34;, instruction=\u0026#34;\u0026#34;\u0026#34;You are a flight booking assistant. Current booking step: {booking_step?} Help users search for and book flights. If they\u0026#39;ve already searched, reference the results in state.\u0026#34;\u0026#34;\u0026#34;, tools=[search_flights, book_flight], output_key=\u0026#34;last_response\u0026#34; ) # --- Runner and Session --- async def main(): session_service = InMemorySessionService() runner = Runner( agent=booking_agent, app_name=\u0026#34;booking_app\u0026#34;, session_service=session_service ) session = await session_service.create_session( app_name=\u0026#34;booking_app\u0026#34;, user_id=\u0026#34;user1\u0026#34;, state={\u0026#34;booking_step\u0026#34;: \u0026#34;start\u0026#34;, \u0026#34;user:name\u0026#34;: \u0026#34;Ravi\u0026#34;} ) # Turn 1: Search msg1 = Content(parts=[Part(text=\u0026#34;Find flights from NYC to Paris\u0026#34;)], role=\u0026#34;user\u0026#34;) async for event in runner.run_async( user_id=\u0026#34;user1\u0026#34;, session_id=session.id, new_message=msg1 ): if event.is_final_response() and event.content: print(f\u0026#34;Agent: {event.content.parts[0].text}\u0026#34;) # Check state after Turn 1 s = await session_service.get_session( app_name=\u0026#34;booking_app\u0026#34;, user_id=\u0026#34;user1\u0026#34;, session_id=session.id ) print(f\u0026#34;State: booking_step={s.state.get(\u0026#39;booking_step\u0026#39;)}\u0026#34;) print(f\u0026#34;State: origin={s.state.get(\u0026#39;origin\u0026#39;)}\u0026#34;) # Turn 2: Book msg2 = Content(parts=[Part(text=\u0026#34;Book UA202 please\u0026#34;)], role=\u0026#34;user\u0026#34;) async for event in runner.run_async( user_id=\u0026#34;user1\u0026#34;, session_id=session.id, new_message=msg2 ): if event.is_final_response() and event.content: print(f\u0026#34;Agent: {event.content.parts[0].text}\u0026#34;) # Check final state s = await session_service.get_session( app_name=\u0026#34;booking_app\u0026#34;, user_id=\u0026#34;user1\u0026#34;, session_id=session.id ) print(f\u0026#34;State: booking_step={s.state.get(\u0026#39;booking_step\u0026#39;)}\u0026#34;) print(f\u0026#34;State: user:total_bookings={s.state.get(\u0026#39;user:total_bookings\u0026#39;)}\u0026#34;) asyncio.run(main()) When you run this, you can observe how the agent uses the state updates to track progress.\n1 2 3 4 5 6 7 8 9 10 \u0026gt; python.exe .\\inmemory-track-state.py Agent: OK. I found three flights for you: AA101 at 8:00 AM for $450, UA202 at 11:30 AM for $380, and DL303 at 3:00 PM for $520. State: booking_step=select_flight State: origin=NYC Agent: OK. I have booked flight UA202 for you. It departs at 11:30 AM and the price is $380. State: booking_step=confirmed State: user:total_bookings=1 Understanding how ADK uses Sessions, state, and memory to enable agents to persist conversational state and recall past interactions is important for developing efficient agent workflows. This article explored this in-depth and demonstrated session and memory capabilities using InMemorySessionService and InMemoryMemoryService. The fundamentals you learned about the overall workflow stay the same for other session and memory service types that provide persistence. I recommend that you experiment with those services.\n","date":"2025-12-05T00:00:00Z","excerpt":"LLMs are stateless. Every API call to an LLM is independent. The model does not inherently remember what was said before. Yet meaningful conversations are inherently multi-turn, contextual, and stateful.","permalink":"https://ravichaganti.com/blog/google-adk-sessions-state-and-memory/","series":"Google ADK","title":"Google ADK - Sessions, state, and memory"},{"categories":["Google","Agent Development Kit","Agents"],"content":"In the introduction to Google ADK, we looked at the basics of using ADK to create an agent, connect tools, and also learnt how to define agents using the declarative YAML configuration. The example we implemented used the Agent class. This class is an alias of the LlmAgent class. LlmAgent behavior is non-deterministic as it uses an LLM for deciding what tools to use or how to proceed toward a goal. ADK also offers workflow agents that are more deterministic, control the execution flow of their sub-agents, and operate according to predefined logic. The execution flow of workflow agents depends on their type.\nIn this article today, we will review different agent types.\nLlmAgent We have seen a detailed example of using an Agent or LlmAgent in the earlier part of this series. Let us revisit LlmAgent and understand how this agent type deals with sub-agents. To demonstrate this, we will build a code tutor agent with two sub-agents: a code generator and a code reviewer.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 from google.adk.agents.llm_agent import LlmAgent code_generator_agent = LlmAgent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;code_generator\u0026#39;, description=\u0026#39;Agent that generates code based on user requirements.\u0026#39;, instruction=\u0026#39;\u0026#39;\u0026#39; You are a code generator only and NOT a reviewer. Generate code based on the user\u0026#39;s requirements. Ensure that the code is well-commented and follows best practices. Wrap the code in markdown format for better readability. \u0026#39;\u0026#39;\u0026#39;, ) code_reviewer_agent = LlmAgent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;code_reviewer\u0026#39;, description=\u0026#39;Agent that reviews code and provides feedback.\u0026#39;, instruction=\u0026#39;\u0026#39;\u0026#39; You are a code reviewer. Review the provided code and provide constructive feedback. Suggest improvements and best practices to enhance the code quality. \u0026#39;\u0026#39;\u0026#39;, ) root_agent = LlmAgent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;code_tutor\u0026#39;, description=\u0026#39;Code tutor agent that can help generate or review code.\u0026#39;, instruction=\u0026#39;\u0026#39;\u0026#39; You are an expert code tutor that can help a learner generate or review code in any programming language. For generating code, use only the code_generator_agent. For reviewing code, use only the code_reviewer_agent. Always decide which sub-agent to use based on the user\u0026#39;s request. \u0026#39;\u0026#39;\u0026#39;, sub_agents = [code_generator_agent, code_reviewer_agent], ) In the preceding example, we have three agents. The code_generator_agent can generate code in any language, the code_reviewer_agent reviews code written in any language, and the code_tutor agent chooses and orchestrates the right agent based on the user prompt.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 (.venv) PS C:\\GitHub\\google-adk-101\u0026gt; adk run .\\code_tutor\\ Running agent code_tutor, type exit to exit. [user]: Write a Hello, World program in Python and review it for any enhancements. [code_generator]: ```python # hello_world.py def greet(): \u0026#34;\u0026#34;\u0026#34; This function prints the classic \u0026#34;Hello, World!\u0026#34; message to the console. \u0026#34;\u0026#34;\u0026#34; print(\u0026#34;Hello, World!\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: # Call the greet function when the script is executed directly greet() ``` Now, I will transfer this to the `code_reviewer` agent for any potential enhancements. [code_reviewer]: This is a well-structured and excellent \u0026#34;Hello, World!\u0026#34; program in Python! Here\u0026#39;s a review with some feedback, mostly reinforcing the good practices already in place: **Strengths and Best Practices Observed:** 1. **Function Encapsulation (`greet()`):** You\u0026#39;ve correctly encapsulated the \u0026#34;Hello, World!\u0026#34; logic within a function. This promotes modularity, reusability, and makes the code easier to read and test. 2. **Docstrings:** The `greet()` function includes a clear and concise docstring. This is a best practice for documenting what functions do, which is invaluable for code maintainability and understanding, especially in larger projects. 3. **`if __name__ == \u0026#34;__main__\u0026#34;:` Block:** This is a crucial Python best practice. It ensures that the `greet()` function is only called when the script is executed directly (e.g., `python hello_world.py`). If this file were imported as a module into another script, the `greet()` function would not be automatically executed, preventing unintended side effects. 4. **Readability:** The code is very clean, straightforward, and easy to understand. **Potential Enhancements (Minor/General Best Practices):** For this specific \u0026#34;Hello, World!\u0026#34; program, your code is already highly optimized and follows best practices. There isn\u0026#39;t much to \u0026#34;enhance\u0026#34; without making it overly complex. However, for future reference in more complex applications, you might consider: 1. **Type Hinting (for more complex functions):** While not applicable here as `greet()` takes no arguments and implicitly returns `None`, for functions that accept parameters or return values, adding type hints can improve code readability, help catch errors during development with static analysis tools (like MyPy), and serve as self-documentation. *Example of type hinting (not for your current `greet` function, but for illustration):* ```python def greet_person(name: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34; Greets a person by name. \u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; # print(greet_person(\u0026#34;Alice\u0026#34;)) ``` **Conclusion:** Your \u0026#34;Hello, World!\u0026#34; program is very well-written, adheres to Pythonic best practices, and is a perfect example for its purpose. Great job! [user]: Given the prompt, the code_tutor agent first transfers control to the code_generator_agent, then to the code_reviewer_agent, for review of the generated code for any improvements. The code_tutor agent is still non-deterministic because it uses an LLM to choose which agent to transfer to.\nWorkflow agents Another category of agents that ADK offers is the workflow agents. Workflow agents offer a deterministic execution flow. In the earlier example, an LlmAgent orchestrated the workflow based on the prompt. Workflow agents use a predefined logic to orchestrate the sub-agents. With this type of agent, an LLM is not needed to determine the execution order; rather, the agent\u0026rsquo;s type determines the execution flow.\nThere are three types of workflow agents.\nSequential Agent As the name suggests, this type of agent executes sub-agents in a sequence, in the order they are specified. Let us look at an example.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 from google.adk.agents import LlmAgent, SequentialAgent code_generator_agent = LlmAgent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;code_generator\u0026#39;, description=\u0026#39;Agent that generates code based on user requirements.\u0026#39;, instruction=\u0026#39;\u0026#39;\u0026#39; You are a code generator only and NOT a reviewer. Generate code based on the user\u0026#39;s requirements. Ensure that the code is well-commented and follows best practices. Wrap the code in markdown format for better readability. \u0026#39;\u0026#39;\u0026#39;, ) code_reviewer_agent = LlmAgent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;code_reviewer\u0026#39;, description=\u0026#39;Agent that reviews code and provides feedback.\u0026#39;, instruction=\u0026#39;\u0026#39;\u0026#39; You are a code reviewer. Review the provided code and provide constructive feedback. Suggest improvements and best practices to enhance the code quality. \u0026#39;\u0026#39;\u0026#39;, ) root_agent = SequentialAgent( name=\u0026#39;code_tutor\u0026#39;, description=\u0026#39;Code tutor agent that can help generate or review code.\u0026#39;, sub_agents = [code_generator_agent, code_reviewer_agent], ) This sequential agent is similar to the previous example. It is a code tutor. In the previous example, a sub-agent was chosen based on the user prompt. Depending on the prompt, either or both agents get executed. With a sequential agent, both agents are executed in the order specified in the sub_agents list.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 (.venv) PS C:\\GitHub\\google-adk-101\u0026gt; adk run .\\code_tutor\\ Running agent code_tutor, type exit to exit. [user]: Write a Hello, World program in Python. [code_generator]: ```python # This is a simple Python program that prints \u0026#34;Hello, World!\u0026#34; to the console. def hello_world(): \u0026#34;\u0026#34;\u0026#34; Prints the classic \u0026#34;Hello, World!\u0026#34; message. \u0026#34;\u0026#34;\u0026#34; print(\u0026#34;Hello, World!\u0026#34;) # The entry point of the program. if __name__ == \u0026#34;__main__\u0026#34;: # Call the function to display the message. hello_world() ``` [code_reviewer]: This is a well-written \u0026#34;Hello, World!\u0026#34; program that goes beyond the absolute minimum to demonstrate good programming practices. Here\u0026#39;s a review: **Positive Aspects:** 1. **Readability:** The code is extremely clear, easy to understand, and well-commented. 2. **Function Definition (`hello_world()`):** Encapsulating the print statement within a function is excellent practice. Even for a single line, it promotes modularity and makes the code reusable and testable. 3. **Docstring:** The function includes a clear docstring, which is crucial for explaining the purpose of functions, especially in larger projects. 4. **`if __name__ == \u0026#34;__main__\u0026#34;:` block:** This is a fundamental Python best practice. It ensures that the `hello_world()` function is only called when the script is executed directly, not when it\u0026#39;s imported as a module into another script. This makes your code reusable. 5. **Comments:** The comments explain the purpose of the program and the entry point clearly, which is helpful for beginners or anyone new to the codebase. **Suggestions for Improvement (Minor and Contextual):** For a program of this simplicity, there\u0026#39;s very little to \u0026#34;improve\u0026#34; as it already follows best practices. However, here are some points to consider for more complex scenarios: 1. **Type Hinting (Advanced but good to know):** For functions that take arguments or return values, adding type hints improves code clarity and allows for static analysis tools (like MyPy) to catch potential errors. * *Example (if `hello_world` were to return a string):* ```python def hello_world() -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34; Returns the classic \u0026#34;Hello, World!\u0026#34; message. \u0026#34;\u0026#34;\u0026#34; return \u0026#34;Hello, World!\u0026#34; if __name__ == \u0026#34;__main__\u0026#34;: message = hello_world() print(message) ``` * For the current `hello_world()` that just prints and implicitly returns `None`, type hints aren\u0026#39;t strictly necessary but could be added as `def hello_world() -\u0026gt; None:`. However, for such a simple function, this might be considered overkill by some. 2. **Minimalism vs. Best Practices:** While the current code demonstrates excellent practices, the *absolute most minimal* \u0026#34;Hello, World!\u0026#34; would just be `print(\u0026#34;Hello, World!\u0026#34;)`. The provided code wisely prioritizes good structure over extreme minimalism, which is commendable. **Conclusion:** This is an excellent example of a \u0026#34;Hello, World!\u0026#34; program. It\u0026#39;s not just functional, but also showcases fundamental Python best practices like modularity, documentation, and proper script execution. Well done! [user]: Sequential agents, any workflow agent for that matter, do not require an LLM, as they simply orchestrate the task using sub-agents in a predefined order.\nThe second type of workflow agent is a parallel agent.\nParallel Agent A parallel agent executes its sub-agents concurrently. Let us look at an example to understand this.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 from google.adk.agents import LlmAgent, SequentialAgent, ParallelAgent code_generator_agent_g25 = LlmAgent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;code_generator_g25\u0026#39;, description=\u0026#39;Agent that generates code based on user requirements.\u0026#39;, instruction=\u0026#39;\u0026#39;\u0026#39; You are a code generator only and NOT a reviewer. Generate code based on the user\u0026#39;s requirements. Ensure the code is well commented and follows best practices. Wrap the code in markdown format for better readability. \u0026#39;\u0026#39;\u0026#39;, output_key=\u0026#39;g25_code\u0026#39; ) code_generator_agent_g30 = LlmAgent( model=\u0026#39;gemini-3-flash-preview\u0026#39;, name=\u0026#39;code_generator_g30\u0026#39;, description=\u0026#39;Agent that generates code based on user requirements.\u0026#39;, instruction=\u0026#39;\u0026#39;\u0026#39; You are a code generator only and NOT a reviewer. Generate code based on the user\u0026#39;s requirements. Ensure that the code is well-commented and follows best practices. Wrap the code in markdown format for better readability. \u0026#39;\u0026#39;\u0026#39;, output_key=\u0026#39;g30_code\u0026#39; ) parallel_code_generator = ParallelAgent( name=\u0026#39;parallel_code_generator\u0026#39;, description=\u0026#39;Parallel agent to generate code using different models.\u0026#39;, sub_agents=[code_generator_agent_g25, code_generator_agent_g30], ) code_reviewer_agent = LlmAgent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;code_reviewer\u0026#39;, description=\u0026#39;Agent that reviews code and provides feedback.\u0026#39;, instruction=\u0026#39;\u0026#39;\u0026#39; You are a code reviewer. Review and compare the code by the two code generator agents and select the best one. code_generator_agent_g25 output {g25_code}. code_generator_agent_g30 output {g30_code}. Explain the reasons for your selection. \u0026#39;\u0026#39;\u0026#39;, ) root_agent = SequentialAgent( name=\u0026#39;code_tutor\u0026#39;, description=\u0026#39;Code tutor agent that can help generate or review code.\u0026#39;, sub_agents = [parallel_code_generator, code_reviewer_agent], ) This example uses the parallel agent construct to generate code for the same prompt with two different models. The code_generator_agent_g25 uses Gemini 2.5 Flash, and Gemini 3.0 Flash is used by the code_generator_agent_g30 agent.\nThis example implements output keys to save the agent\u0026rsquo;s response directly into the session\u0026rsquo;s state. We will learn more about this later. For now, understand that you can specify the output_key within the LlmAgent definition to specify the key used to retrieve an agent\u0026rsquo;s response within a session. This example also implements a sequential agent that combines the parallel agent with the code_reviewer_agent to select the best code. The code_reviewer_agent receives the output from the parallel agent and takes an action based on its instructions. The output keys defined in the code generator agents are specified within the reviewer agent\u0026rsquo;s instructions.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 PS C:\\GitHub\\google-adk-101\u0026gt; adk run .\\code_tutor\\ Running agent code_tutor, type exit to exit. [user]: Write a program to determine LCM of two numbers. [code_generator_g25]: To determine the Least Common Multiple (LCM) of two numbers, we can use the relationship between LCM and the Greatest Common Divisor (GCD) of the two numbers. The formula is: `LCM(a, b) = |a * b| / GCD(a, b)` First, we need a function to calculate the GCD, which can be efficiently done using the Euclidean algorithm. ```python import math def gcd(a: int, b: int) -\u0026gt; int: \u0026#34;\u0026#34;\u0026#34; Calculates the Greatest Common Divisor (GCD) of two non-negative integers using the Euclidean algorithm. Args: a (int): The first integer. b (int): The second integer. Returns: int: The Greatest Common Divisor of a and b. \u0026#34;\u0026#34;\u0026#34; # The math.gcd function in Python\u0026#39;s standard library # already implements the Euclidean algorithm efficiently. return math.gcd(a, b) def lcm(a: int, b: int) -\u0026gt; int: \u0026#34;\u0026#34;\u0026#34; Calculates the Least Common Multiple (LCM) of two integers. The formula used is: LCM(a, b) = |a * b| / GCD(a, b). Handles edge cases where one or both numbers might be zero. The LCM of any number and zero is considered zero. Args: a (int): The first integer. b (int): The second integer. Returns: int: The Least Common Multiple of a and b. \u0026#34;\u0026#34;\u0026#34; if a == 0 or b == 0: return 0 # LCM of any number with 0 is 0 # Calculate the absolute product to handle negative inputs correctly, # though LCM is typically defined for positive integers. # If a and b are non-zero, their GCD will be non-zero. # We use abs() to ensure the result is positive, as LCM is # conventionally a positive value. return abs(a * b) // gcd(a, b) # --- Main part of the program --- if __name__ == \u0026#34;__main__\u0026#34;: print(\u0026#34;--- LCM Calculator ---\u0026#34;) # Example 1: Positive integers num1_a = 4 num1_b = 6 result1 = lcm(num1_a, num1_b) print(f\u0026#34;The LCM of {num1_a} and {num1_b} is: {result1}\u0026#34;) # Expected: 12 # Example 2: Another pair of positive integers num2_a = 15 num2_b = 20 result2 = lcm(num2_a, num2_b) print(f\u0026#34;The LCM of {num2_a} and {num2_b} is: {result2}\u0026#34;) # Expected: 60 # Example 3: Prime numbers num3_a = 7 num3_b = 11 result3 = lcm(num3_a, num3_b) print(f\u0026#34;The LCM of {num3_a} and {num3_b} is: {result3}\u0026#34;) # Expected: 77 # Example 4: One number is a multiple of the other num4_a = 8 num4_b = 16 result4 = lcm(num4_a, num4_b) print(f\u0026#34;The LCM of {num4_a} and {num4_b} is: {result4}\u0026#34;) # Expected: 16 # Example 5: One of the numbers is zero num5_a = 0 num5_b = 10 result5 = lcm(num5_a, num5_b) print(f\u0026#34;The LCM of {num5_a} and {num5_b} is: {result5}\u0026#34;) # Expected: 0 # Example 6: Both numbers are zero num6_a = 0 num6_b = 0 result6 = lcm(num6_a, num6_b) print(f\u0026#34;The LCM of {num6_a} and {num6_b} is: {result6}\u0026#34;) # Expected: 0 # Example 7: User input try: user_num1 = int(input(\u0026#34;\\nEnter the first number: \u0026#34;)) user_num2 = int(input(\u0026#34;Enter the second number: \u0026#34;)) user_lcm = lcm(user_num1, user_num2) print(f\u0026#34;The LCM of {user_num1} and {user_num2} is: {user_lcm}\u0026#34;) except ValueError: print(\u0026#34;Invalid input. Please enter integers only.\u0026#34;) except Exception as e: print(f\u0026#34;An error occurred: {e}\u0026#34;) ``` [code_generator_g30]: This program calculates the Least Common Multiple (LCM) of two numbers using the relationship between the Greatest Common Divisor (GCD) and the LCM. The formula used is: `LCM(a, b) = abs(a * b) / GCD(a, b)`. ```python def calculate_gcd(a: int, b: int) -\u0026gt; int: \u0026#34;\u0026#34;\u0026#34; Calculates the Greatest Common Divisor (GCD) using the Euclidean Algorithm. Args: a (int): First integer. b (int): Second integer. Returns: int: The greatest common divisor of a and b. \u0026#34;\u0026#34;\u0026#34; while b: a, b = b, a % b return a def calculate_lcm(a: int, b: int) -\u0026gt; int: \u0026#34;\u0026#34;\u0026#34; Calculates the Least Common Multiple (LCM) of two numbers. Args: a (int): First integer. b (int): Second integer. Returns: int: The least common multiple of a and b. \u0026#34;\u0026#34;\u0026#34; # LCM of 0 and any number is 0 if a == 0 or b == 0: return 0 # Using the formula: LCM(a, b) = |a * b| / GCD(a, b) # We use integer division (//) to ensure the result is an integer return abs(a * b) // calculate_gcd(a, b) def main(): \u0026#34;\u0026#34;\u0026#34; Main function to handle user input and display the result. \u0026#34;\u0026#34;\u0026#34; print(\u0026#34;--- LCM Calculator ---\u0026#34;) try: # Prompt user for input num1 = int(input(\u0026#34;Enter the first number: \u0026#34;)) num2 = int(input(\u0026#34;Enter the second number: \u0026#34;)) # Calculate LCM result = calculate_lcm(num1, num2) # Display the result print(f\u0026#34;The LCM of {num1} and {num2} is: {result}\u0026#34;) except ValueError: print(\u0026#34;Invalid input! Please enter integer values.\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: main() ``` ### Key Features of this Implementation: 1. **Efficiency**: It uses the Euclidean Algorithm to find the GCD first, which is much faster than iterating through multiples for large numbers. 2. **Robustness**: It handles cases involving zero (LCM with zero is zero). 3. **Best Practices**: * Uses **type hinting** (`a: int`) for better code clarity. * Includes a `try-except` block to handle non-integer user inputs gracefully. * Modularizes logic into specific functions (`calculate_gcd`, `calculate_lcm`, and `main`). * Uses the `if __name__ == \u0026#34;__main__\u0026#34;:` block to allow the script to be imported without executing the main logic immediately. [code_reviewer]: I will compare the two code submissions and select the better one based on clarity, efficiency, correctness, robustness, and adherence to best practices. ### Review of `code_generator_agent_g25` **Pros:** 1. **Leverages Standard Library:** It correctly identifies and utilizes `math.gcd` for calculating the Greatest Common Divisor. This is a significant advantage as `math.gcd` is highly optimized, reliable, and generally preferred over a custom implementation in production code. 2. **Clear Functionality:** The `lcm` function is concise and correctly implements the formula `LCM(a, b) = |a * b| / GCD(a, b)`, handling the zero case (`LCM(x, 0) = 0`). 3. **Comprehensive Examples:** The `if __name__ == \u0026#34;__main__\u0026#34;:` block includes a wide range of example test cases (positive integers, prime numbers, multiples, zero inputs) that clearly demonstrate the function\u0026#39;s behavior and correctness. This is excellent for testing and understanding. 4. **Robust Error Handling:** The user input section includes `try-except` blocks for `ValueError` (for non-integer input) and a general `Exception` for other unexpected errors. 5. **Good Docstrings and Type Hinting:** Clear docstrings explain the purpose, arguments, and return values, and type hints are used effectively. **Cons:** 1. The `gcd` function is essentially a wrapper around `math.gcd`. While it adds a docstring, it could potentially be omitted, and `math.gcd` called directly within `lcm` to reduce one level of indirection, or its docstring could mention it\u0026#39;s a wrapper. However, having a dedicated `gcd` function is not a significant drawback. ### Review of `code_generator_agent_g30` **Pros:** 1. **Correct Logic:** Both `calculate_gcd` and `calculate_lcm` functions are logically correct and implement the Euclidean algorithm and the LCM formula accurately. 2. **Good Structure:** It separates the GCD, LCM, and main execution logic into distinct functions (`calculate_gcd`, `calculate_lcm`, `main`). 3. **Robustness for Zero:** Correctly handles the LCM of zero with any number. 4. **Good Docstrings and Type Hinting:** Similar to g25, it uses clear docstrings and type hints. 5. **Error Handling:** Includes a `try-except` block for `ValueError` in user input. **Cons:** 1. **Manual GCD Implementation:** While functionally correct, reimplementing the Euclidean algorithm when `math.gcd` is readily available in the standard library is generally less efficient and less Pythonic. `math.gcd` is implemented in C and is typically faster. 2. **Limited Examples:** The `main` function focuses solely on user input and does not provide pre-defined example test cases. This makes it less immediately illustrative compared to g25\u0026#39;s comprehensive examples. 3. **Less Comprehensive Error Handling:** Only handles `ValueError` for user input, whereas g25 also catches a broader `Exception`. ### Conclusion Both agents provide valid and correct solutions for calculating the LCM. However, `code_generator_agent_g25` stands out as the superior choice due to the following reasons: 1. **Efficiency and Best Practice (GCD):** `g25` correctly utilizes the `math.gcd` function from Python\u0026#39;s standard library. This is the recommended approach as it\u0026#39;s optimized, tested, and reliable. `g30` re-implements the Euclidean algorithm, which, while correct, is unnecessary when a superior built-in option exists. 2. **Demonstrative Examples:** `g25` includes a wide array of test cases in its `if __name__ == \u0026#34;__main__\u0026#34;:` block, covering various scenarios including edge cases (like zero inputs). These examples are invaluable for understanding how the function behaves, for verification, and for demonstrating its robustness. `g30` only includes user input, lacking these illustrative examples. 3. **Robust Error Handling:** `g25` includes a more comprehensive error handling block in its `main` function (`except ValueError` and `except Exception`), making it slightly more robust. **The best code is generated by `code_generator_agent_g25`.** [user]: The third type of workflow agent is the loop agent.\nLoop Agent A loop agent repeatedly runs a sequence of agents for a specified number of iterations or until a stop condition is met. We can refine our earlier examples to add a review-refine loop to code generation. Let us look at the example.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 from google.adk.agents import LlmAgent, SequentialAgent, LoopAgent code_generator_agent = LlmAgent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;code_generator\u0026#39;, description=\u0026#39;Agent that generates code based on user requirements.\u0026#39;, instruction=\u0026#39;\u0026#39;\u0026#39; You are a code generator only and NOT a reviewer. Generate code based on the user\u0026#39;s requirements. Ensure that the code is well-commented and follows best practices. Output code only and wrap the code in markdown format for better readability. \u0026#39;\u0026#39;\u0026#39;, output_key=\u0026#39;generated_code\u0026#39;, ) code_reviewer_agent = LlmAgent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;code_reviewer\u0026#39;, description=\u0026#39;Agent that reviews code and provides feedback.\u0026#39;, instruction=\u0026#39;\u0026#39;\u0026#39; You are a code reviewer. Review the code generated by the code generator. Generate code: {generated_code} Provide constructive feedback and suggest improvements. Do not generate the code yourself. \u0026#39;\u0026#39;\u0026#39;, output_key=\u0026#39;review_feedback\u0026#39;, ) code_refiner_agent = LlmAgent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;code_refiner\u0026#39;, description=\u0026#39;Agent that refines code based on feedback.\u0026#39;, instruction=\u0026#39;\u0026#39;\u0026#39; You are a code refiner. You must consider the feedback provided by the code reviwer agent and respond with refined code. Code to update: {generated_code} Review feedback: {review_feedback} Ensure that the code is well-commented and follows best practices. Output code only and wrap the code in markdown format for better readability. \u0026#39;\u0026#39;\u0026#39;, output_key=\u0026#39;generated_code\u0026#39;, ) loop_agent = LoopAgent( name=\u0026#39;parallel_code_generator\u0026#39;, description=\u0026#39;Parallel agent to generate code using different models.\u0026#39;, sub_agents=[code_reviewer_agent,code_refiner_agent], max_iterations=3, ) root_agent = SequentialAgent( name=\u0026#39;code_tutor_agent\u0026#39;, description=\u0026#39;An agent that generates, reviews, and refines code based on user requirements.\u0026#39;, sub_agents=[code_generator_agent, loop_agent], ) In this example, a loop agent is defined to iterate between the review and refine process. The initial code generation is performed by the code_generator_agent, and then the review-refine loop refines the code. This loop runs for maximum iterations as specified by the max_iterations property in the LoopAgent definition.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 PS C:\\GitHub\\google-adk-101\u0026gt; adk run .\\code_tutor\\ [user]: Write a program to determine GCD of two numbers. [code_generator]: ```python def gcd(a, b): \u0026#34;\u0026#34;\u0026#34; Calculates the Greatest Common Divisor (GCD) of two non-negative integers using the Euclidean algorithm. The Euclidean algorithm is an efficient method for computing the greatest common divisor (GCD) of two integers (numbers), the largest number that divides them both without a remainder. Args: a (int): The first non-negative integer. b (int): The second non-negative integer. Returns: int: The Greatest Common Divisor (GCD) of a and b. \u0026#34;\u0026#34;\u0026#34; # Ensure both numbers are non-negative. If negative inputs are provided, # the absolute values are used, as GCD is typically defined for positive integers. a = abs(a) b = abs(b) # The Euclidean algorithm works by repeatedly applying the division algorithm # until the remainder is 0. The GCD is the last non-zero remainder. # In each step, we replace the larger number with the smaller number, # and the smaller number with the remainder of the division. while b: # The line \u0026#39;a, b = b, a % b\u0026#39; simultaneously updates \u0026#39;a\u0026#39; and \u0026#39;b\u0026#39;. # \u0026#39;a\u0026#39; takes the previous value of \u0026#39;b\u0026#39;. # \u0026#39;b\u0026#39; takes the remainder of \u0026#39;a\u0026#39; divided by \u0026#39;b\u0026#39;. a, b = b, a % b # When \u0026#39;b\u0026#39; becomes 0, \u0026#39;a\u0026#39; holds the GCD. return a # --- Example Usage --- if __name__ == \u0026#34;__main__\u0026#34;: print(\u0026#34;--- GCD Calculator ---\u0026#34;) # Test cases num1 = 48 num2 = 18 result = gcd(num1, num2) print(f\u0026#34;The GCD of {num1} and {num2} is: {result}\u0026#34;) # Expected: 6 num1 = 101 num2 = 103 result = gcd(num1, num2) print(f\u0026#34;The GCD of {num1} and {num2} is: {result}\u0026#34;) # Expected: 1 (prime numbers) num1 = 60 num2 = 24 result = gcd(num1, num2) print(f\u0026#34;The GCD of {num1} and {num2} is: {result}\u0026#34;) # Expected: 12 num1 = 7 num2 = 0 result = gcd(num1, num2) print(f\u0026#34;The GCD of {num1} and {num2} is: {result}\u0026#34;) # Expected: 7 (GCD(n, 0) = |n|) num1 = 0 num2 = 0 result = gcd(num1, num2) print(f\u0026#34;The GCD of {num1} and {num2} is: {result}\u0026#34;) # Expected: 0 (GCD(0, 0) is usually defined as 0) num1 = -48 num2 = 18 result = gcd(num1, num2) print(f\u0026#34;The GCD of {num1} and {num2} is: {result}\u0026#34;) # Expected: 6 (handles negative numbers by taking absolute value) # User input example try: user_num1 = int(input(\u0026#34;\\nEnter the first number: \u0026#34;)) user_num2 = int(input(\u0026#34;Enter the second number: \u0026#34;)) user_result = gcd(user_num1, user_num2) print(f\u0026#34;The GCD of {user_num1} and {user_num2} is: {user_result}\u0026#34;) except ValueError: print(\u0026#34;Invalid input. Please enter integers only.\u0026#34;) ``` [code_reviewer]: This is a very well-written and robust implementation of the Greatest Common Divisor (GCD) using the Euclidean algorithm. The code is clear, efficient, and handles various edge cases gracefully. Here\u0026#39;s a constructive review with minor suggestions for improvement: **Positive Aspects:** 1. **Correctness:** The `gcd` function correctly implements the Euclidean algorithm and produces the right results for all standard and edge cases (positive, zero, negative inputs). .... **Suggestions for Improvement:** 1. **Docstring Consistency (Minor):** * The docstring states that `a` and `b` are \u0026#34;non-negative integers.\u0026#34; However, the first two lines of the function `a = abs(a)` and `b = abs(b)` explicitly handle negative integers. .... **Conclusion:** The code is excellent. It\u0026#39;s a complete, correct, and highly readable solution for calculating the GCD. The suggestions above are mostly stylistic or for added clarity/robustness in documentation, and do not detract from the quality of the core implementation. [code_refiner]: ```python import math def gcd(a: int, b: int) -\u0026gt; int: \u0026#34;\u0026#34;\u0026#34; Calculates the Greatest Common Divisor (GCD) of two integers using the Euclidean algorithm. The Euclidean algorithm is an efficient method for computing the greatest common divisor (GCD) of two integers (numbers), the largest number that divides them both without a remainder. Negative inputs are handled by taking their absolute values, as GCD is typically defined for non-negative integers. Args: a (int): The first integer. b (int): The second integer. .... ``` [code_reviewer]: The `code_refiner` has done an excellent job incorporating the previous feedback. This revised code is significantly improved and demonstrates best practices. Here\u0026#39;s a breakdown of the positive aspects and how the suggestions were addressed: **Positive Aspects and Improvements:** 1. **Comprehensive Docstring:** The docstring is now perfectly aligned with the function\u0026#39;s behavior. * It correctly states that `a` and `b` are `int` (any integer). * It explicitly clarifies: \u0026#34;Negative inputs are handled by taking their absolute values, as GCD is typically defined for non-negative integers.\u0026#34; This fully addresses the docstring consistency suggestion. .... **Conclusion:** This is an excellent piece of code. All previous feedback has been thoughtfully incorporated, leading to a robust, well-documented, and highly educational implementation of the GCD function. Great work! [code_refiner]: ```python \u0026#34;\u0026#34;\u0026#34; This module provides a custom implementation of the Greatest Common Divisor (GCD) function using the Euclidean algorithm, along with example usage and comparison to Python\u0026#39;s built-in `math.gcd()` function. It\u0026#39;s intended for educational purposes to demonstrate the algorithm\u0026#39;s logic. \u0026#34;\u0026#34;\u0026#34; .... ``` [code_reviewer]: This revised code is truly excellent and demonstrates a very high standard of Python programming and documentation. All previous feedback has been thoroughly addressed, resulting in a robust, clear, and highly educational module. ... **Conclusion:** This code is exceptionally well-written. It is correct, robust, highly readable, well-documented, and serves its educational purpose perfectly. There are no further improvements needed. This is an exemplary solution. [code_refiner]: ```python .... ``` [user]: As you can see, the review-refine loop runs for 3 iterations and completes. The reviewer provided feedback in the second iteration itself that the refined code is excellent. However, the refiner continued to generate the code refinement. We can use the sub-agent\u0026rsquo;s escalation to terminate the loop and avoid unnecessary iterations.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 from google.adk.agents import LlmAgent, SequentialAgent, LoopAgent from google.adk.tools.tool_context import ToolContext def exit_loop(tool_context: ToolContext): \u0026#34;\u0026#34;\u0026#34;Call this function ONLY when the reviewer indicates APPROVED, signaling the iterative process should end.\u0026#34;\u0026#34;\u0026#34; print(f\u0026#34;[Tool Call] exit_loop triggered by {tool_context.agent_name}\u0026#34;) tool_context.actions.escalate = True tool_context.actions.skip_summarization = True return {} code_generator_agent = LlmAgent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;code_generator\u0026#39;, description=\u0026#39;Agent that generates code based on user requirements.\u0026#39;, instruction=\u0026#39;\u0026#39;\u0026#39; You are a code generator only and NOT a reviewer. Generate code based on the user\u0026#39;s requirements. Ensure that the code is well-commented and follows best practices. Output code only and wrap the code in markdown format for better readability. \u0026#39;\u0026#39;\u0026#39;, output_key=\u0026#39;generated_code\u0026#39;, ) code_reviewer_agent = LlmAgent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;code_reviewer\u0026#39;, description=\u0026#39;Agent that reviews code and provides feedback.\u0026#39;, instruction=\u0026#39;\u0026#39;\u0026#39; You are a code reviewer. Review the code generated by the code generator. Generate code: {generated_code} Provide constructive feedback and suggest improvements. Do not generate the code yourself. If the refined code is satisfactory, respond with \u0026#34;APPROVED\u0026#34; only and nothing else. Else, provide specific feedback on what needs to be improved. \u0026#39;\u0026#39;\u0026#39;, output_key=\u0026#39;review_feedback\u0026#39;, ) code_refiner_agent = LlmAgent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;code_refiner\u0026#39;, description=\u0026#39;Agent that refines code based on feedback.\u0026#39;, instruction=\u0026#39;\u0026#39;\u0026#39; You are a code refiner. You must consider the feedback provided by the code reviwer agent and respond with refined code. Ensure that the code is well-commented and follows best practices. Code to update: {generated_code} Review feedback: {review_feedback} If the feedback is \u0026#34;APPROVED\u0026#34;, you must call \u0026#39;exit_loop\u0026#39; function and DO NOT output any code or text. Else output refined code only and wrap the code in markdown format for better readability. \u0026#39;\u0026#39;\u0026#39;, tools=[exit_loop], output_key=\u0026#39;generated_code\u0026#39;, ) loop_agent = LoopAgent( name=\u0026#39;parallel_code_generator\u0026#39;, description=\u0026#39;Parallel agent to generate code using different models.\u0026#39;, sub_agents=[code_reviewer_agent,code_refiner_agent], max_iterations=3, ) root_agent = SequentialAgent( name=\u0026#39;code_tutor_agent\u0026#39;, description=\u0026#39;An agent that generates, reviews, and refines code based on user requirements.\u0026#39;, sub_agents=[code_generator_agent, loop_agent], ) Taking a cue from the ADK documentation, the exit_loop is used to implement loop termination. To make this happen, we need a signal from the reviewer agent. This is done by modifying the instructions given to both the reviewer and refiner agents. When the code refined by the refiner is satisfactory, the reviewer will respond with a completion word or phrase. In this example, it is APPROVED. When the refiner agent receives a response from the reviewer that contains only the completion word or phrase, it will call the specified tool to terminate the loop. Within this function, tool_context.actions.escalate = True indicates that the control be escalated to a higher-level agent and tool_context.actions.skip_summarization = True indicates that there is no need to call the LLM to summarize the reviewer agent\u0026rsquo;s response.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 PS C:\\GitHub\\google-adk-101\u0026gt; adk run .\\code_tutor\\ Running agent code_tutor_agent, type exit to exit. [user]: Write a Python program to determine the factorial of a number. [code_generator]: ```python .... ``` [code_reviewer]: The code is well-structured, easy to read, and includes good docstrings and comments. It correctly handles the base cases (n=0) and negative inputs, as well as non-integer inputs. The `__main__` block provides a robust user interface with input validation and error handling. However, there\u0026#39;s an opportunity to improve the separation of concerns within the `factorial` function itself. **Feedback and Suggestions for Improvement:** .... [code_refiner]: ```python .... ``` [code_reviewer]: APPROVED async for event in agen: [Tool Call] exit_loop triggered by code_refiner [user]: As you have seen in these examples, it is possible to combine different workflow agents into a single workflow. This comes in handy when you want to create complex agentic workflows using Google ADK. As we proceed in this series, we will explore several such examples. Stay tuned.\n","date":"2025-12-03T00:00:00Z","excerpt":"Google Agent Development Kit (ADK) is a flexible and modular framework for developing and deploying AI agents. It offers different types agents.","permalink":"https://ravichaganti.com/blog/google-adk-types-of-agents/","series":"Google ADK","title":"Google ADK - types of agents"},{"categories":["Google","Agent Development Kit","Agents"],"content":"Every other week, we see a new framework claiming to simplifythe development of AI agents. Thanks to the growing interest in agentic AI. Earlier this year, Google released its framework for developing agents, which it called the Agent Development Kit (ADK).\nYouTube Video This video requires your consent to load content from YouTube, which may set tracking cookies. Enable YouTube Video ID: zgrOwow_uTQ Bo Yang, tech lead of the ADK project, wrote about how ADK was born and described how 100 lines of code became the most sought-after agent development framework. It is fascinating to read how ADK evolved into what it is today.\nSo, what is ADK? What makes it special when there are a million other frameworks out there? In this series of posts, we will explore answers to these questions.\nIntroduction Google ADK provides an open foundation for simplifying agent and agentic workflow development. With ADK, you can use not just Google\u0026rsquo;s Gemini models but models from any provider, making model agnostic. The agents and workflows developed using ADK are deployment-agnostic and can run on your local machine, in Google Cloud, or in your own data center. True to its developers\u0026rsquo; intentions, ADK supports interoperability with agents built in other frameworks via the Agent-to-Agent (A2A) protocol. In my work with various agent development frameworks, I found ADK to be very easy to get started with while providing powerful constructs for building enterprise-ready agents and agentic workflows. With ADK, you can create agents in Python, Go, and Java programming languages. All examples in this series of articles on ADK will be written in Python.\nLet us explore further using code.\nTo get started with creating agents, all you need is the google-adk package.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 C:\\\u0026gt; pip install google-adk C:\\\u0026gt; adk --help Usage: adk [OPTIONS] COMMAND [ARGS]... Agent Development Kit CLI tools. Options: --version Show the version and exit. --help Show this message and exit. Commands: api_server Starts a FastAPI server for agents. conformance Conformance testing tools for ADK. create Creates a new app in the current folder with prepopulated agent template. deploy Deploys agent to hosted environments. eval Evaluates an agent given the eval sets. eval_set Manage Eval Sets. run Runs an interactive CLI for a certain agent. web Starts a FastAPI server with Web UI for agents. The adk command-line provides the necessary capabilities to create, run, deploy, and evaluate agents. Using the adk web command, you can converse with the agents and debug their execution. Let us start with creating an agent using the adk create command. When you run this command, you will be prompted for a model to use, a backend for the agent deployment and services, and an API key if you have chosen the Google Gemini model.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 C:\\\u0026gt; adk create code_agent Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_API_KEY Agent created in C:\\GitHub\\google-adk-101\\code_agent: - .env - __init__.py - agent.py As seen in the output, this command generates the necessary scaffolding. The .env file contains environment variables, including GOOGLE_API_KEY for Google Gemini models. The __init__.py file serves as the agent entry point and imports the agent defined in agent.py. The agent.py contains the agent definition. Let us explore this further.\n1 2 3 4 5 6 7 8 from google.adk.agents.llm_agent import Agent root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;root_agent\u0026#39;, description=\u0026#39;A helpful assistant for user questions.\u0026#39;, instruction=\u0026#39;Answer user questions to the best of your knowledge\u0026#39;, ) Google ADK supports multiple agent types. The one we created is an LlmAgent. This category of agents uses an LLM as the core reasoning engine and is non-deterministic. In the preceding code, you can replace Agent with LlmAgent as both refer to the LLM Agent implementation. We will look at other types of agents later in this series.\nIn the agent definition above, the model identifies the LLM agent to use for reasoning, and the instruction specifies the agent\u0026rsquo;s behavior. This scaffold is sufficient to try running this agent.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 C:\\\u0026gt; adk run .\\code_agent\\ .... [user]: Hello, how can you help? [root_agent]: Hello! I\u0026#39;m here to help answer your questions and provide information to the best of my knowledge. Here are some examples of what I can do: * **Answer questions on a wide range of topics:** History, science, technology, culture, current events, and more. * **Explain concepts:** Describe how things work, define terms, or break down complex ideas. * **Provide summaries:** Give you a concise overview of a topic or document. * **Help with creative tasks:** Brainstorm ideas, write short pieces of text, or suggest options. * **Give recommendations:** Suggest books, movies, places, or tools based on your interests. Just tell me what you need, and I\u0026#39;ll do my best to assist you! To open the dev-ui web interface, run the adk web command.\nThe ADK web interface is a great resource for debugging agent conversations and interactions with other agents and tools.\nADK provides a few built-in tools for our agents. The following example shows the built-in google-search tool for grounding LLMs with Google search results.\n1 2 3 4 5 6 7 8 9 10 11 12 13 from google.adk.agents.llm_agent import Agent from google.adk.tools import google_search root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;condition\u0026#34;}\u0026#39;, tools=[ google_search, ], ) When you run this agent using adk run, you will be presented with a conversational prompt.\n1 2 3 4 5 C:\\\u0026gt; adk run .\\weather_agent\\ .... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;19 °C\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;Partly cloudy\u0026#34;} You can also supply Python functions as tools.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 from google.adk.agents.llm_agent import Agent import os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}°C\u0026#34; else: return \u0026#34;City not found\u0026#34; root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;}\u0026#39;, tools=[ get_weather ], ) The get_weather function uses the OpenWeatherMap API to retrieve weather data for any location.\n1 2 3 4 5 C:\\\u0026gt; adk run .\\code_agent\\ ... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;18.66°C\u0026#34;} Google ADK also supports the Model Context Protocol and can use other agents as tools. We will learn more about this later.\nDeclarative Configuration What we have seen so far is an imperative-based approach to implementing agents in ADK. We can also do the same declaratively using a YAML configuration. Let us look at this method.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 C:\\\u0026gt; adk create weather_agent --type=config Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_GOOGLE_API_KEY Agent created in C:\\GitHub\\google-adk-101\\weather_agent: - .env - __init__.py - root_agent.yaml For creating a YAML-based agent configuration, you need to specify the --type=config optional parameter to the adk create command. This command generates root_agent.yaml instead of agent.py. If you intend to write declarative agents like this, ensure that you have root_agent.yaml always in the agent folder.\n1 2 3 4 name: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash You can add tools to this configuration.\n1 2 3 4 5 6 name: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: google_search This is how built-in tools can be specified. But what about functional tools, such as the get_weather tool we saw earlier? To use function tools within declarative configuration, we need to place them in a Python module and place that module under the agent package. For example, the get_weather function above can be added to the tools.py module.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 import os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}°C\u0026#34; else: return \u0026#34;City not found\u0026#34; The folder structure of the agent folder will be as follows:\n1 2 3 4 5 6 7 8 C:\\\u0026gt; tree /F /A .\\weather_agent\\ Folder PATH listing Volume serial number is 987A-295A C:\\WEATHER_AGENT .env root_agent.yaml tools.py __init__.py To reference the tool in the YAML configuration, we must provide the fully qualified path to the function.\n1 2 3 4 5 6 name: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: weather_agent.tools.get_weather With the basics out of the way, let\u0026rsquo;s dive deep into developing agents and agentic workflows with Google ADK.\n","date":"2025-12-01T00:00:00Z","excerpt":"Google Agent Development Kit (ADK) is a flexible and modular framework for developing and deploying AI agents.","permalink":"https://ravichaganti.com/blog/introduction-to-google-agent-development-kit/","series":"Google ADK","title":"Introduction to Google Agent Development Kit"},{"categories":["Microsoft Agent Framework","Workflows","Agents"],"content":"In this series on Microsoft Agent Framework (MAF), we\u0026rsquo;ve explored sequential workflows where agents process tasks in a fixed order, concurrent workflows where agents work in parallel, handoff workflows where agents transfer control based on context, and group chat workflows where agents engage in turn-based discussions. Each pattern has its strengths, but they all share one limitation: the orchestration logic is predetermined at design time.\nWhat if you need a workflow that can adapt its routing decisions based on what agents discover during execution? Enter Magentic workflows, the most sophisticated orchestration pattern in MAF, where an LLM-powered manager autonomously coordinates specialized agents, making real-time decisions about who to invoke next based on intermediate results.\nThe Problem: Static Orchestration Isn\u0026rsquo;t Always Enough Consider a complex task like investment due diligence. A sequential workflow might work: research the company, analyze financials, assess risks, write a report. But what happens when the risk assessment reveals concerning findings? In a static workflow, you\u0026rsquo;d proceed to the report anyway. In reality, you\u0026rsquo;d want to go back and dig deeper into those risks before making a recommendation.\nThis is where magentic workflows shine. They enable:\nDynamic routing: The manager decides which agent to invoke based on the current state of the conversation Conditional branching: Different paths through the workflow based on intermediate results Feedback loops: The ability to revisit earlier agents when new information warrants it Autonomous coordination: The LLM manager handles the orchestration logic without explicit programming What is a Magentic Workflow? A magentic workflow consists of two key components:\nParticipant agents: Specialized agents, each with distinct tools and expertise Manager agent: An LLM-powered orchestrator that plans the task, selects which agent to invoke, monitors progress, and determines when to complete or replan Unlike group chat where agents take turns based on a selection function you define, magentic workflows delegate the entire orchestration decision to the manager LLM. The manager maintains a \u0026ldquo;task ledger\u0026rdquo; tracking facts discovered, the current plan, and progress toward completion.\nExample: Investment Due Diligence Workflow Let\u0026rsquo;s build a practical example that demonstrates the power of magentic orchestration. We\u0026rsquo;ll create an investment research workflow with four specialized agents:\nAgent Role Tools MarketResearcher Gathers market context, news, analyst opinions Web search, financial news FinancialAnalyst Analyzes fundamentals and financial health Yahoo Finance API, Python RiskAssessor Evaluates risks and assigns a risk score Risk assessment InvestmentAdvisor Synthesizes findings into a recommendation Report generation Here\u0026rsquo;s the key feature. If the RiskAssessor returns a high risk score (\u0026gt;7), the manager will loop back to the MarketResearcher for deeper investigation before proceeding to the final recommendation.\nSetting Up the Environment First, install the required packages:\n1 pip install agent-framework azure-identity tavily-python yfinance python-dotenv Create a .env file with your configuration:\n1 2 AZURE_OPENAI_ENDPOINT=https://your-endpoint.openai.azure.com/ TAVILY_API_KEY=your-tavily-api-key Defining the Tools Each agent needs specialized tools. Let\u0026rsquo;s start with the research and analysis tools:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 from agent_framework import ai_function from tavily import TavilyClient import yfinance as yf import os tavily_client = TavilyClient(api_key=os.getenv(\u0026#34;TAVILY_API_KEY\u0026#34;)) @ai_function def search_web(query: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Search the web for general information about markets, companies, or trends.\u0026#34;\u0026#34;\u0026#34; result = tavily_client.search(query=query, max_results=5) return str(result) @ai_function def search_financial_news(company_or_topic: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Search for recent financial news, analyst opinions, and market sentiment.\u0026#34;\u0026#34;\u0026#34; query = f\u0026#34;{company_or_topic} stock news analyst rating 2024\u0026#34; result = tavily_client.search(query=query, max_results=5) return f\u0026#34;FINANCIAL NEWS for {company_or_topic}:\\n{str(result)}\u0026#34; The financial analysis tool uses Yahoo Finance to fetch real market data:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 @ai_function def analyze_financials(ticker: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Get key financial metrics using Yahoo Finance.\u0026#34;\u0026#34;\u0026#34; try: stock = yf.Ticker(ticker.upper()) info = stock.info name = info.get(\u0026#39;longName\u0026#39;, ticker) price = info.get(\u0026#39;currentPrice\u0026#39;, \u0026#39;N/A\u0026#39;) pe_ratio = info.get(\u0026#39;trailingPE\u0026#39;, \u0026#39;N/A\u0026#39;) profit_margin = info.get(\u0026#39;profitMargins\u0026#39;, \u0026#39;N/A\u0026#39;) market_cap = info.get(\u0026#39;marketCap\u0026#39;, \u0026#39;N/A\u0026#39;) # Format market cap if isinstance(market_cap, (int, float)): if market_cap \u0026gt;= 1e12: market_cap = f\u0026#34;${market_cap/1e12:.2f}T\u0026#34; elif market_cap \u0026gt;= 1e9: market_cap = f\u0026#34;${market_cap/1e9:.2f}B\u0026#34; result = f\u0026#34;\u0026#34;\u0026#34;FINANCIAL ANALYSIS for {name} ({ticker.upper()}) {\u0026#39;=\u0026#39; * 50} - Current Price: ${price} - Market Cap: {market_cap} - P/E Ratio: {pe_ratio} - Profit Margin: {profit_margin * 100:.1f}% if profit_margin else \u0026#39;N/A\u0026#39; \u0026#34;\u0026#34;\u0026#34; # Add risk flags for unprofitable companies if profit_margin and profit_margin \u0026lt; 0: result += \u0026#34;\\nRISK FLAG: Company is UNPROFITABLE (negative margins)\u0026#34; return result except Exception as e: return f\u0026#34;Error fetching data for {ticker}: {str(e)}\u0026#34; The risk assessment tool is where the conditional logic lives:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 @ai_function def assess_risk(analysis_summary: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Evaluate investment risks and return a risk score (1-10).\u0026#34;\u0026#34;\u0026#34; summary_lower = analysis_summary.lower() risk_score = 3 # Base score risk_factors = [] # Check for high-risk indicators if \u0026#34;unprofitable\u0026#34; in summary_lower or \u0026#34;negative margin\u0026#34; in summary_lower: risk_score += 3 risk_factors.append(\u0026#34;Company is not profitable - high cash burn risk\u0026#34;) if \u0026#34;biotech\u0026#34; in summary_lower or \u0026#34;clinical\u0026#34; in summary_lower: risk_score += 2 risk_factors.append(\u0026#34;Clinical stage biotech - binary FDA approval risk\u0026#34;) if \u0026#34;crypto\u0026#34; in summary_lower or \u0026#34;bitcoin\u0026#34; in summary_lower: risk_score += 2 risk_factors.append(\u0026#34;High volatility asset - significant drawdown risk\u0026#34;) risk_score = max(1, min(10, risk_score)) result = f\u0026#34;\u0026#34;\u0026#34;RISK ASSESSMENT =============== RISK SCORE: {risk_score}/10 {\u0026#34;[HIGH RISK]\u0026#34; if risk_score \u0026gt; 7 else \u0026#34;[Moderate]\u0026#34; if risk_score \u0026gt; 4 else \u0026#34;[Low Risk]\u0026#34;} RISK FACTORS IDENTIFIED: \u0026#34;\u0026#34;\u0026#34; for i, factor in enumerate(risk_factors, 1): result += f\u0026#34;{i}. {factor}\\n\u0026#34; # This message triggers the feedback loop if risk_score \u0026gt; 7: result += f\u0026#34;\u0026#34;\u0026#34; HIGH RISK ALERT: Risk score exceeds threshold. RECOMMEND: Request MarketResearcher to conduct deeper research on: {risk_factors[0]} \u0026#34;\u0026#34;\u0026#34; return result Creating the Specialized Agents Now we create four agents, each with distinct instructions and tools:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 from agent_framework.azure import AzureOpenAIChatClient from azure.identity import DefaultAzureCredential chat_client = AzureOpenAIChatClient( credential=DefaultAzureCredential(), ) # Market Researcher - gathers context and news market_researcher = chat_client.create_agent( name=\u0026#34;MarketResearcher\u0026#34;, instructions=\u0026#34;\u0026#34;\u0026#34;You are a market research analyst. YOUR ROLE: - Gather market context, industry trends, and competitive landscape - Find recent news and analyst opinions - Identify market sentiment and catalysts Use search_web for broad context and search_financial_news for recent analyst takes. If asked to do a \u0026#34;deep dive\u0026#34; on specific risks, focus your search on those factors.\u0026#34;\u0026#34;\u0026#34;, tools=[search_web, search_financial_news], ) # Financial Analyst - evaluates fundamentals financial_analyst = chat_client.create_agent( name=\u0026#34;FinancialAnalyst\u0026#34;, instructions=\u0026#34;\u0026#34;\u0026#34;You are a financial analyst specializing in fundamental analysis. YOUR ROLE: - Analyze financial metrics (P/E, margins, growth, debt) - Evaluate company fundamentals and financial health - Be objective about both strengths and weaknesses\u0026#34;\u0026#34;\u0026#34;, tools=[analyze_financials], ) # Risk Assessor - identifies and scores risks risk_assessor = chat_client.create_agent( name=\u0026#34;RiskAssessor\u0026#34;, instructions=\u0026#34;\u0026#34;\u0026#34;You are a risk management specialist. YOUR ROLE: - Evaluate investment risks based on gathered information - Assign a risk score from 1-10 - Identify specific risk factors CRITICAL: If risk score \u0026gt; 7, recommend deeper research before final recommendation.\u0026#34;\u0026#34;\u0026#34;, tools=[assess_risk], ) # Investment Advisor - synthesizes final recommendation investment_advisor = chat_client.create_agent( name=\u0026#34;InvestmentAdvisor\u0026#34;, instructions=\u0026#34;\u0026#34;\u0026#34;You are a senior investment advisor. THIS IS THE FINAL STEP - After you complete your work, the task is DONE. YOUR ROLE: - Synthesize all research into a recommendation: BUY / HOLD / SELL / AVOID - Save the final report using save_report tool - After saving, confirm \u0026#34;TASK COMPLETE - Report saved successfully\u0026#34; \u0026#34;\u0026#34;\u0026#34;, tools=[save_report], ) Creating the Manager Agent The manager agent is the brain of the magentic workflow. Its instructions define how agents are coordinated:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 manager_agent = chat_client.create_agent( name=\u0026#34;InvestmentManager\u0026#34;, instructions=\u0026#34;\u0026#34;\u0026#34;You are the Investment Due Diligence Manager. YOUR TEAM: - MarketResearcher: Gathers market context, news, analyst opinions - FinancialAnalyst: Analyzes fundamentals, metrics, financial health - RiskAssessor: Evaluates risks, assigns risk score (1-10) - InvestmentAdvisor: Synthesizes findings and saves final report WORKFLOW: 1. MarketResearcher - gather market context and news 2. FinancialAnalyst - analyze fundamentals 3. RiskAssessor - evaluate risks and get risk score 4. DECISION POINT (if risk score \u0026gt; 7): - Request ONE deep dive from MarketResearcher on the main risk - Then proceed to step 5 5. InvestmentAdvisor - synthesize and save final report TERMINATION: - The task is COMPLETE when InvestmentAdvisor confirms \u0026#34;Report saved\u0026#34; - Do NOT call any more agents after InvestmentAdvisor completes\u0026#34;\u0026#34;\u0026#34; ) Notice how the manager\u0026rsquo;s instructions encode the conditional logic: \u0026ldquo;if risk score \u0026gt; 7, request ONE deep dive.\u0026rdquo; This is the power of magentic. The routing logic is expressed in natural language and interpreted by the LLM.\nBuilding the Workflow With all components defined, we build the magentic workflow using MagenticBuilder:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 from agent_framework import MagenticBuilder workflow = ( MagenticBuilder() .participants( market_researcher=market_researcher, financial_analyst=financial_analyst, risk_assessor=risk_assessor, investment_advisor=investment_advisor, ) .with_standard_manager( agent=manager_agent, max_round_count=15, # Maximum orchestration rounds max_stall_count=3, # Replan after this many stalls ) .build() ) Key configuration options:\nmax_round_count: Limits total orchestration rounds to prevent infinite loops max_stall_count: If no progress is made for this many rounds, the manager replans Running and Monitoring the Workflow Execute the workflow and observe the orchestration events:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 from agent_framework import ( WorkflowOutputEvent, ExecutorInvokedEvent, ExecutorCompletedEvent, WorkflowStatusEvent, WorkflowRunState, ) import asyncio async def main(): task = \u0026#34;Evaluate investing in Beam Therapeutics (BEAM), a small-cap biotech company.\u0026#34; agent_sequence = [] async for event in workflow.run_stream(task): if isinstance(event, ExecutorInvokedEvent): agent_sequence.append(event.executor_id) print(f\u0026#34;\u0026gt;\u0026gt; Invoking: {event.executor_id}\u0026#34;) elif isinstance(event, ExecutorCompletedEvent): print(f\u0026#34; Completed: {event.executor_id}\u0026#34;) elif isinstance(event, WorkflowStatusEvent): if event.state == WorkflowRunState.IDLE: print(f\u0026#34;\\nWorkflow completed!\u0026#34;) print(f\u0026#34;Agent sequence: {\u0026#39; -\u0026gt; \u0026#39;.join(agent_sequence)}\u0026#34;) elif isinstance(event, WorkflowOutputEvent): print(f\u0026#34;\\nFinal output received\u0026#34;) asyncio.run(main()) Observing Dynamic Routing in Action When we run this workflow with a high-risk biotech stock like BEAM, here\u0026rsquo;s what happens:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 \u0026gt;\u0026gt; Invoking: magentic_orchestrator Completed: magentic_orchestrator \u0026gt;\u0026gt; Invoking: agent_market_researcher Completed: agent_market_researcher \u0026gt;\u0026gt; Invoking: magentic_orchestrator Completed: magentic_orchestrator \u0026gt;\u0026gt; Invoking: agent_financial_analyst Completed: agent_financial_analyst \u0026gt;\u0026gt; Invoking: magentic_orchestrator Completed: magentic_orchestrator \u0026gt;\u0026gt; Invoking: agent_risk_assessor Completed: agent_risk_assessor \u0026gt;\u0026gt; Invoking: magentic_orchestrator \u0026lt;- Manager sees high risk score Completed: magentic_orchestrator \u0026gt;\u0026gt; Invoking: agent_market_researcher \u0026lt;- LOOPS BACK for deep dive! Completed: agent_market_researcher \u0026gt;\u0026gt; Invoking: magentic_orchestrator Completed: magentic_orchestrator \u0026gt;\u0026gt; Invoking: agent_investment_advisor \u0026lt;- Now proceeds to final step Completed: agent_investment_advisor Workflow completed! Agent sequence: orchestrator -\u0026gt; market_researcher -\u0026gt; orchestrator -\u0026gt; financial_analyst -\u0026gt; orchestrator -\u0026gt; risk_assessor -\u0026gt; orchestrator -\u0026gt; market_researcher -\u0026gt; orchestrator -\u0026gt; investment_advisor Notice the feedback loop: after the risk assessor returns a high score, the manager autonomously decides to invoke the market researcher again for deeper research on the identified risks. This adaptive behavior is impossible with static workflow patterns.\nCompare this to a low-risk stock like Microsoft (MSFT), where the workflow proceeds directly without the feedback loop:\n1 2 orchestrator -\u0026gt; market_researcher -\u0026gt; orchestrator -\u0026gt; financial_analyst -\u0026gt; orchestrator -\u0026gt; risk_assessor -\u0026gt; orchestrator -\u0026gt; investment_advisor The same workflow code produces different execution paths based on the data discovered during execution.\nHow Magentic Differs from Other Patterns Pattern Orchestration Routing Logic Use Case Sequential Fixed order Predetermined Pipelines with known steps Concurrent Parallel execution Fan-out/fan-in Independent subtasks Handoff Agent-to-agent transfer Explicit handoff calls Escalation, specialization Group Chat Turn-based Selection function Brainstorming, debate Magentic LLM manager Dynamic, adaptive Complex tasks with conditional logic Magentic is the most flexible but also the most resource-intensive pattern. The manager LLM is invoked between every agent execution to evaluate progress and decide the next step. Use it when:\nThe workflow requires conditional branching based on intermediate results You need feedback loops for iterative refinement The optimal sequence of agents isn\u0026rsquo;t known at design time Task complexity warrants the overhead of LLM-based orchestration Magentic workflows represent the most sophisticated orchestration pattern in MAF, bridging the gap between rigid automation and truly autonomous multi-agent systems. When your task requires adaptive decision-making that can\u0026rsquo;t be predetermined, magentic is the pattern to reach for.\n","date":"2025-10-20T00:00:00Z","excerpt":"If you need a workflow that can adapt its routing decisions based on what agents discover during execution, look no further. Magentic is for you!","permalink":"https://ravichaganti.com/blog/magentic-workflows-in-microsoft-agent-framework/","series":"Microsoft Agent Framework","title":"Magentic workflows in Microsoft Agent Framework"},{"categories":["Microsoft Agent Framework","Workflows","Agents"],"content":"In the previous articles of this series, we explored three powerful workflow patterns in Microsoft Agent Framework (MAF):\nSequential: Agents process in a fixed order, like a pipeline. Concurrent: Agents work in parallel, with results aggregated. Handoff: A coordinator routes requests to specialist agents. But what happens when you need agents to discuss, debate, and build on each other\u0026rsquo;s ideas, like a brainstorming session or a committee meeting? This is where the GroupChat workflow pattern shines. In this article, we\u0026rsquo;ll explore how to build dynamic multi-agent conversations using GroupChatBuilder, where a moderator (or selection function) orchestrates turn-based discussions among participant agents.\nThe GroupChat workflow is ideal for scenarios where:\nMultiple perspectives are needed on a single topic. Ideas need to evolve through discussion and refinement. Agents should react to each other\u0026rsquo;s outputs, not just the original input. Consensus or synthesis emerges from collaborative dialogue. To understand this better, let us start with an example. Let\u0026rsquo;s build a classic brainstorming setup with three agents: a creative who generates ideas, a critic who challenges them, and a synthesizer who combines the best elements. Each agent needs a distinct perspective defined in its instructions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 creative = chat_client.create_agent( name=\u0026#34;Creative\u0026#34;, instructions=\u0026#34;You are a creative thinker. Generate bold, innovative ideas.\u0026#34;, ) critic = chat_client.create_agent( name=\u0026#34;Critic\u0026#34;, instructions=\u0026#34;You are a critical thinker. Identify potential issues and challenges.\u0026#34;, ) synthesizer = chat_client.create_agent( name=\u0026#34;Synthesizer\u0026#34;, instructions=\u0026#34;You synthesize ideas. Combine the best elements into actionable plans.\u0026#34;, ) The heart of a GroupChat workflow is the speaker selection function. This function decides who speaks next based on the current state of the conversation. The speaker selection function receives the conversation state.\n1 2 3 4 5 GroupChatStateSnapshot = TypedDict(\u0026#39;GroupChatStateSnapshot\u0026#39;, { \u0026#39;participants\u0026#39;: dict, # Dictionary of participant agents (name → agent) \u0026#39;history\u0026#39;: list, # Conversation history so far \u0026#39;round_index\u0026#39;: int, # Current round number (0-indexed) }) Field Description participants A dictionary where keys are agent names history List of messages exchanged so far round_index How many turns have been taken A simple way to select speakers is to use a round-robin method. In this approach, each agent speaks in order.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 def select_speaker(state: GroupChatStateSnapshot) -\u0026gt; str | None: \u0026#34;\u0026#34;\u0026#34;Select the next speaker in round-robin order.\u0026#34;\u0026#34;\u0026#34; participants = state[\u0026#34;participants\u0026#34;] round_index = state[\u0026#34;round_index\u0026#34;] # End after 6 rounds (2 full cycles through 3 agents) if round_index \u0026gt;= 6: return None # Returning None ends the conversation # Get the list of participant names participant_names = list(participants.keys()) # Select based on round index return participant_names[round_index % len(participant_names)] As defined in this function, the next participant name is returned. To terminate the conversation, this function returns None.\nWith the speaker selection in place, let us build the workflow.\n1 2 3 4 5 6 workflow = ( GroupChatBuilder() .set_select_speakers_func(select_speaker) .participants([creative, critic, synthesizer]) .build() ) The set_select_speakers_func(func) method sets the speaker selection function. participants([...]) registers the agents that can participate, and build()creates the executable workflow. We will use the well-known pattern for running the workflow.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 async def main(): print(\u0026#34;Running group chat workflow: Creative ↔ Critic ↔ Synthesizer\u0026#34;) print(\u0026#34;=\u0026#34; * 60) print(\u0026#34;Topic: How can we make AI agents more trustworthy?\u0026#34;) print(\u0026#34;=\u0026#34; * 60) output_evt: WorkflowOutputEvent | None = None async for event in workflow.run_stream(\u0026#34;How can we make AI agents more trustworthy?\u0026#34;): if isinstance(event, ExecutorInvokedEvent): print(f\u0026#34;⚡ Starting: {event.executor_id}\u0026#34;) elif isinstance(event, ExecutorCompletedEvent): print(f\u0026#34;✓ Completed: {event.executor_id}\u0026#34;) elif isinstance(event, WorkflowStatusEvent): if event.state == WorkflowRunState.IDLE: print(\u0026#34;\\n✅ Group chat completed!\u0026#34;) elif isinstance(event, WorkflowOutputEvent): output_evt = event The WorkflowOutputEvent.data contains the full conversation as a list of messages:\n1 2 3 4 5 6 7 8 9 10 11 12 13 if output_evt: print(\u0026#34;\\n\u0026#34; + \u0026#34;=\u0026#34; * 60) print(\u0026#34;GROUP CHAT TRANSCRIPT\u0026#34;) print(\u0026#34;=\u0026#34; * 60) messages = output_evt.data if isinstance(messages, list): for i, msg in enumerate(messages, start=1): if hasattr(msg, \u0026#39;role\u0026#39;): name = msg.author_name or (\u0026#34;assistant\u0026#34; if msg.role == Role.ASSISTANT else \u0026#34;user\u0026#34;) print(f\u0026#34;\\n{\u0026#39;-\u0026#39; * 60}\u0026#34;) print(f\u0026#34;{i:02d} [{name}]\u0026#34;) print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\u0026#34;) print(msg.text) Here\u0026rsquo;s the complete code for a brainstorming GroupChat.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 from agent_framework import ( GroupChatBuilder, GroupChatStateSnapshot, Role, WorkflowOutputEvent, ExecutorInvokedEvent, ExecutorCompletedEvent, WorkflowStatusEvent, WorkflowRunState, ) from agent_framework.azure import AzureOpenAIChatClient from azure.identity import DefaultAzureCredential import asyncio from dotenv import load_dotenv load_dotenv() # Create chat client chat_client = AzureOpenAIChatClient( credential=DefaultAzureCredential(), ) # Create participant agents creative = chat_client.create_agent( name=\u0026#34;Creative\u0026#34;, instructions=\u0026#34;You are a creative thinker. Generate bold, innovative ideas.\u0026#34;, ) critic = chat_client.create_agent( name=\u0026#34;Critic\u0026#34;, instructions=\u0026#34;You are a critical thinker. Identify potential issues and challenges.\u0026#34;, ) synthesizer = chat_client.create_agent( name=\u0026#34;Synthesizer\u0026#34;, instructions=\u0026#34;You synthesize ideas. Combine the best elements into actionable plans.\u0026#34;, ) # Simple round-robin speaker selection def select_speaker(state: GroupChatStateSnapshot) -\u0026gt; str | None: participants = state[\u0026#34;participants\u0026#34;] round_index = state[\u0026#34;round_index\u0026#34;] # End after 6 rounds (2 full cycles) if round_index \u0026gt;= 6: return None # participants is a dict with agent names as keys participant_names = list(participants.keys()) return participant_names[round_index % len(participant_names)] # Build group chat workflow workflow = ( GroupChatBuilder() .set_select_speakers_func(select_speaker) .participants([creative, critic, synthesizer]) .build() ) async def main(): print(\u0026#34;Running group chat workflow: Creative ↔ Critic ↔ Synthesizer\u0026#34;) print(\u0026#34;=\u0026#34; * 60) print(\u0026#34;Topic: How can we make AI agents more trustworthy?\u0026#34;) print(\u0026#34;=\u0026#34; * 60) output_evt: WorkflowOutputEvent | None = None async for event in workflow.run_stream(\u0026#34;How can we make AI agents more trustworthy?\u0026#34;): if isinstance(event, ExecutorInvokedEvent): print(f\u0026#34;⚡ Starting: {event.executor_id}\u0026#34;) elif isinstance(event, ExecutorCompletedEvent): print(f\u0026#34;✓ Completed: {event.executor_id}\u0026#34;) elif isinstance(event, WorkflowStatusEvent): if event.state == WorkflowRunState.IDLE: print(\u0026#34;\\n✅ Group chat completed!\u0026#34;) elif isinstance(event, WorkflowOutputEvent): output_evt = event # Display the final conversation if output_evt: print(\u0026#34;\\n\u0026#34; + \u0026#34;=\u0026#34; * 60) print(\u0026#34;GROUP CHAT TRANSCRIPT\u0026#34;) print(\u0026#34;=\u0026#34; * 60) messages = output_evt.data if isinstance(messages, list): for i, msg in enumerate(messages, start=1): if hasattr(msg, \u0026#39;role\u0026#39;): name = msg.author_name or (\u0026#34;assistant\u0026#34; if msg.role == Role.ASSISTANT else \u0026#34;user\u0026#34;) print(f\u0026#34;\\n{\u0026#39;-\u0026#39; * 60}\u0026#34;) print(f\u0026#34;{i:02d} [{name}]\u0026#34;) print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\u0026#34;) print(msg.text) else: print(output_evt.data) if __name__ == \u0026#34;__main__\u0026#34;: asyncio.run(main()) Here\u0026rsquo;s what you can expect when you run this workflow.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 \u0026gt; python.exe .\\04-group_chat.py Running group chat workflow: Creative ↔ Critic ↔ Synthesizer ============================================================ Topic: How can we make AI agents more trustworthy? ============================================================ ⚡ Starting: groupchat_orchestrator_08ab99b0 ✓ Completed: groupchat_orchestrator_08ab99b0 ⚡ Starting: groupchat_agent:Creative ✓ Completed: groupchat_agent:Creative ⚡ Starting: groupchat_orchestrator_08ab99b0 ✓ Completed: groupchat_orchestrator_08ab99b0 ⚡ Starting: groupchat_agent:Critic ✓ Completed: groupchat_agent:Critic ⚡ Starting: groupchat_orchestrator_08ab99b0 ✓ Completed: groupchat_orchestrator_08ab99b0 ⚡ Starting: groupchat_agent:Synthesizer ✓ Completed: groupchat_agent:Synthesizer ⚡ Starting: groupchat_orchestrator_08ab99b0 ✓ Completed: groupchat_orchestrator_08ab99b0 ⚡ Starting: groupchat_agent:Creative ✓ Completed: groupchat_agent:Creative ⚡ Starting: groupchat_orchestrator_08ab99b0 ✓ Completed: groupchat_orchestrator_08ab99b0 ⚡ Starting: groupchat_agent:Critic ✓ Completed: groupchat_agent:Critic ⚡ Starting: groupchat_orchestrator_08ab99b0 ✓ Completed: groupchat_orchestrator_08ab99b0 ⚡ Starting: groupchat_agent:Synthesizer ✓ Completed: groupchat_agent:Synthesizer ⚡ Starting: groupchat_orchestrator_08ab99b0 ✓ Completed: groupchat_orchestrator_08ab99b0 ✅ Group chat completed! ============================================================ GROUP CHAT TRANSCRIPT ============================================================ ..... The round-robin method for speaker selection is simple but does not necessarily reflect real-world brainstorming. In the following sections, we will look at slightly more advanced methods for selecting a speaker in a GroupChat workflow.\nConditional selection based on consent In this pattern, we let the content of the conversation drive the speaker selection. Let us look at the implementation.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def content_aware_selection(state: GroupChatStateSnapshot) -\u0026gt; str | None: \u0026#34;\u0026#34;\u0026#34;Select speaker based on what was just said.\u0026#34;\u0026#34;\u0026#34; history = state[\u0026#34;history\u0026#34;] round_index = state[\u0026#34;round_index\u0026#34;] if round_index \u0026gt;= 9: return None if round_index == 0: return \u0026#34;Creative\u0026#34; last_turn = history[-1] if history else None last_message = \u0026#34;\u0026#34; if last_turn and hasattr(last_turn, \u0026#39;message\u0026#39;): last_message = last_turn.message.lower() if isinstance(last_turn.message, str) else \u0026#34;\u0026#34; if \u0026#34;idea\u0026#34; in last_message or \u0026#34;propose\u0026#34; in last_message: return \u0026#34;Critic\u0026#34; if \u0026#34;concern\u0026#34; in last_message or \u0026#34;risk\u0026#34; in last_message: return \u0026#34;Creative\u0026#34; return \u0026#34;Synthesizer\u0026#34; In this pattern, based on the creative or critic agent\u0026rsquo;s response, we choose the next speaker. Again, this may not always be the right thing. Imagine a chatty, opinionated co-worker in a brainstorming session. They end up taking a lot of time. To address this, we can use a weighted selection where a few agents are selected more often than others.\nPriority-Based Selection In this method, you can prioritize one speaker over others.\n1 2 3 4 5 6 7 8 9 10 def weighted_selection(state: GroupChatStateSnapshot) -\u0026gt; str | None: \u0026#34;\u0026#34;\u0026#34;Weighted speaker selection - Creative speaks more often.\u0026#34;\u0026#34;\u0026#34; round_index = state[\u0026#34;round_index\u0026#34;] if round_index \u0026gt;= 8: return None # Pattern: Creative, Critic, Creative, Synthesizer, repeat pattern = [\u0026#34;Creative\u0026#34;, \u0026#34;Critic\u0026#34;, \u0026#34;Creative\u0026#34;, \u0026#34;Synthesizer\u0026#34;] return pattern[round_index % len(pattern)] In this example, the creative agent is given greater weight than the others.\nLast speaker exclusion In this pattern, we ensure that the same agent doesn\u0026rsquo;t speak twice in a row.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def no_repeat_selection(state: GroupChatStateSnapshot) -\u0026gt; str | None: \u0026#34;\u0026#34;\u0026#34;Prevent the same agent from speaking consecutively.\u0026#34;\u0026#34;\u0026#34; participants = state[\u0026#34;participants\u0026#34;] history = state[\u0026#34;history\u0026#34;] round_index = state[\u0026#34;round_index\u0026#34;] if round_index \u0026gt;= 6: return None participant_names = list(participants.keys()) # Find who spoke last last_speaker = None for msg in reversed(history): if hasattr(msg, \u0026#39;author_name\u0026#39;) and msg.author_name: last_speaker = msg.author_name break # Filter out last speaker available = [p for p in participant_names if p != last_speaker] # Simple rotation among available speakers return available[round_index % len(available)] In this method, from the message history, we identify who spoke last and remove that speaker from the list of potential participants.\nLLM-based speaker selection For scenarios that require dynamic speaker selection, you can assign an LLM to be the manager.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 moderator = chat_client.create_agent( name=\u0026#34;Moderator\u0026#34;, instructions=\u0026#34;\u0026#34;\u0026#34;You are a discussion moderator. Based on the conversation so far, decide who should speak next: - \u0026#39;Creative\u0026#39; when new ideas are needed - \u0026#39;Critic\u0026#39; when ideas need evaluation - \u0026#39;Synthesizer\u0026#39; when it\u0026#39;s time to consolidate Return ONLY the agent name, nothing else.\u0026#34;\u0026#34;\u0026#34;, temperature=0.3, ) workflow = ( GroupChatBuilder() .set_manager(moderator, display_name=\u0026#34;Moderator\u0026#34;) .participants([creative, critic, synthesizer]) .with_max_rounds(10) .build() ) In this approach, the moderator agent becomes the manager of the conversation and selects the appropriate agent based on the goal. This agent has access to the full conversation history, returns the name of the next speaker, and can make dynamic decisions based on context.\nIn addition to speaker selection, identifying and implementing an appropriate termination condition is important. There are multiple ways to do this.\nReturn None from Selection Function The simplest approach is to return None when you want to stop:\n1 2 3 4 def select_speaker(state: GroupChatStateSnapshot) -\u0026gt; str | None: if state[\u0026#34;round_index\u0026#34;] \u0026gt;= 10: return None # End after 10 rounds # ... selection logic Implementing a custom termination condition We can use with_termination_condition() for complex logic.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def check_consensus(state: GroupChatStateSnapshot) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;End when agents reach consensus.\u0026#34;\u0026#34;\u0026#34; for entry in state[\u0026#34;history\u0026#34;]: text = entry.text.upper() if hasattr(entry, \u0026#39;text\u0026#39;) else \u0026#34;\u0026#34; if \u0026#34;CONSENSUS REACHED\u0026#34; in text or \u0026#34;AGREED\u0026#34; in text: return True return False workflow = ( GroupChatBuilder() .set_select_speakers_func(select_speaker) .participants([creative, critic, synthesizer]) .with_termination_condition(check_consensus) .build() ) As shown in the LLM-based speaker selection example, one additional way to terminate a conversation is to specify a maximum number of rounds using with_max_rounds().\nThe GroupChat workflow pattern enables dynamic, turn-based conversations between multiple AI agents. Key takeaways.\nSpeaker selection is the control mechanism. You can use functions or an LLM manager. All agents share the conversation history, enabling true dialogue. Termination conditions prevent infinite loops. Design complementary personas for productive discussions. Use streaming events to monitor conversation progress GroupChat excels when you need agents to build on each other\u0026rsquo;s ideas; brainstorm, debate, code-review, and collaborate on problem-solving.\n","date":"2025-10-18T00:00:00Z","excerpt":"Building dynamic multi-agent discussions where AI agents collaborate, debate, and converge on solutions with GroupChatBuilder","permalink":"https://ravichaganti.com/blog/group-chat-workflows-in-microsoft-agent-framework/","series":"Microsoft Agent Framework","title":"Group chat workflows in Microsoft Agent Framework"},{"categories":["Docker","cagent","Agents"],"content":"Docker is one company that has been really taking advantage of the AI wave. True to its philosophy, it is helping developers with new tools and frameworks to simplify AI application development. Starting with Docker MCP catalog, MCP toolkit, Model Runner, MCP gateway, and cagent, Docker is certainly at the forefront of AI agent developer experience. In this article, we will get started with cagent.\nIntroduction Docker cagent is an open-source, multi-agent AI runtime that lets you build, orchestrate, and share teams of specialized AI agents defined declaratively in YAML. Instead of wiring together complex agent frameworks in code, you describe what each agent does, which tools it can access, and how agents delegate to one another. cagent handles the rest: model communication, tool orchestration, context isolation, and inter-agent coordination. It ships bundled with Docker Desktop 4.49+ and can distribute agent configurations as OCI artifacts through Docker Hub, treating agents with the same rigor as container images. Written in Go and currently labeled experimental, cagent represents Docker\u0026rsquo;s bet that the agent ecosystem needs the same standardization, portability, and trust infrastructure that containers brought to application deployment.\nWith the evolution of various agent frameworks, we are moving from a simple request-response pattern for implementing monolithic agents to orchestrating specialized agents. cagent helps simplify this transition with its declarative approach.\ncagent sits at the top of Docker\u0026rsquo;s AI stack. Docker Model Runner (DMR) provides local inference with no API keys. The MCP Gateway orchestrates external tools in isolated containers. The MCP Catalog offers curated tool servers on Docker Hub. cagent ties these layers together. It uses DMR for local models, routes tool calls through the MCP Gateway, and packages agent configurations as OCI artifacts for distribution. If Docker containers standardized how applications ship, cagent aims to standardize how AI agents ship.\nArchitecture cagent\u0026rsquo;s architecture centers on a few core concepts that govern how agents are defined, how they communicate, and how they access tools. Every agent configuration has a root agent, which is the entry point that receives user messages. The root agent can work alone or coordinate a team through two delegation mechanisms.\nSub-agents Sub-agents implement hierarchical task delegation. A parent agent assigns a specific task to a child agent using an auto-generated transfer_task tool. The child agent executes in its own isolated context. Each child agent has its own model, instructions, and toolset and returns results to the parent. The parent retains control and can delegate to multiple sub-agents in a sequence or combine their outputs. Sub-agents can have sub-agents themselves, enabling arbitrarily deep hierarchies.\nHandoffs The handoff mechanism implements peer-to-peer conversation transfer. When an agent encounters a topic outside its expertise, it hands the entire conversation to a more suitable peer using the transfer_to_agent tool. The receiving agent takes over completely. This pattern works well for routing conversations between domain experts without a central coordinator.\nEach agent maintains its own conversation context. When a root agent delegates to a sub-agent, the sub-agent receives only the specific task description, not the parent\u0026rsquo;s full conversation history. This isolation keeps contexts focused and prevents token-bloat in deep hierarchies.\nGetting started Docker Desktop 4.49+ includes cagent integration. After updating Docker Desktop, verify with:\n1 2 3 PS\u0026gt; cagent version cagent version v1.19.4 Commit: 7e631ef3f39ee56e4c0f2bb39eab73444c040601 You can also install cagent using the standalone binary available on the GitHub releases page.\ncagent supports models from Anthropic, Gemini, and OpenAI. For managed model providers, it is important to supply the API keys as environment variables.\n1 2 3 export ANTHROPIC_API_KEY=your_key export OPENAI_API_KEY=your_key export GOOGLE_API_KEY=your_key cagent also supports local models run using Docker Model Runner and an OpenAI-compatible endpoint.\nBasic agent cagent uses a YAML configuration file for defining the agentic application. The YAML skeleton structure is as shown below.\n1 2 3 4 5 version: \u0026#34;2\u0026#34; # Schema version (current: \u0026#34;2\u0026#34;) agents: {} # Required — agent definitions models: {} # Optional — named model configurations rag: {} # Optional — RAG knowledge sources metadata: {} # Optional — author, license, readme The \u0026ldquo;2\u0026rdquo; schema version is the current standard and is shipped with Docker Desktop 4.49+. Older files without a version field are treated as v1 and remain backward-compatible. The v2 schema supports the full feature set, including RAG, structured output, thinking budgets, and advanced delegation.\nWe will dive into configuration options as we progress in this walk-through. Let us start with a basic agent.\n1 2 3 4 5 6 7 8 9 agents: root: model: openai/gpt-4o description: A helpful coding assistant instruction: | You are a helpful coding assistant. Help me write and understand code. When I ask you to write code, please provide it in a markdown code block. When I ask you to explain code, please provide a clear, concise explanation. This basic agent uses a simple configuration and uses openai/gpt-4o for the model. You can run this agent using the cagent exec command.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 PS\u0026gt; cagent exec .\\cagent\\01-basic.yaml \u0026#34;Write a Python program to determine the nth factorial.\u0026#34; --- Agent: root --- To determine the nth factorial, you can use a simple iterative function. Here\u0026#39;s a Python program that calculates the factorial of a number \\( n \\): ```python def factorial(n): if n \u0026lt; 0: raise ValueError(\u0026#34;Factorial is not defined for negative numbers\u0026#34;) result = 1 for i in range(2, n + 1): result *= i return result # Example usage: n = 5 print(f\u0026#34;The factorial of {n} is: {factorial(n)}\u0026#34;) ``` ### Explanation: - This function `factorial` computes the factorial of a non-negative integer \\( n \\). - If \\( n \\) is less than 0, it raises a `ValueError` since factorials are only defined for non-negative integers. - It initializes `result` to 1 because the factorial of 0 is 1 by definition. - It uses a for loop to multiply `result` by each integer from 2 up to \\( n \\), inclusive. This iteratively calculates the factorial. - Finally, it returns the calculated `result`. You can replace `n = 5` with any non-negative integer to find its factorial. cagent exec takes a prompt and generates a response. If you prefer a conversational interface, you can use the cagent run command. This command opens a simple yet beautiful TUI.\nLet us try the same with a local model run using DMR.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 models: local-gemma: provider: dmr model: ai/gemma3:4B base_url: http://localhost:12434/v1 agents: root: model: local-gemma description: A helpful coding assistant instruction: | You are a helpful coding assistant. Help me write and understand code. When I ask you to write code, please provide it in a markdown code block. When I ask you to explain code, please provide a clear, concise explanation. In this example, we use the models object from the configuration to define the agent\u0026rsquo;s model configuration. local-gemma is the model configuration name we can reference in the agent configuration. If you are running this on a Windows system, ensure you also provide the base_url. By default, cagent tries to connect to a local socket for the model endpoint. This configuration can be run the same way we did earlier. The other supported model fields in the model configuration include provider, model, max_tokens, temperature, top_p, frequency_penalty, presence_penalty, base_url (for custom endpoints), token_key, parallel_tool_calls, and provider_opts for provider-specific settings. Alloy models let you rotate between multiple models by separating names with commas in the model field.\nThe model specification we saw in the first example is the inline shorthand specification. The prefix in the model name, such as openai or dmr, is used to route to the appropriate provider. model: auto lets cagent auto-select a provider based on available API keys.\nA simple multi-agent Now that we have a basic agent working, let us explore the real power of cagent. In this example, we will build a small team: a coordinator agent that delegates research to a researcher and writing to a writer. Each agent has its own model, instructions, and tools.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 version: \u0026#34;2\u0026#34; agents: root: model: openai/gpt-4o description: Coordinator that manages a research and writing team instruction: | You are a project coordinator. When a user asks you to write about a topic: 1. First, delegate research to the researcher agent 2. Then, pass the research findings to the writer agent 3. Present the final output to the user Always delegate. Do not research or write yourself. sub_agents: [researcher, writer] researcher: model: openai/gpt-4o-mini description: Researches topics and gathers key facts instruction: | You are a research specialist. When given a topic, provide a structured summary of key facts, important details, and relevant context. Be thorough but concise. writer: model: openai/gpt-4o-mini description: Writes polished content from research notes instruction: | You are a skilled technical writer. When given research notes, transform them into a well-structured, engaging article. Use clear language and logical flow. Do NOT use bullet points. There are a few important things to observe in this configuration. The root agent is always the entry point. It is the only agent that interacts directly with the user. The sub_agents field lists the agents that the root agent can delegate tasks to. Each sub-agent has its own model, description, and instruction. The description field is particularly important in multi-agent setups since cagent uses it to help the parent agent decide which sub-agent to call and what task to assign.\nNotice how we are using gpt-4o for the coordinator (which needs to understand complex instructions and make delegation decisions) and the cheaper gpt-4o-mini for the researcher and writer (which handles more focused, well-defined tasks). This is a practical cost-optimization pattern: use your most capable model where judgment matters, and cheaper models where the task is well scoped.\nLet us run this agent and see how it works.\n1 PS\u0026gt; cagent exec .\\cagent\\03-multi-agent.yaml \u0026#34;Write about the history of containerization in software\u0026#34; When you run this, you will see the delegation in action. cagent clearly labels which agent is active at each step. The root agent interprets the user request, delegates research to the researcher, receives findings, and then hands those findings to the writer for polishing.\nThere is also an alternative delegation mechanism called handoffs. Unlike sub-agents, where the parent retains control, a handoff transfers the entire conversation to another agent. This is useful when you want domain experts to take over completely.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 agents: root: model: openai/gpt-4o description: Routes user queries to the right expert instruction: | You are a routing agent. Based on the user\u0026#39;s question: - If it\u0026#39;s about Python, hand off to python_expert - If it\u0026#39;s about Go, hand off to go_expert - For general questions, answer directly handoffs: [python_expert, go_expert] python_expert: model: openai/gpt-4o-mini description: Expert Python developer instruction: | You are an expert Python developer. Help users with Python-related questions, code reviews, and debugging. go_expert: model: openai/gpt-4o-mini description: Expert Go developer instruction: | You are an expert Go developer. Help users with Go-related questions, code reviews, and debugging. The key difference: sub_agents uses a transfer_task tool in which the parent assigns a specific task and receives the results. handoffs uses a transfer_to_agent tool where the conversation moves entirely to the new agent. Think of sub-agents as a manager delegating work and handoffs as a receptionist routing you to the right department.\nThis article covered the foundations, how cagent\u0026rsquo;s architecture works with sub-agents and handoffs, and how to get started with basic and multi-agent configurations. But we have barely scratched the surface of what cagent can do. In the next parts of the series, we will dive into using tools and MCP, RAG with cagent, evaluating and sharing agents, and, finally, a complex multi-agent workflow that brings together all the learnings.\n","date":"2025-10-16T00:00:00Z","excerpt":"Docker cagent is an open-source, multi-agent AI runtime that lets you build, orchestrate, and share teams of specialized AI agents — all defined declaratively in YAML.","permalink":"https://ravichaganti.com/blog/docker-cagent-an-introduction/","series":"Docker cagent","title":"Docker cagent - An introduction"},{"categories":["Microsoft Agent Framework","Workflows","Agents"],"content":"In the previous articles, we explored Sequential and Concurrent workflow patterns in the Microsoft Agent Framework (MAF). Today, we dive into one of the most practical and widely-applicable patterns: the handoff workflow. The handoff pattern models real-world scenarios in which a conversation or task is transferred from one agent to another based on context, expertise, or role. Think of it like a customer support call center—you first speak with a frontline agent who assesses your issue and then routes you to the appropriate specialist.\nUnlike sequential workflows (where agents execute in a fixed order) or concurrent workflows (where agents work in parallel), handoff workflows are dynamic and conditional. The flow is determined at runtime based on the conversation context. Handoff workflows are useful in scenarios like:\nCustomer support systems: Route to refund, shipping, or technical specialists. Intake and triage: Assess requests and delegate to domain experts. Escalation workflows: Pass complex issues to senior agents. Multi-department routing: Direct inquiries to sales, support, or billing. Before building our handoff workflow, let\u0026rsquo;s understand the core concepts.\nThe coordinator is the entry point agent that assesses incoming requests and decides which specialist to hand off to. It acts as a dispatcher or router. The specialists are the participant agents. These are domain-specific agents with focused expertise. Each specialist handles a particular type of request. The framework automatically provides handoff tools to the coordinator, enabling it to transfer control to specialists. The agent simply calls the appropriate handoff function when it determines the right specialist. The interaction mode controls how the workflow handles the conversation:\n\u0026quot;autonomous\u0026quot;: Agents operate without waiting for user input between turns. \u0026quot;interactive\u0026quot;: Pauses for user input during the conversation. A termination condition is a function that examines the conversation and decides when the workflow should end. This prevents infinite loops and ensures conversations reach a natural conclusion.\nBuilding a customer support handoff system Let\u0026rsquo;s build a customer support system step by step. We\u0026rsquo;ll start simple and progressively add sophistication. First, let us define the coordinator.\n1 2 3 4 5 6 7 8 9 10 11 coordinator = chat_client.create_agent( name=\u0026#34;coordinator_agent\u0026#34;, instructions=\u0026#34;\u0026#34;\u0026#34;You are a frontline customer support agent. Assess the customer\u0026#39;s issue and hand off to the appropriate specialist: - For refund requests → hand off to refund_agent - For shipping issues → hand off to shipping_agent - For technical problems → hand off to technical_agent Use the handoff tools to transfer the conversation. After the specialist resolves the issue, thank the customer and end the conversation.\u0026#34;\u0026#34;\u0026#34;, ) Notice how the instructions explicitly tell the agent:\nWhat types of issues exist? Which specialist handles each type? How to perform the handoff (using tools). The agent names in the instructions (refund_agent and shipping_agent) must match the actual agent names; this is how the coordinator knows which handoff tool to invoke.\nWe will now build the participant or the specialist agents. Each specialist has focused expertise and clear instructions on how to handle their domain:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 refund_agent = chat_client.create_agent( name=\u0026#34;refund_agent\u0026#34;, instructions=\u0026#34;\u0026#34;\u0026#34;You handle refund requests. Ask for order details and process refunds. Once you have helped the customer, provide a clear resolution and end your response. Do not hand off back to the coordinator - just provide your final answer.\u0026#34;\u0026#34;\u0026#34;, ) shipping_agent = chat_client.create_agent( name=\u0026#34;shipping_agent\u0026#34;, instructions=\u0026#34;\u0026#34;\u0026#34;You resolve shipping issues. Track packages and update delivery status. For order #12345, inform the customer that you\u0026#39;ve located the package and it will be delivered within 2 business days. Provide a clear resolution and end your response. Do not ask follow-up questions.\u0026#34;\u0026#34;\u0026#34;, ) technical_agent = chat_client.create_agent( name=\u0026#34;technical_agent\u0026#34;, instructions=\u0026#34;\u0026#34;\u0026#34;You solve technical problems. Troubleshoot issues step by step. Once you have helped the customer, provide a clear resolution and end your response. Do not hand off back to the coordinator - just provide your final answer.\u0026#34;\u0026#34;\u0026#34;, ) With the coordinator and specialist agents in place, we need a termination condition. The termination condition prevents runaway conversations. It\u0026rsquo;s a function that returns True when the workflow should stop.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def should_terminate(conversation: list) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;End when the conversation includes resolution keywords or reaches length limit.\u0026#34;\u0026#34;\u0026#34; # Safety limit: end after too many turns if len(conversation) \u0026gt;= 6: return True # Content-based termination: look for resolution indicators for msg in conversation: if hasattr(msg, \u0026#39;text\u0026#39;) and msg.text: text_lower = msg.text.lower() resolution_keywords = [\u0026#39;resolved\u0026#39;, \u0026#39;delivered\u0026#39;, \u0026#39;refunded\u0026#39;, \u0026#39;fixed\u0026#39;, \u0026#39;thank you for contacting\u0026#39;] if any(word in text_lower for word in resolution_keywords): return True return False This function checks two conditions:\nConversation length: Prevents runaway loops by limiting turns. Resolution keywords: Detects when the issue has been resolved With the termination function ready, we can build the handoff workflow.\n1 2 3 4 5 6 7 8 workflow = ( HandoffBuilder(name=\u0026#34;customer_support\u0026#34;) .participants([coordinator, refund_agent, shipping_agent, technical_agent]) .set_coordinator(coordinator) .with_interaction_mode(\u0026#34;autonomous\u0026#34;) .with_termination_condition(should_terminate) .build() ) Let\u0026rsquo;s break down each builder method:\nMethod Purpose HandoffBuilder(name=\u0026quot;...\u0026quot;) Creates a named workflow for identification .participants([...]) Registers all agents that can participate .set_coordinator(agent) Designates the entry-point agent .with_interaction_mode(\u0026quot;autonomous\u0026quot;) Runs without waiting for user input .with_termination_condition(fn) Sets custom logic to end the workflow .build() Compiles the workflow configuration To see the handoff workflow in action, we can use streaming events.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 async def main(): print(\u0026#34;Running handoff workflow: Coordinator → Specialists\u0026#34;) print(\u0026#34;=\u0026#34; * 60) output_evt: WorkflowOutputEvent | None = None async for event in workflow.run_stream(\u0026#34;I never received my order #12345\u0026#34;): if isinstance(event, ExecutorInvokedEvent): print(f\u0026#34;⚡ Starting: {event.executor_id}\u0026#34;) elif isinstance(event, ExecutorCompletedEvent): print(f\u0026#34;✓ Completed: {event.executor_id}\u0026#34;) elif isinstance(event, WorkflowStatusEvent): if event.state == WorkflowRunState.IDLE: print(\u0026#34;\\n✅ Workflow completed!\u0026#34;) elif isinstance(event, WorkflowOutputEvent): output_evt = event # Display the conversation history if output_evt: print(\u0026#34;\\nCONVERSATION HISTORY\u0026#34;) messages = output_evt.data for msg in messages: if hasattr(msg, \u0026#39;role\u0026#39;): name = msg.author_name or msg.role.value print(f\u0026#34;[{name}]: {msg.text}\u0026#34;) The streaming API emits different event types that let you track workflow progress:\nEvent Type When It Fires Use Case ExecutorInvokedEvent An agent starts processing Show \u0026ldquo;Agent X is working\u0026hellip;\u0026rdquo; ExecutorCompletedEvent An agent finishes Update progress indicators WorkflowStatusEvent Workflow state changes Detect completion or errors WorkflowOutputEvent Final output is ready Capture the result The WorkflowRunState.IDLE status indicates that the workflow has finished processing.\nHere is the complete example.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 from agent_framework import ( HandoffBuilder, Role, WorkflowOutputEvent, ExecutorInvokedEvent, ExecutorCompletedEvent, WorkflowStatusEvent, WorkflowRunState, ) from agent_framework.azure import AzureOpenAIChatClient from azure.identity import DefaultAzureCredential import asyncio from dotenv import load_dotenv load_dotenv() chat_client = AzureOpenAIChatClient( credential=DefaultAzureCredential(), ) # Coordinator agent coordinator = chat_client.create_agent( name=\u0026#34;coordinator_agent\u0026#34;, instructions=\u0026#34;\u0026#34;\u0026#34;You are a frontline customer support agent. Assess the customer\u0026#39;s issue and hand off to the appropriate specialist: - For refund requests → hand off to refund_agent - For shipping issues → hand off to shipping_agent - For technical problems → hand off to technical_agent Use the handoff tools to transfer the conversation. After the specialist resolves the issue, thank the customer and end the conversation.\u0026#34;\u0026#34;\u0026#34;, ) # Specialist agents refund_agent = chat_client.create_agent( name=\u0026#34;refund_agent\u0026#34;, instructions=\u0026#34;\u0026#34;\u0026#34;You handle refund requests. Ask for order details and process refunds. Once you have helped the customer, provide a clear resolution and end your response. Do not hand off back to the coordinator - just provide your final answer.\u0026#34;\u0026#34;\u0026#34;, ) shipping_agent = chat_client.create_agent( name=\u0026#34;shipping_agent\u0026#34;, instructions=\u0026#34;\u0026#34;\u0026#34;You resolve shipping issues. Track packages and update delivery status. For order #12345, inform the customer that you\u0026#39;ve located the package and it will be delivered within 2 business days. Provide a clear resolution and end your response. Do not ask follow-up questions.\u0026#34;\u0026#34;\u0026#34;, ) technical_agent = chat_client.create_agent( name=\u0026#34;technical_agent\u0026#34;, instructions=\u0026#34;\u0026#34;\u0026#34;You solve technical problems. Troubleshoot issues step by step. Once you have helped the customer, provide a clear resolution and end your response. Do not hand off back to the coordinator - just provide your final answer.\u0026#34;\u0026#34;\u0026#34;, ) def should_terminate(conversation: list) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;End when the conversation includes resolution keywords or reaches length limit.\u0026#34;\u0026#34;\u0026#34; if len(conversation) \u0026gt;= 6: return True for msg in conversation: if hasattr(msg, \u0026#39;text\u0026#39;) and msg.text: text_lower = msg.text.lower() if any(word in text_lower for word in [\u0026#39;resolved\u0026#39;, \u0026#39;delivered\u0026#39;, \u0026#39;refunded\u0026#39;, \u0026#39;fixed\u0026#39;, \u0026#39;thank you for contacting\u0026#39;]): return True return False workflow = ( HandoffBuilder(name=\u0026#34;customer_support\u0026#34;) .participants([coordinator, refund_agent, shipping_agent, technical_agent]) .set_coordinator(coordinator) .with_interaction_mode(\u0026#34;autonomous\u0026#34;) # Complete without prompting for user input .with_termination_condition(should_terminate) .build() ) async def main(): print(\u0026#34;Running handoff workflow: Coordinator → Specialists\u0026#34;) print(\u0026#34;=\u0026#34; * 60) print(\u0026#34;Customer issue: I never received my order #12345\u0026#34;) print(\u0026#34;=\u0026#34; * 60) output_evt: WorkflowOutputEvent | None = None async for event in workflow.run_stream(\u0026#34;I never received my order #12345\u0026#34;): if isinstance(event, ExecutorInvokedEvent): print(f\u0026#34;⚡ Starting: {event.executor_id}\u0026#34;) elif isinstance(event, ExecutorCompletedEvent): print(f\u0026#34;✓ Completed: {event.executor_id}\u0026#34;) elif isinstance(event, WorkflowStatusEvent): if event.state == WorkflowRunState.IDLE: print(\u0026#34;\\n✅ Workflow completed!\u0026#34;) elif isinstance(event, WorkflowOutputEvent): output_evt = event # Display the final conversation if output_evt: print(\u0026#34;\\n\u0026#34; + \u0026#34;=\u0026#34; * 60) print(\u0026#34;CONVERSATION HISTORY\u0026#34;) print(\u0026#34;=\u0026#34; * 60) messages = output_evt.data if isinstance(messages, list): for i, msg in enumerate(messages, start=1): if hasattr(msg, \u0026#39;role\u0026#39;): name = msg.author_name or (\u0026#34;assistant\u0026#34; if msg.role == Role.ASSISTANT else \u0026#34;user\u0026#34;) print(f\u0026#34;\\n{\u0026#39;-\u0026#39; * 60}\u0026#34;) print(f\u0026#34;{i:02d} [{name}]\u0026#34;) print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\u0026#34;) print(msg.text) else: print(output_evt.data) if __name__ == \u0026#34;__main__\u0026#34;: asyncio.run(main()) Here is the expected output when you run this workflow.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 \u0026gt; python.exe .\\03-handoff.py Running handoff workflow: Coordinator → Specialists ============================================================ Customer issue: I never received my order #12345 ============================================================ ⚡ Starting: input-conversation ✓ Completed: input-conversation ⚡ Starting: coordinator_agent ✓ Completed: coordinator_agent ⚡ Starting: handoff-coordinator ✓ Completed: handoff-coordinator ⚡ Starting: shipping_agent ✓ Completed: shipping_agent ⚡ Starting: handoff-coordinator ✓ Completed: handoff-coordinator ✅ Workflow completed! ============================================================ CONVERSATION HISTORY ============================================================ ------------------------------------------------------------ 01 [user] ------------------------------------------------------------ I never received my order #12345 ------------------------------------------------------------ 02 [shipping_agent] ------------------------------------------------------------ Thank you — I’ve located the package for order #12345. It’s back in transit with the carrier and is scheduled to be delivered within 2 business days (by Tuesday, February 3, 2026). No action is needed on your part; we’ll continue to monitor until delivery. Apologies for the delay — the issue is resolved. In the advanced implementation of this workflow, a specialist can hand off to other specialists.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 senior_technical_agent = chat_client.create_agent( name=\u0026#34;senior_technical_agent\u0026#34;, instructions=\u0026#34;\u0026#34;\u0026#34;You handle escalated technical issues that require deep expertise. You receive cases from the technical_agent.\u0026#34;\u0026#34;\u0026#34;, ) technical_agent = chat_client.create_agent( name=\u0026#34;technical_agent\u0026#34;, instructions=\u0026#34;\u0026#34;\u0026#34;You solve technical problems. For complex issues you cannot resolve, hand off to senior_technical_agent.\u0026#34;\u0026#34;\u0026#34;, ) ..... workflow = ( HandoffBuilder(name=\u0026#34;support\u0026#34;) .set_coordinator(coordinator) .participants([refund_agent, shipping_agent, technical_agent]) .add_handoff(refund_agent, technical_agent) # Refund can transfer to Technical .add_handoff(shipping_agent, refund_agent) # Shipping can transfer to Refund .build() ) For scenarios that require a human-in-the-loop, you can set the interaction mode to interactive.\n1 2 3 4 5 6 7 workflow = ( HandoffBuilder(name=\u0026#34;customer_support\u0026#34;) .participants([coordinator, refund_agent, shipping_agent]) .set_coordinator(coordinator) .with_interaction_mode(\u0026#34;interactive\u0026#34;) .build() ) This pauses the workflow at natural points, allowing users to provide additional information or confirm actions.\nThe Handoff pattern enables dynamic, context-aware routing between agents. This is perfect for customer support, intake systems, and any scenario where different experts handle different concerns. By combining a coordinator, specialized agents, and smart termination logic, you can build sophisticated multi-agent systems that feel natural and efficient. When using this workflow:\nBe explicit about when and to whom an agent should hand off. Always include a length-based termination to prevent infinite loops. Ensure each agent excels at exactly one thing. In the next article, we\u0026rsquo;ll explore the group chat orchestration pattern for even more complex multi-agent collaboration scenarios.\n","date":"2025-10-16T00:00:00Z","excerpt":"Learn how to orchestrate AI agents in a concurrent workflow using Microsoft Agent Framework's HandoffBuilder","permalink":"https://ravichaganti.com/blog/handoff-workflows-in-microsoft-agent-framework/","series":"Microsoft Agent Framework","title":"Handoff workflows in Microsoft Agent Framework"},{"categories":["Microsoft Agent Framework","Workflows","Agents"],"content":"In the [previous article](Sequential workflows in Microsoft Agent Framework | Ravikanth Chaganti), we explored the sequential workflow pattern, a straightforward approach where agents process tasks one after another in a defined order. While sequential workflows are powerful for pipeline-style processing, they have a limitation: speed. When agents don\u0026rsquo;t depend on each other\u0026rsquo;s output, running them one at a time is inefficient.\nHere\u0026rsquo;s where the concurrent workflow pattern comes into play. This pattern allows multiple agents to work in parallel, dramatically reducing total execution time and enabling scenarios that require diverse perspectives to be analyzed simultaneously.\nIn this article, we\u0026rsquo;ll build up from a simple example to a sophisticated multi-analyst system that gathers insights from different perspectives and synthesizes them into a unified executive summary.\nThe concurrent pattern is useful in scenarios such as:\nIndependent analysis: Multiple agents can analyze the same input without needing each other\u0026rsquo;s results. Time-sensitive operations: You need results fast, and parallelism can speed things up. Diverse perspectives: You want varied viewpoints on the same topic (technical, business, ethical, and so on). Redundancy/voting: Multiple agents tackle the same problem, and you aggregate their answers. Let us explore this further using an example. In this example, we will create three analyst agents that analyze a topic across three facets: technical, business, and ethical.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 from agent_framework import ( ConcurrentBuilder, WorkflowOutputEvent, ExecutorInvokedEvent, ExecutorCompletedEvent, WorkflowStatusEvent, WorkflowRunState, ) from agent_framework.azure import AzureOpenAIChatClient from azure.identity import DefaultAzureCredential import asyncio from dotenv import load_dotenv load_dotenv() # Create chat client chat_client = AzureOpenAIChatClient( credential=DefaultAzureCredential(), ) # Create three specialized analyst agents technical_analyst = chat_client.create_agent( name=\u0026#34;TechnicalAnalyst\u0026#34;, instructions=\u0026#34;Analyze the topic from a technical/engineering perspective. Be concise.\u0026#34;, ) business_analyst = chat_client.create_agent( name=\u0026#34;BusinessAnalyst\u0026#34;, instructions=\u0026#34;Analyze the topic from a business/market perspective. Be concise.\u0026#34;, ) ethical_analyst = chat_client.create_agent( name=\u0026#34;EthicalAnalyst\u0026#34;, instructions=\u0026#34;Analyze the topic from an ethical/societal perspective. Be concise.\u0026#34;, ) # Build the concurrent workflow workflow = ( ConcurrentBuilder() .participants([technical_analyst, business_analyst, ethical_analyst]) .build() ) async def main(): print(\u0026#34;Running concurrent analysis with 3 analysts...\u0026#34;) print(\u0026#34;=\u0026#34; * 60) async for event in workflow.run_stream(\u0026#34;Analyze the impact of autonomous vehicles\u0026#34;): if isinstance(event, ExecutorInvokedEvent): print(f\u0026#34;⚡ Starting: {event.executor_id}\u0026#34;) elif isinstance(event, ExecutorCompletedEvent): print(f\u0026#34;✓ Completed: {event.executor_id}\u0026#34;) elif isinstance(event, WorkflowStatusEvent): if event.state == WorkflowRunState.IDLE: print(\u0026#34;\\n✅ Workflow completed!\u0026#34;) elif isinstance(event, WorkflowOutputEvent): print(f\u0026#34;\\n{\u0026#39;=\u0026#39;*60}\u0026#34;) print(\u0026#34;RESULTS\u0026#34;) print(f\u0026#34;{\u0026#39;=\u0026#39;*60}\u0026#34;) for msg in event.data: name = msg.author_name or \u0026#34;Assistant\u0026#34; print(f\u0026#34;\\n--- {name} ---\u0026#34;) print(msg.text) if __name__ == \u0026#34;__main__\u0026#34;: asyncio.run(main()) In this concurrent workflow, three agents are created, each with a different analytical perspective. ConcurrentBuilder creates a workflow that runs all participants in parallel. All three agents receive the same input: Analyze the impact of autonomous vehicles. They execute simultaneously, returning results as they complete.\nThe ConcurrentBuilder is a fluent builder pattern for constructing concurrent workflows:\n1 2 3 4 5 workflow = ( ConcurrentBuilder() .participants([agent1, agent2, agent3]) # Agents to run in parallel .build() # Finalize and return the workflow ) Unlike SequentialBuilder, where agents run one after another, passing output forward, ConcurrentBuilder fans out the input to all participants simultaneously.\nWith the help of events, we can track the progress of the agents as they complete.\n1 2 3 4 5 6 7 8 9 10 11 12 $ python concurrent.py Running concurrent analysis with 3 analysts... ============================================================ ⚡ Starting: TechnicalAnalyst ⚡ Starting: BusinessAnalyst ⚡ Starting: EthicalAnalyst ✓ Completed: BusinessAnalyst ✓ Completed: TechnicalAnalyst ✓ Completed: EthicalAnalyst .... ✅ Workflow completed! So far, we have received individual responses from each agent. But what if we want to combine their insights into a unified summary? This is where aggregators come in.\nWe can extend this workflow by adding an aggregator. It would be nice if another agent could take the response from the parallel workflow and summarize it for us.\nBuilding a custom aggregator function An aggregator is an async function that receives all agent responses and produces a combined output.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 from agent_framework import AgentExecutorResponse, ChatMessage, Role # Create a summarizer agent to synthesize results summarizer = chat_client.create_agent( name=\u0026#34;Summarizer\u0026#34;, instructions=\u0026#34;\u0026#34;\u0026#34;You are an expert synthesizer. You will receive analyses from three perspectives: - Technical/Engineering - Business/Market - Ethical/Societal Create a unified executive summary that: 1. Highlights key insights from each perspective 2. Identifies common themes 3. Notes tensions or trade-offs 4. Provides actionable recommendations Keep the summary concise but comprehensive.\u0026#34;\u0026#34;\u0026#34;, ) async def aggregate_with_summarizer(results: list[AgentExecutorResponse]) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Aggregate all analyst responses using the summarizer agent.\u0026#34;\u0026#34;\u0026#34; # Step 1: Collect all analyses analyses = [] for r in results: agent_name = r.executor_id # Get the last assistant message (the analysis) for msg in reversed(r.agent_run_response.messages): if msg.role.value == \u0026#34;assistant\u0026#34;: analyses.append(f\u0026#34;## {agent_name} Analysis\\n{msg.text}\u0026#34;) break # Step 2: Create prompt for summarizer combined = \u0026#34;\\n\\n---\\n\\n\u0026#34;.join(analyses) prompt = f\u0026#34;Please synthesize the following three analyses into a unified executive summary:\\n\\n{combined}\u0026#34; # Step 3: Run summarizer agent response = await summarizer.run([ChatMessage(role=Role.USER, text=prompt)]) return response.text This aggregator function can be used with ConcurrentBuilder().\n1 2 3 4 5 6 workflow = ( ConcurrentBuilder() .participants([technical_analyst, business_analyst, ethical_analyst]) .with_aggregator(aggregate_with_summarizer) .build() ) The .with_aggregator() method tells the workflow what to do with all the parallel results:\nAll three analysts run concurrently. Once all are complete, the results are passed to aggregate_with_summarizer. The summarizer creates a unified executive summary. The summary becomes the final WorkflowOutputEvent.data Here is the complete working example.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 from agent_framework import ( ConcurrentBuilder, AgentExecutorResponse, ChatMessage, Role, WorkflowOutputEvent, ExecutorInvokedEvent, ExecutorCompletedEvent, WorkflowStatusEvent, WorkflowRunState, ) from agent_framework.azure import AzureOpenAIChatClient from azure.identity import DefaultAzureCredential import asyncio from dotenv import load_dotenv load_dotenv() # Create chat client chat_client = AzureOpenAIChatClient( credential=DefaultAzureCredential(), ) # Create agents with different perspectives technical_analyst = chat_client.create_agent( name=\u0026#34;TechnicalAnalyst\u0026#34;, instructions=\u0026#34;Analyze the topic from a technical/engineering perspective. Be concise.\u0026#34;, ) business_analyst = chat_client.create_agent( name=\u0026#34;BusinessAnalyst\u0026#34;, instructions=\u0026#34;Analyze the topic from a business/market perspective. Be concise.\u0026#34;, ) ethical_analyst = chat_client.create_agent( name=\u0026#34;EthicalAnalyst\u0026#34;, instructions=\u0026#34;Analyze the topic from an ethical/societal perspective. Be concise.\u0026#34;, ) # Create aggregator/summarizer agent summarizer = chat_client.create_agent( name=\u0026#34;Summarizer\u0026#34;, instructions=\u0026#34;\u0026#34;\u0026#34;You are an expert synthesizer. You will receive analyses from three different perspectives: - Technical/Engineering - Business/Market - Ethical/Societal Your job is to create a unified executive summary that: 1. Highlights the key insights from each perspective 2. Identifies common themes across all analyses 3. Notes any tensions or trade-offs between perspectives 4. Provides actionable recommendations Keep the summary concise but comprehensive.\u0026#34;\u0026#34;\u0026#34;, ) # Custom aggregator function that uses the summarizer agent async def aggregate_with_summarizer(results: list[AgentExecutorResponse]) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Aggregate all analyst responses using the summarizer agent.\u0026#34;\u0026#34;\u0026#34; # Collect all analyses analyses = [] for r in results: agent_name = r.executor_id # Get the last assistant message (the analysis) for msg in reversed(r.agent_run_response.messages): if msg.role.value == \u0026#34;assistant\u0026#34;: analyses.append(f\u0026#34;## {agent_name} Analysis\\n{msg.text}\u0026#34;) break # Create prompt for summarizer combined = \u0026#34;\\n\\n---\\n\\n\u0026#34;.join(analyses) prompt = f\u0026#34;Please synthesize the following three analyses into a unified executive summary:\\n\\n{combined}\u0026#34; # Run summarizer agent response = await summarizer.run([ChatMessage(role=Role.USER, text=prompt)]) return response.text # Build concurrent workflow with custom aggregator workflow = ( ConcurrentBuilder() .participants([technical_analyst, business_analyst, ethical_analyst]) .with_aggregator(aggregate_with_summarizer) .build() ) async def main(): # Run the workflow with streaming events print(\u0026#34;Running concurrent analysis with 3 analysts + summarizer...\u0026#34;) print(\u0026#34;=\u0026#34; * 60) output_evt: WorkflowOutputEvent | None = None async for event in workflow.run_stream(\u0026#34;Analyze the impact of autonomous vehicles\u0026#34;): if isinstance(event, ExecutorInvokedEvent): print(f\u0026#34;⚡ Starting: {event.executor_id}\u0026#34;) elif isinstance(event, ExecutorCompletedEvent): print(f\u0026#34;✓ Completed: {event.executor_id}\u0026#34;) elif isinstance(event, WorkflowStatusEvent): if event.state == WorkflowRunState.IDLE: print(\u0026#34;\\n✅ Workflow completed!\u0026#34;) elif isinstance(event, WorkflowOutputEvent): output_evt = event # Display the aggregated summary if output_evt: print(f\u0026#34;\\n{\u0026#39;=\u0026#39;*60}\u0026#34;) print(\u0026#34;EXECUTIVE SUMMARY (Aggregated by Summarizer Agent)\u0026#34;) print(f\u0026#34;{\u0026#39;=\u0026#39;*60}\u0026#34;) print(output_evt.data) if __name__ == \u0026#34;__main__\u0026#34;: asyncio.run(main()) The design of an aggregator is important. Consider the following for designing a meaningful aggregator.\nWhat insights should be preserved from each agent? How should conflicts between perspectives be handled? What format does the final output need to be in? Agents in a concurrent workflow may complete at different speeds. The concurrent workflow waits for all to finish before aggregating. If you need partial results, consider using the events to track individual completions.\nIn the next article, we\u0026rsquo;ll explore the handoff workflow pattern, in which agents can dynamically transfer control to each other based on the conversation context. This is perfect for scenarios like customer support, where a coordinator routes requests to specialized agents. Stay tuned!\n","date":"2025-10-14T00:00:00Z","excerpt":"Learn how to orchestrate AI agents in a concurrent workflow using Microsoft Agent Framework's ConcurrentBuilder","permalink":"https://ravichaganti.com/blog/concurrent-workflows-in-microsoft-agent-framework/","series":"Microsoft Agent Framework","title":"Concurrent workflows in Microsoft Agent Framework"},{"categories":["Microsoft Agent Framework","Workflows","Agents"],"content":"A single AI agent is powerful and can do impressive things. But real-world applications often need multiple specialists working together, with each agent handling a specific task before passing results to the next. As with humans, in the agentic world we also need to break complex problems into manageable pieces and have specialized agents handle each task. We need one agent to review another\u0026rsquo;s work and handle complex tool invocation.\nTrying to cram all of this into one agent leads to:\nConfused outputs (the agent doesn\u0026rsquo;t know which \u0026ldquo;hat\u0026rdquo; to wear) Context overload (too many tools and instructions) Difficult debugging (hard to know what went wrong) Microsoft Agent Framework (MAF) makes this orchestration simple with its workflow patterns. Workflows in MAF connect multiple agents (and other executors) into a graph that defines how data flows between them. Think of it as the \u0026ldquo;plumbing\u0026rdquo; that connects your agents together. In this article, we\u0026rsquo;ll focus on the most fundamental pattern: Sequential Workflows.\nSequential Workflows The Sequential Pattern is the simplest and most intuitive workflow pattern. Agents process data sequentially, each building on the previous agent\u0026rsquo;s output.\nIn a sequential flow, each agent receives the accumulated conversation history (including all previous messages), processes the input, generates a response, appends it to the conversation, and passes the updated conversation to the next agent. These workflows are perfect for content pipelines (research → write → edit → publish), data transformation chains (extract → transform → load), review processes (draft → review → approve), and step-by-step analysis (collect data → analyze → summarize).\nBuilding a Sequential Workflow: Step by Step Let\u0026rsquo;s build a content creation pipeline that demonstrates the sequential pattern. We\u0026rsquo;ll start simple and gradually add complexity.\nFirst, install the required packages:\n1 2 $ pip install agent-framework-azure-ai --pre $ pip install python-dotenv azure-identity Set up your environment variables (create a .env file):\n1 2 AZURE_OPENAI_ENDPOINT=\u0026#34;https://your-resource.openai.azure.com/\u0026#34; AZURE_OPENAI_CHAT_DEPLOYMENT_NAME=\u0026#34;gpt-4o\u0026#34; Let\u0026rsquo;s start by importing everything we need:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from agent_framework import ( SequentialBuilder, ChatMessage, Role, WorkflowOutputEvent, ExecutorInvokedEvent, ExecutorCompletedEvent, WorkflowStatusEvent, WorkflowRunState, ) from agent_framework.azure import AzureOpenAIChatClient from azure.identity import DefaultAzureCredential import asyncio from typing import Any from dotenv import load_dotenv load_dotenv() SequentialBuilder in the MAF package agent_framework is used to build a sequential workflow. We will discuss other imports as we progress further.\nWe need a chat client for the workflow.\n1 2 3 chat_client = AzureOpenAIChatClient( credential=DefaultAzureCredential(), ) The DefaultAzureCredential handles authentication automatically. It tries multiple authentication methods in order:\nEnvironment variables Managed Identity Azure CLI (az login) Now let\u0026rsquo;s create three specialized agents for our content pipeline:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 researcher = chat_client.create_agent( name=\u0026#34;Researcher\u0026#34;, instructions=\u0026#34;Research the given topic and provide key facts and statistics.\u0026#34;, ) writer = chat_client.create_agent( name=\u0026#34;Writer\u0026#34;, instructions=\u0026#34;Take the research and write an engaging article draft.\u0026#34;, ) editor = chat_client.create_agent( name=\u0026#34;Editor\u0026#34;, instructions=\u0026#34;Review the draft for clarity, grammar, and engagement. Provide the final version.\u0026#34;, ) A sequential workflow is created using the SequentialBuilder().\n1 2 3 4 5 workflow = ( SequentialBuilder() .participants([researcher, writer, editor]) .build() ) That\u0026rsquo;s it! Just two method calls:\n.participants([...])- List your agents in the order they should execute. .build()- Construct the workflow. Under the hood, this creates:\nAn executor for each agent Edges connecting them in sequence A start point (first agent) and an end point (last agent) Running the Workflow with Streaming MAF supports real-time event streaming during workflow execution. This gives you visibility into what\u0026rsquo;s happening:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 async def main(): # Run the workflow with streaming events print(\u0026#34;Running sequential workflow: Researcher → Writer → Editor\u0026#34;) print(\u0026#34;=\u0026#34; * 60) output_evt: WorkflowOutputEvent | None = None async for event in workflow.run_stream(\u0026#34;Write about the future of AI agents\u0026#34;): if isinstance(event, ExecutorInvokedEvent): print(f\u0026#34;⚡ Starting: {event.executor_id}\u0026#34;) elif isinstance(event, ExecutorCompletedEvent): print(f\u0026#34;✓ Completed: {event.executor_id}\u0026#34;) elif isinstance(event, WorkflowStatusEvent): if event.state == WorkflowRunState.IDLE: print(\u0026#34;\\n✅ Workflow completed!\u0026#34;) elif isinstance(event, WorkflowOutputEvent): output_evt = event Let\u0026rsquo;s understand each event type:\nEvent When It Fires What It Contains ExecutorInvokedEvent When an agent starts processing executor_id - the agent\u0026rsquo;s name ExecutorCompletedEvent When an agent finishes executor_id WorkflowStatusEvent When the workflow state changes state - RUNNING, IDLE, ERROR WorkflowOutputEvent When the workflow produces output data - the final result The final output contains the complete conversation history.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # Display the final conversation if output_evt: print(\u0026#34;\\n\u0026#34; + \u0026#34;=\u0026#34; * 60) print(\u0026#34;FINAL CONVERSATION\u0026#34;) print(\u0026#34;=\u0026#34; * 60) messages: list[ChatMessage] | Any = output_evt.data for i, msg in enumerate(messages, start=1): name = msg.author_name or (\u0026#34;assistant\u0026#34; if msg.role == Role.ASSISTANT else \u0026#34;user\u0026#34;) print(f\u0026#34;\\n{\u0026#39;-\u0026#39; * 60}\u0026#34;) print(f\u0026#34;{i:02d} [{name}]\u0026#34;) print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\u0026#34;) print(msg.text) if __name__ == \u0026#34;__main__\u0026#34;: asyncio.run(main()) The WorkflowOutputEvent.data contains a list of ChatMessage objects with:\nrole: USER or ASSISTANT author_name: The agent\u0026rsquo;s name (for assistant messages) text: The message content Here is the complete example.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 from agent_framework import ( SequentialBuilder, ChatMessage, Role, WorkflowOutputEvent, ExecutorInvokedEvent, ExecutorCompletedEvent, WorkflowStatusEvent, WorkflowRunState, ) from agent_framework.azure import AzureOpenAIChatClient from azure.identity import DefaultAzureCredential import asyncio from typing import Any from dotenv import load_dotenv load_dotenv() # Create chat client chat_client = AzureOpenAIChatClient( credential=DefaultAzureCredential(), ) # Create specialized agents researcher = chat_client.create_agent( name=\u0026#34;Researcher\u0026#34;, instructions=\u0026#34;Research the given topic and provide key facts and statistics.\u0026#34;, ) writer = chat_client.create_agent( name=\u0026#34;Writer\u0026#34;, instructions=\u0026#34;Take the research and write an engaging article draft.\u0026#34;, ) editor = chat_client.create_agent( name=\u0026#34;Editor\u0026#34;, instructions=\u0026#34;Review the draft for clarity, grammar, and engagement. Provide the final version.\u0026#34;, ) async def main(): # Build sequential workflow workflow = ( SequentialBuilder() .participants([researcher, writer, editor]) .build() ) # Run the workflow with streaming events print(\u0026#34;Running sequential workflow: Researcher → Writer → Editor\u0026#34;) print(\u0026#34;=\u0026#34; * 60) output_evt: WorkflowOutputEvent | None = None async for event in workflow.run_stream(\u0026#34;Write about the future of AI agents\u0026#34;): if isinstance(event, ExecutorInvokedEvent): print(f\u0026#34;⚡ Starting: {event.executor_id}\u0026#34;) elif isinstance(event, ExecutorCompletedEvent): print(f\u0026#34;✓ Completed: {event.executor_id}\u0026#34;) elif isinstance(event, WorkflowStatusEvent): if event.state == WorkflowRunState.IDLE: print(\u0026#34;\\n✅ Workflow completed!\u0026#34;) elif isinstance(event, WorkflowOutputEvent): output_evt = event # Display the final conversation if output_evt: print(\u0026#34;\\n\u0026#34; + \u0026#34;=\u0026#34; * 60) print(\u0026#34;FINAL CONVERSATION\u0026#34;) print(\u0026#34;=\u0026#34; * 60) messages: list[ChatMessage] | Any = output_evt.data for i, msg in enumerate(messages, start=1): name = msg.author_name or (\u0026#34;assistant\u0026#34; if msg.role == Role.ASSISTANT else \u0026#34;user\u0026#34;) print(f\u0026#34;\\n{\u0026#39;-\u0026#39; * 60}\u0026#34;) print(f\u0026#34;{i:02d} [{name}]\u0026#34;) print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\u0026#34;) print(msg.text) if __name__ == \u0026#34;__main__\u0026#34;: asyncio.run(main()) When you run this workflow, you\u0026rsquo;ll see something like:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 Running sequential workflow: Researcher → Writer → Editor ============================================================ ⚡ Starting: Researcher ✓ Completed: Researcher ⚡ Starting: Writer ✓ Completed: Writer ⚡ Starting: Editor ✓ Completed: Editor ✅ Workflow completed! ============================================================ FINAL CONVERSATION ============================================================ ------------------------------------------------------------ 01 [user] ------------------------------------------------------------ Write about the future of AI agents ------------------------------------------------------------ 02 [Researcher] ------------------------------------------------------------ # Key Facts and Statistics on AI Agents ## Market Growth - The AI agent market is projected to reach $47.1 billion by 2030 - Autonomous AI systems are expected to handle 85% of customer interactions by 2027 ... ------------------------------------------------------------ 03 [Writer] ------------------------------------------------------------ # The Future of AI Agents: A New Era of Intelligent Automation The rise of AI agents represents one of the most transformative shifts... [Engaging article draft based on the research] ------------------------------------------------------------ 04 [Editor] ------------------------------------------------------------ # The Future of AI Agents: Reshaping How We Work and Live .... Sequential workflows in MAF provide a simple yet powerful way to orchestrate multiple AI agents. With just a few lines of code, you can create sophisticated pipelines where:\nEach agent specializes in one task Data flows naturally from one agent to the next You have full visibility through event streaming The framework handles all the orchestration complexity The SequentialBuilder makes this pattern incredibly easy to implement. You just define your agents, list them in order, and let MAF handle the rest.\n","date":"2025-10-12T00:00:00Z","excerpt":"Learn how to orchestrate AI agents in a step-by-step pipeline using Microsoft Agent Framework's SequentialBuilder","permalink":"https://ravichaganti.com/blog/sequential-workflows-in-microsoft-agent-framework/","series":"Microsoft Agent Framework","title":"Sequential workflows in Microsoft Agent Framework"},{"categories":["Microsoft Agent Framework","Foundry","Agents"],"content":"In my earlier post on Getting Started with Foundry Agents, I covered the basics of creating agents using the Azure AI Foundry Agents service and the azure-ai-projects SDK. In another article in this series, we looked at creating Azure AI agents with Microsoft Agent Framework (MAF). Agents created with MAF are local only where MAF acts as the runtime. In this follow-up, we will examine how to create Foundry agents using MAF.\nWhen working directly with the native SDK, the typical pattern involves:\nCreating an AIProjectClient Defining an agent with PromptAgentDefinition Managing conversations manually Handling responses through the OpenAI client wrapper Here\u0026rsquo;s what that looks like with the native SDK:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 from azure.ai.projects import AIProjectClient from azure.ai.projects.models import PromptAgentDefinition project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.create_version( agent_name=agent_name, definition=PromptAgentDefinition( model=foundry_model, instructions=instruction, ), ) # Manually manage conversations conversation = openai_client.conversations.create() response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;your question here\u0026#34;, ) While this approach gives you fine-grained control, it requires explicit conversation management and more boilerplate code. This is where the Microsoft Agent Framework comes into play. We have already seen the basics of MAF in an earlier article in this series. So, let\u0026rsquo;s look at the persistent agent pattern.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 import os import asyncio from dotenv import load_dotenv from typing import Annotated from agent_framework.azure import AzureAIClient from azure.identity.aio import AzureCliCredential from pydantic import Field load_dotenv() def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}°C\u0026#34; else: return \u0026#34;City not found\u0026#34; async def main() -\u0026gt; None: async with AzureCliCredential() as credential: async with ( AzureAIClient( credential=credential, ).create_agent( name=\u0026#34;WeatherAgent\u0026#34;, instructions=\u0026#34;You are a weatherman. Provide accurate and concise weather information based on user queries. Use provided tools to fetch current weather data.\u0026#34;, tools=get_weather, ) as agent, ): result = await agent.run(\u0026#34;What should I wear in Austin, Texas?\u0026#34;) print(f\u0026#34;Agent: {result}\u0026#34;) asyncio.run(main()) In both scenarios, an agent gets created in the Foundry.\nAs in the previous scenario, the agent is persisted in Foundry. Note that we are using AzureAIClient rather than AzureAIAgentClient. This is the recommended approach.\nTo use an existing Foundry agent with MAF, specify use_latest_version=True in the AzureAIClient definition.\nIn the subsequent parts of this series, we will examine other types of agents we can create with MAF.\n","date":"2025-10-10T00:00:00Z","excerpt":"The Microsoft Agent Framework represents a significant improvement in developer experience over the raw Foundry SDK. While the native SDK gives you complete control, the Agent Framework provides less boilerplate, type-safe tools, consistent patterns, and resource safety.","permalink":"https://ravichaganti.com/blog/building-persistent-ai-agents-with-microsoft-agent-framework-and-microsoft-foundry/","series":"Microsoft Agent Framework","title":"Building persistent AI Agents with Microsoft Agent Framework and Microsoft Foundry"},{"categories":["Microsoft Agent Framework","Agents"],"content":"The Microsoft Agent Framework (MAF) is an open-source development kit that combines the best ideas from Semantic Kernel and AutoGen projects. It provides a flexible foundation for building AI agents that can:\nProcess user inputs using Large Language Models (LLMs) Call tools and MCP servers to perform actions. Generate intelligent, context-aware responses. Manage conversation state across interactions. In this article, we\u0026rsquo;ll explore different examples that showcase different capabilities of AI agents.\nBefore diving into the examples, make sure you have:\nPython 3.10+ installed Azure CLI installed and authenticated (az login) Microsoft Foundry project set up with a deployed model. Required packages installed: 1 pip install agent-framework[azure-ai] azure-identity python-dotenv Environment variables configured: 1 2 export AZURE_AI_PROJECT_ENDPOINT=\u0026#34;https://\u0026lt;your-project\u0026gt;.services.ai.azure.com/api/projects/\u0026lt;project-id\u0026gt;\u0026#34; export AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-4o-mini\u0026#34; Basic Agent Let\u0026rsquo;s start with the simplest possible agent - one that requires minimal setup and demonstrates the core concepts.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 import asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( project_endpoint=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34;, model_deployment_name=\u0026#34;gpt-4o\u0026#34;, credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a weather man. Provide accurate and concise weather information based on user queries.\u0026#34;, ) as agent, ): query = \u0026#34;What\u0026#39;s the weather like in Bengaluru?\u0026#34; result = await agent.run(query) print(f\u0026#34;Agent: {result}\\n\u0026#34;) asyncio.run(main()) All Azure AI agent operations are async. Using the async/await pattern for better performance and scalability. The async with pattern ensures proper resource cleanup, preventing connection leaks. Both the credential and agent are managed as async context managers, ensuring proper cleanup. AzureCliCredential authenticates using your Azure CLI session - no need to manage API keys in your code or environment variables.\nAzureAIAgentClient is used to create an AI agent client. It accepts:\nproject_endpoint: Your Azure AI Foundry project URL. This can also be supplied using an environment variable AZURE_AI_PROJECT_ENDPOINT\nmodel_deployment_name: The deployed model to use. This can also be supplied using an environment variable AZURE_AI_MODEL_DEPLOYMENT_NAME\ncredential: Your authentication credential.\nagent_name: A human-readable name of the agent created using this client.\nThe instructions parameter defines the agent\u0026rsquo;s persona and behavior. This is the system prompt that guides the agent\u0026rsquo;s response. The agent.run() method sends a message and returns the complete response.\nStreaming Responses For a better user experience, especially with longer responses, you often want to stream the output as it\u0026rsquo;s generated rather than waiting for the complete response.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 import asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a weather man. Provide accurate and concise weather information based on user queries.\u0026#34;, ) as agent, ): print(\u0026#34;Agent: \u0026#34;, end=\u0026#34;\u0026#34;, flush=True) async for chunk in agent.run_stream(\u0026#34;Tell me a short story\u0026#34;): if chunk.text: print(chunk.text, end=\u0026#34;\u0026#34;, flush=True) print() asyncio.run(main()) Instead of waiting for the complete response, run_stream() returns an async iterator that yields chunks as they arrive. Each chunk may contain text (or may be empty for control messages), so we check if chunk.text before printing. Using flush=True ensures each chunk appears immediately in the console, creating a typing effect. Also, notice that the project_endpoint and model_deployment_name are not provided in this example. The AzureAIAgentClient uses the values from the environment variables. Streaming responses are well-suited for interactive applications such as chatbots, command-line tools, long-form content (e.g., code generation), and user interfaces that require immediate feedback.\nTool Calling This is where agents become truly powerful. By giving agents access to tools (functions), they can interact with the real world - fetch data from APIs, perform calculations, or execute any Python code you define.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 from pandas._libs.hashtable import mode import asyncio from typing import Annotated from pydantic import Field from dotenv import load_dotenv import os from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential load_dotenv() def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}°C\u0026#34; else: return \u0026#34;City not found\u0026#34; async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( model_deployment_name=\u0026#34;gpt-4o\u0026#34;, credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a weather man. Provide accurate and concise weather information based on user queries. Use provided tools to fetch current weather data.\u0026#34;, tools=[get_weather], ) as agent, ): result = await agent.run(\u0026#34;I am travelling to Austin Texas tomorrow, what should I wear?\u0026#34;) print(result.text) asyncio.run(main()) The get_weather function is a typed tool function that uses Python type annotations, including Annotated and Pydantic\u0026rsquo;s Field, to describe its parameters. This metadata is automatically converted to the function schema that the LLM understands. You can pass your function(s) to the tools parameter when creating the agent. You can pass a single function or a list of functions. The agent automatically decides when to call your tool based on the user\u0026rsquo;s query. Ask about the weather? It calls get_weather. Ask about something else? It uses its built-in knowledge.\nThis example connects to the OpenWeatherMap API to fetch current weather data, demonstrating how agents can interact with external services. The store=True parameter enables conversation persistence on the server side.\nWhen you ask, \u0026ldquo;What should I wear in Austin, Texas?\u0026rdquo;, the agent:\nRecognizes it needs weather information Calls get_weather(\u0026quot;Austin Texas\u0026quot;) Receives the temperature data Combines that with its knowledge about clothing to give a contextual recommendation This pattern is fundamental to building agents that can take real-world actions.\nCode Interpreter What if your agent needs to perform complex calculations or data analysis? The Code Interpreter tool allows agents to write and execute Python code in a sandboxed environment.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 import asyncio from agent_framework import HostedCodeInterpreterTool from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from dotenv import load_dotenv load_dotenv() async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( model_deployment_name=\u0026#34;gpt-4o\u0026#34;, credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a Python coding expert. Use the provided tools to execute Python code to answer user queries accurately.\u0026#34;, tools=HostedCodeInterpreterTool(), ) as agent, ): result = await agent.run(\u0026#34;Calculate the 100th prime number.\u0026#34;) print(result.text) asyncio.run(main()) HostedCodeInterpreterTool is a built-in tool that provides a sandboxed Python execution environment hosted in Azure. The agent can write and run code without affecting your local system. The code runs in an isolated environment, allowing the agent to experiment without risk. With this, you can ask the agent to calculate the 100th prime number, and it will:\nWrite Python code to find prime numbers\nExecute the code in the sandbox\nReturn the result (541, by the way!)\nHosted code tools are important when your agent needs to handle scenarios such as complex mathematical formulas, statistical analysis, parsing, transforming, analyzing data, and generating data visualizations.\nThe Microsoft Agent Framework makes it remarkably easy to build sophisticated AI agents. Starting from a basic agent that takes less than 15 lines of code, you can progressively add capabilities like streaming, tool calling, code interpretation, and persistence. The key insight is that agents aren\u0026rsquo;t just chat interfaces - they\u0026rsquo;re intelligent systems that can reason about problems and take real actions. By giving them tools, you extend their capabilities beyond language into the real world. The future of AI is agentic, and with the Microsoft Agent Framework, you have everything you need to build that future today.\nResources Microsoft Agent Framework Documentation Microsoft Foundry OpenWeatherMap API (for the tool calling example) ","date":"2025-10-04T00:00:00Z","excerpt":"The world of AI agents is evolving rapidly, and Microsoft's Agent Framework provides a powerful, unified foundation for building intelligent agents that can reason, take actions, and interact with users naturally. In this blog post, I'll walk you through a series of practical examples that demonstrate how to create and use Azure AI agents using MAF and Python.","permalink":"https://ravichaganti.com/blog/building-ai-agents-with-microsoft-agent-framework/","series":"Microsoft Agent Framework","title":"Building AI Agents with Microsoft Agent Framework"},{"categories":["Microsoft","Foundry","Agents"],"content":"Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and capabilities such as observability, guardrails, and enterprise-ready governance for building AI agents. With the Foundry Agent service, developers can develop agents locally and deploy them to different environments seamlessly, leveraging the building blocks provided by Microsoft Foundry. Foundry Agent service provides the runtime that manages conversations, orchestrates tool calls, and integrates with identify and observability systems. As discussed in the previous article, agents use LLMs to reason and make decisions. Agents use tools to perform actions. Agents can participate in workflows or interact with other agents to achieve a bigger goal. This is achieved by treating agents as composable units.\nFigure 1: Agents are composable units (Source: Microsoft)\nIf you are looking to develop production-ready agents for your enterprise needs, the Foundry Agent service provides the necessary building blocks to help you achieve your goals. You can use the Foundry portal or the Foundry SDK to create agents. Before we switch gears to the Microsoft Agent Framework, let us quickly review how to use the Foundry SDK to create agents.\nFirst, let us install the necessary packages.\n1 2 $ pip install azure-ai-projects --pre $ pip install openai azure-identity python-dotenv You must create a Foundry project, deploy a model for the agent, and set the necessary environment variables.\n1 2 AZURE_AI_FOUNDRY_PROJECT_ENDPOINT=\u0026lt;Foundry-Endpoint\u0026gt; AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-5-mini\u0026#34; For this example, we will use Azure CLI to authenticate and DefaultAzureCredential to retrieve the authentication token within the Python program. Therefore, ensure that you have logged in using az login command.\nThe following example demonstrates creating an agent and chatting with the agent.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 import os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient from azure.ai.projects.models import PromptAgentDefinition load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] foundry_model = os.environ[\u0026#34;AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME\u0026#34;] instruction = \u0026#34;You are a helpful python programming tutor. Answer questions with code samples and explanation.\u0026#34; project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.create_version( agent_name=agent_name, definition=PromptAgentDefinition( model=foundry_model, instructions=instruction, ), ) openai_client = project_client.get_openai_client() conversation = openai_client.conversations.create() response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;how do I create a python function?\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;Build a python function that returns fibonacci sequence up to n\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) AIProjectClient() creates a Foundry client instance, which can be used to create an agent. This is done using agents.create_version() method. This creates an agent in the Foundry.\nTo chat with this agent, we will need an OpenAI client. This is created using the get_openai_client() method. Using this client, we can create a conversation to use with the agent and then chat with the agent. This enables carrying multi-turn conversations.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 C:\\\u0026gt; python.exe .\\00-microsoft-foundry-agent-create-agent.py Response output: A Python function is defined with the def keyword, a name, an optional parameter list, and a block of statements. It can optionally return a value with return. Here are concise examples and explanations. 1) Basic function ```python def greet(name): \u0026#34;\u0026#34;\u0026#34;Return a greeting string for name.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; print(greet(\u0026#34;Alice\u0026#34;)) # Hello, Alice! ``` .... If you want, tell me what kind of function you need and I’ll show a tailored example. Response output: Here are two common interpretations of \u0026#34;Fibonacci up to n\u0026#34; and simple, clear functions for each. Both start the sequence 0, 1, 1, 2, 3, ... 1) \u0026#34;Up to n terms\u0026#34; — return the first n Fibonacci numbers ```python from typing import List def fib_n_terms(n: int) -\u0026gt; List[int]: \u0026#34;\u0026#34;\u0026#34;Return a list of the first n Fibonacci numbers (starting with 0). fib_n_terms(0) -\u0026gt; [] fib_n_terms(1) -\u0026gt; [0] fib_n_terms(5) -\u0026gt; [0, 1, 1, 2, 3] \u0026#34;\u0026#34;\u0026#34; if n \u0026lt;= 0: return [] if n == 1: return [0] seq = [0, 1] while len(seq) \u0026lt; n: seq.append(seq[-1] + seq[-2]) return seq # Example print(fib_n_terms(6)) # [0, 1, 1, 2, 3, 5] ``` .... Notes - Time complexity for these is O(n) (n = number of terms produced or upper bound on produced terms). - Use the \u0026#34;n terms\u0026#34; function when the caller wants a fixed-length sequence; use the \u0026#34;up to value\u0026#34; or generator when limited by a maximum value. - If you prefer the sequence to start at 1, 1 instead of 0, 1, I can show that variant too. Chat with an existing agent To chat with an existing Foundry agent, we need to obtain an agent instance. This is done using project_client.agents.get(agent_name=\u0026quot;PyTutor\u0026quot;) method.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient import os from dotenv import load_dotenv load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.get(agent_name=agent_name) print(f\u0026#34;Retrieved agent: {agent.name}\u0026#34;) openai_client = project_client.get_openai_client() # Reference the agent to get a response response = openai_client.responses.create( input=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me what you can help with.\u0026#34;}], extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent.name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) Once we have an agent instance, we can use the OpenAI client to chat with the agent and retrieve a response.\n1 2 3 4 5 6 7 C:\\\u0026gt; python.exe .\\01-microsoft-foundry-agent-chat-with-existing-agent.py Retrieved agent: PyTutor Response output: I’m a Python programming tutor — I can help with pretty much anything related to learning, writing, debugging, and improving Python code. Here’s a quick summary of what I can do and how I’ll help you. What I can help with - Basics: syntax, variables, control flow, functions, classes, modules, file I/O. .... As shown earlier, agents created with the Foundry SDK are available in the Foundry Portal. You can customize the agents further using the portal functionality.\nℹ️ Last updated: 7th December 2025 ","date":"2025-10-03T00:00:00Z","excerpt":"Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and other aspects such as observability, guardrails, and enterprise-ready governance for creating AI agents.","permalink":"https://ravichaganti.com/blog/getting-started-with-foundry-agents/","series":"Microsoft Foundry","title":"Getting Started with Foundry Agents"},{"categories":["Microsoft","Agent Framework","Agents"],"content":"I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nFigure 1: Agent Framework (Source: Microsoft)\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. In addition, it supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 import asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, resulting in more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 from agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\nℹ️ Last updated: 6th December 2025 ","date":"2025-10-02T00:00:00Z","excerpt":"Microsoft Agent Framework is an open-source development kit for building AI agents and multi-agent workflows.","permalink":"https://ravichaganti.com/blog/introduction-to-microsoft-agent-framework/","series":"Microsoft Agent Framework","title":"Introduction to Microsoft Agent Framework"},{"categories":["inferencing","local serving"],"content":"In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\n1 winget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\n1 2 3 4 PS C:\\\u0026gt; foundry service status 🟢 Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\n1 2 3 4 5 PS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... 🔴 Service is stopped. 🟢 Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\n1 2 PS C:\\\u0026gt; foundry cache location 💾 Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 PS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\n1 2 3 4 5 6 7 8 9 10 PS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? 🧠 Thinking... 🤖 Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\n1 2 3 PS C:\\\u0026gt; foundry model load qwen2.5-0.5b 🕓 Loading model... 🟢 Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\n1 2 3 4 PS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID 🟢 qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\n1 2 PS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 PS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 import openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nℹ️ Last updated: 18th November 2025 ","date":"2025-07-17T00:00:00Z","excerpt":"There are several options available for running Large Language Model (LLM) inference locally. Foundry Local by Microsoft is a new entrant.","permalink":"https://ravichaganti.com/blog/local-model-serving-using-foundry-local/","series":"Local LLM Serving","title":"Local model serving - Using Foundry Local"},{"categories":["inferencing","local serving"],"content":"In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub. Manage local models Run and interact with models both from the command line and the Docker Desktop GUI. Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 PS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\n1 2 3 PS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\n1 2 3 4 PS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\n1 2 3 4 5 6 7 8 PS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\n1 2 3 4 5 6 7 8 9 10 11 curl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; ℹ️ Last updated: 30th September 2025 ","date":"2025-07-16T00:00:00Z","excerpt":"Docker Model Runner — a faster, simpler way to run and test AI models locally, right from your existing workflow. Whether you’re experimenting with the latest LLMs or deploying to production, Model Runner brings the performance and control you need, without the friction.","permalink":"https://ravichaganti.com/blog/local-model-serving-using-docker-model-runner/","series":"Local LLM Serving","title":"Local model serving - Using Docker model runner"},{"categories":["inferencing","local serving"],"content":"In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\n1 2 3 4 5 6 7 8 9 10 11 12 curl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nℹ️ Last updated: 25th September 2025 ","date":"2025-07-15T00:00:00Z","excerpt":"There are several options available for running Large Language Model (LLM) inference locally. LM Studio is one such option. It is more comprehensive and offers some great features.","permalink":"https://ravichaganti.com/blog/local-model-serving-using-lm-studio/","series":"Local LLM Serving","title":"Local model serving - Using LM Studio"},{"categories":["inferencing","local serving"],"content":"As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 PS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\n1 2 3 4 5 6 7 8 9 10 11 PS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ▕███████████████▏ 2.0 GB pulling 966de95ca8a6: 100% ▕███████████████▏ 1.4 KB pulling fcc5a6bec9da: 100% ▕███████████████▏ 7.7 KB pulling a70ff7e570d9: 100% ▕███████████████▏ 6.0 KB pulling 56bb8bd477a5: 100% ▕███████████████▏ 96 B pulling 34bb5ab01051: 100% ▕███████████████▏ 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\n1 2 3 4 5 6 7 PS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\n1 2 3 PS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\n1 2 3 4 5 6 PS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 PS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\n1 2 PS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve ℹ️ The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails. Ollama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\n1 2 3 4 5 6 7 8 9 10 11 from ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n1 2 (.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nℹ️ Last updated: 29th September 2025 ","date":"2025-07-14T00:00:00Z","excerpt":"There are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.","permalink":"https://ravichaganti.com/blog/local-model-serving-using-ollama/","series":"Local LLM Serving","title":"Local model serving - Using Ollama"},{"categories":["autogen","azure-openai","python"],"content":"Microsoft released [version 0.5.1 of the AutoGen](Release python-v0.5.1 · microsoft/autogen) stable build last week, and I quickly reviewed the release notes to verify if there were any breaking changes to what I had been writing so far using AutoGen. There is code refactoring to change the structure of base types. This release provides enhanced support for structured output. When working directly with model clients, you can set to a Pydantic model. Here is an example from the release notes.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 import asyncio from typing import Literal from autogen_core import CancellationToken from autogen_ext.models.openai import OpenAIChatCompletionClient from pydantic import BaseModel # Define the structured output format. class AgentResponse(BaseModel): thoughts: str response: Literal[\u0026#34;happy\u0026#34;, \u0026#34;sad\u0026#34;, \u0026#34;neutral\u0026#34;] model_client = OpenAIChatCompletionClient(model=\u0026#34;gpt-4o-mini\u0026#34;) # Generate a response using the tool. response = await model_client.create( messages=[ SystemMessage(content=\u0026#34;Analyze input text sentiment using the tool provided.\u0026#34;), UserMessage(content=\u0026#34;I am happy.\u0026#34;, source=\u0026#34;user\u0026#34;), ], json_ouput=AgentResponse, ) print(response.content) # Should be a structured output. # {\u0026#34;thoughts\u0026#34;: \u0026#34;The user is happy.\u0026#34;, \u0026#34;response\u0026#34;: \u0026#34;happy\u0026#34;} If you are using AssistantAgent, you can use the output_content_type to a Pydantic model that describes the output format. The agent will automatically reflect on the tool call result and generate a StructuredMessage with the output content type.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 import asyncio from typing import Literal from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.messages import TextMessage from autogen_agentchat.ui import Console from autogen_core import CancellationToken from autogen_core.tools import FunctionTool from autogen_ext.models.openai import OpenAIChatCompletionClient from pydantic import BaseModel # Define the structured output format. class AgentResponse(BaseModel): thoughts: str response: Literal[\u0026#34;happy\u0026#34;, \u0026#34;sad\u0026#34;, \u0026#34;neutral\u0026#34;] # Define the function to be called as a tool. def sentiment_analysis(text: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Given a text, return the sentiment.\u0026#34;\u0026#34;\u0026#34; return \u0026#34;happy\u0026#34; if \u0026#34;happy\u0026#34; in text else \u0026#34;sad\u0026#34; if \u0026#34;sad\u0026#34; in text else \u0026#34;neutral\u0026#34; # Create a FunctionTool instance with `strict=True`, # which is required for structured output mode. tool = FunctionTool(sentiment_analysis, description=\u0026#34;Sentiment Analysis\u0026#34;, strict=True) # Create an OpenAIChatCompletionClient instance that supports structured output. model_client = OpenAIChatCompletionClient( model=\u0026#34;gpt-4o-mini\u0026#34;, ) # Create an AssistantAgent instance that uses the tool and model client. agent = AssistantAgent( name=\u0026#34;assistant\u0026#34;, model_client=model_client, tools=[tool], system_message=\u0026#34;Use the tool to analyze sentiment.\u0026#34;, output_content_type=AgentResponse, ) stream = agent.on_messages_stream([TextMessage(content=\u0026#34;I am happy today!\u0026#34;, source=\u0026#34;user\u0026#34;)], CancellationToken()) await Console(stream) This release also added support for Azure AI Search as a tool, along with enhancements to SelectorGroupChat and code executors.\nWhile I was testing the older examples I had built for this series, I encountered a deprecation warning.\n1 2 .venv\\Lib\\site-packages\\autogen_ext\\models\\openai\\_openai_client.py:379: UserWarning: Missing required field \u0026#39;structured_output\u0026#39; in ModelInfo. This field will be required in a future version of AutoGen. validate_model_info(self._model_info) I dug a little deeper and understood that the ModelInfo object with the following structure is necessary in the future.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 model_info = ModelInfo( vision=False, structured_output=False, function_calling=False, streaming=False, json_output=False, family=\u0026#34;gpt-4o\u0026#34;, ) aopenai_client = AzureOpenAIChatCompletionClient( model=os.getenv(\u0026#34;AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\u0026#34;), api_key=os.getenv(\u0026#34;AZURE_OPENAI_API_KEY\u0026#34;), api_version=os.getenv(\u0026#34;AZURE_OPENAI_API_VERSION\u0026#34;), endpoint=os.getenv(\u0026#34;AZURE_OPENAI_ENDPOINT\u0026#34;), model_info=model_info, ) If you are using agents or need tool calling, you must set the function_calling to True.\n","date":"2025-04-06T00:00:00Z","excerpt":"Know what went into AutoGen 0.5.1 -- the open framework for creating multi-agent systems.","permalink":"https://ravichaganti.com/blog/changes-in-autogen-release-0_5_1/","series":"AutoGen","title":"Changes in Autogen release 0.5.1"},{"categories":null,"content":"In today\u0026rsquo;s article, you will build upon what you learned in the earlier article about the basics of kro. To demonstrate a few more kro concepts, we will use the famous sample voting application.\nWhile not very complex, this application has enough moving parts to define dependencies. In this article, we shall look at the custom API needed to instantiate this voting application on Kubernetes. In this example, we must bring up the redis and db services as ClusterIP services before the vote and result services, which are NodePort type.\nFirst, let us define the schema for all input parameters.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 apiVersion: kro.run/v1alpha1 kind: ResourceGraphDefinition metadata: name: voteapplication spec: schema: apiVersion: v1alpha1 kind: VoteApplication spec: name: string redis: name: string | default=\u0026#34;redis\u0026#34; svcName: string | default=\u0026#34;redis\u0026#34; imageName: string | default=\u0026#34;redis:alpine\u0026#34; port: integer | default=6379 db: name: string | default=\u0026#34;db\u0026#34; svcName: string | default=\u0026#34;db\u0026#34; imageName: string | default=\u0026#34;postgres:15-alpine\u0026#34; userName: string | default=\u0026#34;postgres\u0026#34; password: string | default=\u0026#34;postgres\u0026#34; port: integer | default=5432 worker: name: string | default=\u0026#34;worker\u0026#34; imageName: string | default=\u0026#34;dockersamples/examplevotingapp_worker\u0026#34; vote: name: string | default=\u0026#34;vote\u0026#34; imageName: string | default=\u0026#34;dockersamples/examplevotingapp_vote\u0026#34; port: integer | default=80 nodePort: integer | default=31000 svcName: string | default=\u0026#34;vote\u0026#34; result: name: string | default=\u0026#34;result\u0026#34; imageName: string | default=\u0026#34;dockersamples/examplevotingapp_result\u0026#34; port: integer | default=80 nodePort: integer | default=31001 svcName: string | default=\u0026#34;result\u0026#34; status: {} To deploy the containers as pods, we must define name, imageName, and port. We also need svcName and nodePort as input values for services associated with these deployments. For now, let us leave the status field empty. We will this later. Let us now look at the redis deployment and service.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 resources: - id: redisDeployment template: apiVersion: apps/v1 kind: Deployment metadata: name: ${schema.spec.redis.name} spec: replicas: 1 selector: matchLabels: app: ${schema.spec.redis.name} template: metadata: labels: app: ${schema.spec.redis.name} spec: containers: - name: ${schema.spec.redis.name} image: ${schema.spec.redis.imageName} ports: - containerPort: ${schema.spec.redis.port} - id: redisService template: apiVersion: v1 kind: Service metadata: name: ${schema.spec.redis.svcName} spec: selector: ${redisDeployment.spec.selector.matchLabels} ports: - protocol: TCP port: ${schema.spec.redis.port} targetPort: ${schema.spec.redis.port} By using selector: ${redisDeployment.spec.selector.matchLabels}, we define an explicit dependency on redisDeployment. The next resources are the db deployment and service.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 - id: dbDeployment template: apiVersion: apps/v1 kind: Deployment metadata: name: ${schema.spec.db.name} spec: replicas: 1 selector: matchLabels: app: ${schema.spec.db.name} template: metadata: labels: app: ${schema.spec.db.name} spec: containers: - name: ${schema.spec.db.name} image: ${schema.spec.db.imageName} env: - name: POSTGRES_USER value: ${schema.spec.db.userName} - name: POSTGRES_PASSWORD value: ${schema.spec.db.password} ports: - containerPort: ${schema.spec.db.port} - id: dbService template: apiVersion: v1 kind: Service metadata: name: ${schema.spec.db.svcName} spec: selector: ${dbDeployment.spec.selector.matchLabels} ports: - protocol: TCP port: ${schema.spec.db.port} targetPort: ${schema.spec.db.port} Note that the redis and db service names must be redis and db, respectively. These names are hard-coded in the application code.\nThe next set of deployments and services are worker, vote, and result.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 - id: workerDeployment template: apiVersion: apps/v1 kind: Deployment metadata: name: ${schema.spec.worker.name} spec: replicas: 1 selector: matchLabels: app: ${schema.spec.worker.name} template: metadata: labels: app: ${schema.spec.worker.name} spec: containers: - name: ${schema.spec.worker.name} image: ${schema.spec.worker.imageName} - id: voteDeployment template: apiVersion: apps/v1 kind: Deployment metadata: name: ${schema.spec.vote.name} spec: replicas: 1 selector: matchLabels: app: ${schema.spec.vote.name} template: metadata: labels: app: ${schema.spec.vote.name} spec: containers: - name: ${schema.spec.vote.name} image: ${schema.spec.vote.imageName} - id: voteService template: apiVersion: v1 kind: Service metadata: name: ${schema.spec.vote.svcName} spec: type: NodePort selector: ${voteDeployment.spec.selector.matchLabels} ports: - protocol: TCP port: ${schema.spec.vote.port} targetPort: ${schema.spec.vote.port} nodePort: ${schema.spec.vote.nodePort} - id: resultDeployment template: apiVersion: apps/v1 kind: Deployment metadata: name: ${schema.spec.result.name} spec: replicas: 1 selector: matchLabels: app: ${schema.spec.result.name} template: metadata: labels: app: ${schema.spec.result.name} spec: containers: - name: ${schema.spec.result.name} image: ${schema.spec.result.imageName} - id: resultService template: apiVersion: v1 kind: Service metadata: name: ${schema.spec.result.svcName} spec: type: NodePort selector: ${resultDeployment.spec.selector.matchLabels} ports: - protocol: TCP port: ${schema.spec.result.port} targetPort: ${schema.spec.result.port} nodePort: ${schema.spec.result.nodePort} The worker deployment does not need any service. However, the end-user accesses the vote and result in pods and, therefore, requires the NodePort service.\nThis resource group definition can be deployed using the kubectl apply command.\n1 $ kubectl apply -f vote-rgd.yaml As we do not need to change any input values to create an instance, we can use the following instance definition to create an instance of the custom API.\n1 2 3 4 5 6 7 8 9 10 11 apiVersion: kro.run/v1alpha1 kind: VoteApplication metadata: name: my-vote-1 spec: name: my-vote-1 redis: {} db: {} worker: {} vote: {} result: {} Again, this can be deployed using the kubectl apply command.\n1 $ kubectl apply -f vote_instance.yaml This is a good start. But, how do we make this more dynamic so that we can use it across multiple environments such as dev, staging, and production. This will the topic of our next article in this series.\n","date":"2025-03-14T00:00:00Z","excerpt":"Extending what you learned in the previous article on the basics of kro, this article demonstrates a few more features of kro with the help of a sample voting application.","permalink":"https://ravichaganti.com/blog/kube-resource-orchestrator-kro-deploying-the-sample-voting-application/","series":"Kubernetes Resource Orchestrator","title":"Kube Resource Orchestrator (KRO) - Deploying the sample voting application"},{"categories":null,"content":"Helm helps application developers and operators manage Kubernetes applications. It allows you to define, install, and upgrade Kubernetes applications. Helm uses a packaging format called charts, a collection of files describing a related set of Kubernetes resources. Other application packaging solutions, such as Kustomize, exist, but nothing comes close to the popularity Helm gained in the Cloud Native space. While Helm is certainly the most popular package manager, it comes with a few limitations. Helm does not offer an efficient dependency management solution. At deployment time, there is no way you can retrieve the dependent resource properties for subsequent resource deployment within the chart. Helm cannot handle custom resource definitions well. We must use values.yaml to supply custom values to the chart and its subcharts. This has no schema validation and, therefore, results in human errors at the time of deployment. Helm does not offer any post-deployment reconciliation support, which is important to ensure any drift in application configuration can be detected and corrected. Companies in the Cloud-native space have been looking at alternatives, and a new open-source project called kro may be that alternative.\nkro (Kube Resource Orchestrator) is an open-source, Kubernetes-native, and cloud-agnostic package manager for Kubernetes applications. It is a joint effort by Microsoft, Amazon, and Google, the three big cloud giants. Kro allows you to define custom Kubernetes APIs that define a group of Kubernetes resources and the logical operations between those resources. kro uses the Common Expression Language (CEL) expressions to define the logical operations, pass values between objects, and calculate the order in which the objects should be created.\nSo, what exactly is kro? Let\u0026rsquo;s explore that. Here is a simplified representation of what kro helps us do.\nAs mentioned earlier, kro allows us to define a custom API. Users and developers can use this custom API to provision instances of a group of resources represented by the custom API. In this image above, when a user creates an instance of the custom API, they will get an instance of the group of resources containing a Redis cache, Postgres DB, and three web service pods.\nSo, how do you get started using kro to define and provision custom APIs? First, you have to install kro.\n1 2 3 4 5 6 7 8 9 $ export KRO_VERSION=$(curl -sL \\ https://api.github.com/repos/kro-run/kro/releases/latest | \\ jq -r \u0026#39;.tag_name | ltrimstr(\u0026#34;v\u0026#34;)\u0026#39; ) $ helm install kro oci://ghcr.io/kro-run/kro/kro \\ --namespace kro \\ --create-namespace \\ --version=${KRO_VERSION} The helm install command creates a new namespace called kro.\n1 2 3 4 5 6 7 8 9 $ kubectl get all -n kro NAME READY STATUS RESTARTS AGE pod/kro-b667bd485-6lvrt 1/1 Running 2 (36m ago) 4d2h NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/kro 1/1 1 1 4d2h NAME DESIRED CURRENT READY AGE replicaset.apps/kro-b667bd485 1 1 1 4d2h Once the necessary resources are created, it is time to define a custom API. This custom API is called a ResourceGraphDefinition or rgd.\nThe following YAML spec is the general schema for a ResourceGraphDefinition.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 apiVersion: kro.run/v1alpha1 kind: ResourceGraphDefinition metadata: name: name-of-the-application spec: schema: # API version of the application apiVersion: v1alpha1 # This is the resource kind for your application kind: Application spec: # This is where you define the properties or configuration options for your application name: string | required=true description=\u0026#34;Name of the instance\u0026#34; replicas: integer | default=1 minimum=1 maximum=3 status: #Status fields to add to your application instance status resources: # An array of Kubernetes objects or custom resources such as another rgd that you want to provision For a working example, we will use what is provided in the documentation for now and build something better in a later post.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 apiVersion: kro.run/v1alpha1 kind: ResourceGraphDefinition metadata: name: my-application spec: schema: apiVersion: v1alpha1 kind: Application spec: name: string image: string | default=\u0026#34;nginx\u0026#34; ingress: enabled: boolean | default=false status: deploymentConditions: ${deployment.status.conditions} availableReplicas: ${deployment.status.availableReplicas} # Define the resources this API will manage. resources: - id: deployment template: apiVersion: apps/v1 kind: Deployment metadata: name: ${schema.spec.name} spec: replicas: 3 selector: matchLabels: app: ${schema.spec.name} template: metadata: labels: app: ${schema.spec.name} spec: containers: - name: ${schema.spec.name} image: ${schema.spec.image} ports: - containerPort: 80 - id: service template: apiVersion: v1 kind: Service metadata: name: ${schema.spec.name}-service spec: selector: ${deployment.spec.selector.matchLabels} ports: - protocol: TCP port: 80 targetPort: 80 - id: ingress includeWhen: - ${schema.spec.ingress.enabled} template: apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: ${schema.spec.name}-ingress annotations: kubernetes.io/ingress.class: alb alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/target-type: ip alb.ingress.kubernetes.io/healthcheck-path: /health alb.ingress.kubernetes.io/listen-ports: \u0026#39;[{\u0026#34;HTTP\u0026#34;: 80}]\u0026#39; alb.ingress.kubernetes.io/target-group-attributes: stickiness.enabled=true,stickiness.lb_cookie.duration_seconds=60 spec: rules: - http: paths: - path: \u0026#34;/\u0026#34; pathType: Prefix backend: service: name: ${service.metadata.name} port: number: 80 In this example, we have a deployment object with an nginx Pod, a ClusterIP service, and an ingress. You can use the interpolation syntax to refer to the values of different spec fields. For example, ${schema.spec.ingress.enabled} gives you the value associated with ingress.enabled from the spec section of the schema.\nAlthough all three are defined in a sequence, there is a dependency. You want the deployment to be ready before creating the service, and similarly, you want the service to be ready before creating an Ingress. In this example, these dependencies can be identified by referring to other objects. For example, the service resource definition uses selector: ${deployment.spec.selector.matchLabels} in its spec. The ingress resource uses name: ${service.metadata.name} in the spec.rules.paths.backend.service. kro uses these clues to build a Directed Acyclic Graph (DAG) for the resources defined in the rgd. You can provision this custom API using the kubectl apply command.\n1 2 3 4 5 6 $ kubectl apply -f rgd.yaml resourcegraphdefinition.kro.run/my-application created $ kubectl get rgd my-application -owide NAME APIVERSION KIND STATE TOPOLOGICALORDER AGE my-application v1alpha1 Application Active [\u0026#34;deployment\u0026#34;,\u0026#34;service\u0026#34;,\u0026#34;ingress\u0026#34;] 64s If everything goes as expected, you should see the rgd in active state and the topological order in which the resources will be provisioned. kro deploys your custom application API definition as a custom resource.\n1 2 $ kubectl get Applications No resources found in default namespace. Remember? Application is the value of the Kind field in our example. So, you have a resource named Applications. Alright. To realize the application through the custom API, you need to create an instance of the custom API.\n1 2 3 4 5 6 7 8 apiVersion: kro.run/v1alpha1 kind: Application metadata: name: my-application-instance spec: name: my-awesome-app ingress: enabled: true You can provide values of all configurable fields from the resource definition within the spec section of the instance. Once again, you can use kubectl apply command to provision this instance.\n1 2 3 4 5 6 $ kubectl apply -f instance.yaml application.kro.run/my-application-instance created $ k get Applications NAME STATE SYNCED AGE my-application-instance IN_PROGRESS False 5s The state of the custom resource instance will remain in progress until all the resources within the group are created successfully.\n1 2 3 4 5 6 7 8 9 10 11 $ kubectl get Applications my-application-instance -owide NAME STATE SYNCED AGE my-application-instance ACTIVE True 3m2s $ kubectl get all | grep my-awesome-app pod/my-awesome-app-9ccb8f99d-62lgx 1/1 Running 0 3m18s pod/my-awesome-app-9ccb8f99d-9989h 1/1 Running 0 3m18s pod/my-awesome-app-9ccb8f99d-zkszm 1/1 Running 0 3m18s service/my-awesome-app-service ClusterIP 10.99.199.102 \u0026lt;none\u0026gt; 80/TCP 3m13s deployment.apps/my-awesome-app 3/3 3 3 3m18s replicaset.apps/my-awesome-app-9ccb8f99d 3 3 3 3m18s As you see from this example, kro was able to create our resource graph definition as a custom resource and create an instance of that custom resource while taking care of the dependencies. You can also see the custom fields that are added to the custom resource instance\u0026rsquo;s status.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 $ kubectl get Applications my-application-instance -o yaml apiVersion: kro.run/v1alpha1 kind: Application metadata: annotations: .... name: my-application-instance namespace: default resourceVersion: \u0026#34;204024\u0026#34; uid: fef4985b-33e3-4643-8e7f-a540948331f5 spec: image: nginx ingress: enabled: true name: my-awesome-app status: availableReplicas: 3 conditions: - lastTransitionTime: \u0026#34;2025-03-13T11:36:12Z\u0026#34; message: Instance reconciled successfully observedGeneration: 1 reason: ReconciliationSucceeded status: \u0026#34;True\u0026#34; type: InstanceSynced deploymentConditions: - lastTransitionTime: \u0026#34;2025-03-13T11:36:11Z\u0026#34; lastUpdateTime: \u0026#34;2025-03-13T11:36:11Z\u0026#34; message: Deployment has minimum availability. reason: MinimumReplicasAvailable status: \u0026#34;True\u0026#34; type: Available - lastTransitionTime: \u0026#34;2025-03-13T11:36:01Z\u0026#34; lastUpdateTime: \u0026#34;2025-03-13T11:36:11Z\u0026#34; message: ReplicaSet \u0026#34;my-awesome-app-9ccb8f99d\u0026#34; has successfully progressed. reason: NewReplicaSetAvailable status: \u0026#34;True\u0026#34; type: Progressing state: ACTIVE This is a basic example and we shall look at something more useful that helps understand different aspects of using kro in a future post.\n","date":"2025-03-09T00:00:00Z","excerpt":"Kube Resource Orchestrator (KRO) introduces a new Kubernetes-native and cloud-agnostic way to group Kubernetes resources.","permalink":"https://ravichaganti.com/blog/kube-resource-orchestrator-kro-the-basics/","series":"Kubernetes Resource Orchestrator","title":"Kube Resource Orchestrator (KRO) - The Basics"},{"categories":["mcp","agents","python","azure"],"content":"The earlier article in this series introduced Anthropic\u0026rsquo;s Model Context Protocol. It presented an example of building a simple MCP server for use with the Claude desktop application. The hello-world example was a very basic implementation of an MCP server. In today\u0026rsquo;s article, we shall extend our knowledge of creating MCP servers to achieve more practical applications. We will build an MCP server to interact with Microsoft Azure resources.\nAnthropic made bootstrap MCP server development easy by providing the create-mcp-server package. To get started, you need to install this locally as a tool.\n1 2 3 4 5 6 # Using uvx (recommended) uvx create-mcp-server # Or using pip pip install create-mcp-server create-mcp-server To create a new MCP server, run the uvx create-mcp-server command and follow the prompts on the screen.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 PS C:\\GitHub\u0026gt; uvx create-mcp-server Creating a new MCP server project using uv. This will set up a Python project with MCP dependency. Let\u0026#39;s begin! Project name (required): azure-mcp-server Project description [A MCP server project]: An MCP server to interact with Azure resources Project version [0.1.0]: Project will be created at: C:\\GitHub\\azure-mcp-server Is this correct? [Y/n]: Y Using CPython 3.13.1 interpreter at: C:\\Program Files\\Python313\\python.exe Creating virtual environment at: .venv Resolved 21 packages in 466ms Built azure-mcp-server @ file:///C:/GitHub/azure-mcp-server Prepared 2 packages in 2.02s Installed 21 packages in 207ms + annotated-types==0.7.0 + anyio==4.8.0 + azure-mcp-server==0.1.0 (from file:///C:/GitHub/azure-mcp-server) + certifi==2025.1.31 + click==8.1.8 + colorama==0.4.6 + h11==0.14.0 + httpcore==1.0.7 + httpx==0.28.1 + httpx-sse==0.4.0 + idna==3.10 + mcp==1.3.0 + pydantic==2.10.6 + pydantic-core==2.27.2 + pydantic-settings==2.8.1 + python-dotenv==1.0.1 + sniffio==1.3.1 + sse-starlette==2.2.1 + starlette==0.46.0 + typing-extensions==4.12.2 + uvicorn==0.34.0 Claude.app detected. Would you like to install the server into Claude.app now? [Y/n]: Y Settings file location: C:\\Users\\ravik\\AppData\\Roaming\\Claude\\claude_desktop_config.json ✅ Created project azure-mcp-server in azure-mcp-server ℹ️ To install dependencies run: cd azure-mcp-server uv sync --dev --all-extras This command sets up all the dependencies needed to build an MCP server. Depending on your choice, it will also add the MCP server to the Claude Desktop configuration. The folder structure will be as follows.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 PS C:\\GitHub\\azure-mcp-server\u0026gt; ls Directory: C:\\GitHub\\azure-mcp-server Mode LastWriteTime Length Name ---- ------------- ------ ---- d----- 23-02-2025 16:26 .venv d----- 23-02-2025 16:26 src -a---- 23-02-2025 19:04 66 .gitattributes -a---- 23-02-2025 19:04 3238 .gitignore -a---- 23-02-2025 16:26 5 .python-version -a---- 23-02-2025 19:04 1088 LICENSE -a---- 23-02-2025 17:31 553 pyproject.toml -a---- 23-02-2025 19:04 146 README.md -a---- 23-02-2025 17:31 46157 uv.lock The src\\azure_mcp_server should contain all the business logic you need to enable Azure resource management integration. By default, it contains a sample MCP Server used to manage notes.\n1 2 3 4 5 6 7 8 9 10 PS C:\\GitHub\\azure-mcp-server\\src\\azure_mcp_server\u0026gt; ls Directory: C:\\GitHub\\azure-mcp-server\\src\\azure_mcp_server Mode LastWriteTime Length Name ---- ------------- ------ ---- d----- 04-03-2025 08:27 __pycache__ -a---- 23-02-2025 17:53 228 .env -a---- 04-03-2025 08:27 6561 server.py -a---- 23-02-2025 16:26 220 __init__.py This sample server implementation is a great start to learning how to implement different capabilities of an MCP server. We shall implement the tools\u0026rsquo; capability and, in the future, look at implementing prompts and resources as well.\nYou must decide how to authenticate to interact with the Azure resource management API. For the purpose of the demonstration, I have used client secret-based authentication. I documented these requirements in an earlier article. The client secret credential is better created by adding the keys and secrets as environment variables. We can use the .env file in the Python project to make this easy. You need to add the following key-value pairs to this file.\n1 2 3 4 AZURE_SUBSCRIPTION_ID=Sub-ID AZURE_TENANT_ID=Tenant-ID AZURE_CLIENT_ID=Client-ID AZURE_CLIENT_SECRET=Secret A .env file requires the load_dotenv function from the python-dotenv package, so add that as a dependency.\n1 uv add python-dotenv We will also need the Azure resource management packages.\n1 2 3 uv add azure.identity uv add azure-mgmt-resource uv add azure-mgmt-subscription With all the package dependencies added to the project, we can move toward adding the necessary tools. This is done in server.py. Before adding the code related to the tools, let us first add the functions needed to talk to the Azure resource management API.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 async def list_subscriptions() -\u0026gt; list[dict[str, Any]]: \u0026#34;\u0026#34;\u0026#34;List all subscriptions in the account. Args: None \u0026#34;\u0026#34;\u0026#34; credential = EnvironmentCredential() subscription_client = SubscriptionClient(credential) subscriptions = subscription_client.subscriptions.list() subscription_list = [] for subscription in list(subscriptions): subscription_info = { \u0026#34;id\u0026#34;: subscription.subscription_id, \u0026#34;name\u0026#34;: subscription.display_name, } subscription_list.append(subscription_info) return subscription_list async def list_resource_groups(subscription_id=None) -\u0026gt; list[dict[str, Any]]: \u0026#34;\u0026#34;\u0026#34;List all resource groups in the subscription. Args: subscription_id (str): The subscription ID. This is an optional parameter. \u0026#34;\u0026#34;\u0026#34; credential = EnvironmentCredential() if subscription_id is None: if \u0026#34;AZURE_SUBSCRIPTION_ID\u0026#34; not in os.environ: raise ValueError(\u0026#34;subscription_id must be provided or set as an environment variable.\u0026#34;) else: subscription_id = os.environ[\u0026#34;AZURE_SUBSCRIPTION_ID\u0026#34;] resource_client = ResourceManagementClient(credential, subscription_id) group_list = resource_client.resource_groups.list() resource_groups = [] for group in list(group_list): resource = { \u0026#34;name\u0026#34;: group.name, \u0026#34;location\u0026#34;: group.location, } resource_groups.append(resource) return resource_groups async def list_resources(resource_group, subscription_id) -\u0026gt; list[dict[str, Any]]: \u0026#34;\u0026#34;\u0026#34;List all resources in the resource group. Args: resource_group (str): The resource group name. subscription_id (str): The subscription ID. This is an optional parameter. \u0026#34;\u0026#34;\u0026#34; credential = EnvironmentCredential() if subscription_id is None: if \u0026#34;AZURE_SUBSCRIPTION_ID\u0026#34; not in os.environ: raise ValueError(\u0026#34;subscription_id must be provided or set as an environment variable.\u0026#34;) else: subscription_id = os.environ[\u0026#34;AZURE_SUBSCRIPTION_ID\u0026#34;] resource_client = ResourceManagementClient(credential, subscription_id) resources = resource_client.resources.list_by_resource_group(resource_group) resource_list = [] for resource in list(resources): resource_info = { \u0026#34;name\u0026#34;: resource.name, \u0026#34;type\u0026#34;: resource.type, \u0026#34;location\u0026#34;: resource.location, } resource_list.append(resource_info) return resource_list These three functions are a basic implementation for getting a list of subscriptions, all resource groups in a subscription, and all resources within a resource group. You must have the docstring inside each function to describe what the function is about and its arguments, and outputs. The code within these functions is self-explanatory. If you need a quick tour of Azure resource management in Python, look at the Azure Python SDK.\nAn MCP server is a JSON RPC server. Every MCP server exposes the list and call tool endpoints. These are defined using the handle_list_tools() and handle_call_tool() functions.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 @server.list_tools() async def handle_list_tools() -\u0026gt; list[types.Tool]: \u0026#34;\u0026#34;\u0026#34; List available tools. Each tool specifies its arguments using JSON Schema validation. \u0026#34;\u0026#34;\u0026#34; return [ types.Tool( name=\u0026#34;list-subscriptions\u0026#34;, description=\u0026#34;List all Azure subscriptions for the authenticated user.\u0026#34;, inputSchema={ \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: {}, \u0026#34;required\u0026#34;: [], }, ), types.Tool( name=\u0026#34;list-resource-groups\u0026#34;, description=\u0026#34;List all resource groups in an Azure subscription.\u0026#34;, inputSchema={ \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;subscription_id\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;}, }, \u0026#34;required\u0026#34;: [], }, ), types.Tool( name=\u0026#34;list-resources\u0026#34;, description=\u0026#34;List all resources in a resource group.\u0026#34;, inputSchema={ \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;subscription_id\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;}, \u0026#34;resource_group\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;} }, \u0026#34;required\u0026#34;: [\u0026#34;resource_group\u0026#34;], }, ) ] @server.call_tool() async def handle_call_tool( name: str, arguments: dict | None ) -\u0026gt; list[types.TextContent | types.ImageContent | types.EmbeddedResource]: \u0026#34;\u0026#34;\u0026#34; Handle tool execution requests. Tools can modify server state and notify clients of changes. \u0026#34;\u0026#34;\u0026#34; if name == \u0026#34;list-subscriptions\u0026#34;: response = await list_subscriptions() respText = \u0026#34;Subscriptions:\\n\u0026#34; for subscription in response: respText += f\u0026#34;ID: {subscription[\u0026#39;id\u0026#39;]}, Name: {subscription[\u0026#39;name\u0026#39;]}\\n\u0026#34; elif name == \u0026#34;list-resource-groups\u0026#34;: subscription_id = arguments.get(\u0026#34;subscription_id\u0026#34;, None) response = await list_resource_groups(subscription_id) respText = f\u0026#34;Resource Groups in {subscription_id}:\\n\u0026#34; for group in response: respText += f\u0026#34;Name: {group[\u0026#39;name\u0026#39;]}, Location: {group[\u0026#39;location\u0026#39;]}\\n\u0026#34; elif name == \u0026#34;list-resources\u0026#34;: subscription_id = arguments.get(\u0026#34;subscription_id\u0026#34;, None) resource_group = arguments.get(\u0026#34;resource_group\u0026#34;) result = await list_resources(resource_group, subscription_id) respText = f\u0026#34;Resources in {resource_group} in the {subscription_id}:\\n\u0026#34; for resource in result: respText += f\u0026#34;Name: {resource[\u0026#39;name\u0026#39;]}, Type: {resource[\u0026#39;type\u0026#39;]}, Location: {resource[\u0026#39;location\u0026#39;]}\\n\u0026#34; else: respText = \u0026#34;Invalid tool name.\u0026#34; return [ types.TextContent( type=\u0026#34;text\u0026#34;, text=respText ) ] These list and call functions are decorated using the list_tools() and call_tools() decorators respectively. The handle_list_tools() returns a list of tools where each element is of type types.Tool. The handle_call_tool() returns the output from the tool call as one of the return types specified in the function signature. Depending on the return type, you must construct the value. In this example, all tools call will respond with a dictionary. This response then gets converted to text content and is returned as types.TextContent type. This type requires type and text properties.\nAs the create-mcp-server command added the tool to the Claude Desktop application, you must be able to see the tools ready for use.\nOnce you confirm the available tools, you can try the following prompts.\nList all subscriptions I have access to in my Azure account Do I have any resource groups in the east-us region? List all virtual machines provisioned in my Research subscription. When you prompt, Claude will ask permission to use the available tools. If allowed, it can call the tools and get you the response.\nWith MCP, the possibilities are endless. I am developing the Azure MCP server as an open-source project, and I will continue to add more tools, prompts, and resources to it. Do check it out and leave a comment.\n","date":"2025-02-25T00:00:00Z","excerpt":"Extend knowledge of creating MCP servers to achieve more practical applications","permalink":"https://ravichaganti.com/blog/building-a-model-context-protocol-server-for-azure/","series":"Model Context Protocol","title":"Building a Model Context Protocol server for Azure"},{"categories":["autogen","azure-openai","python"],"content":"In the previous parts of this series on the Microsoft AutoGen framework, we looked at how to get started with the AutoGen framework. We examined the process of building a simple agent using the prebuilt AssistantAgent in AutoGen and explored the development of a multi-agent team.\nHowever, what is an agent in the context of AutoGen? How do agents communicate with each other? I needed answers to these questions before proceeding to the next step in using AutoGen to build larger, multi-agent applications. So, I started exploring the Core package upon which the autogen-chat is built. This article is a result of that exploration.\nTo understand the fundamentals, I used the quick-start example as a starting point and modified it. While the quick-start example demonstrates two agents coordinating to count down from 10 to 1, the modified example in this article uses two agents to generate ten random prime numbers.\nAn agent in the context of AutoGen is defined as:\nAn agent is a software entity that communicates via messages, maintains state, and performs actions in response to received messages or state changes. These actions may modify the agent’s state and produce external effects, such as updating message logs, sending new messages, executing code, or making API calls.\nEach agent is a self-contained unit of execution that can be developed, tested, and deployed independently.\nLet\u0026rsquo;s dive into the code.\nThe Basics Before anything else, ensure that you have installed the autogen-core package.\n1 $ pip install \u0026#34;autogen-core\u0026#34; AutoGen core is the event-driven actor framework for creating agentic workflows. This package provides message-passing, event-driven agents, as well as local and distributed agent runtimes. In the actor model, an actor is an entity that, in response to a message it receives, can concurrently send a finite number of messages to other actors, create a finite number of new actors, and designate the behavior to be used for the next message it receives. These actions could be carried out in parallel.\nLet us first define the agent and its logic for handling messages.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 from dataclasses import dataclass from typing import Callable import asyncio from sympy import isprime import random from autogen_core import DefaultTopicId, MessageContext, RoutedAgent, default_subscription, message_handler from autogen_core import AgentId, SingleThreadedAgentRuntime @dataclass class Message: content: int @default_subscription class Generator(RoutedAgent): def __init__(self, generate_val: Callable[[], int]) -\u0026gt; None: super().__init__(\u0026#34;A generator agent.\u0026#34;) self._generate_val = generate_val @message_handler async def handle_message(self, message: Message, ctx: MessageContext) -\u0026gt; None: val = self._generate_val() print(f\u0026#34;{\u0026#39;-\u0026#39;*80}\\nGenerator:\\nWe got {message.content} prime numbers. \\nNext number is {val}\u0026#34;) await self.publish_message(Message(content=val), DefaultTopicId()) @default_subscription class Checker(RoutedAgent): def __init__(self, run_until: Callable[[int], bool]) -\u0026gt; None: super().__init__(\u0026#34;A checker agent.\u0026#34;) self.count = 0 self._run_until = run_until self.prime_numbers = [] @message_handler async def handle_message(self, message: Message, ctx: MessageContext) -\u0026gt; None: if message.content not in self.prime_numbers: if isprime(message.content): self.count = self.count+1 self.prime_numbers.append(message.content) if not self._run_until(self.count): print(f\u0026#34;{\u0026#39;-\u0026#39;*80}\\nChecker:\\n{message.content} is a prime number, continue to next.\u0026#34;) await self.publish_message(Message(content=self.count), DefaultTopicId()) else: print(f\u0026#34;{\u0026#39;-\u0026#39;*80}\\nChecker:\\nWe got {self.count} prime numbers, stopping.\u0026#34;) print(f\u0026#34;{\u0026#39;-\u0026#39;*80}\\nPrime numbers generated: {self.prime_numbers}\u0026#34;) else: print(f\u0026#34;{\u0026#39;-\u0026#39;*80}\\nChecker:\\n{message.content} is not a prime number, generate next.\u0026#34;) await self.publish_message(Message(content=self.count), DefaultTopicId()) else: print(f\u0026#34;{\u0026#39;-\u0026#39;*80}\\nChecker:\\n{message.content} exists in the generated prime numbers, continue to next.\u0026#34;) await self.publish_message(Message(content=self.count), DefaultTopicId()) The Message data class defines the messages passed between agents. The Generator agent generates a random number, while the Checker agent verifies whether the random number received as a message is prime. Each agent defines a method for handling messages. This is indicated using the @message_handler decorator. The method you designate as the message handler must be asynchronous and must have three arguments: self, message, and context.\nThe agent runtime is a key component of the AutoGen framework. This provides the communication infrastructure required for agent-to-agent communication. The runtime handles agent creation and agents\u0026rsquo; lifecycle. In this example, we use a SingleThreadedAgentRuntime, which is most suitable for development and standalone applications. This runtime processes all messages using a single asyncio queue. We can create a runtime using the SingleThreadedAgentRuntime() method. Once a runtime is created, we need to register the agents.\n1 2 3 4 5 6 7 8 9 10 11 await Generator.register( runtime, \u0026#34;generator\u0026#34;, lambda: Generator(generate_val=lambda: random.randint(1, 15)), ) await Checker.register( runtime, \u0026#34;checker\u0026#34;, lambda: Checker(run_until=lambda count: count \u0026gt;= 5), ) Within each agent\u0026rsquo;s registration, we supply a lambda function that is used as an action when a message is received. The Generator agent uses random.randint(1, 15) as the lambda while the Checker agent checks for the condition count \u0026gt;= 5.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 async def main(): runtime = SingleThreadedAgentRuntime() await Generator.register( runtime, \u0026#34;generator\u0026#34;, lambda: Generator(generate_val=lambda: random.randint(1, 15)), ) await Checker.register( runtime, \u0026#34;checker\u0026#34;, lambda: Checker(run_until=lambda count: count \u0026gt;= 5), ) runtime.start() await runtime.send_message(Message(1), AgentId(\u0026#34;checker\u0026#34;, \u0026#34;default\u0026#34;)) await runtime.stop_when_idle() asyncio.run(main()) To start the runtime, we need to call runtime.start(). The send_message method can be used to initiate the conversation between the agents. In this example, we send Message(1) as the initial message. runtime.stop_when_idle() specifies that the runtime should stop when it is idle.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 $ python 05-autogencore-basics.py -------------------------------------------------------------------------------- Checker: 1 is not a prime number, generate next. -------------------------------------------------------------------------------- Generator: We got 0 prime numbers. Next number is 8 -------------------------------------------------------------------------------- Checker: 8 is not a prime number, generate next. -------------------------------------------------------------------------------- Generator: We got 0 prime numbers. Next number is 4 -------------------------------------------------------------------------------- Checker: 4 is not a prime number, generate next. -------------------------------------------------------------------------------- Generator: We got 0 prime numbers. Next number is 11 -------------------------------------------------------------------------------- Checker: 11 is a prime number, continue to next. -------------------------------------------------------------------------------- Generator: We got 1 prime numbers. Next number is 7 -------------------------------------------------------------------------------- Checker: 7 is a prime number, continue to next. -------------------------------------------------------------------------------- Generator: We got 2 prime numbers. Next number is 4 -------------------------------------------------------------------------------- Checker: 4 is not a prime number, generate next. -------------------------------------------------------------------------------- Generator: We got 2 prime numbers. Next number is 15 -------------------------------------------------------------------------------- Checker: 15 is not a prime number, generate next. -------------------------------------------------------------------------------- Generator: We got 2 prime numbers. Next number is 8 -------------------------------------------------------------------------------- Checker: 8 is not a prime number, generate next. -------------------------------------------------------------------------------- Generator: We got 2 prime numbers. Next number is 10 -------------------------------------------------------------------------------- Checker: 10 is not a prime number, generate next. -------------------------------------------------------------------------------- Generator: We got 2 prime numbers. Next number is 10 -------------------------------------------------------------------------------- Checker: 10 is not a prime number, generate next. -------------------------------------------------------------------------------- Generator: We got 2 prime numbers. Next number is 7 -------------------------------------------------------------------------------- Checker: 7 exists in the generated prime numbers, continue to next. -------------------------------------------------------------------------------- Generator: We got 2 prime numbers. Next number is 4 -------------------------------------------------------------------------------- Checker: 4 is not a prime number, generate next. -------------------------------------------------------------------------------- Generator: We got 2 prime numbers. Next number is 6 -------------------------------------------------------------------------------- Checker: 6 is not a prime number, generate next. -------------------------------------------------------------------------------- Generator: We got 2 prime numbers. Next number is 6 -------------------------------------------------------------------------------- Checker: 6 is not a prime number, generate next. -------------------------------------------------------------------------------- Generator: We got 2 prime numbers. Next number is 2 -------------------------------------------------------------------------------- Checker: 2 is a prime number, continue to next. -------------------------------------------------------------------------------- Generator: We got 3 prime numbers. Next number is 7 -------------------------------------------------------------------------------- Checker: 7 exists in the generated prime numbers, continue to next. -------------------------------------------------------------------------------- Generator: We got 3 prime numbers. Next number is 11 -------------------------------------------------------------------------------- Checker: 11 exists in the generated prime numbers, continue to next. -------------------------------------------------------------------------------- Generator: We got 3 prime numbers. Next number is 7 -------------------------------------------------------------------------------- Checker: 7 exists in the generated prime numbers, continue to next. -------------------------------------------------------------------------------- Generator: We got 3 prime numbers. Next number is 10 -------------------------------------------------------------------------------- Checker: 10 is not a prime number, generate next. -------------------------------------------------------------------------------- Generator: We got 3 prime numbers. Next number is 7 -------------------------------------------------------------------------------- Checker: 7 exists in the generated prime numbers, continue to next. -------------------------------------------------------------------------------- Generator: We got 3 prime numbers. Next number is 9 -------------------------------------------------------------------------------- Checker: 9 is not a prime number, generate next. -------------------------------------------------------------------------------- Generator: We got 3 prime numbers. Next number is 3 -------------------------------------------------------------------------------- Checker: 3 is a prime number, continue to next. -------------------------------------------------------------------------------- Generator: We got 4 prime numbers. Next number is 3 -------------------------------------------------------------------------------- Checker: 3 exists in the generated prime numbers, continue to next. -------------------------------------------------------------------------------- Generator: We got 4 prime numbers. Next number is 2 -------------------------------------------------------------------------------- Checker: 2 exists in the generated prime numbers, continue to next. -------------------------------------------------------------------------------- Generator: We got 4 prime numbers. Next number is 1 -------------------------------------------------------------------------------- Checker: 1 is not a prime number, generate next. -------------------------------------------------------------------------------- Generator: We got 4 prime numbers. Next number is 6 -------------------------------------------------------------------------------- Checker: 6 is not a prime number, generate next. -------------------------------------------------------------------------------- Generator: We got 4 prime numbers. Next number is 14 -------------------------------------------------------------------------------- Checker: 14 is not a prime number, generate next. -------------------------------------------------------------------------------- Generator: We got 4 prime numbers. Next number is 4 -------------------------------------------------------------------------------- Checker: 4 is not a prime number, generate next. -------------------------------------------------------------------------------- Generator: We got 4 prime numbers. Next number is 8 -------------------------------------------------------------------------------- Checker: 8 is not a prime number, generate next. -------------------------------------------------------------------------------- Generator: We got 4 prime numbers. Next number is 6 -------------------------------------------------------------------------------- Checker: 6 is not a prime number, generate next. -------------------------------------------------------------------------------- Generator: We got 4 prime numbers. Next number is 13 -------------------------------------------------------------------------------- Checker: We got 5 prime numbers, stopping. -------------------------------------------------------------------------------- Prime numbers generated: [11, 7, 2, 3, 13] This is a quick tour of how you can build agents with AutoGen core. We will explore more about different agent patterns in the upcoming articles of this series.\n","date":"2025-02-10T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/understanding-agents-in-autogen-framework/","series":"AutoGen","title":"Understanding Agents in Microsoft AutoGen framework"},{"categories":["autogen","azure-openai","python"],"content":"In the previous article in this series on Microsoft AutoGen, we looked at how to get started with Microsoft AutoGen and create a simple agent. We built a weather agent capable of retrieving real-time weather data. Through this demonstration, we gained an understanding of the basics of implementing an agent using the AutoGen framework. In this article, we will examine the process of building a team of agents or a multi-agent team to achieve a slightly more complex goal. We will start this exploration by building a single-agent team. Yes, you read that correctly. AutoGen allows you to create a single-agent team. This is useful in running an agent in a loop until a goal is achieved.\nA single-agent team For this demonstration, we will construct an agent that identifies five random prime numbers within a specified range. The task is simple. The agent must use the tools to generate a random number, check if it is prime, and repeat this process until five prime numbers are found. Let\u0026rsquo;s first examine the tools.\n1 2 3 4 5 6 7 def generate_number(start: int, end: int) -\u0026gt; int: \u0026#34;\u0026#34;\u0026#34;Generate a random number.\u0026#34;\u0026#34;\u0026#34; return random.randint(start,end) def check_is_prime_number(number: int) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;check if the random number is a prime.\u0026#34;\u0026#34;\u0026#34; return isprime(number) Let us now define an agent and equip it with the necessary tools.\n1 2 3 4 5 6 7 8 9 10 11 12 prime_number_assistant = AssistantAgent( \u0026#34;prime_number_assistant\u0026#34;, model_client=model_client, tools=[generate_number, check_is_prime_number], system_message=\u0026#34;\u0026#34;\u0026#34;\u0026#34; You are a helpful AI assistant, use the tools to generate random number and check if the number is prime or not. Skip already generated or verified numbers. Respond with DONE when all required prime numbers are found and return all prime numbers you found.\u0026#34; \u0026#34;\u0026#34;\u0026#34; ) termination_condition = TextMentionTermination(\u0026#34;DONE\u0026#34;) This definition should be familiar. This was mentioned in the earlier article. We use the AssistantAgent and give it a model to interact with. This is one of the predefined agents in the AutoGen library and can use tools to perform actions. We specify the system message to define the role the agent needs to play clearly. We need to tell the agent when to stop. This is done by specifying a termination condition. AutoGen offers various types of conditions, and in this example, we will utilize the TextMentionTermination condition. This indicates to the agent that the conversation should be terminated when the specified text is mentioned.\nWe can now create a team. A simple method for creating a team is to use the RoundRobinGroupChat. A round-robin group chat is a team that runs a group chat, with participants taking turns in a round-robin fashion to publish a message to all. As we have only one agent in the team, it will be the only speaker.\nLet us put this all together.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.conditions import TextMentionTermination from autogen_agentchat.teams import RoundRobinGroupChat from autogen_ext.models.openai import AzureOpenAIChatCompletionClient from autogen_agentchat.ui import Console from dotenv import load_dotenv import os import asyncio import random from sympy import isprime load_dotenv(override=True) api_key = os.getenv(\u0026#34;AZURE_OPENAI_API_KEY\u0026#34;) model = os.getenv(\u0026#34;AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\u0026#34;) endpoint = os.getenv(\u0026#34;AZURE_OPENAI_ENDPOINT\u0026#34;) api_version = os.getenv(\u0026#34;AZURE_OPENAI_API_VERSION\u0026#34;) model_client = AzureOpenAIChatCompletionClient( model=model, api_key=api_key, endpoint=endpoint, api_version=api_version, ) def generate_number(start: int, end: int) -\u0026gt; int: \u0026#34;\u0026#34;\u0026#34;Generate a random number.\u0026#34;\u0026#34;\u0026#34; return random.randint(start,end) def check_is_prime_number(number: int) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;check if the random number is a prime.\u0026#34;\u0026#34;\u0026#34; return isprime(number) async def main(): prime_number_assistant = AssistantAgent( \u0026#34;prime_number_assistant\u0026#34;, model_client=model_client, tools=[generate_number, check_is_prime_number], system_message=\u0026#34;\u0026#34;\u0026#34;\u0026#34; You are a helpful AI assistant, use the tools to generate random number and check if the number is prime or not. Skip already generated or verified numbers. Respond with DONE when all required prime numbers are found and return all prime numbers you found.\u0026#34; \u0026#34;\u0026#34;\u0026#34; ) termination_condition = TextMentionTermination(\u0026#34;DONE\u0026#34;) team = RoundRobinGroupChat( [prime_number_assistant], termination_condition=termination_condition, ) await team.reset() await Console(team.run_stream(task=\u0026#34;Find 5 random prime numbers between 1 and 15.\u0026#34;)) asyncio.run(main()) The team.run_stream is similar to the on_messages_stream() we saw in the earlier example. It runs the team and generates a stream of messages. When the agents finish, it produces a TaksResult.\nLet\u0026rsquo;s see this agent in action.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 $ python 03-single-agent-team.py ---------- user ---------- Find 5 random prime numbers between 1 and 15. ---------- prime_number_assistant ---------- [FunctionCall(id=\u0026#39;call_FIOXaa8IiGRUI70sEeFgoQis\u0026#39;, arguments=\u0026#39;{\u0026#34;start\u0026#34;:1,\u0026#34;end\u0026#34;:15}\u0026#39;, name=\u0026#39;generate_number\u0026#39;)] ---------- prime_number_assistant ---------- [FunctionExecutionResult(content=\u0026#39;7\u0026#39;, name=\u0026#39;generate_number\u0026#39;, call_id=\u0026#39;call_FIOXaa8IiGRUI70sEeFgoQis\u0026#39;, is_error=False)] ---------- prime_number_assistant ---------- 7 ---------- prime_number_assistant ---------- [FunctionCall(id=\u0026#39;call_SukPoqLsvtMAoPQDzBRum13W\u0026#39;, arguments=\u0026#39;{\u0026#34;number\u0026#34;:7}\u0026#39;, name=\u0026#39;check_is_prime_number\u0026#39;)] ---------- prime_number_assistant ---------- [FunctionExecutionResult(content=\u0026#39;True\u0026#39;, name=\u0026#39;check_is_prime_number\u0026#39;, call_id=\u0026#39;call_SukPoqLsvtMAoPQDzBRum13W\u0026#39;, is_error=False)] ---------- prime_number_assistant ---------- True ---------- prime_number_assistant ---------- The number 7 is a prime number! I\u0026#39;ll continue finding more prime numbers. ---------- prime_number_assistant ---------- [FunctionCall(id=\u0026#39;call_D4jzf4NPU6YNazxR0Nei9fdq\u0026#39;, arguments=\u0026#39;{\u0026#34;start\u0026#34;:1,\u0026#34;end\u0026#34;:15}\u0026#39;, name=\u0026#39;generate_number\u0026#39;)] ---------- prime_number_assistant ---------- [FunctionExecutionResult(content=\u0026#39;5\u0026#39;, name=\u0026#39;generate_number\u0026#39;, call_id=\u0026#39;call_D4jzf4NPU6YNazxR0Nei9fdq\u0026#39;, is_error=False)] ---------- prime_number_assistant ---------- 5 ---------- prime_number_assistant ---------- [FunctionCall(id=\u0026#39;call_5HPqrpLlV9ifiRLgExtCcKNe\u0026#39;, arguments=\u0026#39;{\u0026#34;number\u0026#34;:5}\u0026#39;, name=\u0026#39;check_is_prime_number\u0026#39;)] ---------- prime_number_assistant ---------- [FunctionExecutionResult(content=\u0026#39;True\u0026#39;, name=\u0026#39;check_is_prime_number\u0026#39;, call_id=\u0026#39;call_5HPqrpLlV9ifiRLgExtCcKNe\u0026#39;, is_error=False)] ---------- prime_number_assistant ---------- True ---------- prime_number_assistant ---------- The number 5 is a prime number! I now have two prime numbers: 7 and 5. Continuing the search... ..... ---------- prime_number_assistant ---------- [FunctionCall(id=\u0026#39;call_TIirUcHBylgJ9m2akMNdmxDm\u0026#39;, arguments=\u0026#39;{\u0026#34;start\u0026#34;:1,\u0026#34;end\u0026#34;:15}\u0026#39;, name=\u0026#39;generate_number\u0026#39;)] ---------- prime_number_assistant ---------- [FunctionExecutionResult(content=\u0026#39;7\u0026#39;, name=\u0026#39;generate_number\u0026#39;, call_id=\u0026#39;call_TIirUcHBylgJ9m2akMNdmxDm\u0026#39;, is_error=False)] ---------- prime_number_assistant ---------- 7 ---------- prime_number_assistant ---------- [FunctionCall(id=\u0026#39;call_u6J6zXAYEkYuSQ7uX65MhX1y\u0026#39;, arguments=\u0026#39;{\u0026#34;start\u0026#34;:1,\u0026#34;end\u0026#34;:15}\u0026#39;, name=\u0026#39;generate_number\u0026#39;)] ---------- prime_number_assistant ---------- [FunctionExecutionResult(content=\u0026#39;4\u0026#39;, name=\u0026#39;generate_number\u0026#39;, call_id=\u0026#39;call_u6J6zXAYEkYuSQ7uX65MhX1y\u0026#39;, is_error=False)] ---------- prime_number_assistant ---------- 4 ---------- prime_number_assistant ---------- [FunctionCall(id=\u0026#39;call_uibluZI0IljGmn9TXgFEN2P0\u0026#39;, arguments=\u0026#39;{\u0026#34;number\u0026#34;:4}\u0026#39;, name=\u0026#39;check_is_prime_number\u0026#39;)] ---------- prime_number_assistant ---------- [FunctionExecutionResult(content=\u0026#39;False\u0026#39;, name=\u0026#39;check_is_prime_number\u0026#39;, call_id=\u0026#39;call_uibluZI0IljGmn9TXgFEN2P0\u0026#39;, is_error=False)] ---------- prime_number_assistant ---------- False ---------- prime_number_assistant ---------- [FunctionCall(id=\u0026#39;call_E2KrXXeIe7VpLzITrjcAavip\u0026#39;, arguments=\u0026#39;{\u0026#34;start\u0026#34;:1,\u0026#34;end\u0026#34;:15}\u0026#39;, name=\u0026#39;generate_number\u0026#39;)] ---------- prime_number_assistant ---------- [FunctionExecutionResult(content=\u0026#39;5\u0026#39;, name=\u0026#39;generate_number\u0026#39;, call_id=\u0026#39;call_E2KrXXeIe7VpLzITrjcAavip\u0026#39;, is_error=False)] ---------- prime_number_assistant ---------- 5 ---------- prime_number_assistant ---------- [FunctionCall(id=\u0026#39;call_Pm7KRu6DvRrdCVQz2ZeUlwG1\u0026#39;, arguments=\u0026#39;{\u0026#34;start\u0026#34;:1,\u0026#34;end\u0026#34;:15}\u0026#39;, name=\u0026#39;generate_number\u0026#39;)] ---------- prime_number_assistant ---------- [FunctionExecutionResult(content=\u0026#39;5\u0026#39;, name=\u0026#39;generate_number\u0026#39;, call_id=\u0026#39;call_Pm7KRu6DvRrdCVQz2ZeUlwG1\u0026#39;, is_error=False)] ---------- prime_number_assistant ---------- 5 ---------- prime_number_assistant ---------- [FunctionCall(id=\u0026#39;call_FYdvVQMvuBwUj8pPVr7RwbaH\u0026#39;, arguments=\u0026#39;{\u0026#34;start\u0026#34;:1,\u0026#34;end\u0026#34;:15}\u0026#39;, name=\u0026#39;generate_number\u0026#39;)] ---------- prime_number_assistant ---------- [FunctionExecutionResult(content=\u0026#39;2\u0026#39;, name=\u0026#39;generate_number\u0026#39;, call_id=\u0026#39;call_FYdvVQMvuBwUj8pPVr7RwbaH\u0026#39;, is_error=False)] ---------- prime_number_assistant ---------- 2 ---------- prime_number_assistant ---------- [FunctionCall(id=\u0026#39;call_aXZrxX6rj7Mfk5drOg2W8FLQ\u0026#39;, arguments=\u0026#39;{\u0026#34;number\u0026#34;:2}\u0026#39;, name=\u0026#39;check_is_prime_number\u0026#39;)] ---------- prime_number_assistant ---------- [FunctionExecutionResult(content=\u0026#39;True\u0026#39;, name=\u0026#39;check_is_prime_number\u0026#39;, call_id=\u0026#39;call_aXZrxX6rj7Mfk5drOg2W8FLQ\u0026#39;, is_error=False)] ---------- prime_number_assistant ---------- True ---------- prime_number_assistant ---------- The number 2 is a prime number! So far, I have three prime numbers: 7, 5, and 2. Let\u0026#39;s find the remaining two. ---------- prime_number_assistant ---------- [FunctionCall(id=\u0026#39;call_FZ4hFXuqsATSItZoZ5stXue0\u0026#39;, arguments=\u0026#39;{\u0026#34;start\u0026#34;:1,\u0026#34;end\u0026#34;:15}\u0026#39;, name=\u0026#39;generate_number\u0026#39;)] ---------- prime_number_assistant ---------- [FunctionExecutionResult(content=\u0026#39;5\u0026#39;, name=\u0026#39;generate_number\u0026#39;, call_id=\u0026#39;call_FZ4hFXuqsATSItZoZ5stXue0\u0026#39;, is_error=False)] ---------- prime_number_assistant ---------- 5 ---------- prime_number_assistant ---------- [FunctionCall(id=\u0026#39;call_4IbHsZYIq4Gw65URuXRCzT7T\u0026#39;, arguments=\u0026#39;{\u0026#34;start\u0026#34;:1,\u0026#34;end\u0026#34;:15}\u0026#39;, name=\u0026#39;generate_number\u0026#39;)] ---------- prime_number_assistant ---------- [FunctionExecutionResult(content=\u0026#39;11\u0026#39;, name=\u0026#39;generate_number\u0026#39;, call_id=\u0026#39;call_4IbHsZYIq4Gw65URuXRCzT7T\u0026#39;, is_error=False)] ---------- prime_number_assistant ---------- 11 ---------- prime_number_assistant ---------- [FunctionCall(id=\u0026#39;call_tLYwXnVPWV3j5Vu1Y0o4frEG\u0026#39;, arguments=\u0026#39;{\u0026#34;number\u0026#34;:11}\u0026#39;, name=\u0026#39;check_is_prime_number\u0026#39;)] ---------- prime_number_assistant ---------- [FunctionExecutionResult(content=\u0026#39;True\u0026#39;, name=\u0026#39;check_is_prime_number\u0026#39;, call_id=\u0026#39;call_tLYwXnVPWV3j5Vu1Y0o4frEG\u0026#39;, is_error=False)] ---------- prime_number_assistant ---------- True ---------- prime_number_assistant ---------- The number 11 is a prime number! So far, the prime numbers I\u0026#39;ve found are 7, 5, 2, and 11. Only one more to go. ---------- prime_number_assistant ---------- [FunctionCall(id=\u0026#39;call_0tR2vGJ1QaTd05Ca4ZCQQ2RC\u0026#39;, arguments=\u0026#39;{\u0026#34;start\u0026#34;:1,\u0026#34;end\u0026#34;:15}\u0026#39;, name=\u0026#39;generate_number\u0026#39;)] ---------- prime_number_assistant ---------- [FunctionExecutionResult(content=\u0026#39;13\u0026#39;, name=\u0026#39;generate_number\u0026#39;, call_id=\u0026#39;call_0tR2vGJ1QaTd05Ca4ZCQQ2RC\u0026#39;, is_error=False)] ---------- prime_number_assistant ---------- 13 ---------- prime_number_assistant ---------- [FunctionCall(id=\u0026#39;call_1S9DTlNPcASj3bmSTM4NK3XS\u0026#39;, arguments=\u0026#39;{\u0026#34;number\u0026#34;:13}\u0026#39;, name=\u0026#39;check_is_prime_number\u0026#39;)] ---------- prime_number_assistant ---------- [FunctionExecutionResult(content=\u0026#39;True\u0026#39;, name=\u0026#39;check_is_prime_number\u0026#39;, call_id=\u0026#39;call_1S9DTlNPcASj3bmSTM4NK3XS\u0026#39;, is_error=False)] ---------- prime_number_assistant ---------- True ---------- prime_number_assistant ---------- The number 13 is a prime number! I have now found 5 prime numbers: **7, 5, 2, 11, and 13**. DONE For brevity, I removed a few lines from the output. As you can see, the agent calls the generate_number and check_is_prime_number tools as necessary. Once all five random prime numbers are found, the agent terminates. If the random number generated has already been verified as a prime or non-prime, the model skips calling the check_is_prime_number tool. Essentially, it maintains the state without requiring us to provide it to the agent explicitly. As mentioned in the system message, the agent terminates once all five prime numbers are found.\nLet\u0026rsquo;s now examine the process of creating a multi-agent team.\nA multi-agent team Creating a multi-agent team is not very different. Let us take the same task of deriving five random prime numbers. Instead of using a single agent, we will now distribute the work across two agents. Once again, we will use the predefined AssistantAgent type for this and continue to use the RoundRobinGroupChat for the team.\nLet us look at the code.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 import asyncio from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.base import TaskResult from autogen_agentchat.conditions import ExternalTermination, TextMentionTermination from autogen_agentchat.teams import RoundRobinGroupChat from autogen_agentchat.ui import Console from autogen_core import CancellationToken from autogen_ext.models.openai import AzureOpenAIChatCompletionClient import random from sympy import isprime import os from dotenv import load_dotenv load_dotenv(override=True) def generate_number(start: int, end: int) -\u0026gt; int: \u0026#34;\u0026#34;\u0026#34;Generate a random number.\u0026#34;\u0026#34;\u0026#34; return random.randint(start,end) def check_is_prime_number(number: int) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;check if the random number is a prime.\u0026#34;\u0026#34;\u0026#34; return isprime(number) async def main(): api_key = os.getenv(\u0026#34;AZURE_OPENAI_API_KEY\u0026#34;) model = os.getenv(\u0026#34;AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\u0026#34;) endpoint = os.getenv(\u0026#34;AZURE_OPENAI_ENDPOINT\u0026#34;) api_version = os.getenv(\u0026#34;AZURE_OPENAI_API_VERSION\u0026#34;) model_client = AzureOpenAIChatCompletionClient( model=model, api_key=api_key, endpoint=endpoint, api_version=api_version, ) generator_agent = AssistantAgent( \u0026#34;generator\u0026#34;, description=\u0026#34;An agent that generates random numbers.\u0026#34;, model_client=model_client, tools=[generate_number], system_message=\u0026#34;\u0026#34;\u0026#34; You are a numbers wizard. Use the tools to generate random number. Maintain a list of generated numbers. If a new number is generated, check if it is already in the list. If it is, generate a new number. If it is not, add it to the list and return the number. \u0026#34;\u0026#34;\u0026#34;, ) verifier_agent = AssistantAgent( \u0026#34;verifier\u0026#34;, description=\u0026#34;An agent that verifies if a number is prime.\u0026#34;, model_client=model_client, tools=[check_is_prime_number], system_message=\u0026#34;\u0026#34;\u0026#34;\u0026#34; You are a powerful calculator, use the tools to verify if a number is prime number. Skip already verified numbers. Respond with DONE when all required prime numbers are found and return all prime numbers you found.\u0026#34; \u0026#34;\u0026#34;\u0026#34; ) text_termination = TextMentionTermination(\u0026#34;DONE\u0026#34;) team = RoundRobinGroupChat([generator_agent, verifier_agent], termination_condition=text_termination) await team.reset() await Console(team.run_stream(task=\u0026#34;Find 5 random prime numbers between 1 and 15.\u0026#34;)) result = asyncio.run(main()) print(result) We define two instances of AssistantAgent \u0026ndash; The generator_agent and the verifier_agent. We give each of these agents the tools they need. The behavior of two agents performing this task is different from the single-agent team.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 $ python 04-multi-agent-team.py ---------- user ---------- Find 5 random prime numbers between 1 and 15. ---------- generator ---------- [FunctionCall(id=\u0026#39;call_894jeNfW7tCkahg4oFunruAq\u0026#39;, arguments=\u0026#39;{\u0026#34;start\u0026#34;: 1, \u0026#34;end\u0026#34;: 15}\u0026#39;, name=\u0026#39;generate_number\u0026#39;), FunctionCall(id=\u0026#39;call_fH1d93NA2BHFcDyr8hlINvvJ\u0026#39;, arguments=\u0026#39;{\u0026#34;start\u0026#34;: 1, \u0026#34;end\u0026#34;: 15}\u0026#39;, name=\u0026#39;generate_number\u0026#39;), FunctionCall(id=\u0026#39;call_eyKVFeDGGoe5uf2I7JIKddM4\u0026#39;, arguments=\u0026#39;{\u0026#34;start\u0026#34;: 1, \u0026#34;end\u0026#34;: 15}\u0026#39;, name=\u0026#39;generate_number\u0026#39;), FunctionCall(id=\u0026#39;call_FnghutlmxBIQf74a6oGruByH\u0026#39;, arguments=\u0026#39;{\u0026#34;start\u0026#34;: 1, \u0026#34;end\u0026#34;: 15}\u0026#39;, name=\u0026#39;generate_number\u0026#39;), FunctionCall(id=\u0026#39;call_vG6bzFg1EW1RIlHxGLQE52ii\u0026#39;, arguments=\u0026#39;{\u0026#34;start\u0026#34;: 1, \u0026#34;end\u0026#34;: 15}\u0026#39;, name=\u0026#39;generate_number\u0026#39;)] ---------- generator ---------- [FunctionExecutionResult(content=\u0026#39;15\u0026#39;, name=\u0026#39;generate_number\u0026#39;, call_id=\u0026#39;call_894jeNfW7tCkahg4oFunruAq\u0026#39;, is_error=False), FunctionExecutionResult(content=\u0026#39;11\u0026#39;, name=\u0026#39;generate_number\u0026#39;, call_id=\u0026#39;call_fH1d93NA2BHFcDyr8hlINvvJ\u0026#39;, is_error=False), FunctionExecutionResult(content=\u0026#39;1\u0026#39;, name=\u0026#39;generate_number\u0026#39;, call_id=\u0026#39;call_eyKVFeDGGoe5uf2I7JIKddM4\u0026#39;, is_error=False), FunctionExecutionResult(content=\u0026#39;2\u0026#39;, name=\u0026#39;generate_number\u0026#39;, call_id=\u0026#39;call_FnghutlmxBIQf74a6oGruByH\u0026#39;, is_error=False), FunctionExecutionResult(content=\u0026#39;10\u0026#39;, name=\u0026#39;generate_number\u0026#39;, call_id=\u0026#39;call_vG6bzFg1EW1RIlHxGLQE52ii\u0026#39;, is_error=False)] ---------- generator ---------- 15 11 1 2 10 ---------- verifier ---------- [FunctionCall(id=\u0026#39;call_2iCvwnnwpaboUgSVVNhRVl9O\u0026#39;, arguments=\u0026#39;{\u0026#34;number\u0026#34;: 15}\u0026#39;, name=\u0026#39;check_is_prime_number\u0026#39;), FunctionCall(id=\u0026#39;call_Su29tYoERzRYyredK2s4MeCj\u0026#39;, arguments=\u0026#39;{\u0026#34;number\u0026#34;: 11}\u0026#39;, name=\u0026#39;check_is_prime_number\u0026#39;), FunctionCall(id=\u0026#39;call_zAZEyK1RgQlxKlfKI2cwwP1W\u0026#39;, arguments=\u0026#39;{\u0026#34;number\u0026#34;: 1}\u0026#39;, name=\u0026#39;check_is_prime_number\u0026#39;), FunctionCall(id=\u0026#39;call_1aJQIBX9w063rjtLSGjZEmPW\u0026#39;, arguments=\u0026#39;{\u0026#34;number\u0026#34;: 2}\u0026#39;, name=\u0026#39;check_is_prime_number\u0026#39;), FunctionCall(id=\u0026#39;call_YaT7FkojUoHBR8ADvQkeJVAh\u0026#39;, arguments=\u0026#39;{\u0026#34;number\u0026#34;: 10}\u0026#39;, name=\u0026#39;check_is_prime_number\u0026#39;)] ---------- verifier ---------- [FunctionExecutionResult(content=\u0026#39;False\u0026#39;, name=\u0026#39;check_is_prime_number\u0026#39;, call_id=\u0026#39;call_2iCvwnnwpaboUgSVVNhRVl9O\u0026#39;, is_error=False), FunctionExecutionResult(content=\u0026#39;True\u0026#39;, name=\u0026#39;check_is_prime_number\u0026#39;, call_id=\u0026#39;call_Su29tYoERzRYyredK2s4MeCj\u0026#39;, is_error=False), FunctionExecutionResult(content=\u0026#39;False\u0026#39;, name=\u0026#39;check_is_prime_number\u0026#39;, call_id=\u0026#39;call_zAZEyK1RgQlxKlfKI2cwwP1W\u0026#39;, is_error=False), FunctionExecutionResult(content=\u0026#39;True\u0026#39;, name=\u0026#39;check_is_prime_number\u0026#39;, call_id=\u0026#39;call_1aJQIBX9w063rjtLSGjZEmPW\u0026#39;, is_error=False), FunctionExecutionResult(content=\u0026#39;False\u0026#39;, name=\u0026#39;check_is_prime_number\u0026#39;, call_id=\u0026#39;call_YaT7FkojUoHBR8ADvQkeJVAh\u0026#39;, is_error=False)] ---------- verifier ---------- False True False True False ---------- generator ---------- The random numbers generated are: **15, 11, 1, 2, and 10**. Upon verification of whether they\u0026#39;re prime or not: - **15**: Not prime (divisible by 3 and 5). - **11**: Prime. - **1**: Not prime (prime numbers start from 2). - **2**: Prime. - **10**: Not prime (divisible by 2 and 5). Let\u0026#39;s filter and continue generating new random prime numbers until we obtain 5 unique prime numbers between 1 and 15. So far, we have two prime numbers (**11** and **2**). Let me generate three more. ---------- generator ---------- [FunctionCall(id=\u0026#39;call_sJiRDadKeNIYRP36h8H636Dy\u0026#39;, arguments=\u0026#39;{\u0026#34;start\u0026#34;: 1, \u0026#34;end\u0026#34;: 15}\u0026#39;, name=\u0026#39;generate_number\u0026#39;), FunctionCall(id=\u0026#39;call_0edqKSkqWOiuA2E0fgNbO6Vu\u0026#39;, arguments=\u0026#39;{\u0026#34;start\u0026#34;: 1, \u0026#34;end\u0026#34;: 15}\u0026#39;, name=\u0026#39;generate_number\u0026#39;), FunctionCall(id=\u0026#39;call_J4qdolJj9xCFCa7gqr0nP8hE\u0026#39;, arguments=\u0026#39;{\u0026#34;start\u0026#34;: 1, \u0026#34;end\u0026#34;: 15}\u0026#39;, name=\u0026#39;generate_number\u0026#39;)] ---------- generator ---------- [FunctionExecutionResult(content=\u0026#39;15\u0026#39;, name=\u0026#39;generate_number\u0026#39;, call_id=\u0026#39;call_sJiRDadKeNIYRP36h8H636Dy\u0026#39;, is_error=False), FunctionExecutionResult(content=\u0026#39;8\u0026#39;, name=\u0026#39;generate_number\u0026#39;, call_id=\u0026#39;call_0edqKSkqWOiuA2E0fgNbO6Vu\u0026#39;, is_error=False), FunctionExecutionResult(content=\u0026#39;4\u0026#39;, name=\u0026#39;generate_number\u0026#39;, call_id=\u0026#39;call_J4qdolJj9xCFCa7gqr0nP8hE\u0026#39;, is_error=False)] ---------- generator ---------- 15 8 4 ---------- verifier ---------- [FunctionCall(id=\u0026#39;call_VF51HDANEX28kesdjEGmHdQi\u0026#39;, arguments=\u0026#39;{\u0026#34;number\u0026#34;: 15}\u0026#39;, name=\u0026#39;check_is_prime_number\u0026#39;), FunctionCall(id=\u0026#39;call_biagEfBKc8tM5CT6INB5Cvz8\u0026#39;, arguments=\u0026#39;{\u0026#34;number\u0026#34;: 8}\u0026#39;, name=\u0026#39;check_is_prime_number\u0026#39;), FunctionCall(id=\u0026#39;call_4JzDMXXf3a8fAcPGTCwRVW9w\u0026#39;, arguments=\u0026#39;{\u0026#34;number\u0026#34;: 4}\u0026#39;, name=\u0026#39;check_is_prime_number\u0026#39;)] ---------- verifier ---------- [FunctionExecutionResult(content=\u0026#39;False\u0026#39;, name=\u0026#39;check_is_prime_number\u0026#39;, call_id=\u0026#39;call_VF51HDANEX28kesdjEGmHdQi\u0026#39;, is_error=False), FunctionExecutionResult(content=\u0026#39;False\u0026#39;, name=\u0026#39;check_is_prime_number\u0026#39;, call_id=\u0026#39;call_biagEfBKc8tM5CT6INB5Cvz8\u0026#39;, is_error=False), FunctionExecutionResult(content=\u0026#39;False\u0026#39;, name=\u0026#39;check_is_prime_number\u0026#39;, call_id=\u0026#39;call_4JzDMXXf3a8fAcPGTCwRVW9w\u0026#39;, is_error=False)] ---------- verifier ---------- False False False ---------- generator ---------- The newly generated random numbers are: **15, 8, and 4**. Upon verification: - **15**: Not prime (divisible by 3 and 5). - **8**: Not prime (divisible by 2). - **4**: Not prime (divisible by 2). Still, we only have two prime numbers (**11** and **2**) so far. Let me generate more until we reach 5 unique prime numbers. ---------- generator ---------- [FunctionCall(id=\u0026#39;call_XsmXqy5UAvptmxCcCazNfwl6\u0026#39;, arguments=\u0026#39;{\u0026#34;start\u0026#34;: 1, \u0026#34;end\u0026#34;: 15}\u0026#39;, name=\u0026#39;generate_number\u0026#39;), FunctionCall(id=\u0026#39;call_MHznHp9uqfg4NVc113zmUPuO\u0026#39;, arguments=\u0026#39;{\u0026#34;start\u0026#34;: 1, \u0026#34;end\u0026#34;: 15}\u0026#39;, name=\u0026#39;generate_number\u0026#39;), FunctionCall(id=\u0026#39;call_4cGoESKrN7cNcUcsiMYGThQM\u0026#39;, arguments=\u0026#39;{\u0026#34;start\u0026#34;: 1, \u0026#34;end\u0026#34;: 15}\u0026#39;, name=\u0026#39;generate_number\u0026#39;)] ---------- generator ---------- [FunctionExecutionResult(content=\u0026#39;6\u0026#39;, name=\u0026#39;generate_number\u0026#39;, call_id=\u0026#39;call_XsmXqy5UAvptmxCcCazNfwl6\u0026#39;, is_error=False), FunctionExecutionResult(content=\u0026#39;4\u0026#39;, name=\u0026#39;generate_number\u0026#39;, call_id=\u0026#39;call_MHznHp9uqfg4NVc113zmUPuO\u0026#39;, is_error=False), FunctionExecutionResult(content=\u0026#39;2\u0026#39;, name=\u0026#39;generate_number\u0026#39;, call_id=\u0026#39;call_4cGoESKrN7cNcUcsiMYGThQM\u0026#39;, is_error=False)] ---------- generator ---------- 6 4 2 ---------- verifier ---------- Skipping already verified number \u0026#34;2\u0026#34;. ---------- verifier ---------- [FunctionCall(id=\u0026#39;call_aZY6dD90AaxB7pZM6jTwroyU\u0026#39;, arguments=\u0026#39;{\u0026#34;number\u0026#34;: 6}\u0026#39;, name=\u0026#39;check_is_prime_number\u0026#39;), FunctionCall(id=\u0026#39;call_RAMqj1UNgU0fxpHx1ggCtWgm\u0026#39;, arguments=\u0026#39;{\u0026#34;number\u0026#34;: 4}\u0026#39;, name=\u0026#39;check_is_prime_number\u0026#39;)] ---------- verifier ---------- [FunctionExecutionResult(content=\u0026#39;False\u0026#39;, name=\u0026#39;check_is_prime_number\u0026#39;, call_id=\u0026#39;call_aZY6dD90AaxB7pZM6jTwroyU\u0026#39;, is_error=False), FunctionExecutionResult(content=\u0026#39;False\u0026#39;, name=\u0026#39;check_is_prime_number\u0026#39;, call_id=\u0026#39;call_RAMqj1UNgU0fxpHx1ggCtWgm\u0026#39;, is_error=False)] ---------- verifier ---------- False False ---------- generator ---------- The newly generated random numbers are: **6**, **4**, and **2**. Upon verification: - **6**: Not prime (divisible by 2 and 3). - **4**: Not prime (divisible by 2). - **2**: Already identified as prime, but it is not unique in this process. To summarize, we still only have two unique prime numbers (**11** and **2**). I\u0026#39;ll proceed to generate more numbers until we achieve 5 unique prime numbers. ---------- generator ---------- [FunctionCall(id=\u0026#39;call_xdyKFh35FzGs50YPq5rWSjZF\u0026#39;, arguments=\u0026#39;{\u0026#34;start\u0026#34;: 1, \u0026#34;end\u0026#34;: 15}\u0026#39;, name=\u0026#39;generate_number\u0026#39;), FunctionCall(id=\u0026#39;call_p9xk79X8lLREn2CRr01qZq44\u0026#39;, arguments=\u0026#39;{\u0026#34;start\u0026#34;: 1, \u0026#34;end\u0026#34;: 15}\u0026#39;, name=\u0026#39;generate_number\u0026#39;), FunctionCall(id=\u0026#39;call_DyC3tYI711jOZ3JBD9mOBSF5\u0026#39;, arguments=\u0026#39;{\u0026#34;start\u0026#34;: 1, \u0026#34;end\u0026#34;: 15}\u0026#39;, name=\u0026#39;generate_number\u0026#39;)] ---------- generator ---------- [FunctionExecutionResult(content=\u0026#39;8\u0026#39;, name=\u0026#39;generate_number\u0026#39;, call_id=\u0026#39;call_xdyKFh35FzGs50YPq5rWSjZF\u0026#39;, is_error=False), FunctionExecutionResult(content=\u0026#39;6\u0026#39;, name=\u0026#39;generate_number\u0026#39;, call_id=\u0026#39;call_p9xk79X8lLREn2CRr01qZq44\u0026#39;, is_error=False), FunctionExecutionResult(content=\u0026#39;5\u0026#39;, name=\u0026#39;generate_number\u0026#39;, call_id=\u0026#39;call_DyC3tYI711jOZ3JBD9mOBSF5\u0026#39;, is_error=False)] ---------- generator ---------- 8 6 5 ---------- verifier ---------- [FunctionCall(id=\u0026#39;call_yeaYJyH84xwmy4qy7tcSaCv5\u0026#39;, arguments=\u0026#39;{\u0026#34;number\u0026#34;: 8}\u0026#39;, name=\u0026#39;check_is_prime_number\u0026#39;), FunctionCall(id=\u0026#39;call_63MzDx5TP4OQqtegd2hlJHhk\u0026#39;, arguments=\u0026#39;{\u0026#34;number\u0026#34;: 6}\u0026#39;, name=\u0026#39;check_is_prime_number\u0026#39;), FunctionCall(id=\u0026#39;call_nh4Kbzw8zQt8dsYSk8AbDeWJ\u0026#39;, arguments=\u0026#39;{\u0026#34;number\u0026#34;: 5}\u0026#39;, name=\u0026#39;check_is_prime_number\u0026#39;)] ---------- verifier ---------- [FunctionExecutionResult(content=\u0026#39;False\u0026#39;, name=\u0026#39;check_is_prime_number\u0026#39;, call_id=\u0026#39;call_yeaYJyH84xwmy4qy7tcSaCv5\u0026#39;, is_error=False), FunctionExecutionResult(content=\u0026#39;False\u0026#39;, name=\u0026#39;check_is_prime_number\u0026#39;, call_id=\u0026#39;call_63MzDx5TP4OQqtegd2hlJHhk\u0026#39;, is_error=False), FunctionExecutionResult(content=\u0026#39;True\u0026#39;, name=\u0026#39;check_is_prime_number\u0026#39;, call_id=\u0026#39;call_nh4Kbzw8zQt8dsYSk8AbDeWJ\u0026#39;, is_error=False)] ---------- verifier ---------- False False True ---------- generator ---------- The newly generated random numbers are: **8**, **6**, and **5**. Upon verification: - **8**: Not prime (divisible by 2). - **6**: Not prime (divisible by 2 and 3). - **5**: Prime. Now we have three unique prime numbers: **11**, **2**, and **5**. We need two more. Generating additional numbers. ---------- generator ---------- [FunctionCall(id=\u0026#39;call_jFPQ1YEM80bIwR1E1jGTKVot\u0026#39;, arguments=\u0026#39;{\u0026#34;start\u0026#34;: 1, \u0026#34;end\u0026#34;: 15}\u0026#39;, name=\u0026#39;generate_number\u0026#39;), FunctionCall(id=\u0026#39;call_Kjrf499UylUV232SF4Id4Mca\u0026#39;, arguments=\u0026#39;{\u0026#34;start\u0026#34;: 1, \u0026#34;end\u0026#34;: 15}\u0026#39;, name=\u0026#39;generate_number\u0026#39;), FunctionCall(id=\u0026#39;call_RLKqvfwdY5PZxDos3Zr9rXwI\u0026#39;, arguments=\u0026#39;{\u0026#34;start\u0026#34;: 1, \u0026#34;end\u0026#34;: 15}\u0026#39;, name=\u0026#39;generate_number\u0026#39;)] ---------- generator ---------- [FunctionExecutionResult(content=\u0026#39;2\u0026#39;, name=\u0026#39;generate_number\u0026#39;, call_id=\u0026#39;call_jFPQ1YEM80bIwR1E1jGTKVot\u0026#39;, is_error=False), FunctionExecutionResult(content=\u0026#39;8\u0026#39;, name=\u0026#39;generate_number\u0026#39;, call_id=\u0026#39;call_Kjrf499UylUV232SF4Id4Mca\u0026#39;, is_error=False), FunctionExecutionResult(content=\u0026#39;2\u0026#39;, name=\u0026#39;generate_number\u0026#39;, call_id=\u0026#39;call_RLKqvfwdY5PZxDos3Zr9rXwI\u0026#39;, is_error=False)] ---------- generator ---------- 2 8 2 ---------- verifier ---------- [FunctionCall(id=\u0026#39;call_WUndSjvxaKwx99xlXBcAMZyL\u0026#39;, arguments=\u0026#39;{\u0026#34;number\u0026#34;: 8}\u0026#39;, name=\u0026#39;check_is_prime_number\u0026#39;)] ---------- verifier ---------- [FunctionExecutionResult(content=\u0026#39;False\u0026#39;, name=\u0026#39;check_is_prime_number\u0026#39;, call_id=\u0026#39;call_WUndSjvxaKwx99xlXBcAMZyL\u0026#39;, is_error=False)] ---------- verifier ---------- False ---------- generator ---------- [FunctionCall(id=\u0026#39;call_RK6WeByLnudd85EMQpBAptPD\u0026#39;, arguments=\u0026#39;{\u0026#34;start\u0026#34;: 1, \u0026#34;end\u0026#34;: 15}\u0026#39;, name=\u0026#39;generate_number\u0026#39;), FunctionCall(id=\u0026#39;call_4ltyvOuEGC0boIZO1tKdKowx\u0026#39;, arguments=\u0026#39;{\u0026#34;start\u0026#34;: 1, \u0026#34;end\u0026#34;: 15}\u0026#39;, name=\u0026#39;generate_number\u0026#39;), FunctionCall(id=\u0026#39;call_9xUdr2fX1YEARIbTxQHhkCuX\u0026#39;, arguments=\u0026#39;{\u0026#34;start\u0026#34;: 1, \u0026#34;end\u0026#34;: 15}\u0026#39;, name=\u0026#39;generate_number\u0026#39;)] ---------- generator ---------- [FunctionExecutionResult(content=\u0026#39;2\u0026#39;, name=\u0026#39;generate_number\u0026#39;, call_id=\u0026#39;call_RK6WeByLnudd85EMQpBAptPD\u0026#39;, is_error=False), FunctionExecutionResult(content=\u0026#39;2\u0026#39;, name=\u0026#39;generate_number\u0026#39;, call_id=\u0026#39;call_4ltyvOuEGC0boIZO1tKdKowx\u0026#39;, is_error=False), FunctionExecutionResult(content=\u0026#39;12\u0026#39;, name=\u0026#39;generate_number\u0026#39;, call_id=\u0026#39;call_9xUdr2fX1YEARIbTxQHhkCuX\u0026#39;, is_error=False)] ---------- generator ---------- 2 2 12 ---------- verifier ---------- [FunctionCall(id=\u0026#39;call_RZoKcfjoOgCSRX2f47pQgFqk\u0026#39;, arguments=\u0026#39;{\u0026#34;number\u0026#34;:12}\u0026#39;, name=\u0026#39;check_is_prime_number\u0026#39;)] ---------- verifier ---------- [FunctionExecutionResult(content=\u0026#39;False\u0026#39;, name=\u0026#39;check_is_prime_number\u0026#39;, call_id=\u0026#39;call_RZoKcfjoOgCSRX2f47pQgFqk\u0026#39;, is_error=False)] ---------- verifier ---------- False ---------- generator ---------- [FunctionCall(id=\u0026#39;call_ekpU1QikKZeINATnJNJUwazo\u0026#39;, arguments=\u0026#39;{\u0026#34;start\u0026#34;: 1, \u0026#34;end\u0026#34;: 15}\u0026#39;, name=\u0026#39;generate_number\u0026#39;), FunctionCall(id=\u0026#39;call_5Dhm659YhWbvjPoPyoLN26K8\u0026#39;, arguments=\u0026#39;{\u0026#34;start\u0026#34;: 1, \u0026#34;end\u0026#34;: 15}\u0026#39;, name=\u0026#39;generate_number\u0026#39;), FunctionCall(id=\u0026#39;call_L6HHEr1BiRIbtL1lmwvEiG7S\u0026#39;, arguments=\u0026#39;{\u0026#34;start\u0026#34;: 1, \u0026#34;end\u0026#34;: 15}\u0026#39;, name=\u0026#39;generate_number\u0026#39;)] ---------- generator ---------- [FunctionExecutionResult(content=\u0026#39;13\u0026#39;, name=\u0026#39;generate_number\u0026#39;, call_id=\u0026#39;call_ekpU1QikKZeINATnJNJUwazo\u0026#39;, is_error=False), FunctionExecutionResult(content=\u0026#39;13\u0026#39;, name=\u0026#39;generate_number\u0026#39;, call_id=\u0026#39;call_5Dhm659YhWbvjPoPyoLN26K8\u0026#39;, is_error=False), FunctionExecutionResult(content=\u0026#39;3\u0026#39;, name=\u0026#39;generate_number\u0026#39;, call_id=\u0026#39;call_L6HHEr1BiRIbtL1lmwvEiG7S\u0026#39;, is_error=False)] ---------- generator ---------- 13 13 3 ---------- verifier ---------- [FunctionCall(id=\u0026#39;call_ZlprPJLLfm6CnphIpkeTKJOg\u0026#39;, arguments=\u0026#39;{\u0026#34;number\u0026#34;: 13}\u0026#39;, name=\u0026#39;check_is_prime_number\u0026#39;), FunctionCall(id=\u0026#39;call_0ugsQ0h8YfwuBONedlCkw57k\u0026#39;, arguments=\u0026#39;{\u0026#34;number\u0026#34;: 3}\u0026#39;, name=\u0026#39;check_is_prime_number\u0026#39;)] ---------- verifier ---------- [FunctionExecutionResult(content=\u0026#39;True\u0026#39;, name=\u0026#39;check_is_prime_number\u0026#39;, call_id=\u0026#39;call_ZlprPJLLfm6CnphIpkeTKJOg\u0026#39;, is_error=False), FunctionExecutionResult(content=\u0026#39;True\u0026#39;, name=\u0026#39;check_is_prime_number\u0026#39;, call_id=\u0026#39;call_0ugsQ0h8YfwuBONedlCkw57k\u0026#39;, is_error=False)] ---------- verifier ---------- True True ---------- generator ---------- The newly generated numbers are **13**, **13**, and **3**. Upon verification: - **13**: Prime. - **13**: Already identified above (duplicate). - **3**: Prime. Now we have five unique prime numbers: **11**, **2**, **5**, **13**, and **3**. Final list of random prime numbers between 1 and 15: **11, 2, 5, 13, 3**. ---------- verifier ---------- DONE. The 5 random prime numbers between 1 and 15 are: **11, 2, 5, 13, 3**. The generator_agent generates all five numbers at a time. The verifier_agent finds two prime numbers out of the five. In the next turn, the generator generates only three random numbers as we need only three more prime numbers. This goes on until the task is complete. This is much more efficient than an agent generating one number and the second one verifying the number in a sequence.\nThe team behavior may change based on the type of team we select to implement. We will explore more on this in the next part of this series.\n","date":"2025-02-05T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/teams-of-agents-in-autogen/","series":"AutoGen","title":"Teams of agents in AutoGen"},{"categories":["mcp","agents","python"],"content":"A couple of months ago, Anthropic introduced and open-sourced the Model Context Protocol (MCP). MCP is the new standard for connecting AI models to external data sources and APIs more easily and consistently. With the advances in AI, models are becoming increasingly powerful in reasoning and quality. However, as text-completion machines, these models still lack access to real-time data. AI providers have worked around this using Retrieval Augmented Generation (RAG) and tool calling. Every data source requires custom implementation, and every provider has a way of integrating tools with AI models. MCP addresses these silos by providing a universal, open standard for connecting AI systems with data sources.\nAs Anthropic explains in its documentation, MCP is like a USB-C port for AI applications. Just as USB-C provides a standardized way to connect devices to various peripherals and accessories, MCP provides a standardized way to connect AI models to different data sources and tools.\nThere are a few components in the MCP architecture.\nAn MCP host is an AI application that needs access to external tools and data. These hosts get access to the tools, prompts, and resources an MCP server exposes through an MCP client inside the host application. Anthropic provides a set of sample servers written in Python and TypeScript. Implementing MCP servers and clients is easy if you are familiar with any of these languages. A few community members have implemented MCP SDKs in other languages as well. For example, mcp-golang is a Go implementation of the MCP.\nThis article will look at building a simple MCP server in Python. This example demonstrates how you can get started with developing MCP servers. We shall look at useful MCP server implementations in Python and other languages in future articles.\nGetting started As we will only implement an MCP server today, we must use an existing MCP host with an MCP client. We shall use Claude Desktop for this purpose. Anthropic added support for MCP in Claude Desktop. As of today, MCP servers can provide three types of capabilities: resources, prompts, and tools. This article will look at implementing tool capability in an MCP server.\nAssuming you already have Python and the uv package manager installed on your system, you can run the following commands to create the scaffold for a hello-world MCP server.\n1 2 3 4 5 6 7 8 9 10 11 12 13 # Create a new directory for our project uv init hello-world cd hello-world # Create virtual environment and activate it uv venv .venv\\Scripts\\activate # Install dependencies uv add mcp[cli] # Create our server file new-item hello-world.py We will implement the necessary tools in the hello-world.py.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 from typing import Any from mcp.server.fastmcp import FastMCP mcp = FastMCP(\u0026#34;hello\u0026#34;) @mcp.tool() async def say_hello(name: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Say hello to the user. Args: name (str): The name of the user. \u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; @mcp.tool() async def say_hello_to_everyone() -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Say hello to everyone.\u0026#34;\u0026#34;\u0026#34; return \u0026#34;Hello, everyone!\u0026#34; if __name__ == \u0026#34;__main__\u0026#34;: mcp.run(transport=\u0026#39;stdio\u0026#39;) This hello-world MCP server implements two tools \u0026ndash; say_hello and say_hello_to_everyone. The @mcp.tool() decorator indicates that the functions are MCP tools that a host can use. The doc strings inside the function definitions are important for identifying the right tools.\nWe need to add the MCP server definition to the Claude Desktop configuration to make the MCP host (Claude Desktop) aware of the tools exposed by the MCP server. On a Windows system, this configuration file is located at C:\\Users\\\u0026lt;username\u0026gt;\\AppData\\Roaming\\Claude\\claude_desktop_config.json.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 { \u0026#34;mcpServers\u0026#34;: { \u0026#34;hello-world\u0026#34;: { \u0026#34;command\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.local\\\\bin\\\\uv.exe\u0026#34;, \u0026#34;args\u0026#34;: [ \u0026#34;--directory\u0026#34;, \u0026#34;C:\\\\GitHub\\\\mcp-servers\\\\hello-world\u0026#34;, \u0026#34;run\u0026#34;, \u0026#34;hello.py\u0026#34; ] }, \u0026#34;memory\u0026#34;: { \u0026#34;command\u0026#34;: \u0026#34;npx\u0026#34;, \u0026#34;args\u0026#34;: [ \u0026#34;-y\u0026#34;, \u0026#34;@modelcontextprotocol/server-memory\u0026#34; ] } } } Once you add the tool definition, restart the Claude Desktop application. You should then be able to see the tools available to Claude.\nNow, you are ready to prompt and see Claude you these tools. Once you prompt, if Claude finds a suitable tool, you will be prompted to allow access to the tool.\nThis is it. In the next set of articles in this series on MCP, we will look at developing some useful MCP servers and clients.\n","date":"2025-02-01T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/anthropic-model-context-protocol/","series":"Model Context Protocol","title":"Model Context Protocol by Anthropic for connecting AI models to data"},{"categories":["autogen","azure-openai","python"],"content":"I have been closely following developments in the Agentic AI space. AutoGen from Microsoft Research is emerging as a powerful yet easy-to-use framework for building AI agents and applications. With their re-architected release (0.4.0), they have re-imagined building advanced AI applications. This release introduces an asynchronous, event-driven architecture.\nThis release features a layered architecture with all foundational building blocks for an event-driven system as the core package. The AgentChat layer builds upon the core layer, providing a task-driven API for group chat and code execution, as well as pre-built agents. The extensions layer provides implementations for core interfaces and third-party integrations. This is where all the model clients are implemented. The AutoGen Studio is a low-code interface that enables the rapid prototyping of AI agents, and AutoGen Bench provides developers with tools to benchmark agents\u0026rsquo; performance across different tasks and environments.\nThe Magnetic-One application is a reference implementation that utilizes the AutoGen features, enabling users to perform open-ended web and file-based tasks across various domains.\nIf you want to start with AutoGen, this is the right time. In this article, I will walk through the basics of using AutoGen. We will go from using a simple model client to creating a simple agent. In the later parts of this series, we will look at creating a team of agents. As we do that, we will learn the concepts surrounding AutoGen.\nInstalling AutoGen As with any Python package, creating a virtual environment before installing any packages is always a good practice.\n1 2 3 4 5 $ mkdir -p autogen-101 \u0026amp;\u0026amp; cd autogen-101 $ python -m venv .venv $ source .venv\\bin\\activate $ pip install -U \u0026#34;autogen-agentchat\u0026#34; $ pip install \u0026#34;autogen-ext[openai,azure]\u0026#34; We can start with the autogen-agentchat library for simple AI agents and applications. The autogen-core library will be required for advanced scenarios.\nUsing model clients I chose Azure OpenAI to explore how to use model clients in AutoGen. AutoGen supports different models as extensions. With Azure OpenAI, you can use either key-based authentication or AAD token-based authentication. For the key-based authentication, we will use the following environment variables.\nAZURE_OPENAI_API_KEY: API key to authenticate with the Azure OpenAI service. AZURE_OPENAI_ENDPOINT: API endpoint associated with the Azure OpenAI service. AZURE_OPENAI_CHAT_DEPLOYMENT_NAME: Deployment name given while deploying the model. AZURE_OPENAI_API_VERSION: API version of the Azure OpenAI service To make it easy to retrieve these variable values in the script, we can use the load_dotenv() function.\n1 2 3 4 5 from dotenv import load_dotenv import os import asyncio load_dotenv() For the Azure OpenAI model client, we must import the AzureOpenAIChatCompletionClient from the autogen_ext.models.openai package. To create a conversation with the model, we need to send the prompt as a UserMessage, which is available in the autogen_core.models package.\n1 2 from autogen_ext.models.openai import AzureOpenAIChatCompletionClient from autogen_core.models import UserMessage, AssistantMessage, SystemMessage Let us combine this functionality to retrieve a response to a user prompt from the model.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 async def main(): aopenai_client = AzureOpenAIChatCompletionClient( model=os.getenv(\u0026#34;AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\u0026#34;), api_key=os.getenv(\u0026#34;AZURE_OPENAI_API_KEY\u0026#34;), api_version=os.getenv(\u0026#34;AZURE_OPENAI_API_VERSION\u0026#34;), endpoint=os.getenv(\u0026#34;AZURE_OPENAI_ENDPOINT\u0026#34;), ) result = await aopenai_client.create([ UserMessage( content=\u0026#34;What is AutoGen framework from Microsoft Research?\u0026#34;, source=\u0026#34;user\u0026#34;, ), ]) print(result.content) if __name__ == \u0026#34;__main__\u0026#34;: asyncio.run(main()) Running this program should produce output similar to the following.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 $ python 01-model-client.py AutoGen is an advanced framework developed by Microsoft Research that facilitates the creation of complex agent-based workflows centered around Large Language Models (LLMs). Released as an open-source Python library, AutoGen provides developers with tools to streamline communication and collaboration between LLM-powered agents while enabling dynamic automation and interaction with external APIs or services. ### Key Features of AutoGen: 1. **Agent Collaboration and Communication**: AutoGen allows for the orchestration of intelligent agents (e.g., user agents or assistant agents) that can dynamically interact through dialogues and tasks. These agents are designed to perform complex reasoning, problem-solving, and coordination. 2. **Extensibility and Customization**: AutoGen provides a simple yet powerful interface where developers can define custom agents with specific behaviors, abilities, and APIs. This makes it highly adaptable for diverse use cases. 3. **Streamlined Workflow Automation**: Using AutoGen, developers can build sophisticated workflows where multiple agents collaborate to achieve tasks like information retrieval, code generation, decision-making, content creation, and more. 4. **Ease of Use**: While enabling complex logic and inter-agent collaboration, AutoGen emphasizes simplicity, reducing the time and effort required for developers to design, implement, and manage multi-agent systems. 5. **Hybrid Intelligence**: The framework bridges LLMs and external tools, allowing agents to integrate well with domain-specific APIs, databases, and other services for enhanced operational capabilities. ### Example Use Cases: - **Code Development**: Agents can collaborate to design, write, debug, and review code based on user requirements. - **Business Process Automation**: AutoGen can automate workflows in customer support, data analysis, and report generation by leveraging human-like decision-making agents. - **Creative Content Generation**: Agents can coordinate for tasks requiring writing stories, generating marketing material, or brainstorming ideas collaboratively. ### Components of AutoGen: - **UserAgent**: Simulates a user interacting with the system. - **AssistantAgent**: Represents an LLM-powered agent that actively solves problems or performs delegated tasks. - **Custom Agents**: Developers can design unique agents tailored to specific roles or actions, integrating APIs, static data, or dynamic processes. - **Robust Communication Framework**: Facilitates seamless back-and-forth interaction between agents, with dialogue and task-driven workflows. ### Availability: Microsoft Research released AutoGen on GitHub as an open-source project. Developers can freely access the library, explore examples, and adapt its features to their needs. ### Significance: AutoGen highlights the growing trend of transitioning from single-agent LLM applications to multi-agent systems capable of collaborative intelligence. Such frameworks push the boundaries of what LLMs can achieve, offering opportunities for groundbreaking innovation in AI-driven automation and human-computer interaction. You can find the library and documentation for AutoGen [here on GitHub](https://github.com/microsoft/autogen). You can change the model client and experiment with different hosted or local models.\nUpdate: The 0.4.8 release of AutoGen natively supports Ollama-hosted local models. Here is a quick example of using the Ollama model client. You need to install the autogen-ext[ollama] package.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 from dotenv import load_dotenv import os import asyncio from autogen_ext.models.ollama import OllamaChatCompletionClient from autogen_core.models import UserMessage load_dotenv() async def main(): ollama_client = OllamaChatCompletionClient( model=\u0026#34;llama3.2:3b\u0026#34; ) result = await ollama_client.create([ UserMessage( content=\u0026#34;What is AutoGen framework from Microsoft Research?\u0026#34;, source=\u0026#34;user\u0026#34;, ), ]) print(result.content) if __name__ == \u0026#34;__main__\u0026#34;: asyncio.run(main()) The response may take a few seconds to minutes depending on the system configuration where you run the models locally and the model size.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 $ python .\\01-model-client-ollama.py AutoGen is a tool developed by Microsoft Research that enables the automatic generation of test cases for software applications. It is designed to help improve the quality and coverage of unit tests, integration tests, and UI tests. AutoGen uses a combination of machine learning algorithms and natural language processing (NLP) techniques to analyze source code, identify relevant features, and generate test cases that cover those features. The goal is to reduce the manual effort required to write test cases, making it easier for developers to ensure their application\u0026#39;s functionality is thoroughly tested. Here are some key features of AutoGen: 1. **Source code analysis**: AutoGen analyzes source code to identify relevant functions, methods, and classes. 2. **Feature extraction**: It extracts relevant features from the analyzed source code, such as input parameters, return types, and data structures. 3. **Test case generation**: Based on the extracted features, AutoGen generates test cases that cover those features. 4. **Test case optimization**: It optimizes generated test cases to minimize duplication and ensure maximum coverage. AutoGen can be used in various testing scenarios, including: 1. Unit testing: Generating test cases for individual functions or methods. 2. Integration testing: Testing interactions between components or modules. 3. UI testing: Creating test cases for user interface elements and behaviors. By leveraging machine learning and NLP techniques, AutoGen aims to make the testing process more efficient and effective, allowing developers to focus on other aspects of software development. Would you like to know more about how AutoGen works or its applications in specific industries? As you see, the local model hallucinated about what AutoGen is! :)\nCreate an agent The AgentChat package provides a set of pre-built agents. These include an AssistantAgent, UserProxyAgent, and CodeExecutionAgent, among others. In this article, we will examine the use of the AssistantAgent. Let\u0026rsquo;s dive into the example.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.ui import Console from autogen_ext.models.openai import AzureOpenAIChatCompletionClient import asyncio from dotenv import load_dotenv import os import json import requests from datetime import datetime from typing import Any load_dotenv() api_key = os.getenv(\u0026#34;AZURE_OPENAI_API_KEY\u0026#34;) model = os.getenv(\u0026#34;AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\u0026#34;) endpoint = os.getenv(\u0026#34;AZURE_OPENAI_ENDPOINT\u0026#34;) api_version = os.getenv(\u0026#34;AZURE_OPENAI_API_VERSION\u0026#34;) aoi_client = AzureOpenAIChatCompletionClient( model=model, api_key=api_key, endpoint=endpoint, api_version=api_version, ) def get_weather(city: str, date: datetime = None) -\u0026gt; dict[str, Any]: \u0026#34;\u0026#34;\u0026#34; Get the weather at a given location on a given date or current weather. Args: city: The city name, e.g. Bengaluru. date: Date on which the weather at the given location should be determined. This defaults to the current weather when a date is not specified. Returns: JSON string with the city name, date, and temperature. \u0026#34;\u0026#34;\u0026#34; api_key = os.getenv(\u0026#34;VISUAL_CROSSING_API_KEY\u0026#34;) if date is None: date = datetime.now().strftime(\u0026#34;%Y-%m-%d\u0026#34;) request_url = f\u0026#34;https://weather.visualcrossing.com/VisualCrossingWebServices/rest/services/timeline/{city}/{date}?unitGroup=metric\u0026amp;key={api_key}\u0026amp;contentType=json\u0026#34; response = requests.get(request_url) if response.status_code != 200: return json.dumps({ \u0026#34;error\u0026#34;: \u0026#34;Invalid city name or date\u0026#34; }) else: respJson = response.json() return json.dumps({ \u0026#34;city\u0026#34;: city, \u0026#34;date\u0026#34;: date, \u0026#34;temperature\u0026#34;: respJson[\u0026#34;days\u0026#34;][0][\u0026#34;temp\u0026#34;] }) agent = AssistantAgent( name=\u0026#34;weather_agent\u0026#34;, model_client=aoi_client, tools=[get_weather], system_message=\u0026#34;You are a helpful assistant.\u0026#34;, reflect_on_tool_use=True, model_client_stream=True, ) async def main() -\u0026gt; None: await Console( agent.on_messages_stream( task=\u0026#34;What is the weather in London?\u0026#34; ) ) asyncio.run(main()) The get_weather tool is the same function I used earlier when writing about Azure OpenAI API. It uses VisualCrossing weather API. If you do not have an API key for this service, replace the function body with a dummy return statement.\n1 2 3 4 5 6 7 8 9 10 11 \u0026#34;\u0026#34;\u0026#34; Get the weather at a given location on a given date or current weather. Args: city: The city name, e.g. Bengaluru. date: Date on which the weather at the given location should be determined. This defaults to the current weather when a date is not specified. Returns: JSON string with the city name, date, and temperature. \u0026#34;\u0026#34;\u0026#34; return f\u0026#34;The weather in {city} on {date} is 30 degrees celsius and rainy.\u0026#34; The agent.run_stream() method is a convenient wrapper around the on_messages_stream() method and\nUnlike the function or tool calling when using the OpenAI API directly, the AutoGen agent framework and the agents provide the ability to invoke the tools automatically based on the LLM response. We supply a list of tools available using the tools field.\n1 2 3 4 5 6 7 8 9 $ python 02-assistant-agent.py ---------- user ---------- What is the weather in London? ---------- weather_agent ---------- [FunctionCall(id=\u0026#39;call_Z4evOdxIYSuCsMyXEJXXyToR\u0026#39;, arguments=\u0026#39;{\u0026#34;city\u0026#34;:\u0026#34;London\u0026#34;}\u0026#39;, name=\u0026#39;get_weather\u0026#39;)] ---------- weather_agent ---------- [FunctionExecutionResult(content=\u0026#39;{\u0026#34;city\u0026#34;: \u0026#34;London\u0026#34;, \u0026#34;date\u0026#34;: \u0026#34;2025-03-23\u0026#34;, \u0026#34;temperature\u0026#34;: 11.0}\u0026#39;, name=\u0026#39;get_weather\u0026#39;, call_id=\u0026#39;call_Z4evOdxIYSuCsMyXEJXXyToR\u0026#39;, is_error=False)] ---------- weather_agent ---------- The current temperature in London is 11°C. Let me know if you\u0026#39;d like more details about the weather! The reflect_on_tool_use indicates if the agent should make another model inference using the tool call result. If you set this to False, you will not see the last response.\n1 2 3 4 5 6 7 8 9 $ python 02-assistant-agent.py ---------- user ---------- What is the weather in London? ---------- weather_agent ---------- [FunctionCall(id=\u0026#39;call_1DXbEuxDnHKHKj6d6DhWmueo\u0026#39;, arguments=\u0026#39;{\u0026#34;city\u0026#34;:\u0026#34;London\u0026#34;}\u0026#39;, name=\u0026#39;get_weather\u0026#39;)] ---------- weather_agent ---------- [FunctionExecutionResult(content=\u0026#39;{\u0026#34;city\u0026#34;: \u0026#34;London\u0026#34;, \u0026#34;date\u0026#34;: \u0026#34;2025-03-23\u0026#34;, \u0026#34;temperature\u0026#34;: 11.0}\u0026#39;, name=\u0026#39;get_weather\u0026#39;, call_id=\u0026#39;call_1DXbEuxDnHKHKj6d6DhWmueo\u0026#39;, is_error=False)] ---------- weather_agent ---------- {\u0026#34;city\u0026#34;: \u0026#34;London\u0026#34;, \u0026#34;date\u0026#34;: \u0026#34;2025-03-23\u0026#34;, \u0026#34;temperature\u0026#34;: 11.0} Setting model_client_stream to True or False influences if the result from the tool calling and the model should be streamed or not. We can use any of the four methods \u0026ndash; agent.on_messages(), agent.on_messages_stream(), agent.run(), and agent.run_stream() \u0026ndash; to supply the input prompt to the model.\nAlso, unlike the direct API calling, you don\u0026rsquo;t have to pass the conversation history as a part of the prompt. The AutoGen agents are stateful and maintain the state of the conversation in memory.\nAs you can see, AutoGen framework makes it significantly simple to implement agentic applications. In the later parts of this series, we shall look at some advanced concepts and features of AutoGen.\n","date":"2025-01-30T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/getting-started-with-autogen-framework-for-building-ai-agents-and-applications/","series":"AutoGen","title":"Getting started with AutoGen framework for building AI agents and applications"},{"categories":["azure","openai","python"],"content":"Sometimes, you may want the LLM to perform only minimal changes to what is provided as a prompt. For example, you have a couple of paragraphs of text that you want the LLM to modify to ensure no spelling mistakes. You do not want the LLM to change the overall content, but make sure the misspellings are corrected. This usually helps in reducing the LLM response latency. This is useful in scenarios where you already know a large portion of the expected response and is well-suited for code completion and error detection scenarios. In this part of the series of articles on Azure OpenAI, we will use predicted outputs with Azure OpenAI to build AI applications.\nFor this demonstration, we will use the following paragraph copied from the Azure OpenAI documentation and intentionally modified to induce a few spelling errors.\nAn embedding is a special fromat of data representation that machine learning modles and algorithms can easily use.\nThe embedding is an information dense representation of the sementic meaning of a piece of text.\nEach embedding is a vcetor of floating-point numbers, such that the distance between two embeddings in the\nvector space is correlated with semantic similarity betwien two inputs in the original format.\nI have highlighted the errors induced into the paragraph in bold.\nTo use the predicted outputs feature, we shall provide this as the user prompt to the LLM. Along with this prompt, we will provide certain instructions as well.\nIn the given content, find and fix all spelling mistakes.\nRespond with only the corrected paragraph and highlight the corrected words.\nIn the instructions, we ask LLM to correct all misspellings and then return only the corrected paragraph with corrected words highlighted.\nWe need to tell the LLM that we predict the response to be the same as the prompt, but supplying the prompt text to the prediction field of the client.chat.completions.create() method. Let us look at the complete code.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 from dotenv import load_dotenv import os from openai import AzureOpenAI from rich.console import Console from rich.markdown import Markdown load_dotenv() console = Console() AZURE_OPENAI_API_KEY=os.getenv(\u0026#34;AZURE_OPENAI_API_KEY\u0026#34;) AZURE_OPENAI_ENDPOINT=os.getenv(\u0026#34;AZURE_OPENAI_ENDPOINT\u0026#34;) AZURE_OPENAI_DEPLOYMENT_NAME=os.getenv(\u0026#34;AZURE_OPENAI_DEPLOYMENT_NAME\u0026#34;) client = AzureOpenAI( azure_endpoint = AZURE_OPENAI_ENDPOINT, api_key = AZURE_OPENAI_API_KEY, api_version = \u0026#34;2025-01-01-preview\u0026#34; ) content = \u0026#34;\u0026#34;\u0026#34; An embedding is a special fromat of data representation that machine learning modles and algorithms can easily use. The embedding is an information dense representation of the sementic meaning of a piece of text. Each embedding is a vcetor of floating-point numbers, such that the distance between two embeddings in the vector space is correlated with semantic similarity betwien two inputs in the original format. \u0026#34;\u0026#34;\u0026#34; instructions = \u0026#34;\u0026#34;\u0026#34;\u0026#34; In the given content, find and fix all spelling mistakes. Respond with only the corrected paragraph and highlight the corrected words. \u0026#34;\u0026#34;\u0026#34; response = client.chat.completions.create( model = AZURE_OPENAI_DEPLOYMENT_NAME, messages=[ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: instructions }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: content } ], prediction={ \u0026#34;type\u0026#34;: \u0026#34;content\u0026#34;, \u0026#34;content\u0026#34;: content, } ) md = Markdown(response.choices[0].message.content) console.print(md) When you run this program, you will see LLM respond with all corrected words in the paragraph.\n1 2 3 4 5 $ python.exe .\\09_predicted_output.py An embedding is a special format of data representation that machine learning models and algorithms can easily use. The embedding is an information-dense representation of the semantic meaning of a piece of text. Each embedding is a vector of floating-point numbers, such that the distance between two embeddings in the vector space is correlated with semantic similarity between two inputs in the original format. This feature can be really useful in applications where you want to highlight spelling errors, code linting errors, and so on.\n","date":"2025-01-10T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/predicted-outputs-azure-openai/","series":"Azure OpenAI","title":"Predicted outputs in Azure OpenAI"},{"categories":["azure","openai","python"],"content":"We have learned to perform single- and multi-tool calling with the Azure OpenAI API for chat completions. This part of the series on Azure OpenAI will describe the parallel tool calling feature and how to implement it. Parallel tool calling allows you to perform multiple calls together. This enables parallel execution, result retrieval, and fewer calls to the LLM. Parallelizing tool calls improves overall performance.\nIn a previous example on retrieving weather at a given location, we examined how to iterate over the LLM response for tool calls and append it to the conversation history before making the next LLM API call.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 for tool_call in response_message.tool_calls: if tool_call.function.name == \u0026#34;get_weather\u0026#34;: function_args = json.loads(tool_call.function.arguments) weather_response = get_weather( city_name=function_args.get(\u0026#34;city_name\u0026#34;) ) conversation_history.append({ \u0026#34;role\u0026#34;: \u0026#34;tool\u0026#34;, \u0026#34;tool_call_id\u0026#34;: tool_call.id, \u0026#34;name\u0026#34;: \u0026#34;get_weather\u0026#34;, \u0026#34;content\u0026#34;: weather_response, }) final_response = client.chat.completions.create( model=AZURE_OPENAI_DEPLOYMENT_NAME, messages=conversation_history, tools = tools, ) return final_response.choices[0].message.content In this method, we request the final response from LLM, causing it to use the response from the tool immediately. If we change the prompt from \u0026ldquo;What\u0026rsquo;s the weather like in Bengaluru?\u0026rdquo; to \u0026ldquo;What\u0026rsquo;s the weather like in Bengaluru, London, and Austin?\u0026rdquo;, the above logic will run in a sequence and result in three LLM API calls. We can make a trivial change to this function and reduce the number of LLM API calls.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 for tool_call in response_message.tool_calls: if tool_call.function.name == \u0026#34;get_weather\u0026#34;: function_args = json.loads(tool_call.function.arguments) weather_response = get_weather( city_name=function_args.get(\u0026#34;city_name\u0026#34;) ) conversation_history.append({ \u0026#34;role\u0026#34;: \u0026#34;tool\u0026#34;, \u0026#34;tool_call_id\u0026#34;: tool_call.id, \u0026#34;name\u0026#34;: \u0026#34;get_weather\u0026#34;, \u0026#34;content\u0026#34;: weather_response, }) final_response = client.chat.completions.create( model=AZURE_OPENAI_DEPLOYMENT_NAME, messages=conversation_history, tools = tools, ) return final_response.choices[0].message.content As you see above, we moved the final response block out of the for loop. This sends one consolidated API call to the LLM with all three responses retrieved from the get_weather tool.\n1 2 3 4 5 6 $ python 08_parallel_function_calling.py The current weather conditions are as follows: - **Bengaluru**: 26.6°C - **London**: 4.2°C - **Austin**: 16.9°C This is a short post for today. We will continue to learn about Azure OpenAI in this series of articles.\n","date":"2024-08-20T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/parallel-tool-calling-azure-openai/","series":"Azure OpenAI","title":"Parallel tool calling in Azure OpenAI"},{"categories":["azure","openai","python"],"content":"In this series, we have examined the basics of Azure Open AI, using the chat completions API, streaming responses, and finally, single and multi-tool calling. In today\u0026rsquo;s article, we will examine how to return structured output from the LLM response. We will first examine structured output without function calling and then update the earlier multi-function calling example to output JSON instead of text.\nStructured outputs tell an LLM to follow the schema represented by the response_format parameter of a request to the LLM. We can use Pydantic to build the schema.\nTake a look at this example.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 from dotenv import load_dotenv from pydantic import BaseModel from openai import AzureOpenAI import os load_dotenv() AZURE_OPENAI_API_KEY=os.getenv(\u0026#34;AZURE_OPENAI_API_KEY\u0026#34;) AZURE_OPENAI_ENDPOINT=os.getenv(\u0026#34;AZURE_OPENAI_ENDPOINT\u0026#34;) AZURE_OPENAI_DEPLOYMENT_NAME=os.getenv(\u0026#34;AZURE_OPENAI_DEPLOYMENT_NAME\u0026#34;) client = AzureOpenAI( azure_endpoint = AZURE_OPENAI_ENDPOINT, api_key = AZURE_OPENAI_API_KEY, api_version = \u0026#34;2024-08-01-preview\u0026#34; ) class TravelPlan(BaseModel): city_name: str date: str response = client.beta.chat.completions.parse( model=AZURE_OPENAI_DEPLOYMENT_NAME, messages=[ {\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Extract the city and travel information.\u0026#34;}, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;I will be travelling to London next week.\u0026#34;}, ], response_format=TravelPlan, ) stuctured_output = response.choices[0].message.parsed print(stuctured_output) print(response.choices[0].message.content) First, we must import the BaseModel from pydantic and define the structured output schema as a pydantic base model.\n1 2 3 class TravelPlan(BaseModel): city_name: str date: str We must change the client.chat.completions.create to client.beta.chat.completins.parse and add response_format as a parameter. If you access response.choices[0].message.parsed, you will extract the information returned by the model.\n1 2 $ python .\\07_structured_output-basics.py city_name=\u0026#39;London\u0026#39; date=\u0026#39;Next week\u0026#39; Accessing response.choices[0].message.content retrieves the JSON output for the extracted information.\nIf we need to modify the earlier example of a travel planner program that suggests clothes to wear based on the weather at a location, we repeat what we did in the above example.\nFirst, add a pydantic BaseModel class.\n1 2 3 4 5 class Suggestion(BaseModel): location: str date: str temperature: float cloth_suggestions: str And, change the get_model_response function to replace client.chat.completions.create with client.beta.chat.completions.parse and add response_format as an argument with Suggestion as the value. When you run this program, the model generates structured output as specified in the response format.\n1 2 3 4 $ python .\\07_structured_output-function_call.py Tool call: parse_day with arguments: {\u0026#34;day_string\u0026#34;:\u0026#34;next week\u0026#34;} Tool call: get_weather with arguments: {\u0026#34;city_name\u0026#34;:\u0026#34;Austin\u0026#34;,\u0026#34;date\u0026#34;:\u0026#34;2024-08-25\u0026#34;} {\u0026#34;location\u0026#34;:\u0026#34;Austin\u0026#34;,\u0026#34;date\u0026#34;:\u0026#34;2024-08-25\u0026#34;,\u0026#34;temperature\u0026#34;:35.5,\u0026#34;cloth_suggestions\u0026#34;:\u0026#34;The temperature in Austin next week is expected to be around 35.5°C. This is hot weather, so I recommend carrying light layers such as t-shirts and short-sleeve shirts.\u0026#34;} Structured output can be very useful for multi-step workflows. In the next part of the series, we will see parallel tool calling in Azure OpenAI.\n","date":"2024-08-18T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/structured-output-azure-openai/","series":"Azure OpenAI","title":"Structured output in Azure OpenAI"},{"categories":["azure","openai","python"],"content":"In the last article of this series, we learned about function/tool calling. Based on the prompt, the LLM indicates that we must call the get_weather tool. The LLM finally returns the answer to our prompt using the tool response. However, let us try to add a few more variables to our prompt. The updated prompt will be \u0026ldquo;What\u0026rsquo;s the weather like in Bengaluru next week?\u0026rdquo;.\n1 2 3 $ python .\\05_function_calling.py get_current_time called with location: Bengaluru The current temperature in Bengaluru is approximately 28.2°C. However, for next week\u0026#39;s weather prediction, you\u0026#39;d need a forecast service as I currently provide only current weather information. LLM uses the get_weather tool to determine the current weather but fails to determine next week\u0026rsquo;s weather. This is because we have not provided any tool for the LLM to determine what next week means. Determining the meaning of next week requires the knowledge of the current date and time. This article will demonstrate how to add multiple tool-calling capabilities to our program. With the updated script, you can receive the weather information for a specific date.\nIn this example, I updated the prompt to read, \u0026ldquo;I am going to Austin next Monday. Based on the weather, suggest what kind of clothes I need to carry.\u0026rdquo; Here is how the LLM will respond.\n1 2 3 4 5 6 7 8 9 10 $ python .\\06_multiple_function_calling.py Tool call: parse_day with arguments: {\u0026#34;day_string\u0026#34;:\u0026#34;next Monday\u0026#34;} Tool call: get_weather with arguments: {\u0026#34;city_name\u0026#34;:\u0026#34;Austin\u0026#34;,\u0026#34;date\u0026#34;:\u0026#34;2024-08-19\u0026#34;} The weather in Austin next Monday (August 19, 2024) is expected to be around 37°C. This is hot weather. You should pack light and breathable clothing such as: - T-shirts or short-sleeved shirts - Shorts or lightweight pants - Comfortable walking shoes or sandals We can use the following function to translate the text representation of a day to a date.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 def parse_day(day_string): \u0026#34;\u0026#34;\u0026#34; Parses a day string and returns a datetime.date object. Args: day_string: The string to parse (e.g., \u0026#34;today\u0026#34;, \u0026#34;tomorrow\u0026#34;, \u0026#34;next Monday\u0026#34;). Returns: date as a strning, or invalid input if the input is invalid. \u0026#34;\u0026#34;\u0026#34; cal = parsedatetime.Calendar() time_struct, parse_status = cal.parse(day_string) if parse_status == 0: return \u0026#34;Invalid input\u0026#34; return json.dumps({ \u0026#34;day_string\u0026#34;: day_string, \u0026#34;date\u0026#34;: datetime(*time_struct[:6]).strftime(\u0026#34;%Y-%m-%d\u0026#34;) }) The parse_day function uses the parsedatetime package and returns the date string in YYYY-MM-DD format.\nI am using the VisualCrossing weather API to get weather information for a specified date. We must also update the get_weather function to use the date supplied as the input parameter.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 def get_weather(city_name, date=None): \u0026#34;\u0026#34;\u0026#34; Get the weather at a given location on a given date or current weather. Args: city_name: The city name, e.g. Bengaluru. date: Date on which the weather at the given location should be determined. This defaults to the current weather when a date is not specified. Returns: JSON string with the city name, date, and temperature. \u0026#34;\u0026#34;\u0026#34; api_key = os.getenv(\u0026#34;VISUAL_CROSSING_API_KEY\u0026#34;) if date is None: date = datetime.now().strftime(\u0026#34;%Y-%m-%d\u0026#34;) request_url = f\u0026#34;https://weather.visualcrossing.com/VisualCrossingWebServices/rest/services/timeline/{city_name}/{date}?unitGroup=metric\u0026amp;key={api_key}\u0026amp;contentType=json\u0026#34; response = requests.get(request_url) if response.status_code != 200: return json.dumps({ \u0026#34;error\u0026#34;: \u0026#34;Invalid city name or date\u0026#34; }) else: respJson = response.json() return json.dumps({ \u0026#34;city_name\u0026#34;: city_name, \u0026#34;date\u0026#34;: date, \u0026#34;temperature\u0026#34;: respJson[\u0026#34;days\u0026#34;][0][\u0026#34;temp\u0026#34;] }) With these two tools in place, we can now focus on interacting with the LLM. The get_model_response function takes the conversation history as input and returns the LLM\u0026rsquo;s response.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def get_model_response(conversation_history): client = AzureOpenAI( azure_endpoint = AZURE_OPENAI_ENDPOINT, api_key = AZURE_OPENAI_API_KEY, api_version = \u0026#34;2024-02-01\u0026#34; ) response = client.chat.completions.create( model=AZURE_OPENAI_DEPLOYMENT_NAME, messages=conversation_history, tools=tools, temperature=0.0, tool_choice=\u0026#34;auto\u0026#34;, ) return response The second function we need is a way to invoke the tool indicated by the LLM.\n1 2 3 4 def get_tool_response(tool_name, tool_arguments): tools_args = json.loads(tool_arguments) tool_response = globals()[tool_name](**tools_args) return tool_response The get_tool_response function takes the tool name and arguments as parameters. It then invokes the function and returns response from the tool call. These two functions are straightforward. The real logic for multi-turn tool calling is in the main block.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 question = \u0026#34;I am going to Austin next week. Based on the weather, suggest what kind of cloths I need to carry.\u0026#34; conversation_history = [ {\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;You are a helpful assistant. You should use the tools provided when needed to generate a response.\u0026#34;}, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: question} ] while True: response = get_model_response(conversation_history) response_message = response.choices[0].message if response_message.tool_calls: conversation_history.append({ \u0026#34;role\u0026#34;: \u0026#34;assistant\u0026#34;, \u0026#34;tool_calls\u0026#34;: [tool_call.to_dict() for tool_call in response_message.tool_calls] }) for tool_call in response_message.tool_calls: print(f\u0026#34;Tool call: {tool_call.function.name} with arguments: {tool_call.function.arguments}\u0026#34;) tool_response = get_tool_response(tool_call.function.name, tool_call.function.arguments) conversation_history.append( { \u0026#34;role\u0026#34;: \u0026#34;tool\u0026#34;, \u0026#34;tool_call_id\u0026#34;: tool_call.id, \u0026#34;name\u0026#34;: tool_call.function.name, \u0026#34;content\u0026#34;: tool_response } ) else: print(response_message.content) break In this block of code, we set the conversation history with a system message and the user\u0026rsquo;s prompt. The system prompt tells the language model to use the tools as necessary to generate the response.\nWithin the while loop in this block, we check if the response contains any tool calls. If it does, we use the get_tool_response function to invoke the tool and return the response. We then update the conversation history to add the response from the tool call and invoke the get_model_response function again. This loop goes on until there are no more tool calls. Finally, we will return the final response generated by the LLM to the user.\nThis example demonstrated how to use multiple tool calls to generate the response a user intends to see. In some scenarios, such as another tool using the model\u0026rsquo;s tool calling functionality, you may want to return structured output to the caller. We shall see that in the next part of this series.\n","date":"2024-08-16T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/azure-openai-function-calling-with-multiple-tools/","series":"Azure OpenAI","title":"Implementing multiple tool/function calling when using Azure OpenAI"},{"categories":["azure","openai","python"],"content":"In the last article of this series, we learned how to use the chat completion API. Towards the end, we learned that the LLMs have a knowledge cut-off date and no real-time access to information. However, this can be bridged using the tool/function calling feature of Azure OpenAI service. In this article, we shall learn how to implement tool calling.\nAzure OpenAI\u0026rsquo;s function calling capability lets you connect your language models to external tools and APIs, enabling them to perform a wider range of tasks and access real-time information. This opens up a world of possibilities, allowing your models to interact with the real world in ways never imagined.\nThe Power of Tool Calling Imagine asking your language model to \u0026ldquo;Book a flight from Bengaluru to Delhi next Monday.\u0026rdquo; Without tool calling, the model could only provide general flight information or suggest potential airlines. But with tool calling, it can directly interact with a flight booking API, search for available flights, compare prices, and even make the booking for you!\nThis is just one example of how tool calling can empower your language models. Other use cases include:\nRetrieving real-time data like weather forecasts, stock prices, or news updates Performing actions like sending emails, scheduling appointments, or controlling smart home devices Accessing proprietary information from your databases or internal systems Implementing tools To implement tool calling, you must first implement the necessary tools or functions. For this purpose, we shall build a function that retrieves real-time weather information for a given location. We will use Open Weather Map API for this purpose. You can sign up and get an API key to work with this API.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def get_weather(city_name): \u0026#34;\u0026#34;\u0026#34;Get the current weather for a given location\u0026#34;\u0026#34;\u0026#34; print(f\u0026#34;get_current_time called with location: {city_name}\u0026#34;) city_lower = city_name.lower() api_key = os.getenv(\u0026#34;OPEN_WEATHER_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather?\u0026#34; complete_url = base_url + \u0026#34;appid=\u0026#34; + api_key + \u0026#34;\u0026amp;q=\u0026#34; + city_lower response = requests.get(complete_url) respJson = response.json() return json.dumps({ \u0026#34;city_name\u0026#34;: city_name, \u0026#34;temperature\u0026#34;: respJson[\u0026#34;main\u0026#34;][\u0026#34;temp\u0026#34;] }) This function retrieves weather data for a given city using the OpenWeatherMap API. It accepts the city name as input, constructs the API URL, makes the request, and returns the JSON response containing the temperature. Besides the logic, the structure of this function is important to consider as a tool. The function must have a doc string that describes its purpose. It needs to return a JSON object. Once we have this function, we can look at the code that interacts with the LLM.\nImplementing tool calling We implement tool calling and LLM interaction as the get_response function in this example. Within the function, we must define the available tools, which are identified by the tools list.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def get_response(prompt): # Define the available tools tools = [ { \u0026#34;type\u0026#34;: \u0026#34;function\u0026#34;, \u0026#34;function\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;get_weather\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Get the current weather for a given location\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;city_name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;The city name, e.g. Bengaluru\u0026#34;, }, }, \u0026#34;required\u0026#34;: [\u0026#34;city_name\u0026#34;], }, } } ] # Initialize the conversation history conversation_history = [ {\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;You are a helpful assistant. You should use the tools provided when needed to generate a response. When asked about the weather, return the response in Celsius.\u0026#34;}, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: prompt} ] # Create an Azure OpenAI client client = AzureOpenAI( azure_endpoint = AZURE_OPENAI_ENDPOINT, api_key = AZURE_OPENAI_API_KEY, api_version = \u0026#34;2024-02-01\u0026#34; ) # Send the initial prompt to the model response = client.chat.completions.create( model=AZURE_OPENAI_DEPLOYMENT_NAME, messages=conversation_history, tools=tools, tool_choice=\u0026#34;auto\u0026#34;, ) response_message = response.choices.message # Check if the model wants to use any tools if response_message.tool_calls: # Add the tool call to the conversation history conversation_history.append({ \u0026#34;role\u0026#34;: \u0026#34;assistant\u0026#34;, \u0026#34;tool_calls\u0026#34;: [tool_call.to_dict() for tool_call in response_message.tool_calls] }) # Iterate through the tool calls for tool_call in response_message.tool_calls: if tool_call.function.name == \u0026#34;get_weather\u0026#34;: # Extract the function arguments function_args = json.loads(tool_call.function.arguments) print(f\u0026#34;Function arguments: {function_args}\u0026#34;) # Call the get_weather function with the provided arguments weather_response = get_weather( city_name=function_args.get(\u0026#34;city_name\u0026#34;) ) # Add the tool response to the conversation history conversation_history.append({ \u0026#34;role\u0026#34;: \u0026#34;tool\u0026#34;, \u0026#34;tool_call_id\u0026#34;: tool_call.id, \u0026#34;name\u0026#34;: \u0026#34;get_weather\u0026#34;, \u0026#34;content\u0026#34;: weather_response, }) # Send the updated conversation history back to the model final_response = client.chat.completions.create( model=AZURE_OPENAI_DEPLOYMENT_NAME, messages=conversation_history, tools = tools, ) return final_response.choices.message.content else: # If no tool calls are required, return the initial response return response_message.content We defined a system prompt that tells the LLM to use the tools when available. We add the user-supplied prompt to it and then invoke the API to get a response. If the user\u0026rsquo;s prompt includes a question about the weather at some location, the LLM determines that the get_weather tools must be invoked and indicates the same in response to the user. The response handling function, get_response, must handle this, call the tool with the identified arguments, and send the response back to the LLM as the response from the tool call.\n1 2 3 4 conversation_history.append({ \u0026#34;role\u0026#34;: \u0026#34;assistant\u0026#34;, \u0026#34;tool_calls\u0026#34;: [tool_call.to_dict() for tool_call in response_message.tool_calls] }) I want to re-iterate that calling tools indicated by the LLM is the application\u0026rsquo;s responsibility. LLM never calls the tools directly.\nOnce the tool call response, along with the prior conversation history, is provided to the LLM, the LLM\u0026rsquo;s final response includes the answer to the user\u0026rsquo;s prompt.\nWe can include a call to the get_response function to invoke the workflow.\n1 2 3 4 if __name__ == \u0026#34;__main__\u0026#34;: question = \u0026#34;What\u0026#39;s the weather like in Bengaluru?\u0026#34; response = get_response(question) print(response) Here is the response for a user prompt \u0026ldquo;What\u0026rsquo;s the weather like in Bengaluru?\u0026rdquo;\n1 2 3 (.venv) PS C:\\\u0026gt; python.exe .\\05_function_calling.py get_current_time called with location: Bengaluru The current temperature in Bengaluru is approximately 23°C. If the prompt does not contain any request for weather information, LLM would behave normally.\n1 2 (.venv) PS C:\\\u0026gt; python.exe .\\05_function_calling.py I currently don\u0026#39;t have access to real-time traffic information. You can check platforms such as Google Maps, Waze, or local traffic apps for up-to-date traffic conditions in Bengaluru. Let me know if there\u0026#39;s anything else I can assist you with! This is a good start. This example showed only one tool definition. However, you can provide any number of tools, and as long as those tools are adequately described, the LLM will be able to indicate what tools to call iteratively. In the next article in this series, we will look at that.\n","date":"2024-08-12T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/azure-openai-function-calling/","series":"Azure OpenAI","title":"Implementing tool/function calling when using Azure OpenAI"},{"categories":["azure","openai","python"],"content":"So far in this series, we have looked at the Azure OpenAI completion API, which generates a response for a given prompt. This is a legacy API, and using the chat completion API is recommended. We can build conversational chatbots and similar applications with the chat completion API. This article will examine how to use the Azure OpenAI chat completion API. In the earlier articles, we used the client.completions.create() function to generate a response. We need to use the client.chat.completions.create() in the openai library to build a conversation with the LLM.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 from dotenv import load_dotenv import os from openai import AzureOpenAI from rich.console import Console from rich.markdown import Markdown load_dotenv() console = Console() AZURE_OPENAI_API_KEY=os.getenv(\u0026#34;AZURE_OPENAI_API_KEY\u0026#34;) AZURE_OPENAI_ENDPOINT=os.getenv(\u0026#34;AZURE_OPENAI_ENDPOINT\u0026#34;) AZURE_OPENAI_DEPLOYMENT_NAME=os.getenv(\u0026#34;AZURE_OPENAI_DEPLOYMENT_NAME\u0026#34;) client = AzureOpenAI( azure_endpoint = AZURE_OPENAI_ENDPOINT, api_key = AZURE_OPENAI_API_KEY, api_version = \u0026#34;2024-02-01\u0026#34; ) response = client.chat.completions.create( model = AZURE_OPENAI_DEPLOYMENT_NAME, messages=[ {\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;You are a helpful tutor and an expert in Artificial Intelligence. You should provide a tabular output when the user asks to compare two things.\u0026#34;}, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What is the difference between OpenAI and Azure OpenAI?\u0026#34;}, ], max_tokens=1000 ) md = Markdown(response.choices[0].message.content) console.print(md) This example uses the rich.console and rich.markdown packages to convert the markdown response from the LLM to formatted output for the console. Make sure you add these packages to requirements.txt and install the dependencies.\nThe client.chat.completions.create() function is similar to the earlier one we used. However, this function has many more parameters than the simple completion API. We are most interested in the messages parameter, a list of messages. Each message in the list is a dictionary that contains keys such as role and content. The value of the role key can be developer, system, user, assistant, tool, and function. The developer and system roles are interchangeable and are used to specify the system prompt \u0026ndash; a message that tells the LLM how it should respond. The assistant role is used to indicate the message from the LLM. The user role is used to tag the user-supplied prompt. We shall look at tool and function roles in a later article.\nComing back to this example, we have two messages. A system message that indicates to the LLM how it needs to interpret and respond to the user prompt. The second message is the user prompt. The system prompt is important as it can provide a specific task or instruction to the model. In this example, we ask the LLM to return the response in a tabular format whenever a user asks for a comparison between two things. We will learn more about it when discussing prompt engineering.\nOnce a response is generated, we can supply the response.choices[0].message.content to the Markdown() function to transform the text to a format that can be printed to the console. Finally, using the console.print(md) function, we print the output to the console.\nYou can continue this conversation by adding the response from the LLM as the assistant message and then supply a new user prompt. We do not have any user interface to interact with the model as a conversation. However, we can implement a simple loop to \u0026ldquo;chat\u0026rdquo; with the model at the command line.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 from dotenv import load_dotenv import os from openai import AzureOpenAI load_dotenv() AZURE_OPENAI_API_KEY=os.getenv(\u0026#34;AZURE_OPENAI_API_KEY\u0026#34;) AZURE_OPENAI_ENDPOINT=os.getenv(\u0026#34;AZURE_OPENAI_ENDPOINT\u0026#34;) AZURE_OPENAI_DEPLOYMENT_NAME=os.getenv(\u0026#34;AZURE_OPENAI_DEPLOYMENT_NAME\u0026#34;) client = AzureOpenAI( azure_endpoint = AZURE_OPENAI_ENDPOINT, api_key = AZURE_OPENAI_API_KEY, api_version = \u0026#34;2024-02-01\u0026#34; ) conversation_history = [ {\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;You are a helpful tutor and an expert in Artificial Intelligence. You should provide a tabular output when the user asks to compare two things.\u0026#34;}, ] while True: user_prompt = input(\u0026#34;\\nYou: \u0026#34;) if user_prompt.lower() == \u0026#34;exit\u0026#34;: break conversation_history.append({\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: user_prompt}) response = client.chat.completions.create( model = AZURE_OPENAI_DEPLOYMENT_NAME, messages=conversation_history ) assistant_message = response.choices[0].message.content conversation_history.append({\u0026#34;role\u0026#34;: \u0026#34;assistant\u0026#34;, \u0026#34;content\u0026#34;: assistant_message}) print(\u0026#34;\\nAssistant: \u0026#34; + assistant_message) This updated example is not very different from what we have already tried. It simply prompts the user from a message until it is exit. Both the user_prompt and the response from the LLM, assistant_message, are added to the conversation history. The conversation history is provided to the LLM as messages; therefore, the LLM always has the complete chat context.\nIn general, the LLMs have a knowledge cut-off date. This means the LLM has been trained only until a certain date and will not know the most recent or real-time information, which typically results in hallucinations.\n1 2 3 4 5 6 7 (.venv) PS C:\\\u0026gt; python.exe .\\04_chat_loop.py You: what is the weather in London now? Assistant: I\u0026#39;m unable to provide real-time weather updates because I don\u0026#39;t have access to live data. However, you can check the current weather in London by visiting a trusted weather website, such as [Weather.com](https://www.weather.com) or using a weather app on your smartphone. Feel free to ask if you need guidance on understanding weather forecasts or climate-related concepts! You: One way to address this issue is to supply the LLM with the latest knowledge in the form of Retrieval Augmented Generation (RAG) or using tools/functions. We shall look at tool/function calling in the next article.\n","date":"2024-08-08T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/using-chat-completion-api-azure-openai/","series":"Azure OpenAI","title":"Using chat completion API in Azure OpenAI"},{"categories":["azure","openai","python"],"content":"Responses are streamed to the user interface as they are generated using ChatGPT and similar tools. This eliminates the need for the user to wait until the complete response is generated. In today\u0026rsquo;s article, we shall look at streaming LLM-generated responses when using Azure OpenAI API in Python. In the earlier part of this series, we learned about the client.completions.create() function used to send a prompt to the LLM and retrieve one or more responses. This function supports a parameter called stream when set to True, asks LLM to stream the response as it gets generated. The way to handle this response is a bit different from a standard completion response.\nLet us take a look at the example.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 from dotenv import load_dotenv import os from openai import AzureOpenAI load_dotenv() AZURE_OPENAI_API_KEY_FOR_INSTRUCT=os.getenv(\u0026#34;AZURE_OPENAI_API_KEY_FOR_INSTRUCT\u0026#34;) AZURE_OPENAI_ENDPOINT_FOR_INSTRUCT=os.getenv(\u0026#34;AZURE_OPENAI_ENDPOINT_FOR_INSTRUCT\u0026#34;) AZURE_OPENAI_DEPLOYMENT_NAME_FOR_INSTRUCT=os.getenv(\u0026#34;AZURE_OPENAI_DEPLOYMENT_NAME_FOR_INSTRUCT\u0026#34;) client = AzureOpenAI( api_key=os.getenv(\u0026#34;AZURE_OPENAI_API_KEY_FOR_INSTRUCT\u0026#34;), api_version=\u0026#34;2024-02-01\u0026#34;, azure_endpoint = os.getenv(\u0026#34;AZURE_OPENAI_ENDPOINT_FOR_INSTRUCT\u0026#34;) ) start_phrase = \u0026#39;What is the transformer architecture? Explain self-attention mechanism. Provide the response in 5 bullet points.\u0026#39; response = client.completions.create( model=AZURE_OPENAI_DEPLOYMENT_NAME_FOR_INSTRUCT, prompt=start_phrase, max_tokens=1000, stream=True ) print(start_phrase) for chunk in response: if chunk.choices: if chunk.choices[0].finish_reason != \u0026#39;stop\u0026#39;: text = chunk.choices[0].text print(text, end=\u0026#39;\u0026#39;) In this example, the response arrives at the client as chunks. Each chunk needs to be read to retrieve the partially generated response. Each chunk contains a finish_reason to determine when to stop iterating (finish_reason == 'stop') over the chunks. The print() function appends a new line when printing the chunk. As these are partially generated responses, adding a new line makes the output unreadable. We, therefore, set the end parameter of the ''.\nWhen we run this program, you can see words and lines streamed to the console.\n1 2 3 4 5 6 7 8 (.venv) PS C:\\\u0026gt; python.exe .\\02_completion_stream.py What is the transformer architecture? Explain self-attention mechanism. Provide the response in 5 bullet points. 1. The transformer architecture is a type of neural network used for natural language processing tasks such as machine translation and text generation. 2. It was introduced by Google in 2017 and has since become one of the most popular architectures for NLP tasks due to its ability to handle long sequences and avoid the use of recurrent neural networks. 3. Rather than using recurrent connections, the transformer architecture uses self-attention mechanisms to process input sequences and make predictions. 4. The self-attention mechanism allows the model to focus on specific parts of the input sequence, learning the relationship between different words in a sentence and capturing dependencies between words that are far apart. 5. This is achieved by calculating a weighted sum of the input sequence where the weights are determined by the relevance of each word in the sequence to the current word being processed. This allows the model to learn contextual information and make better predictions for each word in the sequence. In this part, you learned how to stream the responses and deal with the response chunks to build meaningful output. In the next part, we shall look at how to use the chat completion API to build a conversation with the LLM.\n","date":"2024-08-04T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/streaming-completions-azure-openai/","series":"Azure OpenAI","title":"Streaming model responses when using Azure OpenAI"},{"categories":["azure","openai","python"],"content":"Generative AI and OpenAI should not be alien anymore. Several startups are already riding this new wave and creating stunning applications that solve several important use cases. I use GenAI regularly to learn and become more efficient in coding. GitHub Copilot has been a good friend. I experimented with creating Large Language Model (LLM) applications using different providers (OpenAI and Google Gemini) and in different programming languages. OpenAI provides client libraries that can be used with any provider that offers an OpenAI-compatible API. For example, we can use the OpenAI Python library to work with OpenAI and Azure OpenAI services.\nRecently, I started researching the OpenAI Python library and Azure OpenAI service. This series of articles will focus on interacting with the Azure OpenAI service and different language models using the Python library.\nThis series assumes that you have an active Azure subscription and have provisioned Azure OpenAI service. You need to deploy the models of your choice and have the API key and endpoint information handy. For information on getting started with Azure OpenAI service, look at What is Azure OpenAI Service? - Azure AI services | Microsoft Learn.\nLet us start with the basics.\nLike any other Python project, create a virtual environment using your favorite tool before you start. I prefer venv for simple projects and quick programs that I author.\n1 2 3 4 5 6 7 8 9 10 $ python -m venv .venv # activate venv on Linux $ source .venv/bin/activate # activate venv on Windows $ .\\.venv\\Scripts\\Activate.ps1 # Once activated, you will see the prompt change (.venv) PS C:\\\u0026gt; It is a good practice to list all program dependencies in a requirements.txt, so let\u0026rsquo;s create one.\n1 2 openai python-dotenv openai is the Python library to interact with the OpenAI-compatible API. The python-dotenv helps manage environment variables. We can run the pip install -r requirements.txt command within the virtual environment to install these dependencies. At the basic level, there are two types of LLM interactions \u0026ndash; completion and chat completion. The completion is a one-time message/prompt to the LLM, whereas using chat completion, you can converse with the LLM. Let\u0026rsquo;s start with the legacy completion API. The newer models do not support legacy completion. Therefore, the following example uses gpt-35-turbo-instruct.\nAs mentioned earlier, you will need the API key and endpoint. I prefer storing these values as key-value pairs in a .env file and accessing those values as environment variables using the python-dotenv library.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 from dotenv import load_dotenv import os from openai import AzureOpenAI load_dotenv() AZURE_OPENAI_API_KEY_FOR_INSTRUCT=os.getenv(\u0026#34;AZURE_OPENAI_API_KEY_FOR_INSTRUCT\u0026#34;) AZURE_OPENAI_ENDPOINT_FOR_INSTRUCT=os.getenv(\u0026#34;AZURE_OPENAI_ENDPOINT_FOR_INSTRUCT\u0026#34;) AZURE_OPENAI_DEPLOYMENT_NAME_FOR_INSTRUCT=os.getenv(\u0026#34;AZURE_OPENAI_DEPLOYMENT_NAME_FOR_INSTRUCT\u0026#34;) client = AzureOpenAI( api_key=os.getenv(\u0026#34;AZURE_OPENAI_API_KEY_FOR_INSTRUCT\u0026#34;), api_version=\u0026#34;2024-02-01\u0026#34;, azure_endpoint = os.getenv(\u0026#34;AZURE_OPENAI_ENDPOINT_FOR_INSTRUCT\u0026#34;) ) start_phrase = \u0026#39;What would be the tagline if Microsoft Azure was an ice cream shop?\u0026#39; response = client.completions.create( model=AZURE_OPENAI_DEPLOYMENT_NAME_FOR_INSTRUCT, prompt=start_phrase, max_tokens=30 ) print(start_phrase+response.choices[0].text) load_dotenv() function loads all key-value pairs in the .env file as environment variables, which are then retrieved using the os.getenv() function. We can create an Azure OpenAI client using the AzureOpenAI() function in the openai library. This function has several parameters. For the example in this article, we need to set api_key, api_version, and azure_endpoint parameters.\nThe client.completions.create() function sends the prompt string to the API and retrieves the generated response. This function has several parameters that we can use to tweak the LLM\u0026rsquo;s behavior and response. For now, we must supply the model, prompt, and max_tokens. The max_tokens parameter tells the LLM how many tokens it should generate. The response from the LLM will be a complex JSON object. We can retrieve the text from the generated response using response.choices[0].text. Why choices[0]? We shall see that in a bit.\n1 2 3 4 (.venv) PS C:\\\u0026gt; python.exe .\\01_completion.py What would be the tagline if Microsoft Azure was an ice cream shop? \u0026#34;Chill out with Microsoft Azure - serving the coolest cloud flavors!\u0026#34; This is nice! But if you notice ChatGPT or other such applications, more than one response to the prompt is returned, and you can choose from the options. The same behavior can be achieved using the Azure OpenAI library by supplying a parameter n with an integer as a value. This parameter represents the number of completions to generate for each prompt. Here is the modified program.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 from dotenv import load_dotenv import os from openai import AzureOpenAI load_dotenv() AZURE_OPENAI_API_KEY_FOR_INSTRUCT=os.getenv(\u0026#34;AZURE_OPENAI_API_KEY_FOR_INSTRUCT\u0026#34;) AZURE_OPENAI_ENDPOINT_FOR_INSTRUCT=os.getenv(\u0026#34;AZURE_OPENAI_ENDPOINT_FOR_INSTRUCT\u0026#34;) AZURE_OPENAI_DEPLOYMENT_NAME_FOR_INSTRUCT=os.getenv(\u0026#34;AZURE_OPENAI_DEPLOYMENT_NAME_FOR_INSTRUCT\u0026#34;) client = AzureOpenAI( api_key=os.getenv(\u0026#34;AZURE_OPENAI_API_KEY_FOR_INSTRUCT\u0026#34;), api_version=\u0026#34;2024-02-01\u0026#34;, azure_endpoint = os.getenv(\u0026#34;AZURE_OPENAI_ENDPOINT_FOR_INSTRUCT\u0026#34;) ) start_phrase = \u0026#39;What would be the tagline if Microsoft Azure was an ice cream shop?\u0026#39; response = client.completions.create( model=AZURE_OPENAI_DEPLOYMENT_NAME_FOR_INSTRUCT, prompt=start_phrase, max_tokens=30, n=2 ) print(start_phrase) for choice in response.choices: print(choice.text, end=\u0026#34;\u0026#34;) When we execute this, we shall see two responses. We can iterate over the response object and retrieve the generated text.\n1 2 3 4 5 6 (.venv) PS C:\\\u0026gt; python.exe .\\01_completion.py What would be the tagline if Microsoft Azure was an ice cream shop? \u0026#34;Satisfy your cravings for the cloud and creamy treat at Microsoft Azure: The Scoop of Innovation!\u0026#34; \u0026#34;Indulge in the coolness of Microsoft Azure, where tech meets treats!\u0026#34; This is it for today. This is a good start. We will look at streaming completion responses in the next article in this series.\n","date":"2024-08-01T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/getting-started-with-azure-openai/","series":"Azure OpenAI","title":"Getting started with Azure OpenAI"},{"categories":null,"content":"Containers changed the way we develop, package, and distribute applications. This helped teams accelerate development and ensure their code worked consistently across different environments and infrastructures. To a large extent, this enabled write once and run anywhere. Docker made this easy by providing tools to create, manage, and package application code and dependencies as images. We could run a container instance from that image wherever a container runtime exists. Standardizing image and runtime formats helped ensure we could take an image and run it anywhere and not worry about the underlying infrastructure. This also led to the proliferation of microservices-based application architectures, where an application comprises several services running within containers to deliver the expected functionality. Most cloud-native and modern application architectures use microservices to implement the necessary functionality. When developing an application with multiple microservice dependencies, a developer must ensure the development environment is suitably set up to handle all required dependencies.\nVisual Studio Code and many other IDEs offer a way to use containers as local development environments. This ensures that the application code and code dependencies are handled consistently across development, testing, and production environments. Using dev containers for a single container application or service is straightforward.\nThis article uses Docker desktop and Windows Subsystem for Linux 2 to run containers on a Windows PC. Other container engines, such as Podman, can also be used.\nWe need to install the VS Code dev containers extension to get started. Once the extension is installed, we can open the code repository and create a dev container configuration file. We can follow the options within the command palette (F1 or CTRL+SHIFT+P) or write a configuration file. VS Code offers intelligence when authoring a dev container configuration file.\nStart by creating a devcontainer folder at the root of the code repository.\n1 2 $ mkdir .devcontainer $ touch .devcontainer/devcontainer.json The dev container configuration files use JSON format. We can specify how the container should be created in different ways. You can either use an image or create an image from a Dockerfile. In this example, we will use an image from the Microsoft container registry.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 { \u0026#34;image\u0026#34;: \u0026#34;mcr.microsoft.com/vscode/devcontainers/base:jammy\u0026#34;, \u0026#34;customizations\u0026#34;: { \u0026#34;vscode\u0026#34;: { \u0026#34;settings\u0026#34;: {}, \u0026#34;extensions\u0026#34;: [ \u0026#34;golang.go\u0026#34; ] } }, \u0026#34;features\u0026#34;: { \u0026#34;ghcr.io/devcontainers/features/go:1\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34; } }, \u0026#34;remoteUser\u0026#34;: \u0026#34;vscode\u0026#34; } The customizations section of the configuration file specifies properties specific to VS Code, such as extensions. In this example, the Go language VS Code extension is specified. The features section specifies the additional features that should be installed in the container. This example specifies the Go language binaries to be installed. We can open this code repository in a dev container by selecting the \u0026ldquo;Dev containers: Reopen in container\u0026rdquo; option from the command palette.\nThis will trigger pulling the container image from the registry and creating a container.\nOnce the container is created, we can use the development environment as if it is local.\nWhen the application development is complete, we can use a CI/CD pipeline to build the application into whatever the delivery target is \u0026ndash; a binary or a container image \u0026ndash; and distribute it.\nAs mentioned, using dev containers for single-container application development is straightforward. However, the modern applications are not that simple. Cloud-native applications comprise many microservices, and developing such an application requires ensuring the dependent services are also available within the development environment. Fortunately, VS Code dev containers support using Docker Compose for multi-container development environments.\nMulti-container development environment To understand how to create a multi-container development environment, let\u0026rsquo;s first create the necessary folder structure and placeholder files.\n1 2 3 4 5 6 7 8 $ mkdir .devcontainer $ mkdir .devcontainer/go $ mkdir .devcontainer/go/devcontainer.json $ mkdir .devcontainer/mysql $ touch .devcontainer/mysql/decontainer.json $ mkdir src $ touch src/main.go $ touch docker-compose.yml As the above commands suggest, we will create a web application using the Go language and mySQL DB stack for this example. Let us start by authoring the docker compose file first.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 version: \u0026#39;3\u0026#39; services: app: image: mcr.microsoft.com/devcontainers/base:jammy container_name: app restart: unless-stopped volumes: - .:/workspace:cached command: sleep infinity networks: - servicenet links: - \u0026#34;db:database\u0026#34; db: image: mysql:latest container_name: db ports: - \u0026#34;13306:3306\u0026#34; restart: unless-stopped volumes: - app_data:/var/lib/mysql command: --default-authentication-plugin=mysql_native_password environment: MYSQL_ROOT_PASSWORD: vscode MYSQL_USER: vscode MYSQL_PASSWORD: vscode networks: - servicenet volumes: app_data: networks: servicenet: In this compose file, we have two services. The first one is a Go application. It uses the same image as the earlier single container example. Using the links element, we specify a dependency on the second container, the MySQL database. Both these containers connect to the same network called servicenet. The db container is configured to provision a volume (app_data) for the database backend. Using the environment element, we specify the password for the MySQL root account and create a second account used for connecting to the database from the Go application.\n--default-authentication-plugin=mysql_native_password is not recommended in the production environment.\nWe now need to create the dev container configuration for both containers. Let us start with the Go container.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 { \u0026#34;name\u0026#34;: \u0026#34;app\u0026#34;, \u0026#34;dockerComposeFile\u0026#34;: [\u0026#34;../../docker-compose.yml\u0026#34;], \u0026#34;features\u0026#34;: { \u0026#34;ghcr.io/devcontainers/features/go:1\u0026#34;: {} }, \u0026#34;service\u0026#34;: \u0026#34;app\u0026#34;, \u0026#34;shutdownAction\u0026#34;: \u0026#34;stopCompose\u0026#34;, \u0026#34;workspaceFolder\u0026#34;: \u0026#34;/workspace/src\u0026#34;, \u0026#34;remoteUser\u0026#34;: \u0026#34;vscode\u0026#34;, \u0026#34;customizations\u0026#34;: { \u0026#34;vscode\u0026#34;: { \u0026#34;extensions\u0026#34;: [ \u0026#34;golang.go\u0026#34;, \u0026#34;ms-vscode.Go\u0026#34; ] } } } This is similar to the earlier single-container application example. Instead of using an image from the container registry, we use the docker compose file. Also, notice the workspaceFolder element. This maps the source code folder to the container once it runs. This is important because you need the app container as your development environment; all your source code files must be available.\nThe second container we need is a MySQL database container for the application.\n1 2 3 4 5 6 7 { \u0026#34;name\u0026#34;: \u0026#34;db\u0026#34;, \u0026#34;dockerComposeFile\u0026#34;: [\u0026#34;../../docker-compose.yml\u0026#34;], \u0026#34;service\u0026#34;: \u0026#34;mysql\u0026#34;, \u0026#34;shutdownAction\u0026#34;: \u0026#34;none\u0026#34;, \u0026#34;workspaceFolder\u0026#34;: \u0026#34;/workspace/cached\u0026#34; } This is much simpler. Once again, we reference the docker compose file within the repository. Once these files are in place, we can look at the sample Go application code that interacts with the MySQL database.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 package main import ( \u0026#34;database/sql\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log/slog\u0026#34; \u0026#34;os\u0026#34; \u0026#34;text/tabwriter\u0026#34; _ \u0026#34;github.com/go-sql-driver/mysql\u0026#34; ) var dsn = fmt.Sprintf(\u0026#34;%s:%s@tcp(%s:%d)/%s\u0026#34;, \u0026#34;web\u0026#34;, \u0026#34;pass\u0026#34;, \u0026#34;db\u0026#34;, 3306, \u0026#34;cnapp\u0026#34;) type task struct { ID int Title string Status string } func main() { slog.Info(\u0026#34;Connecting to database\u0026#34;, \u0026#34;connection string\u0026#34;, dsn) db, err := openDB(dsn) if err != nil { slog.Error(\u0026#34;Error connecting to database: \u0026#34;, err) } defer db.Close() tasks := []task{} stmt := `SELECT id, title, status FROM tasks` rows, err := db.Query(stmt) if err != nil { slog.Error(\u0026#34;Error querying database: \u0026#34;, err) } defer rows.Close() for rows.Next() { t := task{} err := rows.Scan(\u0026amp;t.ID, \u0026amp;t.Title, \u0026amp;t.Status) if err != nil { slog.Error(\u0026#34;Error scanning row: \u0026#34;, err) } tasks = append(tasks, t) } if err = rows.Err(); err != nil { slog.Error(\u0026#34;Error iterating rows: \u0026#34;, err) } w := tabwriter.NewWriter(os.Stdout, 0, 0, 1, \u0026#39; \u0026#39;, tabwriter.AlignRight|tabwriter.Debug) fmt.Fprintln(w, \u0026#34;\\nID\\tTitle\\tStatus\u0026#34;) for _, t := range tasks { fmt.Fprintf(w, \u0026#34;%d\\t%s\\t%s\\n\u0026#34;, t.ID, t.Title, t.Status) } w.Flush() } func openDB(dsn string) (*sql.DB, error) { db, err := sql.Open(\u0026#34;mysql\u0026#34;, dsn) if err != nil { return nil, err } if err = db.Ping(); err != nil { return nil, err } return db, nil } The above example is only for demonstration purposes. In a real production environment, we would use an external configuration map to supply the database configuration to the application. In this Go application, we connect to the cnapp database and retrieve records from a table called tasks. Once the records are retrieved, we print the same using a tab writer.\nTo test this, you can use the \u0026ldquo;Dev containers: Reopen in container\u0026rdquo; option within the command palette to start the application stack. Once the remote VS Code instance is open, you can verify the Docker stack in the Docker Desktop application.\nAs we see here, both containers are in a running state. Also, within the VS Code instance, you can see that we are already connected to the Go application container. We can attach to the mysql container using the \u0026ldquo;Dev Containers: Attach to running container\u0026hellip;\u0026rdquo; option in the command palette. This connects to the MySQL container as the root user. Once connected, we can create a database and table within the database and add sample records. The following SQL commands can be used for that.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 CREATE DATABASE cnapp CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci; USE cnapp; CREATE TABLE tasks ( id INTEGER NOT NULL PRIMARY KEY AUTO_INCREMENT, title VARCHAR(100) NOT NULL, status VARCHAR(100) NOT NULL ); INSERT INTO tasks (title, status) VALUES ( \u0026#39;Grocery shopping\u0026#39;, \u0026#39;pending\u0026#39; ); INSERT INTO tasks (title, status) VALUES ( \u0026#39;Read book 1\u0026#39;, \u0026#39;complete\u0026#39; ); INSERT INTO tasks (title, status) VALUES ( \u0026#39;Car service\u0026#39;, \u0026#39;complete\u0026#39; ); INSERT INTO tasks (title, status) VALUES ( \u0026#39;Email to manager\u0026#39;, \u0026#39;pending\u0026#39; ); CREATE USER \u0026#39;web\u0026#39;; GRANT SELECT, INSERT, UPDATE, DELETE ON cnapp.* TO \u0026#39;web\u0026#39;; ALTER USER \u0026#39;web\u0026#39; IDENTIFIED BY \u0026#39;pass\u0026#39;; Once the database is prepared, we can run the Go application within the app container.\n1 2 3 4 5 6 7 8 vscode ➜ /workspace/src $ go run main.go 2024/02/03 14:56:19 INFO Connecting to database \u0026#34;connection string\u0026#34;=web:pass@tcp(db:3306)/cnapp ID| Title|Status 1| Grocery shopping|pending 2| Read book 1|complete 3| Car service|complete 4| Email to manager|pending Using the knowledge we gained experimenting with this stack, we can easily add more services to the same docker compose specification.\n","date":"2024-02-03T00:00:00Z","excerpt":"Development containers are a great way to develop modern applications. Cloud-native applications usually implement more than one service to provide the application functionality. Using dev containers for microservices-based application development requires more than one container. This is where using Docker Compose with dev containers is useful. This article explores creating a multi-container development environment using VS Code dev containers and Docker Compose.","permalink":"https://ravichaganti.com/blog/devcontainers-for-cloud-native-application-development/","series":"Containers","title":"Devcontainers for cloud-native application development"},{"categories":null,"content":"Azure Red Hat OpenShift (ARO) service provides fully managed and highly available OpenShift clusters on Azure jointly operated by Microsoft and Red Hat. There are different ways to provision an ARO cluster.\nAzure Portal Azure CLI or Azure PowerShell Azure Resource Manager (ARM) / Bicep templates Red Hat Advanced Cluster Management (RHACM) This article will explore using Bicep language to provision an ARO cluster.\nExamples in this article use Azure CLI for interacting with Azure resources and deployments.\nPrerequisites Before we look at the Bicep template, we need a few prerequisites to be in place. First and foremost, we need an Azure subscription. The subscription must also register the necessary resource providers for the ARO cluster deployment.\n1 2 3 4 5 6 az provider register --namespace \u0026#39;Microsoft.RedHatOpenShift\u0026#39; --wait az provider register --namespace \u0026#39;Microsoft.Compute\u0026#39; --wait az provider register --namespace \u0026#39;Microsoft.Storage\u0026#39; --wait az provider register --namespace \u0026#39;Microsoft.Authorization\u0026#39; --wait ARO_RP_SP_OBJECT_ID=$(az ad sp list --display-name \u0026#34;Azure Red Hat OpenShift RP\u0026#34; --query [0].id -o tsv) We need to create a resource group where the ARO cluster will reside.\n1 2 3 RESOURCEGROUP=aro LOCATION=eastus az group create --name $RESOURCEGROUP --location $LOCATION The user account must have at least contributor and user access administrator permissions to provision an ARO cluster. We shall create a service principal, which will be assigned the necessary roles at the subscription scope later using the Bicep template.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 SUBSCRIPTION_ID=73f21a5e-7cd2-49f8-b314-6b95b971e7c2 SP_RESULT=$(az ad sp create-for-rbac --name \u0026#34;sp-$RESOURCEGROUP-${RANDOM}\u0026#34;) SP_CLIENT_ID=$(jq -r \u0026#39;.appId\u0026#39; \u0026lt;\u0026lt;\u0026lt;\u0026#34;$SP_RESULT\u0026#34;) SP_CLIENT_SECRET=$(jq -r \u0026#39;.password\u0026#39; \u0026lt;\u0026lt;\u0026lt;\u0026#34;$SP_RESULT\u0026#34;) SP_OBJECT_ID=$(az ad sp show --id $SP_CLIENT_ID | jq -r \u0026#39;.id\u0026#39;) az role assignment create --role \u0026#39;User Access Administrator\u0026#39; \\ --scope /subscriptions/$SUBSCRIPTION_ID \\ --assignee-object-id $SP_OBJECT_ID \\ --assignee-principal-type \u0026#39;ServicePrincipal\u0026#39; az role assignment create --role \u0026#39;Contributor\u0026#39; \\ --scope /subscriptions/$SUBSCRIPTION_ID \\ --assignee-object-id $SP_OBJECT_ID \\ --assignee-principal-type \u0026#39;ServicePrincipal\u0026#39; The above commands create a service principal and then set the necessary environment variables for the values we are interested in. We also perform the role assignments to ensure the service principal has the necessary permissions.\nWe need a Red Hat pull secret to enable Operator Hub within the ARO cluster. The content of this pull secret must be available as pull-secret.txt at the same location as the Bicep template.\nARO Bicep template Once the prerequisite configuration is complete, we can start working on the Bicep template. Let us start with a set of parameter definitions.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 @description(\u0026#39;Location\u0026#39;) param location string = resourceGroup().location @description(\u0026#39;Domain Prefix\u0026#39;) param domain string = \u0026#39;\u0026#39; @description(\u0026#39;Name of ARO vNet\u0026#39;) param clusterVnetName string = \u0026#39;aro-vnet\u0026#39; @description(\u0026#39;ARO vNet Address Space\u0026#39;) param clusterVnetCidr string = \u0026#39;10.100.0.0/15\u0026#39; @description(\u0026#39;Worker node subnet address space\u0026#39;) param workerSubnetCidr string = \u0026#39;10.100.70.0/23\u0026#39; @description(\u0026#39;Controlplane node subnet address space\u0026#39;) param cpSubnetCidr string = \u0026#39;10.100.76.0/24\u0026#39; @description(\u0026#39;Controlplane Node VM Type\u0026#39;) param cpVmSize string = \u0026#39;Standard_D8s_v3\u0026#39; @description(\u0026#39;Worker Node VM Type\u0026#39;) param workerVmSize string = \u0026#39;Standard_D4s_v3\u0026#39; @description(\u0026#39;Worker Node Disk Size in GB\u0026#39;) @minValue(128) param workerVmDiskSize int = 128 @description(\u0026#39;Number of Worker Nodes\u0026#39;) @minValue(3) param workerCount int = 3 @description(\u0026#39;Cidr for Pods\u0026#39;) param podCidr string = \u0026#39;10.128.0.0/14\u0026#39; @description(\u0026#39;Cidr of service\u0026#39;) param serviceCidr string = \u0026#39;172.30.0.0/16\u0026#39; @description(\u0026#39;Unique name for the cluster\u0026#39;) param clusterName string @description(\u0026#39;Tags for resources\u0026#39;) param tags object = { env: \u0026#39;Dev\u0026#39; dept: \u0026#39;Ops\u0026#39; } @description(\u0026#39;Api Server Visibility\u0026#39;) @allowed([ \u0026#39;Private\u0026#39; \u0026#39;Public\u0026#39; ]) param apiServerVisibility string = \u0026#39;Public\u0026#39; @description(\u0026#39;Ingress Visibility\u0026#39;) @allowed([ \u0026#39;Private\u0026#39; \u0026#39;Public\u0026#39; ]) param ingressVisibility string = \u0026#39;Public\u0026#39; @description(\u0026#39;Application ID of an Entra client application\u0026#39;) param aadClientId string @description(\u0026#39;Object ID of an Entra client application\u0026#39;) param aadObjectId string @description(\u0026#39;The secret of an Entra client application\u0026#39;) @secure() param aadClientSecret string @description(\u0026#39;The ObjectID of the Resource Provider Service Principal\u0026#39;) param rpObjectId string @description(\u0026#39;Specify if FIPS validated crypto modules are used\u0026#39;) @allowed([ \u0026#39;Enabled\u0026#39; \u0026#39;Disabled\u0026#39; ]) param fips string = \u0026#39;Disabled\u0026#39; @description(\u0026#39;Specify if controlplane VMs are encrypted at host\u0026#39;) @allowed([ \u0026#39;Enabled\u0026#39; \u0026#39;Disabled\u0026#39; ]) param cpEncryptionAtHost string = \u0026#39;Disabled\u0026#39; @description(\u0026#39;Specify if worker VMs are encrypted at host\u0026#39;) @allowed([ \u0026#39;Enabled\u0026#39; \u0026#39;Disabled\u0026#39; ]) param workerEncryptionAtHost string = \u0026#39;Disabled\u0026#39; The parameters in this template help us customize the ARO cluster deployment. There are customizations such as FIPS and encryption at the host disabled by default. The API server and ingress visibility are configured to be public. This template creates three control plane nodes and three worker nodes by default. We cannot change the number of control plane nodes, but the worker count can be customized using the workerCount parameter.\nBefore provisioning any resource dependencies, we need to set up a few variables.\n1 2 3 4 5 6 7 8 9 var contributorRoleDefinitionId = resourceId(\u0026#39;Microsoft.Authorization/roleDefinitions\u0026#39;, \u0026#39;b24988ac-6180-42a0-ab88-20f7382dd24c\u0026#39;) var resourceGroupId = subscriptionResourceId(\u0026#39;${subscription().subscriptionId}\u0026#39;, \u0026#39;Microsoft.Resources/resourceGroups\u0026#39;, \u0026#39;aro-${domain}-${location}\u0026#39;) var cpSubnetId=resourceId(\u0026#39;Microsoft.Network/virtualNetworks/subnets\u0026#39;, clusterVnetName, \u0026#39;controlplane\u0026#39;) var workerSubnetId=resourceId(\u0026#39;Microsoft.Network/virtualNetworks/subnets\u0026#39;, clusterVnetName, \u0026#39;worker\u0026#39;) var pullSecret = loadTextContent(\u0026#39;pull-secret.txt\u0026#39;,\u0026#39;utf-8\u0026#39;) The first variable \u0026ndash; contributorRoleDefinitionId \u0026ndash; is used to retrieve the resource ID of the Contributor role definition. We need this later when we assign permissions on the virtual network resource. The remaining variables defined using the built-in subscriptionResourceId and resourceId functions are used later in the resource definitions.\nTo provision an ARO cluster, we need a virtual network with two subnets \u0026ndash; controlplane and worker. We create this by provisioning a virtual network resource.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 resource clusterVnetName_resource \u0026#39;Microsoft.Network/virtualNetworks@2020-05-01\u0026#39; = { name: clusterVnetName location: location tags: tags properties: { addressSpace: { addressPrefixes: [ clusterVnetCidr ] } subnets: [ { name: \u0026#39;controlplane\u0026#39; properties: { addressPrefix: cpSubnetCidr serviceEndpoints: [ { service: \u0026#39;Microsoft.ContainerRegistry\u0026#39; } ] privateLinkServiceNetworkPolicies: \u0026#39;Disabled\u0026#39; } } { name: \u0026#39;worker\u0026#39; properties: { addressPrefix: workerSubnetCidr serviceEndpoints: [ { service: \u0026#39;Microsoft.ContainerRegistry\u0026#39; } ] } } ] } } This definition creates a virtual network resource with subnets for the control plane and worker nodes. Next, we must use the roleAssignments extension resource to assign contributor access to the service principal and OpenShift Resource Provider on the virtual network resource.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 resource clusterVnetName_aadObjectId \u0026#39;Microsoft.Authorization/roleAssignments@2020-10-01-preview\u0026#39; = { name: guid(aadObjectId, clusterVnetName_resource.id, contributorRoleDefinitionId) scope: clusterVnetName_resource properties: { roleDefinitionId: contributorRoleDefinitionId principalId: aadObjectId principalType: \u0026#39;ServicePrincipal\u0026#39; } } resource clusterVnetName_rpObjectId \u0026#39;Microsoft.Authorization/roleAssignments@2020-10-01-preview\u0026#39; = { name: guid(rpObjectId, clusterVnetName_resource.id, contributorRoleDefinitionId) scope: clusterVnetName_resource properties: { roleDefinitionId: contributorRoleDefinitionId principalId: rpObjectId principalType: \u0026#39;ServicePrincipal\u0026#39; } } With this, we are all set to create an ARO cluster. The following definition helps us with that.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 resource clusterName_resource \u0026#39;Microsoft.RedHatOpenShift/OpenShiftClusters@2023-09-04\u0026#39; = { name: clusterName location: location tags: tags properties: { clusterProfile: { domain: domain resourceGroupId: resourceGroupId pullSecret: pullSecret fipsValidatedModules: fips } networkProfile: { podCidr: podCidr serviceCidr: serviceCidr } servicePrincipalProfile: { clientId: aadClientId clientSecret: aadClientSecret } masterProfile: { vmSize: cpVmSize subnetId: cpSubnetId encryptionAtHost: cpEncryptionAtHost } workerProfiles: [ { name: \u0026#39;worker\u0026#39; vmSize: workerVmSize diskSizeGB: workerVmDiskSize subnetId: workerSubnetId count: workerCount encryptionAtHost: workerEncryptionAtHost } ] apiserverProfile: { visibility: apiServerVisibility } ingressProfiles: [ { name: \u0026#39;default\u0026#39; visibility: ingressVisibility } ] } dependsOn: [ clusterVnetName_resource ] } This is straightforward. The ARO resource requires us to specify the clusterProfile, networkProfile, servicePrincipalProfile, masterProfile, workerProfiles, apiserverProfile, and ingressProfiles. Once this ARO cluster is provisioned, we can retrieve the console and API server URLs using the outputs.\n1 2 3 4 5 output aroCluster object = { clusterName: clusterName_resource.name clusterApiServerUrl: clusterName_resource.properties.apiserverProfile.url clusterConsoleUrl: clusterName_resource.properties.provisioningState == \u0026#39;Succeeded\u0026#39; ? clusterName_resource.properties.consoleProfile.url : \u0026#39;\u0026#39; } This template is ready to be deployed. We can use the az deployment group create command to deploy the template.\n1 2 3 4 5 6 7 8 9 10 11 12 13 DOMAIN=hub ARO_CLUSTER_NAME=hub az deployment group create --name aroDeployment \\ --resource-group $RESOURCEGROUP \\ --template-file main.bicep \\ --parameters location=$LOCATION \\ --parameters domain=$DOMAIN \\ --parameters clusterName=$ARO_CLUSTER_NAME \\ --parameters aadClientId=$SP_CLIENT_ID \\ --parameters aadObjectId=$SP_OBJECT_ID \\ --parameters aadClientSecret=$SP_CLIENT_SECRET \\ --parameters rpObjectId=$ARO_RP_SP_OBJECT_ID The resource group we provisioned gets used as the base resource group. This base resource group houses the virtual network and the ARO cluster. The ARO resource provider creates another resource group called the infrastructure resource group. In the Bicep template, this infrastructure group is identified using the resourceGroupId variable. This infrastructure group can only be managed by the resource provider. The ARO RP provisions a Network Security Group (NSG) within this resource group. We cannot add or remove any inbound or outbound security rules to/from this NSG. This is because all service principals except the one used to create the cluster are denied any API access to this resource group.\nBringing your own NSG is available as a preview feature; we will look at it later.\nSimilarly, this infrastructure resource group will also contain all virtual machines required for the control plane and worker nodes.\nOnce the template deployment is complete, we can see the console and API URLs in the deployment output or within the Azure portal.\n1 2 3 4 5 6 7 8 9 10 \u0026#34;outputs\u0026#34;: { \u0026#34;aroCluster\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;Object\u0026#34;, \u0026#34;value\u0026#34;: { \u0026#34;clusterApiServerUrl\u0026#34;: \u0026#34;https://api.hub.eastus.aroapp.io:6443/\u0026#34;, \u0026#34;clusterConsoleUrl\u0026#34;: \u0026#34;https://console-openshift-console.apps.hub.eastus.aroapp.io/\u0026#34;, \u0026#34;clusterName\u0026#34;: \u0026#34;hub\u0026#34; } } } ARO does not yet support shutting down a cluster. Therefore, we must monitor the usage of the cluster and take the necessary actions, such as deleting the cluster or shutting down virtual machines, as needed.\n","date":"2023-12-01T00:00:00Z","excerpt":"Bicep simplifies provisioning Azure Red Hat OpenShift clusters. This article explains how!","permalink":"https://ravichaganti.com/blog/using-bicep-to-provision-azure-red-hat-openshift-cluster/","series":"Azure Bicep","title":"Using Bicep to provision Azure Red Hat OpenShift cluster"},{"categories":["azure","go"],"content":"So far in this series, you learned how to use different types of credentials that the Azure SDK for Go offers. You used specific credential types to authenticate with Azure in all the examples. You use a set of credential types for the development environment and a different set for production use cases. However, what if you want to use the code unmodified between development and production? What if you must test the code that uses managed identity credentials locally where the type of credentials you must use are different? This is where credential chaining comes to the rescue.\nThe azidentity package offers two ways to implement credential chaining.\nDefaultAzureCredential The DefaultAzureCredential type is most commonly used for applications deployed to Azure. This type combines the production credentials with the development credentials. This credential type attempts to authenticate using different credentials in a specific order.\nEnvironment -\u0026gt; Workload Identity -\u0026gt; Managed Identity -\u0026gt; Azure CLI\nThe chain stops when one of the methods succeeds. Here is an example. This example implements logging of all authentication events to demonstrate that this method attempts different types of credentials through chaining.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 package main import ( \u0026#34;context\u0026#34; \u0026#34;encoding/json\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; azlog \u0026#34;github.com/Azure/azure-sdk-for-go/sdk/azcore/log\u0026#34; \u0026#34;github.com/Azure/azure-sdk-for-go/sdk/azidentity\u0026#34; \u0026#34;github.com/Azure/azure-sdk-for-go/sdk/resourcemanager/resources/armresources\u0026#34; ) const subscriptionID = \u0026#34;21e034f70820\u0026#34; func main() { azlog.SetListener(func(event azlog.Event, s string) { fmt.Println(s) }) azlog.SetEvents(azidentity.EventAuthentication) cred, err := azidentity.NewDefaultAzureCredential(nil) if err != nil { log.Fatal(err) } rcFactory, err := armresources.NewClientFactory(subscriptionID, cred, nil) if err != nil { log.Fatal(err) } rgClient := rcFactory.NewResourceGroupsClient() ctx := context.Background() resultPager := rgClient.NewListPager(nil) resourceGroups := make([]*armresources.ResourceGroup, 0) for resultPager.More() { pageResp, err := resultPager.NextPage(ctx) if err != nil { log.Fatal(err) } resourceGroups = append(resourceGroups, pageResp.ResourceGroupListResult.Value...) } jsonData, err := json.MarshalIndent(resourceGroups, \u0026#34;\\t\u0026#34;, \u0026#34;\\t\u0026#34;) if err != nil { log.Fatal(err) } fmt.Println(string(jsonData)) } You can try and execute this in the development environment first, where you already have Azure CLI credentials locally.\n1 2 3 4 5 6 7 8 9 10 11 12 13 $ go run defaultAzureCred.go Managed Identity Credential will use IMDS managed identity NewDefaultAzureCredential failed to initialize some credentials: EnvironmentCredential: missing environment variable AZURE_TENANT_ID WorkloadIdentityCredential: no client ID specified. Check pod configuration or set ClientID in the options 2023/08/16 15:06:15 DefaultAzureCredential authentication failed GET http://169.254.169.254/metadata/identity/oauth2/token -------------------------------------------------------------------------------- RESPONSE 403 connecting to 169.254.169.254:80: connecting to 169.254.169.254:80: dial tcp 169.254.169.254:80: connectex: A socket operation was attempted to an unreachable network. -------------------------------------------------------------------------------- connecting to 169.254.169.254:80: connecting to 169.254.169.254:80: dial tcp 169.254.169.254:80: connectex: A socket operation was attempted to an unreachable network. -------------------------------------------------------------------------------- exit status 1 As you can see in the output here, the DefaultAzureCredential type attempted EnvironmentCredential and WorkloadIdentityCredential in that order. It then attempted the ManagedIdentityCredential type but failed with an error the \u0026ldquo;socket operation was attempted to an unreachable network\u0026rdquo;. So, what happened to the final type \u0026ndash; AzureCliCredential? Interestingly, it is working as designed. According to the response on one of the issues, the reason this method errors out is because of the way the ManagedIdentityCredential constructor exits once it fails to connect to the metadata endpoint causing the NewDefaultAzureCredential to skip attempting Azure CLI credential.\nHowever, if you execute the same code inside an Azure service (like an Azure VM), you will see that it attempts the Azure CLI credential.\n1 2 3 4 5 6 7 8 9 10 11 12 $ go run defaultAzureCred.go Managed Identity Credential will use IMDS managed identity NewDefaultAzureCredential failed to initialize some credentials: EnvironmentCredential: missing environment variable AZURE_TENANT_ID WorkloadIdentityCredential: no client ID specified. Check pod configuration or set ClientID in the options 2023/08/16 16:18:52 DefaultAzureCredential: failed to acquire a token. Attempted credentials: EnvironmentCredential: missing environment variable AZURE_TENANT_ID WorkloadIdentityCredential: no client ID specified. Check pod configuration or set ClientID in the options ManagedIdentityCredential: no default identity is assigned to this resource AzureCLICredential: Azure CLI not found on path exit status 1 The managed identity endpoint can still be reached inside an Azure VM, even when there is no managed identity (system or user-assigned). This endpoint does not result in a valid token credential; therefore, the NewDefaultAzureCredential() method attempts the Azure CLI credential check.\nAs you see above, the NewDefaultAzureCredential() method implements a fixed authentication flow in a specific order. What if you want to customize the authentication flow? You can achieve that using ChainedTokenCredential.\nChainedTokenCredential The ChainedTokenCredential method can link multiple credentials to be tried sequentially. Here is an example.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 package main import ( \u0026#34;context\u0026#34; \u0026#34;encoding/json\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;github.com/Azure/azure-sdk-for-go/sdk/azcore\u0026#34; azlog \u0026#34;github.com/Azure/azure-sdk-for-go/sdk/azcore/log\u0026#34; \u0026#34;github.com/Azure/azure-sdk-for-go/sdk/azidentity\u0026#34; \u0026#34;github.com/Azure/azure-sdk-for-go/sdk/resourcemanager/resources/armresources\u0026#34; ) const subscriptionID = \u0026#34;5073fd4c-3a1b-4559-8371-21e034f70820\u0026#34; func main() { azlog.SetListener(func(event azlog.Event, s string) { fmt.Println(s) }) azlog.SetEvents(azidentity.EventAuthentication) cli, err := azidentity.NewAzureCLICredential(nil) if err != nil { log.Fatal(err) } env, err := azidentity.NewEnvironmentCredential(nil) if err != nil { log.Fatal(err) } mic, err := azidentity.NewManagedIdentityCredential(nil) if err != nil { log.Fatal(err) } cred, err := azidentity.NewChainedTokenCredential([]azcore.TokenCredential{cli, mic, env}, nil) if err != nil { log.Fatal(err) } rcFactory, err := armresources.NewClientFactory(subscriptionID, cred, nil) if err != nil { log.Fatal(err) } rgClient := rcFactory.NewResourceGroupsClient() ctx := context.Background() resultPager := rgClient.NewListPager(nil) resourceGroups := make([]*armresources.ResourceGroup, 0) for resultPager.More() { pageResp, err := resultPager.NextPage(ctx) if err != nil { log.Fatal(err) } resourceGroups = append(resourceGroups, pageResp.ResourceGroupListResult.Value...) } jsonData, err := json.MarshalIndent(resourceGroups, \u0026#34;\\t\u0026#34;, \u0026#34;\\t\u0026#34;) if err != nil { log.Fatal(err) } fmt.Println(string(jsonData)) } As shown in the code, you must first create the credentials you want to chain together and use the NewChainedTokenCredential() method to specify the authentication flow. When you run this example, you will see that the authentication flow attempts each credential type.\n1 2 3 4 5 6 7 8 9 10 $ go run chainedCredential.go EnvironmentCredential will authenticate with ClientSecretCredential Managed Identity Credential will use IMDS managed identity 2023/08/17 04:54:12 ChainedTokenCredential: failed to acquire a token. Attempted credentials: AzureCLICredential: Azure CLI not found on path ManagedIdentityCredential: no default identity is assigned to this resource ClientSecretCredential: unable to resolve an endpoint: http call(https://login.microsoftonline.com/test/v2.0/.well-known/openid-configuration)(GET) error: reply status code was 400: {\u0026#34;error\u0026#34;:\u0026#34;invalid_tenant\u0026#34;,\u0026#34;error_description\u0026#34;:\u0026#34;AADSTS90002: Tenant \u0026#39;test\u0026#39; not found. Check to make sure you have the correct tenant ID and are signing into the correct cloud. Check with your subscription administrator, this may happen if there are no active subscriptions for the tenant.\\r\\nTrace ID: a21cd225-ecbb-419b-b73e-b78595fd6c00\\r\\nCorrelation ID: 38d1a094-e650-495e-8797-134839aad177\\r\\nTimestamp: 2023-08-17 04:54:12Z\u0026#34;,\u0026#34;error_codes\u0026#34;:[90002],\u0026#34;timestamp\u0026#34;:\u0026#34;2023-08-17 04:54:12Z\u0026#34;,\u0026#34;trace_id\u0026#34;:\u0026#34;a21cd225-ecbb-419b-b73e-b78595fd6c00\u0026#34;,\u0026#34;correlation_id\u0026#34;:\u0026#34;38d1a094-e650-495e-8797-134839aad177\u0026#34;,\u0026#34;error_uri\u0026#34;:\u0026#34;https://login.microsoftonline.com/error?code=90002\u0026#34;} exit status 1 With ChainedTokenCredential, you can customize the authentication flow and use it across both development and production environments.\n","date":"2023-08-16T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/azure-sdk-for-go-authentication-methods-chained-credentials/","series":"Azure Go","title":"Azure SDK for Go - Authentication methods - Chained Credentials"},{"categories":["azure","go"],"content":"When you have applications or services that run in the Azure cloud infrastructure and require access to Azure services, the best way to authenticate to Azure is to use the managed identity. Using managed identity, you can eliminate the need to manage artifacts such as secrets, certificates, and credentials. For example, if you need to access Azure Key Vault to retrieve an API key for authentication purposes. To do this, you must first authenticate to the Key Vault service. With managed identity, you get automatically authenticated.\nThere are two types of managed identities. The Azure.Identity package provides the ManagedIdentityCredential to access Azure tokens via these identities to access target Azure services. This article will teach you how to use these identities in your Go code. You will need an Azure Linux virtual machine to try the examples in this article.\nSystem-assigned The system-assigned managed identity is associated with an Azure resource and is tied to the resource lifecycle. There is only one system-assigned managed identity per resource. And it is not available for every Azure resource. For example, you can create a virtual machine and assign a system-assigned managed identity. This identity can now access authorized (based on role assignments) Azure services from within the virtual machine.\nBefore you proceed, you must enable the system-assigned managed identity on the Azure VM resource where the following code will run.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 package main import ( \u0026#34;context\u0026#34; \u0026#34;encoding/json\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;github.com/Azure/azure-sdk-for-go/sdk/azidentity\u0026#34; \u0026#34;github.com/Azure/azure-sdk-for-go/sdk/resourcemanager/resources/armresources\u0026#34; ) const subscriptionID = \u0026#34;21e034f70820\u0026#34; func main() { cred, err := azidentity.NewManagedIdentityCredential(nil) if err != nil { log.Fatal(err) } rcFactory, err := armresources.NewClientFactory(subscriptionID, cred, nil) if err != nil { log.Fatal(err) } rgClient := rcFactory.NewResourceGroupsClient() ctx := context.Background() resultPager := rgClient.NewListPager(nil) resourceGroups := make([]*armresources.ResourceGroup, 0) for resultPager.More() { pageResp, err := resultPager.NextPage(ctx) if err != nil { log.Fatal(err) } resourceGroups = append(resourceGroups, pageResp.ResourceGroupListResult.Value...) } jsonData, err := json.MarshalIndent(resourceGroups, \u0026#34;\\t\u0026#34;, \u0026#34;\\t\u0026#34;) if err != nil { log.Fatal(err) } fmt.Println(string(jsonData)) } For a system-assigned managed identity, you can call the NewManagedIdentityCredential() method. You can simply run this program to verify if the system-assigned identity is working inside the VM.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 $ go run managedIdentity.go [ { \u0026#34;id\u0026#34;: \u0026#34;/subscriptions/21e034f70820/resourceGroups/cloud-shell-storage-southeastasia\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;southeastasia\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;cloud-shell-storage-southeastasia\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;provisioningState\u0026#34;: \u0026#34;Succeeded\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Resources/resourceGroups\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;/subscriptions/21e034f70820/resourceGroups/NetworkWatcherRG\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;eastus\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;NetworkWatcherRG\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;provisioningState\u0026#34;: \u0026#34;Succeeded\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Resources/resourceGroups\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;/subscriptions/21e034f70820/resourceGroups/DefaultResourceGroup-EUS\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;eastus\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;DefaultResourceGroup-EUS\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;provisioningState\u0026#34;: \u0026#34;Succeeded\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Resources/resourceGroups\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;/subscriptions/21e034f70820/resourceGroups/az\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;eastus\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;az\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;provisioningState\u0026#34;: \u0026#34;Succeeded\u0026#34; }, \u0026#34;tags\u0026#34;: {}, \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Resources/resourceGroups\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;/subscriptions/21e034f70820/resourceGroups/u01_group\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;eastus\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;u01_group\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;provisioningState\u0026#34;: \u0026#34;Succeeded\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Resources/resourceGroups\u0026#34; } ] User-assigned Like any other Azure resource, you can create a user-assigned managed identity as a standalone resource and assign it to one or more Azure resources. You can authorize the user-assigned managed identity to access one or more Azure services. The application consuming the user-assigned identity gets access to the authorized resources. You manage the lifecycle of a user-assigned identity like any other Azure resource.\nBefore proceeding to try the example, you must create and assign the user-assigned identity to the virtual machine.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 package main import ( \u0026#34;context\u0026#34; \u0026#34;encoding/json\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;github.com/Azure/azure-sdk-for-go/sdk/azidentity\u0026#34; \u0026#34;github.com/Azure/azure-sdk-for-go/sdk/resourcemanager/resources/armresources\u0026#34; ) const subscriptionID = \u0026#34;21e034f70820\u0026#34; func main() { clientID := azidentity.ClientID(\u0026#34;08f09d2f-a6e9-48c7-b3f4-9b14934c9952\u0026#34;) opts := azidentity.ManagedIdentityCredentialOptions{ID: clientID} cred, err := azidentity.NewManagedIdentityCredential(\u0026amp;opts) if err != nil { log.Fatal(err) } rcFactory, err := armresources.NewClientFactory(subscriptionID, cred, nil) if err != nil { log.Fatal(err) } rgClient := rcFactory.NewResourceGroupsClient() ctx := context.Background() resultPager := rgClient.NewListPager(nil) resourceGroups := make([]*armresources.ResourceGroup, 0) for resultPager.More() { pageResp, err := resultPager.NextPage(ctx) if err != nil { log.Fatal(err) } resourceGroups = append(resourceGroups, pageResp.ResourceGroupListResult.Value...) } jsonData, err := json.MarshalIndent(resourceGroups, \u0026#34;\\t\u0026#34;, \u0026#34;\\t\u0026#34;) if err != nil { log.Fatal(err) } fmt.Println(string(jsonData)) } In this example, you supply the client ID of the user-assigned identity as an option to the NewManagedIdentityCredential() method. Assuming that you have assigned the necessary roles to this user-assigned identity, you should see the same output as the earlier example.\nThe next article of this series, you will learn about credential chaining. Stay tuned.\n","date":"2023-08-08T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/azure-sdk-for-go-authentication-methods-managed-identity-credential/","series":"Azure Go","title":"Azure SDK for Go - Authentication methods - Managed Identity Credential"},{"categories":["azure","go"],"content":"In the last part of this series, you learned how to use different credential types in a local development environment. Another method you can use within local development and in a hosted/deployed service is providing credentials through environment variables. This is done using the NewEnvironmentCredential() method in the azidentity package.\n1 2 3 4 5 // github.com/rchaganti/azure-go/02-auth101/envCredential.go cred, err := azidentity.NewEnvironmentCredential(nil) if err != nil { log.Fatal(err) } This credential type supports multiple types of authentication in the following order.\nService principal with client secret In this method, you can export the tenant ID, client ID, and client secret of an Azure AD application as environment variables. These variable values get retrieved using NewEnvironmentCredential(), and the client will be authenticated using these values.\nTo use this method, you need an Azure AD application with a client secret, a necessary role assignment for the user, and admin consent should be granted.\nOnce you complete the appropriate Azure AD application registration and configuration, you can set the environment variables for this authentication method.\n1 2 3 $ export AZURE_TENANT_ID=\u0026#39;tenant-id\u0026#39; $ export AZURE_CLIENT_ID=\u0026#39;client-id\u0026#39; $ export AZURE_CLIENT_SECRET=\u0026#39;client-secret\u0026#39; You will receive an appropriate error message if these values are invalid. You will receive the following error if the application does not have the right permissions or role assignment.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 $ go run envCredential.go EnvironmentCredential will authenticate with ClientSecretCredential ClientSecretCredential.GetToken() acquired a token for scope \u0026#34;https://management.core.windows.net//.default\u0026#34;\\n 2023/07/18 04:43:16 GET https://management.azure.com/subscriptions/5073ff70820/resourcegroups -------------------------------------------------------------------------------- RESPONSE 403: 403 Forbidden ERROR CODE: AuthorizationFailed -------------------------------------------------------------------------------- { \u0026#34;error\u0026#34;: { \u0026#34;code\u0026#34;: \u0026#34;AuthorizationFailed\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;The client \u0026#39;8887f900\u0026#39; with object id \u0026#39;8887f900\u0026#39; does not have authorization to perform action \u0026#39;Microsoft.Resources/subscriptions/resourcegroups/read\u0026#39; over scope \u0026#39;/subscriptions/5073ff70820\u0026#39; or the scope is invalid. If access was recently granted, please refresh your credentials.\u0026#34; } } -------------------------------------------------------------------------------- exit status 1 Service principal with a certificate To use a client certificate instead of a client secret, you can use the AZURE_CLIENT_CERTIFICATE_PATH environment variable. If you set both the client secret and certificate variables, the client secret gets precedence.\n1 2 3 $ export AZURE_TENANT_ID=\u0026#39;tenant-id\u0026#39; $ export AZURE_CLIENT_ID=\u0026#39;client-id\u0026#39; $ export AZURE_CLIENT_CERTIFICATE_PATH=\u0026#39;path-to-client-certificate\u0026#39; The prerequisites for using this method are the same as the earlier one. Instead of creating a client secret, you must create a client certificate. If the certificate you created requires a password, you can specify that using the optionalAZURE_CLIENT_CERTIFICATE_PASSWORD variable.\nWith username and password This method is not recommended, but if you prefer username and password-based authentication, you can set the AZURE_CLIENT_ID, AZURE_USERNAME, and AZURE_PASSWORD environment variables.\n1 2 3 $ export AZURE_CLIENT_ID=\u0026#39;client-id\u0026#39; $ export AZURE_USERNAME=\u0026#39;user@domain\u0026#39; $ export AZURE_PASSWORD=\u0026#39;P@ssw0rd\u0026#39; To specify a tenant to authenticate, use the optional AZURE_TENANT_ID environment variable. The authentication methods you saw in the previous article support optional options parameter to customize the behavior. With these options, you can specify additional tenants allowed for multitenant authentication. When using environment credentials, this can be specified using an optional environment variable called AZURE_ADDITIONALLY_ALLOWED_TENANTS. You can specify a comma-separated list of tenant IDs as a value. If you specify \u0026lsquo;*\u0026rsquo; as the value of this variable, you enable requesting tokens from any tenant.\nIf all three methods are set as environment variables, the client secret takes precedence over the others. The Go code for using any of these methods is the same as shown at the beginning of this article. The environment credential method can be used in development and production (hosted/deployed) service scenarios.\nIn the next part of this series, you will learn how to use managed identity credentials within your Go code.\n","date":"2023-07-18T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/azure-sdk-for-go-authentication-methods-environmental-credential/","series":"Azure Go","title":"Azure SDK for Go - Authentication methods - Environmental credential"},{"categories":["azure","go"],"content":"The earlier article introduced you to the Azure SDK for Go. In the example towards the end of that article, you may have noticed how the NewAzureCLICredential method of the azidentity package was used to access locally available Azure CLI credentials. This is perfect for local development and is not recommended for production environments. The Azure SDK supports different types of credentials depending on where your Go code is running. In this article in the Azure SDK for Go series, you will learn about each of these credential types and when you use each of these credentials.\nAll code samples presented in this series will be available at rchaganti/azure-go: Learning Azure SDK for Go (github.com)\nazidentity package The azidentity package provides Azure Active Directory (or should we call it Entra!) authentication support through different types of token credential implementations. Azure SDK clients are lazily initialized; therefore, authentication and authorization do not occur until an actual Azure resource operation is performed. Therefore, the following program will not return an error even when the local Azure CLI credentials are unavailable.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 package main import ( \u0026#34;log\u0026#34; \u0026#34;github.com/Azure/azure-sdk-for-go/sdk/azidentity\u0026#34; ) func main() { _, err := azidentity.NewDefaultAzureCredential(nil) if err != nil { log.Fatal(err) } } This is an important point to remember because many of us think this method should fail when no credentials are available or when the credentials are invalid.\nTo authenticate and access/manage Azure services, your user account needs appropriate permissions/roles assigned to it. In the examples outlined in this series, it is assumed that you have the appropriate permissions.\nCredential Types The azidentity package supports authentication through different types of credentials. At a higher level, these credentials can be classified as development (or local) and production (or hosted service) types. Note that this is not an official classification. The development type credentials are useful in local dev, test, and debug scenarios. This includes:\nAzure CLI credentials Device code credentials Interactive browser-based credentials Username and Password credentials Environment credentials The hosted service or production-type credentials are useful in Azure-hosted service scenarios. This includes:\nEnvironment credentials Managed identity credentials Workload identity credentials The next few sections of this article will walk you through the different credential types for local development, testing, and debugging and show you how to use these credentials to authenticate your Go programs.\nAzure CLI credential In the previous part of this series, you have seen an example that used the NewAzureCLICredential() method to retrieve locally available credentials. This method returns a token credential implemented as the AzureCLICredential type.\nThe NewAzureCLICredential() method takes an optional parameter of type AzureCLICredentialOptions. Using this struct, you can specify the additional tenants for which the credential may acquire tokens. If you need only the home tenant of the logged-in user, you can specify nil as the parameter value.\n1 2 3 4 5 // github.com/rchaganti/azure-go/02-auth101/azcli.go cred, err := azidentity.NewAzureCLICredential(nil) if err != nil { log.Fatal(err) } When you use this method in a program and there are no Azure CLI credentials available locally, the program will exit with an error prompting you to run az login.\n1 2 3 $ go run azcli.go 2023/07/15 06:59:49 AzureCLICredential: ERROR: Please run \u0026#39;az login\u0026#39; to setup account. exit status 1 Interactive browser login When you run az login, you will typically see an interactive browser window open automatically, and you will be prompted to log in to your Azure account. Once the login is successful, the authentication process returns the token credentials that can be cached locally. The same method can be implemented in your Go programs managing Azure or accessing Azure services. This is done using the NewInteractiveBrowserCredential method. You can pass the optional InteractiveBrowserCredentialOptions parameter to modify the allowed tenants, specify a login hint, and enable support for disconnected clouds such as Azure stack. The usage of this method is similar to what you have already seen for Azure CLI credentials.\n1 2 3 4 5 // github.com/rchaganti/azure-go/02-auth101/interactivebrowser.go cred, err := azidentity.NewInteractiveBrowserCredential(nil) if err != nil { log.Fatal(err) } This method fails if the local session has no support for an interactive browser.\n1 2 3 $ go run interactivebrowser.go 2023/07/15 07:14:05 InteractiveBrowserCredential: exec: \u0026#34;xdg-open,x-www-browser,www-browser\u0026#34;: executable file not found in $PATH exit status 1 If a browser is available, you will see a screen similar to what is shown below. You can log in to an authorized account to access Azure services and authenticate.\nDevice code credential The device code base authentication should be familiar if you have used Azure PowerShell or similar tools. This is especially useful when your local environment lacks Azure CLI or a browser for interactive login. This method generates an alpha-numeric code that you need to input into a browser window for authentication. The azidentity package provides the NewDeviceCodeCredential() method to achieve device code-based authentication. This method takes an optional DeviceCodeCredentialOptions parameter using which you can specify additional allowed tenants and a custom prompt.\n1 2 3 4 5 6 7 8 9 10 11 12 // github.com/rchaganti/azure-go/02-auth101/devicecode.go deviceCodeCredOptions := \u0026amp;azidentity.DeviceCodeCredentialOptions{ UserPrompt: func(ctx context.Context, deviceCodeMessage azidentity.DeviceCodeMessage) error { fmt.Printf(\u0026#34;Enter code %s at https://microsoft.com/devicelogin to complete authentication.\\n\u0026#34;, deviceCodeMessage.UserCode) return nil }, } cred, err := azidentity.NewDeviceCodeCredential(deviceCodeCredOptions) if err != nil { log.Fatal(err) } The custom prompt is implemented using the UserPrompt field of the DeviceCodeCredentialOptions struct.\n1 2 $ go run devicecode.go Enter code HMYPYSK8J at https://microsoft.com/devicelogin to complete authentication. In the absence of the custom user prompt, you will receive the default message from Azure AD.\n1 2 $ go run devicecode.go To sign in, use a web browser to open the page https://microsoft.com/devicelogin and enter the code I3MPW7LML to authenticate. Username and Password credential This method of using a username and password is slightly involved and is not recommended for security reasons. However, a developer may use this within the local environment. The NewUsernamePasswordCredential() method in the azidentity package is used for this type of authentication. The account you use must satisfy the following requirements.\nIt should be a school or work account and not a Microsoft account. You need an Azure AD application registration. A client secret must exist within the Azure AD application. The Azure AD application registration must be granted admin consent The NewUsernamePasswordCredential() takes a few parameters such as tenantID, clientID, username, and password and includes UsernamePasswordCredentialOptions that specify additional allowed tenants and enable authentication in disconnected cloud scenarios. tenantID is the Azure tenant ID, and clientID is the Azure AD application ID.\n1 2 3 4 5 // github.com/rchaganti/azure-go/02-auth101/usernamepassword.go cred, err := azidentity.NewUsernamePasswordCredential(tenantID, clientID, username, password, nil) if err != nil { log.Fatal(err) } If all the prerequisites are met and the username and password are valid, executing this program must return a list of resource groups within the subscription.\n1 2 3 4 5 $ go run usernamepassword.go | jq .[].id \u0026#34;/subscriptions/5073ff70820/resourceGroups/cloud-shell-storage-southeastasia\u0026#34; \u0026#34;/subscriptions/5073ff70820/resourceGroups/NetworkWatcherRG\u0026#34; \u0026#34;/subscriptions/5073ff70820/resourceGroups/DefaultResourceGroup-EUS\u0026#34; \u0026#34;/subscriptions/5073ff70820/resourceGroups/az\u0026#34; The use of environment credentials is applicable to hosted/deployed services as well. Therefore, you shall see a demonstration of that in the next part of this series.\n","date":"2023-07-15T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/azure-sdk-for-go-authentication-methods-for-local-dev-environment/","series":"Azure Go","title":"Azure SDK for Go - Authentication methods for local development environment"},{"categories":["azure","go"],"content":"Microsoft Azure provides vast services from basic computing to AI/ML. Azure Resource Manager (ARM) is our entry point to access and manage these services and provides a consistent management layer (APIs) that enables you to work with resources in Azure.\nSource: Azure Resource Manager overview - Azure Resource Manager | Microsoft Learn\nAny resource management request lands at the ARM, get authenticated, and enacted. As the diagram above indicates, there are many ways to interact with ARM. You can choose one or more ways to create and manage your Azure resources based on your preference. Azure Portal provides a click-to-manage experience which is good for managing fewer resources. You can use REST API to access the ARM management API layer directly and perform the CRUD operations against Azure resources. Azure PowerShell and Azure CLI allow you to automate the management tasks at the command-line. These are my preferred methods for working with Azure. These two methods use Azure SDKs to interact with the ARM APIs. Microsoft provides SDKs for different programming languages.\nI will write about Azure SDK for Go and my experience working with it in this series of articles. Microsoft has excellent documentation to get you started. This series of articles is about my notes and experiments as I learn to use Azure SDK for Go.\nSeveral packages are available as a part of the Azure SDK for Go. These packages are classified into two categories \u0026ndash; management plane and data plane. The management plane packages are used to manage Azure resources. The data plane packages are used to interact with Azure resources. For example, the armresources package can be used to create resources and is a management plane package. Whereas the azcontainerregistry package, a data plane package, can be used to manage images in an Azure Container Registry. The minimum Go version you need for using these packages is 1.18.\nAll code samples presented in this series will be available at rchaganti/azure-go: Learning Azure SDK for Go (github.com)\nThe Development Container To help you start with Azure SDK for Go, I suggest using a development environment that can be easily built or rebuilt. I will use a VS code dev container for all examples in this series of articles. Here is the devcontainer.json I am using.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 { \u0026#34;image\u0026#34;: \u0026#34;mcr.microsoft.com/vscode/devcontainers/base\u0026#34;, \u0026#34;customizations\u0026#34;: { \u0026#34;vscode\u0026#34;: { \u0026#34;settings\u0026#34;: {}, \u0026#34;extensions\u0026#34;: [ \u0026#34;golang.go\u0026#34;, \u0026#34;ms-vscode.azure-account\u0026#34; ] } }, \u0026#34;features\u0026#34;: { \u0026#34;ghcr.io/devcontainers/features/go:1\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34; }, \u0026#34;azure-cli\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34; } }, \u0026#34;remoteUser\u0026#34;: \u0026#34;vscode\u0026#34; } This dev container gives you the Go development environment along with Azure CLI. You will also get the Go and Azure account VS Code extensions.\nAll code samples from this series of articles will be available at https://github.com/rchaganti/azure-go. You can clone the repository and start playing with the code samples.\nThe First Step Once the development environment is ready, you can create a project to implement the management or the data plane operations using the Azure SDK for Go.\n1 2 3 4 $ mkdir 01-azstart $ cd 01-azstart $ go mod init azstart $ touch main.go The above set of commands creates a directory and initializes a Go module called azstart. Once this folder structure is ready, you can download your project\u0026rsquo;s necessary Azure Go packages. This can be done using the go get command. For this quick start, you\u0026rsquo;ll need the azidentity and armresources packages.\n1 2 $ go get -u \u0026#34;github.com/Azure/azure-sdk-for-go/sdk/azidentity\u0026#34; $ go get -u \u0026#34;github.com/Azure/azure-sdk-for-go/sdk/resourcemanager/resources/armresources\u0026#34; You must use go get and not just do a go mod tidy. There may be intellisense issues within VS Code otherwise.\nWithin main.go program; you can place the following code. This program lists all resource groups in an Azure subscription. Don\u0026rsquo;t bother about all the details for now. In the subsequent parts of this series, you will learn about different authentication methods and using different packages from the Azure SDK for Go.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 package main import ( \u0026#34;context\u0026#34; \u0026#34;encoding/json\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;github.com/Azure/azure-sdk-for-go/sdk/azidentity\u0026#34; \u0026#34;github.com/Azure/azure-sdk-for-go/sdk/resourcemanager/resources/armresources\u0026#34; ) const subscriptionID = \u0026#34;5073ff70820\u0026#34; func main() { cred, err := azidentity.NewAzureCLICredential(nil) if err != nil { log.Fatal(err) } rcFactory, err := armresources.NewClientFactory(subscriptionID, cred, nil) if err != nil { log.Fatal(err) } rgClient := rcFactory.NewResourceGroupsClient() ctx := context.Background() resultPager := rgClient.NewListPager(nil) resourceGroups := make([]*armresources.ResourceGroup, 0) for resultPager.More() { pageResp, err := resultPager.NextPage(ctx) if err != nil { log.Fatal(err) } resourceGroups = append(resourceGroups, pageResp.ResourceGroupListResult.Value...) } jsonData, err := json.MarshalIndent(resourceGroups, \u0026#34;\\t\u0026#34;, \u0026#34;\\t\u0026#34;) if err != nil { log.Fatal(err) } fmt.Println(string(jsonData)) } The NewAzureCLICredential method in the azidentity package enables you to access locally available Azure credentials within your Go program. You will learn more about this in the next article.\nIf you are using the VS Code dev environment provided at the beginning of this article, you will have Azure CLI within the environment. Before you run this program, you must use az login to authenticate with the Azure cloud.\n1 2 3 4 5 $ go run . | jq .[].id \u0026#34;/subscriptions/5073ff70820/resourceGroups/cloud-shell-storage-southeastasia\u0026#34; \u0026#34;/subscriptions/5073ff70820/resourceGroups/NetworkWatcherRG\u0026#34; \u0026#34;/subscriptions/5073ff70820/resourceGroups/DefaultResourceGroup-EUS\u0026#34; \u0026#34;/subscriptions/5073ff70820/resourceGroups/az\u0026#34; If Azure authentication is successful, this program must return a list of all resource groups in JSON format. Within the dev container, you can use jq to filter the output.\nThis is just a quick start to get you started with the Azure SDK for Go. The next article will teach you about different authentication methods for your Go programs.\n","date":"2023-07-13T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/2023-07-13-getting-started-with-azure-sdk-for-go/","series":"Azure Go","title":"Getting Started with Azure SDK for Go"},{"categories":["powershell"],"content":"I work on a team with members across multiple time zones. It is important to schedule meetings so that team members do not have to stay late or wake up early. Being a command line person, I quickly wrote this PowerShell script to help me determine the time differences.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 param ( [Parameter(Mandatory = $true)] [String[]] $Timezone, [Parameter()] [Datetime] $Target ) if (!$Target) { # Get the local time as the target $Target = Get-Date } $tzObject = [System.Collections.Arraylist]::new() $tzObject.Add( [PSCustomObject]@{ \u0026#39;Timezone\u0026#39; = \u0026#39;Local\u0026#39; \u0026#39;Time\u0026#39; = $Target \u0026#39;DifferenceInHours\u0026#39; = 0 } ) | Out-Null foreach ($tz in $Timezone) { # Get the timezone difference $localTz = [System.TimeZoneInfo]::Local $remoteTz = [System.TimeZoneInfo]::FindSystemTimeZoneById($tz) $tzDifference = [float]($remoteTz.BaseUtcOffset.TotalHours - $localTz.BaseUtcOffset.TotalHours) $remoteTime = [System.TimeZoneInfo]::ConvertTime($Target, $localTz, $remoteTz) $tzObject.Add( [PSCustomObject]@{ \u0026#39;Timezone\u0026#39; = $tz \u0026#39;Time\u0026#39; = $remoteTime \u0026#39;DifferenceInHours\u0026#39; = $tzDifference } ) | Out-Null } return $tzObject This one is simple to use.\n1 2 3 4 5 6 7 8 9 PS\u0026gt; .\\tzDiff.ps1 -Timezone \u0026#39;China Standard Time\u0026#39;,\u0026#39;Eastern Standard Time\u0026#39;,\u0026#39;Central Standard Time\u0026#39;,\u0026#39;Pacific Standard Time\u0026#39; Timezone Time DifferenceInHours -------- ---- ----------------- Local 3/20/2023 10:49:40 AM 0 China Standard Time 3/20/2023 1:19:40 PM 2.5 Eastern Standard Time 3/20/2023 1:19:40 AM -10.5 Central Standard Time 3/20/2023 12:19:40 AM -11.5 Pacific Standard Time 3/19/2023 10:19:40 PM -13.5 When you do not specify any target time to convert, this script uses the current local time and converts to the time zones specified at the command line.\n1 2 3 4 5 6 7 8 9 PS\u0026gt; .\\tzDiff.ps1 -Timezone \u0026#39;China Standard Time\u0026#39;,\u0026#39;Eastern Standard Time\u0026#39;,\u0026#39;Central Standard Time\u0026#39;,\u0026#39;Pacific Standard Time\u0026#39; -Target (Get-Date \u0026#34;3/24/2023 15:30\u0026#34;) Timezone Time DifferenceInHours -------- ---- ----------------- Local 3/24/2023 3:30:00 PM 0 China Standard Time 3/24/2023 6:00:00 PM 2.5 Eastern Standard Time 3/24/2023 6:00:00 AM -10.5 Central Standard Time 3/24/2023 5:00:00 AM -11.5 Pacific Standard Time 3/24/2023 3:00:00 AM -13.5 You can specify a target time to convert using the -Target parameter. You can specify a future target time to schedule the meeting using this parameter.\nI hope you find this useful as well.\n","date":"2023-03-20T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/2023-03-20-determining-time-zone-differences-in-powershell-for-effective-meeting-planning/","series":"PowerShell","title":"Determining time zone differences in PowerShell for effective meeting planning"},{"categories":["wasm","vscode"],"content":"WebAssembly, a.k.a Wasm, defines a portable binary-code format for a stack-based virtual machine. Wasm enables developers to write code that runs in web browsers with near-native performance using languages such as C, C++, Rust, Go, and many other modern languages. Wasm is not a replacement for JavaScript but is designed to complement and run alongside JavaScript.\nI am still learning about Wasm and looking at opportunities to use Wasm in designing modern and cloud-native applications. My focus has been using the Go programming language for Wasm as well. So, when I started learning Wasm, I quickly looked for ways to create a development environment that would help me in my journey toward mastering Wasm. For this, I started looking at VS Code development containers. This resulted in a set of devcontainer features that I can use within a development container definition.\nTinyGo feature The first feature I needed was TinyGo. TinyGo compiles Go code for embedded systems and WebAssembly. This feature installs tinygo in your dev container.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 { \u0026#34;image\u0026#34;: \u0026#34;mcr.microsoft.com/vscode/devcontainers/base\u0026#34;, \u0026#34;settings\u0026#34;: {}, \u0026#34;extensions\u0026#34;: [], \u0026#34;features\u0026#34;: { \u0026#34;ghcr.io/devcontainers/features/go:1\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34; }, \u0026#34;ghcr.io/rchaganti/vsc-devcontainer-features/tinygo:latest\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;0.26.0\u0026#34; } }, \u0026#34;remoteUser\u0026#34;: \u0026#34;vscode\u0026#34; } WebAssembly Binary Toolkit feature The next feature is a set of tools that help learn the internals of Wasm and debug Wasm. These tools are packaged as WABT.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 { \u0026#34;image\u0026#34;: \u0026#34;mcr.microsoft.com/vscode/devcontainers/base\u0026#34;, \u0026#34;settings\u0026#34;: {}, \u0026#34;extensions\u0026#34;: [ \u0026#34;dtsvet.vscode-wasm\u0026#34; ], \u0026#34;features\u0026#34;: { \u0026#34;ghcr.io/devcontainers/features/go:1\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34; }, \u0026#34;ghcr.io/rchaganti/vsc-devcontainer-features/tinygo:latest\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;0.26.0\u0026#34; }, \u0026#34;ghcr.io/rchaganti/vsc-devcontainer-features/wabt:latest\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;1.0.32\u0026#34; } }, \u0026#34;remoteUser\u0026#34;: \u0026#34;vscode\u0026#34; } This dev container definition provides all I need to learn and build Wasm in the Go programming language.\nWhat else? These features install the VS code extensions for TinyGo and WebAssembly. So, you have the full power of VS Code in your development container.\nGo ahead and give this a try! Let me know what else you want to see to help you learn or develop Wasm in VS Code.\n","date":"2023-01-12T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/2023-01-12-webassembly-learning-playground-in-vs-code/","series":"Wasm","title":"WebAssembly (wasm) learning playground in VS Code"},{"categories":["Golang","Go","Cobra"],"content":" Practice makes permanent - Bobby Robson.\nWhen learning a new programming language or any technology for that matter, we need to practice what we learn, and that is when it becomes permanent. I am starting a new series of posts to share what I have tried or built, so that what I have learned— through Go programming books, video courses, and community content—stays with me permanently.\nThe first one in this series is about using Cobra package to build a CLI application.\nWhat you will learn today? Using the Cobra package to add command line parameters The typical program structure when using Cobra package to create a CLI application Converting command line arguments to the relevant data type using strconv package. What is Cobra? Cobra is the most popular and most powerful library for creating modern CLI applications. kubectl, docker, hugo, gh, and several other popular command line tools use Cobra library. Cobra lets us create CLI tools with nested subcommands, global, local and cascading flags among many other powerful features. Talk is cheap, so let us get started with a tiny command line calculator project to understand how to use the Cobra package. This may not be a useful utility but helps in understanding the basics of implementing a CLI application. We can build on this later by looking at more advanced use cases.\nCreating a project To start with, let us create the folder structure we need to build this project.\n1 2 3 4 5 6 7 8 9 $ mkdir gcalc \u0026amp;\u0026amp; cd gcalc $ go mod init github.com/rchaganti/gcalc $ touch main.go $ mkdir -p cmd/gcalc $ mkdir -p pkg/gcalc $ go get -u github.com/spf13/cobra@latest The main project folder is gcalc. We, then, run go mod init to initialize a module within this project folder. I have used a fully-qualified path to the GitHub repo but that is not necessary. The repo need not even exist if we don\u0026rsquo;t intend to share this external world. Finally, to complete the folder structure, we created two folders \u0026ndash; cmd/gcalc for storing all command/subcommand logic and pkg/gcalc for storing the actual arithmetic functions that we will soon implement. The go get command at the end pulls Cobra package and stores it locally.\nWhen using Cobra, all commands get structure as a tree with one root. So, the first thing we need to do is create the root command.\n1 $ touch cmd/gcalc/root.go Within root.go, we will add the root of the command structure that we need to implement. You can copy/paste the following code.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 package gcalc import ( \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;github.com/spf13/cobra\u0026#34; ) var rootCmd = \u0026amp;cobra.Command{ Use: \u0026#34;gcalc\u0026#34;, Short: \u0026#34;gcalc - a commandline calculator for basic arithmetic\u0026#34;, Long: `gcalc is a simple yet powerful command line calculator You can use gcalc for quick calculations at the command line.`, Run: func(cmd *cobra.Command, args []string) {}, } func Execute() { if err := rootCmd.Execute(); err != nil { fmt.Fprintf(os.Stderr, \u0026#34;Ooops. There was an error while executing your CLI \u0026#39;%s\u0026#39;\u0026#34;, err) os.Exit(1) } } In Cobra, every command that we need to add is represented using cobra.Command struct. This struct has many fields, but, for now, let us limit it to minimum needed to implement our calculator.\nTip: Always look at the package / library source code to understand the package better. This is not just better than documentation but also a good way to learn new techniques.\nWithin root.go, we start by declaring and initializing rootCmd variable. The Use, Short, and Long fields are related to the command help. The Run field is what identifies what happens when you invoke this command. Since this the root of the command tree, the Execute() function gets invoked in the main function. Let us see how the main.go looks like.\n1 2 3 4 5 6 7 package main import \u0026#34;github.com/rchaganti/gcalc/cmd/gcalc\u0026#34; func main() { gcalc.Execute() } At this point in time, our CLI is ready for a trial.\n1 2 3 4 5 6 7 8 9 10 $ go run main.go --help gcalc is a simple yet powerful command line calculator You can use gcalc for quick calculations at the command line. Usage: gcalc [flags] Flags: -h, --help help for gcalc Now that we have the CLI root command working well, we can start adding the commands for basic arithmetic. But, before that, let us add the functions that actually perform the arithmetic. We will do this in the pkg/gcalc/gcalc.go.\n1 $ touch pkg/gcalc/gcalc.go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 package gcalc import ( \u0026#34;fmt\u0026#34; \u0026#34;strconv\u0026#34; ) func Add(args []string) (sum int) { for _, i := range args { realInt, err := strconv.Atoi(i) if err != nil { fmt.Println(err) } sum += realInt } return sum } func Subtract(args []string) (diff int) { num1, err := strconv.Atoi(args[0]) if err != nil { fmt.Println(err) } num2, err := strconv.Atoi(args[1]) if err != nil { fmt.Println(err) } return num1 - num2 } func Multiply(args []string) (product int) { product = 1 for _, i := range args { realInt, err := strconv.Atoi(i) if err != nil { fmt.Println(err) } product *= realInt } return product } func Divide(args []string) (dividend int) { num1, err := strconv.Atoi(args[0]) if err != nil { fmt.Println(err) } num2, err := strconv.Atoi(args[1]) if err != nil { fmt.Println(err) } return num1 / num2 } In this package, we define four functions \u0026ndash; Add, Subtract, Multiply, and Divide. This is our business logic. Each function takes a slice of strings as input and return an integer value based on the arithmetic the function implements. Each function takes a slice of strings since that is what we get from the command line. Therefore, we need to convert the values to integer and we do that using the strconv.Atoi() function. The Add and Multiply functions can operate on any number of arguments but the Subtract and Divide functions operate on two arguments only.\nOur package with business logic is ready. But, to consume these functions, we need the commands within our command tree. We can add additional commands by using multiple .go files under cmd/gcalc folder.\n1 2 3 4 $ touch cmd/gcalc/add.go $ touch cmd/gcalc/subtract.go $ touch cmd/gcalc/multiply.go $ touch cmd/gcalc/divide.go While you can have all command registrations in a single file, I believe this a better and clean approach I have seen many CLIs implement.\nLet us populate each of these files in the same order.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 package gcalc import ( \u0026#34;fmt\u0026#34; \u0026#34;github.com/rchaganti/gcalc/pkg/gcalc\u0026#34; \u0026#34;github.com/spf13/cobra\u0026#34; ) var addCmd = \u0026amp;cobra.Command{ Use: \u0026#34;add\u0026#34;, Aliases: []string{\u0026#34;a\u0026#34;}, Short: \u0026#34;Adds a bunch of integers\u0026#34;, Run: func(cmd *cobra.Command, args []string) { res := gcalc.Add(args) fmt.Println(res) }, } func init() { rootCmd.AddCommand(addCmd) } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 package gcalc import ( \u0026#34;fmt\u0026#34; \u0026#34;github.com/rchaganti/gcalc/pkg/gcalc\u0026#34; \u0026#34;github.com/spf13/cobra\u0026#34; ) var subtractCmd = \u0026amp;cobra.Command{ Use: \u0026#34;subtract\u0026#34;, Aliases: []string{\u0026#34;s\u0026#34;}, Short: \u0026#34;Subtracts two integers\u0026#34;, Args: cobra.ExactArgs(2), Run: func(cmd *cobra.Command, args []string) { res := gcalc.Subtract(args) fmt.Println(res) }, } func init() { rootCmd.AddCommand(subtractCmd) } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 package gcalc import ( \u0026#34;fmt\u0026#34; \u0026#34;github.com/rchaganti/gcalc/pkg/gcalc\u0026#34; \u0026#34;github.com/spf13/cobra\u0026#34; ) var multiplyCmd = \u0026amp;cobra.Command{ Use: \u0026#34;multiply\u0026#34;, Aliases: []string{\u0026#34;m\u0026#34;}, Short: \u0026#34;Multiplies a bunch of integers\u0026#34;, Run: func(cmd *cobra.Command, args []string) { res := gcalc.Multiply(args) fmt.Println(res) }, } func init() { rootCmd.AddCommand(multiplyCmd) } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 package gcalc import ( \u0026#34;fmt\u0026#34; \u0026#34;github.com/rchaganti/gcalc/pkg/gcalc\u0026#34; \u0026#34;github.com/spf13/cobra\u0026#34; ) var divideCmd = \u0026amp;cobra.Command{ Use: \u0026#34;divide\u0026#34;, Aliases: []string{\u0026#34;d\u0026#34;}, Short: \u0026#34;Divides two integers\u0026#34;, Args: cobra.ExactArgs(2), Run: func(cmd *cobra.Command, args []string) { res := gcalc.Divide(args) fmt.Println(res) }, } func init() { rootCmd.AddCommand(divideCmd) } The content of all these files is more or less same but there are a couple of things we should note.\nUsing cobra.ExactArgs() method, we can restrict how many command line arguments a user can pass to the command. This is useful within both subtract and divide commands. The second thing we should note here is the init() function in each of these files. This function gets invoked first and helps register the command in the command tree.\nHere is how the final program folder structure should be once you add the remaining command program files.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 $ tree . . ├── cmd │ └── gcalc │ ├── add.go │ ├── divide.go │ ├── multiply.go │ ├── root.go │ └── subtract.go ├── go.mod ├── go.sum ├── main.go └── pkg └── gcalc └── gcalc.go 4 directories, 9 files Alright! Time for another test ride.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 $ go run main.go --help gcalc is a simple yet powerful command line calculator You can use gcalc for quick calculations at the command line. Usage: gcalc [flags] gcalc [command] Available Commands: add Adds a bunch of integers completion Generate the autocompletion script for the specified shell divide Divides two integers help Help about any command multiply Multiplies a bunch of integers subtract Subtracts two integers Flags: -h, --help help for gcalc Use \u0026#34;gcalc [command] --help\u0026#34; for more information about a command. This is good. We have the command help printed as we expect. Let us test each command.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 $ go build -o gcalc main.go $ ./gcalc add 1 2 3 4 5 15 $ ./gcalc subtract 1 2 3 4 5 Error: accepts 2 arg(s), received 5 Usage: gcalc subtract [flags] Aliases: subtract, s Flags: -h, --help help for subtract Ooops. There was an error while executing your CLI \u0026#39;accepts 2 arg(s), received 5\u0026#39; $ ./gcalc subtract 1 2 -1 $ ./gcalc multiply 1 2 3 4 24 $ ./gcalc divide 4 2 2 What if we pass float values instead of integers?\n1 2 3 4 $ ./gcalc add 1 2.0 3.1 5.5 strconv.Atoi: parsing \u0026#34;2.0\u0026#34;: invalid syntax strconv.Atoi: parsing \u0026#34;3.1\u0026#34;: invalid syntax strconv.Atoi: parsing \u0026#34;5.5\u0026#34;: invalid syntax As expected. Our calculator can handle only integers. One method to address this is to consider input as floating point values and return floating point values. Here is how we can update the business logic.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 package gcalc import ( \u0026#34;fmt\u0026#34; \u0026#34;strconv\u0026#34; ) func Add(args []string) (sum float64) { for _, i := range args { realValue, err := strconv.ParseFloat(i, 64) if err != nil { fmt.Println(err) } sum += realValue } return sum } func Subtract(args []string) (diff float64) { num1, err := strconv.ParseFloat(args[0], 64) if err != nil { fmt.Println(err) } num2, err := strconv.ParseFloat(args[1], 64) if err != nil { fmt.Println(err) } return num1 - num2 } func Multiply(args []string) (product float64) { product = 1 for _, i := range args { realValue, err := strconv.ParseFloat(i, 64) if err != nil { fmt.Println(err) } product *= realValue } return product } func Divide(args []string) (divident float64) { num1, err := strconv.ParseFloat(args[0], 64) if err != nil { fmt.Println(err) } num2, err := strconv.ParseFloat(args[1], 64) if err != nil { fmt.Println(err) } return num1 / num2 } With this in place, our calculator can now take both floats and integers or a mix of both types.\n1 2 3 4 5 6 7 8 9 10 11 12 13 $ go build -o gcalc main.go $ ./gcalc add 1 2.0 3.1 5.5 11.6 $ ./gcalc subtract 1.1 2 -0.8999999999999999 $ ./gcalc multiply 1 4 4 $ ./gcalc divide 3 4 0.75 What we have seen so far is a simple implementation of a CLI using Cobra library. You can add global and local flags and nest commands within commands. We shall see that in a future post with when we build something useful than just a calculator. Stay tuned.\n","date":"2022-12-16T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/2022-12-16-applied-go-creating-a-cli-application/","series":"Applied Go","title":"Applied Go - Creating a CLI application"},{"categories":["sigstore","gitsign","containers"],"content":"Once a Sigstore fan, always a fan!\nSigstore has become a good part of my research around SBOM and supply-chain security. I wrote about a cosign VS Code devcontainer feature earlier and then stumbled upon gitsign. In the past, I tried using GPG keys to sign my git commits but could not really sustain that as I kept changing machines and using different development environments. When I first looked at cosign, my initial thought was using it also for git commit signing but as I started reading Sigstore documentation, I discovered gitsign!\nSimilar to my cosign VS Code devcontainer feature, I quickly created a gitsign feature. As a part of this feature install inside the devcontainer, the bootstrap script configures git global options to enable signing of all commits and using gitsign as the X509 program.\nHere is how you can consume this feature in your VS code devcontainer definition.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 { \u0026#34;image\u0026#34;: \u0026#34;mcr.microsoft.com/vscode/devcontainers/base\u0026#34;, \u0026#34;containerEnv\u0026#34;: { \u0026#34;TZ\u0026#34;: \u0026#34;Asia/Calcutta\u0026#34;, \u0026#34;GITSIGN_CONNECTOR_ID\u0026#34;: \u0026#34;https://github.com/login/oauth\u0026#34;, \u0026#34;GITSIGN_FULCIO_URL\u0026#34;: \u0026#34;https://fulcio.sigstore.dev\u0026#34;, \u0026#34;GITSIGN_REKOR_URL\u0026#34;: \u0026#34;https://rekor.sigstore.dev\u0026#34; }, \u0026#34;settings\u0026#34;: {} \u0026#34;extensions\u0026#34;: [ \u0026#34;golang.go\u0026#34; ], \u0026#34;features\u0026#34;: { \u0026#34;ghcr.io/devcontainers/features/go:1\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34; }, \u0026#34;ghcr.io/rchaganti/vsc-devcontainer-features/cosign:latest\u0026#34; : {}, \u0026#34;ghcr.io/rchaganti/vsc-devcontainer-features/gitsign:latest\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;0.4.1\u0026#34; } }, \u0026#34;remoteUser\u0026#34;: \u0026#34;vscode\u0026#34; } As you see in the containerEnv section, you can specify gitsign specific environment variables. For example, if you are using public sigstore instance (can be changed using GITSIGN_FULCIO_URL), you can use either Google or Microsoft or GitHub as auth providers. So, the auth provider of your choice can be specified using the GITSIGN_CONNECTOR_ID environment variable.\nOnce a devcontainer is built from this definition, every git commit will prompt you to retrieve and enter the verification code from the auth provider of your choice. If you have not configured the default auth provider using GITSIGN_CONNECTOR_ID variable, you will be prompted to select a auth provider (assuming you are still using Sigstore public instance.)\nYou will notice on GitHub that these commits show up as unverified.\nThis is because Sigstore is not a part of GitHub\u0026rsquo;s trusted root and there is no way (yet!) for GitHub to verify [using Rekor, for example] that the certificate used to sign the commit was indeed valid at the time of signing. I wish/hope that GitHub implements this support soon!\nThis is it for today! Be back for more Sigstore goodness!\n","date":"2022-12-13T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/2022-12-13-keyless-git-commit-signing-using-sigstore-gitsign-in-a-vscode-devcontainer/","series":null,"title":"Keyless git commit signing using Sigstore gitsign in a VS Code devcontainer"},{"categories":["sigstore","cosign","containers"],"content":"For those following me here or social media will know that I am a big fan of Visual Studio development containers. I wrote a bit about Visual Studio Code development containers. I have been using devcontainers for different development environments extensively. I have a sample repo on GitHub that has all devcontainer definitions that I have been using for different projects. A latest addition to this list is the Sigstore cosign devcontainer feature.\nIf you are new to creating VS Code devcontainer features, you can read my earlier article where I walked through creating a devcontainer feature for Cuelang.\nWhat is cosign? The Sigstore\u0026rsquo;s cosign aims to support artifact signing and verification. This article is not about cosign but here is a quick look at what you can achieve.\n1 2 3 4 5 6 7 8 # Generate key pair (private/public) cosign generate-key-pair # Sign a container image cosign sign --key cosign.key ravikanth/hello-container # Verify signature cosign verify --key cosign.pub ravikanth/hello-container Although the above example shows signing a container image, you can sign any artifact using cosign. I have a few articles around container images and OCI artifacts in drafts where I plan to show the usage of Cosign in-depth. For now, this is just a quick overview.\nComing back to VS Code devcontainers, I use devcontainers for all my development work and as a part of that I wanted to have cosign also available in my devcontainer. So, I created a feature that I can simply include in my devcontainer definition.\nConsuming cosign devcontainer feature All the devcontainer features that I build are available in the GitHub artifact registry.\nFor VS Code or GitHub codespaces to identify a devcontainer, you need to create a .devcontainer folder at the root of the repository. Once this folder is created, copy the JSON contents shown below to a file and save it as devcontainer.json under the .devcontainer folder.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 { \u0026#34;image\u0026#34;: \u0026#34;mcr.microsoft.com/vscode/devcontainers/base\u0026#34;, \u0026#34;containerEnv\u0026#34;: { \u0026#34;TZ\u0026#34;: \u0026#34;Asia/Calcutta\u0026#34; }, \u0026#34;settings\u0026#34;: {}, \u0026#34;extensions\u0026#34;: [ \u0026#34;golang.go\u0026#34; ], \u0026#34;features\u0026#34;: { \u0026#34;ghcr.io/devcontainers/features/go:1\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34; }, \u0026#34;ghcr.io/rchaganti/vsc-devcontainer-features/cosign:latest\u0026#34; : {} }, \u0026#34;remoteUser\u0026#34;: \u0026#34;vscode\u0026#34; } This is it really. Whenever you open this repo in VS Code, it prompts you if you want to open the repository in a devcontainer. If you choose to open in a devcontainer, it will take a few minutes to build the container image and start the container for you.\nIf you need a specific version of the cosign binary, you can specify that using the version option.\n1 2 3 \u0026#34;ghcr.io/rchaganti/vsc-devcontainer-features/cosign:latest\u0026#34; : { \u0026#34;version\u0026#34;: \u0026#34;1.31.1\u0026#34; } In this devcontainer, I have both Go language and cosign binary. Simple. Eh!?\nLet me know how you use cosign today in the comments.\n","date":"2022-12-12T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/2022-12-12-sigstore-cosign-dev-container-feature/","series":null,"title":"Sigstore Cosign VS Code development container feature"},{"categories":["oci","containers"],"content":"In a couple of earlier articles, we looked at Linux constructs used in building container images and the OCI image specification which standardizes the container image format. It is now time to understand how we can create container images. Knowing this is important to optimize the image size and building secure container images. And, of course, this knowledge also helps us appreciate what tools like Docker CLI or Buildah among many others help us achieve. There are many tools to build container images. We shall look at each of these methods in-depth in later parts of this series of articles on container images. For today, we will look into building container images using no tools! Actually, just using built-in OS tools.\nAs we learned already, a container image is a made up of layers packaged as tarballs. Each of these tarballs represent a portion of the file system that is needed for the application in the container to function. For example, here is how the Golang image looks like [on the file system] when you extract it using Skopeo.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 vscode ➜ ~$ cat index.json | jq \u0026#39;.manifests[].digest\u0026#39; \u0026#34;sha256:5e2f436ecda6b9f2ccdd912cfa462323100ba98e4399bdddb212f675d87fb85b\u0026#34; vscode ➜ ~$ cat blobs/sha256/5e2f436ecda6b9f2ccdd912cfa462323100ba98e4399bdddb212f675d87fb85b | jq \u0026#39;.config.digest\u0026#39; \u0026#34;sha256:b47d5cbdc30315fb21d7299ceacc1146985c84230421fd5e4a07e8b009575a11\u0026#34; vscode ➜ ~$ cat blobs/sha256/5e2f436ecda6b9f2ccdd912cfa462323100ba98e4399bdddb212f675d87fb85b | jq \u0026#39;.layers[].digest\u0026#39; \u0026#34;sha256:a8ca11554fce00d9177da2d76307bdc06df7faeb84529755c648ac4886192ed1\u0026#34; \u0026#34;sha256:e4e46864aba2e62ba7c75965e4aa33ec856ee1b1074dda6b478101c577b63abd\u0026#34; \u0026#34;sha256:c85a0be79bfba309d1f05dc40b447aa82b604593531fed1e7e12e4bef63483a5\u0026#34; \u0026#34;sha256:195ea6a58ca87a18477965a6e6a8623112bde82c5b568a29c56ce4581b6e6695\u0026#34; \u0026#34;sha256:52908dc1c386fab0271a2b84b6ef4d96205a98a8c8801169554767172e45d8c7\u0026#34; \u0026#34;sha256:a2b47720d601b6c6c6e7763b4851e25475118d80a76be466ef3aa388abf2defd\u0026#34; \u0026#34;sha256:14a70245b07c7f5056bdd90a3d93e37417ec26542def5a37ac8f19e437562533\u0026#34; index.json at the root of the OCI layout contains a pointer to the manifest that describes the configuration and layers within the image. As seen here, both manifest and image config have file\u0026rsquo;s SHA256 hash as the file name and both files are JSON documents.\nImage config, as we learned earlier, is what the container runtime uses to setup the container. It contains an ordered collection of file system layers and the execution parameters to create the container. What is important for our discussion is at the beginning of the configuration JSON. Here is an example from the golang image.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 vscode ➜ ~$ cat golang/blobs/sha256/b47d5cbdc30315fb21d7299ceacc1146985c84230421fd5e4a07e8b009575a11 | jq . { \u0026#34;created\u0026#34;: \u0026#34;2022-11-16T04:16:44.241793636Z\u0026#34;, \u0026#34;architecture\u0026#34;: \u0026#34;amd64\u0026#34;, \u0026#34;os\u0026#34;: \u0026#34;linux\u0026#34;, \u0026#34;config\u0026#34;: { \u0026#34;Env\u0026#34;: [ \u0026#34;PATH=/go/bin:/usr/local/go/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\u0026#34;, \u0026#34;GOLANG_VERSION=1.19.3\u0026#34;, \u0026#34;GOPATH=/go\u0026#34; ], \u0026#34;Cmd\u0026#34;: [ \u0026#34;bash\u0026#34; ], \u0026#34;WorkingDir\u0026#34;: \u0026#34;/go\u0026#34; }, \u0026#34;rootfs\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;layers\u0026#34;, \u0026#34;diff_ids\u0026#34;: [ \u0026#34;sha256:ccba29d6937047c719a6c048a7038d3907590fbb8556418d119469b2ad4f95bc\u0026#34;, \u0026#34;sha256:f1c1f22985847a54884b1448b7e41f2852c7f4fab4b71014ba38cc2990cdc9f1\u0026#34;, \u0026#34;sha256:80231db1194c4e9941e3c4e00c704f527889863ea2222b691ff7f8583b912aa0\u0026#34;, \u0026#34;sha256:89c3244a87b279c98478c9248c9cbfba18ca16866e38c54f0134b31bbfd3ab27\u0026#34;, \u0026#34;sha256:6a7921805df03a2c5bbc8a24979077ce43b466d1b4ace36afd045e51fdbbe24f\u0026#34;, \u0026#34;sha256:de3f6c83fe06d179f2cdc189bd123dd159c78b2b80cd6739fde25988b1142925\u0026#34;, \u0026#34;sha256:6201ebf6567d8a29c3458610efb2b681fec4b8c127668d8d223f504520ed351e\u0026#34; ] }, .... Each layer in the image is a gzipped tarball, once again, containing the SHA256 hash as the file name. So, if we need to build a container image by hand, we need to really follow the same process of packaging each of our image layers. Let us look at that.\nBuilding a hello-cloud image To understand the process of building a container image, we shall use a simple hello-cloud Go program. Our image will contain only one layer \u0026ndash; the hello-cloud program. Here is our simple Go program.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; ) func main() { var name string if len(os.Args) == 2 { name = os.Args[1] } if name == \u0026#34;\u0026#34; { fmt.Println(\u0026#34;Hello, 🌍!\u0026#34;) } else { fmt.Println(\u0026#34;Hello,\u0026#34;, name, \u0026#34;👋\u0026#34;) } } Nothing fancy here. This program takes an optional input. This optional input can be your name or whatever you like. If there is no input, it prints Hello, 🌍!. You can build the binary using go build hello-cloud.go command.\n1 2 3 4 vscode ➜ ~/hello-cloud$ ./hello-cloud Hello, 🌍! vscode ➜ ~/hello-cloud$ ./hello-cloud \u0026#34;Cloud Native Central\u0026#34; Hello, Cloud Native Central 👋 Go programs are statically linked and therefore have no OS dependencies. Let us now look at creating a folder structure needed for this.\n1 2 3 4 5 6 vscode ➜ ~/hello-cloud$ tree imageLayer/ imageLayer/ ├── bin └── tmp 2 directories, 0 files We shall now move the hello-world binary into the bin folder.\n1 2 3 4 5 6 7 vscode ➜ ~/hello-cloud$ mv hello-cloud imageLayer/bin/ vscode ➜ ~/hello-cloud$ tree imageLayer/ imageLayer/ ├── bin │ └── hello-cloud └── tmp We have the contents of our image layer ready. We can now package this into a gzipped tarball.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 vscode ➜ ~/hello-cloud$ cd imageLayer/ vscode ➜ ~/hello-cloud/imageLayer$ tar -czvf ../hello-cloud.tar.gz * bin/ bin/hello-cloud tmp/ vscode ➜ ~/hello-world/imageLayer$ tree ../ ../ ├── hello-cloud.tar.gz └── imageLayer ├── bin │ └── hello-cloud └── tmp 3 directories, 2 files This is good. However, to be able to run a container from this image layer, we need to feed tell the container runtime how to use the layers and the execution parameters to setup the container. So, let us step back a bit and see what the earlier example of golang image\u0026rsquo;s configuration JSON contained. Every container image is specific to an operating system and architecture. The golang image that we pulled is a linux/amd64 image. Our hello-cloud binary is built on a Linux amd64 system as well.\nTo be able to invoke our hello-cloud binary, we need to set the right environment variables. Also, we want this binary to get invoked when the container starts. These environment and execution parameters needs to be within the config section of the configuration JSON.\nSo, here is how our configuration JSON will start.\n1 2 3 4 5 6 7 8 9 10 11 12 13 { \u0026#34;architecture\u0026#34;: \u0026#34;amd64\u0026#34;, \u0026#34;os\u0026#34;: \u0026#34;linux\u0026#34;, \u0026#34;config\u0026#34;: { \u0026#34;Env\u0026#34;: [\u0026#34;PATH=/bin\u0026#34;], \u0026#34;Entrypoint\u0026#34;: [ \u0026#34;hello-cloud\u0026#34; ], \u0026#34;Cmd\u0026#34;: [ \u0026#34;Cloud Native Central!\u0026#34; ] } } We set the Entrypoint to the hello-world binary and the Cmd array to a single argument that will be passed to the binary.\nThe next important thing that needs to be added is the DiffID of the image layer. The diff ID is the digest over the layer\u0026rsquo;s uncompressed tarball. We can obtainthis using the sha256sum command.\n1 2 vscode ➜ ~$ gunzip \u0026lt; hello-world/hello-cloud.tar.gz | sha256sum 690fa2812df903a4cee676e28389e154fe022f2a26fb29c4c97e9c6583fa5bc6 - We need to add the serialized version of this digest to the configuration JSON. So, here is how our final configuration JSON looks like.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 { \u0026#34;architecture\u0026#34;: \u0026#34;amd64\u0026#34;, \u0026#34;os\u0026#34;: \u0026#34;linux\u0026#34;, \u0026#34;config\u0026#34;: { \u0026#34;Env\u0026#34;: [ \u0026#34;PATH=/bin\u0026#34; ], \u0026#34;Entrypoint\u0026#34;: [ \u0026#34;hello-cloud\u0026#34; ], \u0026#34;Cmd\u0026#34;: [ \u0026#34;Cloud Native Central!\u0026#34; ] }, \u0026#34;rootfs\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;layers\u0026#34;, \u0026#34;diff_ids\u0026#34;: [ \u0026#34;sha256:690fa2812df903a4cee676e28389e154fe022f2a26fb29c4c97e9c6583fa5bc6\u0026#34; ] }, \u0026#34;history\u0026#34;: [ { \u0026#34;created_by\u0026#34;: \u0026#34;Ravikanth C\u0026#34; } ] } As we saw in the golang image example earlier, we need a manifest that combines configuration JSON and the layers. These artifacts are referenced using their digest values as well. So, let us generate the digests for both configuration JSON and the layer tar.gz. We will also need the size in bytes which can be derived from the ls output. The layer digest is different from the DiffID we captured earlier. The layer digest identifies the compressed content and DiffID, as mentioned earlier, identifies the uncompressed content. Container runtimes deal with uncompressed layer content and therefore the configuration JSON must include the digest of the uncompressed layer.\n1 2 3 4 5 6 7 8 9 10 11 vscode ➜ ~/hello-cloud$ sha256sum \u0026lt; hello-world.tar.gz e251e682f7e64cff127020015be3254313b3a7900f54d82ecf34653192be4c21 - vscode ➜ ~/hello-cloud$ sha256sum \u0026lt; configuration.json da17376c4c02667544fb07a8788cfa4c294f89464534d3a76a45ee004cebacf5 - vscode ➜ ~/hello-cloud$ ls -l total 1064 -rw-r--r-- 1 rchaganti rchaganti 364 Nov 27 13:19 configuration.json -rw-r--r-- 1 rchaganti rchaganti 1074934 Nov 27 13:42 hello-world.tar.gz drwxr-xr-x 4 rchaganti rchaganti 4096 Nov 27 11:11 imageLayer We need to add the appropriate mediaType for both configuration JSON and each layer. Here is how our manifest.json looks like.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 { \u0026#34;schemaVersion\u0026#34;: 2, \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.oci.image.manifest.v1+json\u0026#34;, \u0026#34;config\u0026#34;: { \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.oci.image.config.v1+json\u0026#34;, \u0026#34;size\u0026#34;: 277, \u0026#34;digest\u0026#34;: \u0026#34;sha256:da17376c4c02667544fb07a8788cfa4c294f89464534d3a76a45ee004cebacf5\u0026#34; }, \u0026#34;layers\u0026#34;: [ { \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.oci.image.layer.v1.tar+gzip\u0026#34;, \u0026#34;size\u0026#34;: 1074934, \u0026#34;digest\u0026#34;: \u0026#34;sha256:e251e682f7e64cff127020015be3254313b3a7900f54d82ecf34653192be4c21\u0026#34; } ] } Alright. Do we have the image ready? Not quite yet. We need to push it to an image registry. While, on surface, it looks like a container image is a singular entity but in reality it is not. As we have already seen, a container image includes a bunch of layers, a configuration, and a manifest. When Docker CLI or any other tool pushes images to a registry, these tools do so in a specific sequence. Since we are building this without any tools such as Docker CLI, we need to push these artifacts manually using the Docker registry HTTP API.\nHere is the workflow.\nLet us start. We need to first authenticate with Docker Hub. There are many ways to do this but we will use token authentication.\n1 2 3 vscode ➜ ~/hello-cloud$ USERNAME=ravikanth vscode ➜ ~/hello-cloud$ REPO=\u0026#34;${USERNAME}/hello-cloud\u0026#34; vscode ➜ ~/hello-cloud$ TOKEN=$(curl -v -u \u0026#34;$USERNAME\u0026#34; \u0026#34;https://auth.docker.io/token?service=registry.docker.io\u0026amp;scope=repository:$REPO:pull,push\u0026#34; | jq \u0026#39;.token\u0026#39; | cut -d\\\u0026#34; -f2) The above command will prompt for a password to authenticate with Docker Hub and retrieve the token from the response.\nUpload blobs Once we have the auth token, we need to upload each blob (layers and configuration JSON) to the repository. Let us start with hello-world.tar.gz.\n1 2 3 4 5 vscode ➜ ~/hello-cloud$ LOCATION=$(curl -i https://registry.hub.docker.com/v2/$REPO/blobs/uploads/ -H \u0026#34;Authorization: Bearer $TOKEN\u0026#34; -d \u0026#39;\u0026#39; | grep location | cut -d\u0026#34; \u0026#34; -f2 | tr -d \u0026#39;\\r\u0026#39;) vscode ➜ ~/hello-cloud$ LOCATION=$(curl -i $LOCATION -X PATCH -H \u0026#34;Content-Length: $LAYERSIZE\u0026#34; -H \u0026#34;Authorization: Bearer $TOKEN\u0026#34; -H \u0026#34;Content-Type: application/octet-stream\u0026#34; --data-binary @e251e682f7e64cff127020015be3254313b3a7900f54d82ecf34653192be4c21 | grep location | cut -d\u0026#34; \u0026#34; -f2 | tr -d \u0026#39;\\r\u0026#39;) vscode ➜ ~/hello-cloud$ curl \u0026#34;$LOCATION\u0026amp;digest=sha256:$LAYERDIGEST\u0026#34; -X PUT -H \u0026#34;Authorization: Bearer $TOKEN\u0026#34; -v The first command starts an upload and retrieves the location for the upload. The second command uses the location from first command and uploads the blob. Finally, the third command commits the upload at the location returned by second command and specifies the digest of the blob.\nwe just need to repeat these steps for each layer and configuration JSON. Since we have only one layer, let us just push the configuration JSON as well.\n1 2 3 4 5 vscode ➜ ~/hello-cloud$ LOCATION=$(curl -i https://registry.hub.docker.com/v2/$REPO/blobs/uploads/ -H \u0026#34;Authorization: Bearer $TOKEN\u0026#34; -d \u0026#39;\u0026#39; | grep location | cut -d\u0026#34; \u0026#34; -f2 | tr -d \u0026#39;\\r\u0026#39;) vscode ➜ ~/hello-cloud$ LOCATION=$(curl -i $LOCATION -X PATCH -H \u0026#34;Authorization: Bearer $TOKEN\u0026#34; -H \u0026#34;Content-Type: application/octet-stream\u0026#34; --data-binary @4500bf72b8876e180985cb9d30dcbfb4b37c6212af4a1e66f6a59e9c44e4430e | grep location | cut -d\u0026#34; \u0026#34; -f2 | tr -d \u0026#39;\\r\u0026#39;) vscode ➜ ~/hello-cloud$ curl \u0026#34;$LOCATION\u0026amp;digest=sha256:$CONFIGDIGEST\u0026#34; -X PUT -H \u0026#34;Authorization: Bearer $TOKEN\u0026#34; -v Upload the manifest Finally, we need to upload the manifest.\n1 vscode ➜ ~/hello-cloud$ curl \u0026#34;https://registry.hub.docker.com/v2/$REPO/manifests/latest\u0026#34; -X PUT -H \u0026#34;Content-Type: application/vnd.oci.image.manifest.v1+json\u0026#34; -H \u0026#34;Authorization: Bearer $TOKEN\u0026#34; --data-binary @manifest.json -v Once the manifest upload is complete, you can find the image on Docker Hub.\nCan we not run the image we created without pushing it to Docker Hub or another registry? We can! Let us save that for a different day. For now, we will pull the image we just pushed and try to run a container.\n1 2 3 4 5 6 7 8 9 10 11 vscode ➜ ~$ docker pull ravikanth/hello-cloud:latest latest: Pulling from ravikanth/hello-cloud Digest: sha256:a23f26c0df0206f3135ca7c7c02d896b9d9fa43373fb6f5c8be8c7615b4e4402 Status: Image is up to date for ravikanth/hello-cloud:latest docker.io/ravikanth/hello-cloud:latest vscode ➜ ~$ docker run ravikanth/hello-cloud:latest Hello, Cloud Native Central! 👋 vscode ➜ ~$ docker run ravikanth/hello-cloud:latest Readers! Hello, Readers! 👋 This is it! What\u0026rsquo;s next? We shall look at some tools to build these container images. Stay tuned.\n","date":"2022-11-28T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/2022-11-28-building-container-images-using-no-tools/","series":"Container Images","title":"Building container images - using no tools"},{"categories":["azure","Kubernetes","azure bicep"],"content":"In an earlier article, I wrote about provisioning a virtual Kubernetes cluster with kubeadm](https://ravichaganti.com/blog/2022-11-13-installing-and-configuring-kubernetes-cluster-using-kubeadm-on-ubuntu/). I use this method on a laptop with limited resources. This is good to a large extent but not good enough when I want to scale my experiments and learning. This is where I began planning my Kubernetes lab on Azure. I needed something I could spin quickly with no manual intervention. Given my interest in the Bicep language, I naturally wrote a Bicep template to perform this deployment.\nShameless plug: If you are new to Bicep language or know nothing about Bicep, take a look at my Azure Biicep - Zero to Hero book.\nThis article is not about how I used Bicep templates or the techniques [there are certainly a few lessons I learned] I followed in building the Bicep template. Instead, it is more about how you can use this template and what you will get when you deploy it. I am adding a bonus chapter to my Azure Bicep book to describe the techniques I used in this Bicep template.\nAnnouncing K8sAzLab As a part of this experiment to automate Kubernetes deployment on Azure, I created a set of Bicep modules and a Bicep template that uses these modules. This is available in my GitHub repo github.com/rchaganti/k8sazlab.\nHere is how you use this template. First, start by cloning this repository.\n1 git clone https://github.com/rchaganti/K8sAzLab.git This repository contains a devcontainer definition that you can use to start a container (assuming you have Docker Desktop installed) with the necessary tooling to build and deploy Bicep templates. This is my preferred way of development these days. If you do not have Docker Desktop or do not prefer devcontainers, you can simply install Bicep binary on the local machine to provision this template.\nBefore you deploy template, the main.bicep template contains a few parameters that are needed for template deployment.\nParameter Name Description Default Value location Location for all resources created using this template resourceGroup().location storageAccountName Specifies the name of the Azure Storage account storageFileShareName Specifies the SMB share name for sharing files between nodes temp numCP Number of control plane VMs 1 numWorker Number of worker VMs 3 username Username for the Linux VM ubuntu authenticationType Type of authentication to use on the Virtual Machine. SSH key is recommended password passwordOrKey SSH Key or password for the Virtual Machine. SSH key is recommended cniPlugin CNI plugin to install calico cniCidr CNI Pod Network CIDR 10.244.0.0/16 This is still a very early version of my work and gets you from nothing to a fully functional Kubernetes cluster with a single control plane node in under 8 minutes.\nAt the moment, this can only support single control plane node. I have not added HA configuration yet and will do that in the coming days/weeks. For CNI, Calico is supported and I plan to add Cilium support soon. The overall structure of the module enables extending the overall automation in a easy manner. A storage account and an SMB share are created for the purpose of sharing the kubeadm join command between control plane and worker nodes.\nHere is how you provision the template using Azure CLI.\n1 2 3 az deployment group create --template-file main.bicep \\ --parameters storageAccountName=someUniqueName \\ --resource-group k8s The resource group that you specify in the above command must already exist. You will be prompted to enter a password / ssh key.\nAt the end of deployment, you will see the ssh commands to connect to each node in the cluster. You can query the deployment output using the following command.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 vscode ➜ /workspaces/azk8slab $ az deployment group show -g k8s -n main --query properties.outputs { \u0026#34;vmInfo\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;Array\u0026#34;, \u0026#34;value\u0026#34;: [ { \u0026#34;connect\u0026#34;: \u0026#34;ssh ubuntu@cplane1lmwuauibze44k.eastus.cloudapp.azure.com\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;cplane1\u0026#34; }, { \u0026#34;connect\u0026#34;: \u0026#34;ssh ubuntu@worker1lmwuauibze44k.eastus.cloudapp.azure.com\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;worker1\u0026#34; }, { \u0026#34;connect\u0026#34;: \u0026#34;ssh ubuntu@worker2lmwuauibze44k.eastus.cloudapp.azure.com\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;worker2\u0026#34; }, { \u0026#34;connect\u0026#34;: \u0026#34;ssh ubuntu@worker3lmwuauibze44k.eastus.cloudapp.azure.com\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;worker3\u0026#34; } ] } } You can connect to the control plane node as the user you specified (default is ubuntu.) and verify if all nodes are in ready state or not and if all control plane pods are running or not.\nThis is it for now. I spent a good part of my weekend validating this template and making sure it is ready to be shared. Nothing is perfect and I may have missed a few corner cases. If you run into any issues, you know the drill! If you want to contribute or suggest ideas, please feel free to add a discussion thread.\n","date":"2022-11-26T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/2022-11-26-fully-automated-kubernetes-cluster-deployment-on-azure-in-under-8-minutes/","series":"Azure Bicep","title":"Fully automated Kubernetes cluster deployment on Azure in under 8 minutes"},{"categories":null,"content":"In an earlier article, I had written about provisioning a virtual Kubernetes cluster using kubeadm. I use this method on a laptop with limited resources. This is good to a large extent but not good enough when I want to scale my experiments and learning. This is where I started looking at creating my Kubernetes lab on Azure. I needed something that could spin fast and without manual intervention. Given my interest in the Bicep language, I wrote a Bicep template to perform this deployment.\nShameless plug: If you are new to the Bicep language or know nothing about Bicep, look at my Azure Bicep - Zero to Hero book.\nThis article is not about how I used Bicep templates or the techniques [there are certainly a few lessons I learned] I followed in building the Bicep template. Instead, it is more about how to use this template and what you get when you deploy it. I am adding a bonus chapter to my Azure Bicep book to describe my techniques in this Bicep template.\nAnnouncing K8sAzLab As a part of this experiment to automate Kubernetes deployment on Azure, I created a set of Bicep modules and a Bicep template that uses these modules. This is available in my GitHub repo github.com/rchaganti/k8sazlab.\nHere is how you use this template. First, start by cloning this repository.\n1 git clone https://github.com/rchaganti/K8sAzLab.git This repository contains a devcontainer definition you can use to start a container (assuming you have Docker Desktop installed) with the necessary tooling to build and deploy Bicep templates. This is my preferred way of development these days. If you do not have Docker Desktop or do not prefer devcontainers, you can install Bicep binary on the local machine to provision this template.\nBefore you deploy the template, the main.bicep template contains a few parameters that are needed for template deployment.\nParameter Name Description Default Value location Location for all resources created using this template resourceGroup().location storageAccountName Specifies the name of the Azure Storage account storageFileShareName Specifies the SMB share name for sharing files between nodes temp numCP Number of control plane VMs 1 numWorker Number of worker VMs 3 username Username for the Linux VM ubuntu authenticationType Type of authentication to use on the Virtual Machine. SSH key is recommended password passwordOrKey SSH Key or password for the Virtual Machine. SSH key is recommended cniPlugin CNI plugin to install calico cniCidr CNI Pod Network CIDR 10.244.0.0/16 This is still a very early version of my work and gets you from nothing to a fully functional Kubernetes cluster with a single control plane node in under 8 minutes.\nAt the moment, this can only support a single control plane node. I have not added HA configuration yet and will do that in the coming days/weeks. For CNI, Calico is supported, and I plan to add Cilium support soon. The overall structure of the module enables extending the overall automation in an easy manner. A storage account and an SMB share are created to share the kubeadm join command between the control plane and worker nodes.\nHere is how you provision the template using Azure CLI.\n1 2 3 az deployment group create --template-file main.bicep \\ --parameters storageAccountName=someUniqueName \\ --resource-group k8s The resource group that you specify in the above command must already exist. You will be prompted to enter a password / ssh key.\nAt the end of deployment, you will see the ssh commands to connect to each node in the cluster. You can query the deployment output using the following command.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 vscode ➜ /workspaces/azk8slab $ az deployment group show -g k8s -n main --query properties.outputs { \u0026#34;vmInfo\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;Array\u0026#34;, \u0026#34;value\u0026#34;: [ { \u0026#34;connect\u0026#34;: \u0026#34;ssh ubuntu@cplane1lmwuauibze44k.eastus.cloudapp.azure.com\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;cplane1\u0026#34; }, { \u0026#34;connect\u0026#34;: \u0026#34;ssh ubuntu@worker1lmwuauibze44k.eastus.cloudapp.azure.com\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;worker1\u0026#34; }, { \u0026#34;connect\u0026#34;: \u0026#34;ssh ubuntu@worker2lmwuauibze44k.eastus.cloudapp.azure.com\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;worker2\u0026#34; }, { \u0026#34;connect\u0026#34;: \u0026#34;ssh ubuntu@worker3lmwuauibze44k.eastus.cloudapp.azure.com\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;worker3\u0026#34; } ] } } You can connect to the control plane node as the user you specified (default is Ubuntu.) and verify if all nodes are in a ready state and if all control plane pods are running or not.\nThis is it for now. I spent most of my weekend validating this template and ensuring it is ready to be shared. Nothing is perfect, and I may have missed a few corner cases. If you run into any issues, you know the drill! Please add a [discussion thread](rchaganti/K8sAzLab · Discussions · GitHub) if you want to contribute or suggest ideas.\n","date":"2022-11-26T00:00:00Z","excerpt":"Kubeadm is the go to tool for Kubernetes administrators. Understand how to use this tool combined with Bicep to provision a virtualized K8s cluster.","permalink":"https://ravichaganti.com/blog/fully-automated-kubernetes-cluster-deployment-on-azure-in-under-8-minutes/","series":"Azure Bicep","title":"Fully Automated Kubernetes Cluster Deployment on Azure in Under 8 Minutes"},{"categories":["kubernetes","cloudnative"],"content":"This article is more like a note to myself, but this may help if you are looking at configuring a Kubernetes cluster on Ubuntu.\nThere are many ways to install and configure Kubernetes cluster for learning and development purposes. You can use Docker Desktop or Rancher Desktop or https://podman-desktop.io/ or minikube or microk8s to create a single node cluster for your development work quickly. I am a big fan of Docker Desktop and use that for any quick experiments. However, I need a multi-node cluster with additional services for anything more than simple deployment. I use Ubuntu virtual machines and configure a Kubernetes cluster using kubeadm.\nIn this article, I walk through the steps I followed to set up a virtual Kubernetes cluster.\nVirtual machine configuration To ensure I have enough resources in the cluster, I created four Ubuntu 22.04 LTS virtual machines on my Windows 11 system. Each of these VMs is configured with two virtual CPUs and 2GB of virtual memory. It is recommended to configure each virtual machine with a static IP address. I created an external virtual switch to enable Internet connectivity within the Ubuntu guest OS. Of the four virtual machines, I chose to use one as the control plane node and the other three as worker nodes.\nContainer runtime Kubernetes uses a Container Runtime Interface (CRI) compliant container runtime to orchestrate containers in Pods.\nMany runtimes are supported within Kubernetes. The most popular ones include Docker (via cri-dockerd), containerd, and CRI-O. The choice of a runtime depends on several factors, such as performance, isolation needs, and security. For this virtual cluster. I chose containerd as the runtime for my virtual cluster.\nContainer Network Interface (CNI) Kubernetes requires a CNI-compatible Pod network addon for Pods within the cluster to communicate with each other. You can choose from many open-source and commercial CNI plugins to implement the Pod network. Once again, you need to consider factors such as ease of deployment, performance, security, and resource consumption to choose the Pod network addon that is correct for your Kubernetes cluster and the workload you plan to run.\nI chose Calico as the pod network addon for the ease of deployment and considering the fact that I have some prior experience with Calico.\nPreparing control plane and worker nodes Each node in the Kubernetes cluster the following components.\nA container runtime Kubectl - The command line interface to Kubernetes API Kubelet - Agent on each node that receives work from the scheduler Kubeadm - Tool to automate deployment and configuration of a Kubernetes cluster Before going into this, you must ensure that nodes that will be a part of the Kubernetes cluster can communicate with each other and the firewall ports required for node-to-node communication are open.\nThe following network ports must be open for inbound TCP traffic on the control plane node.\n6443 2379:2380 10250 10257 10259 179 (required for Calico) On the worker nodes, you should configure to allow incoming TCP traffic on the following ports.\n10250 30000:32767 On Ubuntu, you can use ufw command to perform this configuration.\n1 sudo ufw allow proto tcp from any to any port 6443,2379,2380,10250,10257,10259,179 To see the bridged traffic, you must disable swap and configure IPv4 forwarding and IP tables on each node. Before all this, ensure each node has the latest and greatest packages. You will needcurl as well on the node to download certain packages.\n1 2 sudo apt-get update sudo apt-get install -y apt-transport-https ca-certificates curl You must disable swap on each node that will be a part of the Kubernetes cluster.\n1 2 sudo swapoff -a sudo sed -i \u0026#39;/^\\/swap\\.img/s/^/#/\u0026#39; /etc/fstab You must also check if the swap is listed in the /etc/fstab and either comment or remove the line.\nNext, configure IPv4 forwarding and IP tables on each node.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 #Enable IP tables bridge traffic on all nodes # https://kubernetes.io/docs/setup/production-environment/container-runtimes/#forwarding-ipv4-and-letting-iptables-see-bridged-traffic cat \u0026lt;\u0026lt;EOF | sudo tee /etc/modules-load.d/k8s.conf overlay br_netfilter EOF sudo modprobe overlay sudo modprobe br_netfilter # sysctl params required by setup, params persist across reboots cat \u0026lt;\u0026lt;EOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-iptables = 1 net.bridge.bridge-nf-call-ip6tables = 1 net.ipv4.ip_forward = 1 EOF # Apply sysctl params without reboot sudo sysctl --system Installing containerd The next set of commands download the latest release of containerd from GitHub and configure it. You need to run this on each node.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 CONTAINERD_VERSION=\u0026#34;1.7.4\u0026#34; RUNC_VERSION=\u0026#34;1.1.9\u0026#34; CNI_PLUGINS_VERSION=\u0026#34;1.3.0\u0026#34; # Install containerd curl -Lo /tmp/containerd-${CONTAINERD_VERSION}-linux-amd64.tar.gz \\ https://github.com/containerd/containerd/releases/download/v${CONTAINERD_VERSION}/containerd-${CONTAINERD_VERSION}-linux-amd64.tar.gz sudo tar Cxzvf /usr/local /tmp/containerd-${CONTAINERD_VERSION}-linux-amd64.tar.gz # Install runc curl -Lo /tmp/runc.amd64 https://github.com/opencontainers/runc/releases/download/v${RUNC_VERSION}/runc.amd64 sudo install -m 755 /tmp/runc.amd64 /usr/local/sbin/runc # clean up containerd and runc files rm -rf /tmp/containerd-${CONTAINERD_VERSION}-linux-amd64.tar.gz /tmp/runc.amd64 # install containerd config sudo mkdir -p /etc/containerd containerd config default | sudo tee /etc/containerd/config.toml sudo sed -i \u0026#39;s/SystemdCgroup = false/SystemdCgroup = true/\u0026#39; /etc/containerd/config.toml sudo curl -Lo /etc/systemd/system/containerd.service https://raw.githubusercontent.com/containerd/cri/master/contrib/systemd-units/containerd.service sudo systemctl daemon-reload sudo systemctl enable --now containerd sudo systemctl status containerd --no-pager # Install CNI plugins curl -Lo /tmp/cni-plugins-amd64-v${CNI_PLUGINS_VERSION}.tgz \\ https://github.com/containernetworking/plugins/releases/download/v${CNI_PLUGINS_VERSION}/cni-plugins-linux-amd64-v${CNI_PLUGINS_VERSION}.tgz sudo mkdir -p /opt/cni/bin sudo tar Cxzvf /opt/cni/bin /tmp/cni-plugins-amd64-v${CNI_PLUGINS_VERSION}.tgz # clean up CNI plugins rm -rf /tmp/cni-plugins-amd64-v${CNI_PLUGINS_VERSION}.tgz Installing kubeadm, kubelet, and kubectl These three tools are needed on each node.\n1 2 3 4 5 6 7 sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.28/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg echo \u0026#39;deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.28/deb/ /\u0026#39; | sudo tee /etc/apt/sources.list.d/kubernetes.list sudo apt-get update sudo apt-get install -y kubelet kubeadm kubectl sudo apt-mark hold kubelet kubeadm kubectl The above commands download and install the three tools that we need on each node. Once installed, we mark the packages as held so that they don\u0026rsquo;t get automatically upgraded or removed.\nInitialize Kubernetes cluster Once the prerequisite configuration is complete, you can initialize the Kubernetes cluster using kubeadm init command.\n1 2 3 4 5 6 7 8 9 IPADDR=$(hostname -I) APISERVER=$(hostname -s) NODENAME=$(hostname -s) POD_NET=\u0026#34;10.244.0.0/16\u0026#34; sudo kubeadm init --apiserver-advertise-address=$IPADDR \\ --apiserver-cert-extra-sans=$APISERVER \\ --pod-network-cidr=$POD_NET \\ --node-name $NODENAME This command starts a few preflight checks and the necessary Pods to start the Kubernetes control plane. At the end of successful execution, you will see output similar to what is shown here.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Your Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config You should now deploy a Pod network to the cluster. Run \u0026#34;kubectl apply -f [podnetwork].yaml\u0026#34; with one of the options listed at: /docs/concepts/cluster-administration/addons/ You can now join any number of machines by running the following on each node as root: kubeadm join \u0026lt;control-plane-host\u0026gt;:\u0026lt;control-plane-port\u0026gt; --token \u0026lt;token\u0026gt; --discovery-token-ca-cert-hash sha256:\u0026lt;hash\u0026gt; Before proceeding or clearing the screen output, copy the kubeadm join command. You need this to join the worker nodes to the Kubernetes cluster.\nPrepare kube config Before installing the Pod network addon, you need to make sure you prepare the kubectl config file. kubeadm init command provides the necessary commands to do this.\n1 2 3 mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Once this is done, verify if the Kubernetes control plane objects can be queried or not.\n1 kubectl get nodes This command will show only the control plane node and be shown as NotReady. This is because the Pod network is not ready. You can now install the Pod network addon.\nInstalling Calico Installing Calico is just two steps. First, we install the opertor.\n1 2 curl -Lo /tmp/tigera-operator.yaml https://raw.githubusercontent.com/projectcalico/calico/v3.24.5/manifests/tigera-operator.yaml kubectl create -f /tmp/tigera-operator.yaml Next, we need to install the custom resources.\n1 curl -Lo /tmp/custom-resources.yaml https://raw.githubusercontent.com/projectcalico/calico/v3.24.5/manifests/custom-resources.yaml In this YAML, you need to modify the the spec.calicoNetwork.ipPools.cidr to match what you specified as the argument to --pod-network-cidr. Once this modification is complete, you can implement the custom resources.\n1 2 3 CIDR=\u0026#39;10.244.0.0/16\u0026#39; sed -i \u0026#34;s|192.168.0.0/16|$CIDR|\u0026#34; /tmp/custom-resources.yaml kubectl create -f /tmp/custom-resources.yaml You need to wait for the Calico Pods to transition to Ready state before you can proceed towards joining the worker nodes to the cluster.\n1 $ watch kubectl get pods -n calico-system Once all Calico pods in the calico-system namespace are online and ready, you can check if the control plane node is in ready state or not using kubectl get nodes command.\nFinally, you can move on to joining all worker nodes the cluster. You need to run the command you copied from the kubeadm init command on each worker node.\n1 2 $ kubeadm join IP:6443 --token token \\ --discovery-token-ca-cert-hash hash Note: IP, token, and hash in the command you copied will be different.\nThe node joining process takes a few minutes. On the control plane node, you can run watch kubectl get nodes command wait until all nodes come online and transition to ready state.\n1 2 3 4 5 6 $ kubectl get nodes NAME STATUS ROLES AGE VERSION cplane01 Ready control-plane 4h42m v1.28.1 node01 Ready \u0026lt;none\u0026gt; 4h28m v1.28.1 node02 Ready \u0026lt;none\u0026gt; 4h28m v1.28.1 node03 Ready \u0026lt;none\u0026gt; 4h28m v1.28.1 You should also verify if all control plane pods are online and ready or not.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 $ kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE calico-apiserver calico-apiserver-f975659ff-fscmd 1/1 Running 2 (3h56m ago) 4h31m calico-apiserver calico-apiserver-f975659ff-jq9hn 1/1 Running 2 (3h56m ago) 4h31m calico-system calico-kube-controllers-6b57db7fd6-grkdh 1/1 Running 2 (3h56m ago) 4h33m calico-system calico-node-2kfq2 1/1 Running 2 (3h55m ago) 4h29m calico-system calico-node-6h65z 1/1 Running 2 (3h56m ago) 4h33m calico-system calico-node-f4vml 0/1 Running 2 (3h55m ago) 4h29m calico-system calico-node-rfpdz 0/1 Running 2 (3h55m ago) 4h29m calico-system calico-typha-75884b99f4-dhrmp 1/1 Running 3 (3h55m ago) 4h29m calico-system calico-typha-75884b99f4-sss9d 1/1 Running 3 (3h56m ago) 4h33m kube-system coredns-565d847f94-knrf8 1/1 Running 2 (3h56m ago) 4h42m kube-system coredns-565d847f94-mtxrs 1/1 Running 2 (3h56m ago) 4h42m kube-system etcd-cplane01 1/1 Running 2 (3h56m ago) 4h42m kube-system kube-apiserver-cplane01 1/1 Running 2 (3h56m ago) 4h42m kube-system kube-controller-manager-cplane01 1/1 Running 2 (3h56m ago) 4h42m kube-system kube-proxy-9s7c7 1/1 Running 2 (3h55m ago) 4h29m kube-system kube-proxy-dq5rc 1/1 Running 2 (3h55m ago) 4h29m kube-system kube-proxy-kfs78 1/1 Running 2 (3h56m ago) 4h42m kube-system kube-proxy-zl7sb 1/1 Running 2 (3h55m ago) 4h29m kube-system kube-scheduler-cplane01 1/1 Running 2 (3h56m ago) 4h42m tigera-operator tigera-operator-6bb5985474-cmnxp 1/1 Running 3 (3h56m ago) 4h35m This is it. You now have a four node Kubernetes cluster that you can use for your learning, development, and even production (if you are brave enough!).\n","date":"2022-11-13T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/2022-11-13-installing-and-configuring-kubernetes-cluster-using-kubeadm-on-ubuntu/","series":"Kubernetes","title":"Installing and Configuring Kubernetes cluster using kubeadm on Ubuntu"},{"categories":null,"content":"There are many ways to install and configure a Kubernetes cluster for learning and development purposes. We can use Docker Desktop, Rancher Desktop, Podman Desktop, minikube, or microk8s to quickly create a single-node cluster for our development work. These are good for quick development work but not so much when we need a multi-node cluster with additional services. For such a scenario, we can use virtual machines and configure a Kubernetes cluster using kubeadm.\nThis article examines the steps necessary to set up a virtual Kubernetes cluster.\nVirtual machine configuration We can create four Ubuntu 22.04 LTS virtual machines (cloud-based or on a local system) to ensure the cluster has enough resources. Each of these VMs is configured with two virtual CPUs and 2GB of virtual memory. It is recommended to configure each virtual machine with a static IP address. In the case of a local virtualization environment, we should create an external virtual switch to enable Internet connectivity within the Ubuntu guest OS. We use one of these four VMs as the control plane node and the other three as worker nodes.\nContainer runtime Kubernetes uses a Container Runtime Interface (CRI) compliant container runtime to orchestrate containers in Pods.\nMany runtimes are supported within Kubernetes. The most popular ones include Docker (via cri-dockerd), containerd, and CRI-O. The choice of a runtime depends on several factors, such as performance, isolation needs, and security. We shall use containerd as the runtime for this virtual cluster.\nContainer Network Interface (CNI) Kubernetes requires a CNI-compatible Pod network addon for Pods within the cluster to communicate with each other. We can choose from many open-source and commercial CNI plugins to implement the Pod network. Once again, we must consider factors such as ease of deployment, performance, security, and resource consumption to choose the Pod network addon correct for our Kubernetes cluster and the workload we plan to run.\nFor this article, we choose Calico as the pod network addon for the ease of deployment.\nPreparing control plane and worker nodes Each node in the Kubernetes cluster has the following components.\nA container runtime Kubectl - The command line interface to Kubernetes API Kubelet - Agent on each node that receives work from the scheduler Kubeadm - Tool to automate deployment and configuration of a Kubernetes cluster Before going into this, we must ensure that nodes that will be a part of the Kubernetes cluster can communicate with each other and the firewall ports required for node-to-node communication are open.\nThe following network ports must be open for inbound TCP traffic on the control plane node.\n6443 2379:2380 10250 10257 10259 179 (required for Calico) On the worker nodes, we should configure to allow incoming TCP traffic on the following ports.\n10250 30000:32767 On Ubuntu, we can use ufw command to perform this configuration.\n1 sudo ufw allow proto tcp from any to any port 6443,2379,2380,10250,10257,10259,179 To see the bridged traffic, we must disable swap and configure IPv4 forwarding and IP tables on each node. Before all this, ensure each node has the latest and greatest packages. We will need curl as well on the node to download certain packages.\n1 2 sudo apt-get update sudo apt-get install -y apt-transport-https ca-certificates curl We must disable swap on each node that will be a part of the Kubernetes cluster.\n1 2 sudo swapoff -a sudo sed -i \u0026#39;/^\\/swap\\.img/s/^/#/\u0026#39; /etc/fstab We must also check if the swap is listed in the /etc/fstab and either comment or remove the line.\nNext, configure IPv4 forwarding and IP tables on each node.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 #Enable IP tables to bridge traffic on all nodes # https://kubernetes.io/docs/setup/production-environment/container-runtimes/#forwarding-ipv4-and-letting-iptables-see-bridged-traffic cat \u0026lt;\u0026lt;EOF | sudo tee /etc/modules-load.d/k8s.conf overlay br_netfilter EOF sudo modprobe overlay sudo modprobe br_netfilter # sysctl params required by setup, params persist across reboots cat \u0026lt;\u0026lt;EOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-iptables = 1 net.bridge.bridge-nf-call-ip6tables = 1 net.ipv4.ip_forward = 1 EOF # Apply sysctl params without reboot sudo sysctl --system Installing containerd The next set of commands download the latest release of containerd from GitHub and configure it. We need to run this on each node.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 CONTAINERD_VERSION=\u0026#34;1.7.4\u0026#34; RUNC_VERSION=\u0026#34;1.1.9\u0026#34; CNI_PLUGINS_VERSION=\u0026#34;1.3.0\u0026#34; # Install containerd curl -Lo /tmp/containerd-${CONTAINERD_VERSION}-linux-amd64.tar.gz \\ https://github.com/containerd/containerd/releases/download/v${CONTAINERD_VERSION}/containerd-${CONTAINERD_VERSION}-linux-amd64.tar.gz sudo tar Cxzvf /usr/local /tmp/containerd-${CONTAINERD_VERSION}-linux-amd64.tar.gz # Install runc curl -Lo /tmp/runc.amd64 https://github.com/opencontainers/runc/releases/download/v${RUNC_VERSION}/runc.amd64 sudo install -m 755 /tmp/runc.amd64 /usr/local/sbin/runc # clean up containerd and runc files rm -rf /tmp/containerd-${CONTAINERD_VERSION}-linux-amd64.tar.gz /tmp/runc.amd64 # install containerd config sudo mkdir -p /etc/containerd containerd config default | sudo tee /etc/containerd/config.toml sudo sed -i \u0026#39;s/SystemdCgroup = false/SystemdCgroup = true/\u0026#39; /etc/containerd/config.toml sudo curl -Lo /etc/systemd/system/containerd.service https://raw.githubusercontent.com/containerd/cri/master/contrib/systemd-units/containerd.service sudo systemctl daemon-reload sudo systemctl enable --now containerd sudo systemctl status containerd --no-pager # Install CNI plugins curl -Lo /tmp/cni-plugins-amd64-v${CNI_PLUGINS_VERSION}.tgz \\ https://github.com/containernetworking/plugins/releases/download/v${CNI_PLUGINS_VERSION}/cni-plugins-linux-amd64-v${CNI_PLUGINS_VERSION}.tgz sudo mkdir -p /opt/cni/bin sudo tar Cxzvf /opt/cni/bin /tmp/cni-plugins-amd64-v${CNI_PLUGINS_VERSION}.tgz # clean up CNI plugins rm -rf /tmp/cni-plugins-amd64-v${CNI_PLUGINS_VERSION}.tgz Installing kubeadm, kubelet, and kubectl These three tools are needed on each node.\n1 2 3 4 5 6 7 sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.28/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg echo \u0026#39;deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.28/deb/ /\u0026#39; | sudo tee /etc/apt/sources.list.d/kubernetes.list sudo apt-get update sudo apt-get install -y kubelet kubeadm kubectl sudo apt-mark hold kubelet kubeadm kubectl The above commands download and install the three tools we need on each node. Once installed, we mark the packages as held so they don\u0026rsquo;t get automatically upgraded or removed.\nInitialize Kubernetes cluster Once the prerequisite configuration is complete, we can initialize the Kubernetes cluster using kubeadm init command.\n1 2 3 4 5 6 7 8 9 IPADDR=$(hostname -I) APISERVER=$(hostname -s) NODENAME=$(hostname -s) POD_NET=\u0026#34;10.244.0.0/16\u0026#34; sudo kubeadm init --apiserver-advertise-address=$IPADDR \\ --apiserver-cert-extra-sans=$APISERVER \\ --pod-network-cidr=$POD_NET \\ --node-name $NODENAME This command starts a few preflight checks and the necessary Pods to start the Kubernetes control plane. At the end of successful execution, we will see output similar to what is shown here.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Your Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config You should now deploy a Pod network to the cluster. Run \u0026#34;kubectl apply -f [podnetwork].yaml\u0026#34; with one of the options listed at: /docs/concepts/cluster-administration/addons/ You can now join any number of machines by running the following on each node as root: kubeadm join \u0026lt;control-plane-host\u0026gt;:\u0026lt;control-plane-port\u0026gt; --token \u0026lt;token\u0026gt; --discovery-token-ca-cert-hash sha256:\u0026lt;hash\u0026gt; Before proceeding or clearing the screen output, copy the kubeadm join command. We need this to join the worker nodes to the Kubernetes cluster.\nPrepare kubeconfig Before installing the Pod network addon, we need to make sure we prepare the kubectl config file. kubeadm init command provides the necessary commands to do this.\n1 2 3 mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Once this is done, verify if the Kubernetes control plane objects can be queried.\n1 kubectl get nodes This command will show only the control plane node and be shown as NotReady. This is because the Pod network is not ready. We can now install the Pod network addon.\nInstalling Calico Installing Calico is just two steps. First, we install the operator.\n1 2 curl -Lo /tmp/tigera-operator.yaml https://raw.githubusercontent.com/projectcalico/calico/v3.24.5/manifests/tigera-operator.yaml kubectl create -f /tmp/tigera-operator.yaml Next, we need to install the custom resources.\n1 curl -Lo /tmp/custom-resources.yaml https://raw.githubusercontent.com/projectcalico/calico/v3.24.5/manifests/custom-resources.yaml In this YAML, we must modify the spec.calicoNetwork.ipPools.cidr to match what we specified as the argument to --pod-network-cidr. Once this modification is complete, we can implement the custom resources.\n1 2 3 CIDR=\u0026#39;10.244.0.0/16\u0026#39; sed -i \u0026#34;s|192.168.0.0/16|$CIDR|\u0026#34; /tmp/custom-resources.yaml kubectl create -f /tmp/custom-resources.yaml We need to wait for the Calico Pods to transition to the Ready state before proceeding toward joining the worker nodes to the cluster.\n1 $ watch kubectl get pods -n calico-system Once all Calico pods in the calico-system namespace are online and ready, we can check if the control plane node is in a ready state or not using the kubectl get nodes command.\nFinally, we can move on to joining all worker nodes in the cluster. we must run the command we copied from the kubeadm init command on each worker node.\n1 2 $ kubeadm join IP:6443 --token token \\ --discovery-token-ca-cert-hash hash Note: IP, token, and hash in the copied command will differ.\nThe node joining process takes a few minutes. We can run the watch kubectl get nodes command on the control plane node, wait until all nodes come online, and transition to the ready state.\n1 2 3 4 5 6 $ kubectl get nodes NAME STATUS ROLES AGE VERSION cplane01 Ready control-plane 4h42m v1.28.1 node01 Ready \u0026lt;none\u0026gt; 4h28m v1.28.1 node02 Ready \u0026lt;none\u0026gt; 4h28m v1.28.1 node03 Ready \u0026lt;none\u0026gt; 4h28m v1.28.1 We should also verify whether all control plane pods are online and ready.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 $ kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE calico-apiserver calico-apiserver-f975659ff-fscmd 1/1 Running 2 (3h56m ago) 4h31m calico-apiserver calico-apiserver-f975659ff-jq9hn 1/1 Running 2 (3h56m ago) 4h31m calico-system calico-kube-controllers-6b57db7fd6-grkdh 1/1 Running 2 (3h56m ago) 4h33m calico-system calico-node-2kfq2 1/1 Running 2 (3h55m ago) 4h29m calico-system calico-node-6h65z 1/1 Running 2 (3h56m ago) 4h33m calico-system calico-node-f4vml 0/1 Running 2 (3h55m ago) 4h29m calico-system calico-node-rfpdz 0/1 Running 2 (3h55m ago) 4h29m calico-system calico-typha-75884b99f4-dhrmp 1/1 Running 3 (3h55m ago) 4h29m calico-system calico-typha-75884b99f4-sss9d 1/1 Running 3 (3h56m ago) 4h33m kube-system coredns-565d847f94-knrf8 1/1 Running 2 (3h56m ago) 4h42m kube-system coredns-565d847f94-mtxrs 1/1 Running 2 (3h56m ago) 4h42m kube-system etcd-cplane01 1/1 Running 2 (3h56m ago) 4h42m kube-system kube-apiserver-cplane01 1/1 Running 2 (3h56m ago) 4h42m kube-system kube-controller-manager-cplane01 1/1 Running 2 (3h56m ago) 4h42m kube-system kube-proxy-9s7c7 1/1 Running 2 (3h55m ago) 4h29m kube-system kube-proxy-dq5rc 1/1 Running 2 (3h55m ago) 4h29m kube-system kube-proxy-kfs78 1/1 Running 2 (3h56m ago) 4h42m kube-system kube-proxy-zl7sb 1/1 Running 2 (3h55m ago) 4h29m kube-system kube-scheduler-cplane01 1/1 Running 2 (3h56m ago) 4h42m tigera-operator tigera-operator-6bb5985474-cmnxp 1/1 Running 3 (3h56m ago) 4h35m This is it. We now have a four-node Kubernetes cluster that we can use for our learning, development, and production too!\n","date":"2022-11-13T00:00:00Z","excerpt":"Kubeadm is a handy tool to configure Kubernetes clusters. This article explains how to create a K8s cluster using Ubuntu VMs.","permalink":"https://ravichaganti.com/blog/installing-and-configuring-kubernetes-cluster-using-kubeadm-on-ubuntu/","series":"Kubernetes","title":"Installing and Configuring Kubernetes Cluster using Kubeadm on Ubuntu"},{"categories":["aci","containers","Azure","Bicep"],"content":"In the previous article, we looked at getting started with Azure Container Instances service, learned some basic concepts of ACI, and saw an example of provisioning a hello-world application. In this article, we shall extend this knowledge by provisioning a multi-container application to ACI using Bicep templates. You can also use a YAML method of provisioning multi-container applications to ACI. We shall look at this later.\nAs we learned earlier, a container group resembles a Kubernetes pod. The containers within the group share resources such as network and storage. The resource usage of containers can be controlled at the container group level. As a general rule of thumb, we should group containers with similar lifecycle and resource requirements into a container group. The multi-container groups are available for Linux containers only at the moment. Based on the region, there will be additional restrictions in terms of resource allocation.\nLet us start with an example.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 @description(\u0026#39;Name for the container group\u0026#39;) param name string = \u0026#39;aciVoteApplication\u0026#39; @description(\u0026#39;Location for all resources.\u0026#39;) param location string = resourceGroup().location @description(\u0026#39;The behavior of Azure runtime if container has stopped.\u0026#39;) @allowed([ \u0026#39;Always\u0026#39; \u0026#39;Never\u0026#39; \u0026#39;OnFailure\u0026#39; ]) param restartPolicy string = \u0026#39;Always\u0026#39; resource containerGroup \u0026#39;Microsoft.ContainerInstance/containerGroups@2021-09-01\u0026#39; = { name: name location: location properties: { containers: [ { name: \u0026#39;azure-vote-backend\u0026#39; properties: { image: \u0026#39;mcr.microsoft.com/oss/bitnami/redis:6.0.8\u0026#39; ports: [ { port: 6379 protocol: \u0026#39;TCP\u0026#39; } ] environmentVariables: [ { name: \u0026#39;ALLOW_EMPTY_PASSWORD\u0026#39; value: \u0026#39;yes\u0026#39; } ] resources: { requests: { cpu: 1 memoryInGB: 1 } } } } { name: \u0026#39;azure-vote-frontend\u0026#39; properties: { image: \u0026#39;mcr.microsoft.com/azuredocs/azure-vote-front:v2\u0026#39; ports: [ { port: 80 protocol: \u0026#39;TCP\u0026#39; } ] environmentVariables: [ { name: \u0026#39;REDIS\u0026#39; value: \u0026#39;localhost\u0026#39; } ] resources: { requests: { cpu: 1 memoryInGB: 1 } } } } ] osType: \u0026#39;Linux\u0026#39; restartPolicy: restartPolicy ipAddress: { type: \u0026#39;Public\u0026#39; ports: [ { port: 80 protocol: \u0026#39;TCP\u0026#39; } ] } } } output containerIPv4Address string = containerGroup.properties.ipAddress.ip This example is like the earlier single-container ACI example. However, we are adding a second container in this multi-container group example. Each container within an ACI group can reference the other using localhost only. Therefore, in the example above, we are mapping the REDIS environment variable to localhost instead of the name of the Redis container.\nWe can provision this ACI container group using the following Azure CLI command.\n1 az deployment group create --template-file aci-mcg.bicep --resource-group bicep Output from the Bicep template gives us the IP address of the container group, which can be used to access the voting application that we just provisioned to the ACI container group.\nThis is it. In a future article, we shall see how to provision applications across multiple multi-container groups and what role virtual networks play in such a scenario.\n","date":"2022-11-03T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/2022-11-03-understanding-and-using-azure-container-instances-service-multi-container-apps-using-bicep/","series":"Containers","title":"Understanding and using Azure Container Instances service - Multi-container Applications using Bicep"},{"categories":["aci","containers","Azure","Bicep"],"content":"In an earlier article, we looked at different containerization options in Microsoft Azure. In today\u0026rsquo;s article, we shall start with Azure Container Instances (ACI) and learn the fundamentals. In a later article, we will learn how to perform multi-container applications to ACI.\nAzure Container Instances Azure Container Instances (ACI) service offers a quick and easy way to run containers in the Microsoft Azure cloud. It is as simple as running a container locally using the Docker engine. ACI does not provide full container orchestration. ACI is great for applications that require faster startup times and hassle-free management. ACI runs inside Azure VMs and therefore provides isolation enabled by virtualization.\nLet us start by provisioning a simple container instance using a Bicep template. This is an example from Microsoft\u0026rsquo;s documentation on ACI.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 @description(\u0026#39;Name for the container group\u0026#39;) param name string = \u0026#39;acibicep\u0026#39; @description(\u0026#39;Location for all resources.\u0026#39;) param location string = resourceGroup().location @description(\u0026#39;Container image to deploy. Should be of the form repoName/imagename:tag for images stored in public Docker Hub, or a fully qualified URI for other registries. Images from private registries require additional registry credentials.\u0026#39;) param image string = \u0026#39;mcr.microsoft.com/azuredocs/aci-helloworld\u0026#39; @description(\u0026#39;Port to open on the container and the public IP address.\u0026#39;) param port int = 80 @description(\u0026#39;The number of CPU cores to allocate to the container.\u0026#39;) param cpuCores int = 1 @description(\u0026#39;The amount of memory to allocate to the container in gigabytes.\u0026#39;) param memoryInGb int = 2 @description(\u0026#39;The behavior of Azure runtime if container has stopped.\u0026#39;) @allowed([ \u0026#39;Always\u0026#39; \u0026#39;Never\u0026#39; \u0026#39;OnFailure\u0026#39; ]) param restartPolicy string = \u0026#39;Always\u0026#39; resource containerGroup \u0026#39;Microsoft.ContainerInstance/containerGroups@2021-09-01\u0026#39; = { name: name location: location properties: { containers: [ { name: name properties: { image: image ports: [ { port: port protocol: \u0026#39;TCP\u0026#39; } ] resources: { requests: { cpu: cpuCores memoryInGB: memoryInGb } } } } ] osType: \u0026#39;Linux\u0026#39; restartPolicy: restartPolicy ipAddress: { type: \u0026#39;Public\u0026#39; ports: [ { port: port protocol: \u0026#39;TCP\u0026#39; } ] } } } output containerIPv4Address string = containerGroup.properties.ipAddress.ip To provision this, we can use Azure CLI commands.\n1 az deployment group create --template-file aci.bicep --resource-group bicep Once provisioned, the template returns the public IP address of the container instance. If we browse to this IP, we shall see the following displayed in the browser.\nLet us now look at the Bicep template\u0026rsquo;s contents to understand more about ACI.\nContainer Groups In ACI, the containers are created within a container group. The resource type used to create a container group is Microsoft.ContainerInstance/containerGroups. A container group is a top-level resource in ACI. A container group is simply a group of containers scheduled on the same host. Like a Kubernetes pod, all containers within the container group share the lifecycle, network, and storage.\nAt the time of writing, only Linux containers are supported in a multi-container group.\nA container group resource definition contains a collection of container specifications. In the example above, we specified only one container to be created within the group. The properties object within the container resource contains additional properties, such as the image to be pulled for creating the containers, ports to publish, and resource requests and limits.\nEach container within this collection is allocated resources specified in the container definition. For example, in the above template, we allocate one CPU core and 2GB of memory to the container. This is called a resource request. We can also set a resource limit. The resource limits of a container instance must be greater than or equal to the resource request. When the resource limit property is not set in the container instance property, the container\u0026rsquo;s maximum resource usage will be the same as its resource request.\nWe will learn more about resource allocation in ACI in a later post.\nWe can specify the restart policy at the container group level. This is done using the restartPolicy property within the container group properties. This property has three possible values \u0026ndash; Always, Never, and OnFailure \u0026ndash; with Always being the default. Container instances are billed by the second, so using an appropriate restart policy is important. If the restart policy is set to Never or OnFailure, the containers in the container group are stopped and set to Terminated state once the application exits. This configuration will be useful for running serverless workloads that just perform the function and exit. For such containerized applications, setting restartPolicy to Always can be disastrous as ACI continues to restart the container even after the application exits.\nWhat happens when we have multiple containers in a container group? We shall explore that in the next article!\n","date":"2022-11-02T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/2022-11-02-understanding-and-using-azure-container-instances-service-the-basics/","series":"Containers","title":"Understanding and using Azure Container Instances service - The Basics"},{"categories":["oci","containers","Azure"],"content":"Containerization is everywhere \u0026ndash; from on-premises data centers to the edge and in the cloud. We have been using containers right from development to production. I wrote about VS Code development containers and how I have standardized my development environment using devcontainers. There are several options for companies to run containerized applications in production. we can run individual application containers on a container host using engines like Docker. We can use tools like Docker Compose or Swarm for slightly complex multi-container, multi-host environments. However, it becomes complex to manage such environments. This needs us to implement a physical or virtualized server environment and handle all operational overhead associated with running our data center.\nMany cloud providers offer managed environments to run containerized applications. Microsoft Azure, too, offers several options for running containerized applications.\nAzure Container Instances (ACI) ACI service offers the capability to spin a container in the Azure cloud, similar to running a container from an image using the docker run command. With ACI, we can quickly deploy and scale containers for running short-lived and bursty workloads. We don\u0026rsquo;t need to worry about owning and deploying any infrastructure. ACI offers faster startup times. We can access the container instance using a Fully-Qualified Domain Name (FQDN). ACI integrates with other Azure services, such as Virtual Networks and Storage. We can use Azure PowerShell or Azure CLI to provision container instances in the Azure cloud.\nACI uses the concept of container groups to group together multiple containers, deploy to the same host, share the same network, and share any mounted volumes. We can have up to 60 containers and 20 volumes per container group.\nACI service runs containers inside a Hyper-V virtual machine to provide higher isolation levels. We can use resource requests to allocate CPU and memory per container group. ACI is not a container orchestrator like Kubernetes. Azure Container Instances can be virtual Azure Kubernetes Service (AKS) nodes.\nAzure Kubernetes Service (AKS) Azure Kubernetes Service is the hosted Kubernetes service from Azure. When using AKS, the operational overhead is offloaded to Azure. This is done by automatically provisioning the Kubernetes control plane. This control plane is provided at no additional cost, and we only pay for the worker nodes. An AKS cluster contains at least one node, an Azure virtual machine running the Kubernetes node components, and a container runtime. AKS supports thousands of nodes in a single cluster. In an AKS cluster, nodes are grouped as node pools based on the configuration. These node pools can contain both Windows and Linux nodes.\nAKS offers a built-in load balancer, automated scaling, and upgrades. We can use the recently used Kubernetes Fleet Manager (preview) for resource propagation and multi-cluster load balancing. While AKS offers an excellent way to run cloud-native applications and removes the overhead of managing a Kubernetes cluster, developers must still be aware of creating Kubernetes deployments specs written in YAML and managing the health of applications. This is where Azure Container Apps (ACA) help.\nAzure Container Apps Azure Container Apps (ACA) offer a fully managed serverless platform on which we can run microservices ad containerized applications. With ACA, we can run application code packaged in container format and not worry about managing cloud infrastructure. A group of containers share a secure environment within which all containers share the same network and write logs to the same destination. Azure handles the updates, resource balancing, scaling, and failover procedures. ACA supports Distributed Application Runtime (DAPR) and Kubernetes Event-Driven Autoscaling (KEDA) and is best suited for applications that implement microservice and event-driven architectures.\nIn future posts, we will dive into each of these services, understand the concepts, implement applications, and finally gain the knowledge to help us decide to choose one of the services for our applications.\n","date":"2022-11-01T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/2022-11-01-containerization-in-azure/","series":"Azure","title":"Containerization in Microsoft Azure"},{"categories":["oci","containers","Azure","Azure Bicep"],"content":"In the previous articles in this series, we looked at how to build container images and understand the OCI image specification that governs how the images are built and dealt with. Once the images are built, we may want to push them to a central repository for sharing with a larger community or a private container registry such as the Azure Container Registry (ACR).\nIn today\u0026rsquo;s article, we shall learn how to create an Azure Container Registry, push an image, and consume it back in the local environment. Let\u0026rsquo;s start!\nAzure Container Registry ACR is a managed registry service based on open-source Docker Registry 2.0. Within our Azure subscription, we can create an instance of the ACR resource and use it for publishing the container images. The following Bicep template can be used to provision ACR.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 @minLength(5) @maxLength(50) @description(\u0026#39;Specify a globally unique name\u0026#39;) param acrName string @description(\u0026#39;Specify a location where the ACR must be created\u0026#39;) param location string = resourceGroup().location @description(\u0026#39;Specify the pricing tier of for ACR\u0026#39;) param acrSku string = \u0026#39;Basic\u0026#39; resource acrResource \u0026#39;Microsoft.ContainerRegistry/registries@2021-06-01-preview\u0026#39; = { name: acrName location: location sku: { name: acrSku } properties: { adminUserEnabled: false } } @description(\u0026#39;Login server information\u0026#39;) output loginServer string = acrResource.properties.loginServer We can provision this template using Azure PowerShell or Azure CLI.\n1 az deployment group create --resource-group bicep --template-file acr.bicep --parameters acrName=\u0026#39;acrbicepex\u0026#39; This Bicep template returns the login server FQDN as the output.\n1 2 3 4 5 6 \u0026#34;outputs\u0026#34;: { \u0026#34;loginServer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;String\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;acrbicepex.azurecr.io\u0026#34; } } Once an instance of ACR is available within the subscription, we can start pushing images to this registry and eventually pull images from this private registry. Optionally, we can import Docker Hub images to ACR.\nPushing images to ACR To try pushing images to ACR, let us first pull an image using docker CLI.\n1 docker pull ravikanth/bicep We must re-tag (alias) the pulled image to indicate the remote Azure Container Registry we provisioned.\n1 docker tag ravikanth/bicep acrbicepex.azurecr.io/bicep:latest Before we can push to the ACR, we need to authenticate. For this, we need the Azure CLI.\n1 2 az login az acr login --name acrbicepex.azurecr.io We are now ready to push the image. This can be done using the docker push command.\n1 docker push acrbicepex.azurecr.io/bicep:latest Pulling an image from ACR Pulling an image from ACR is the same as pulling one from the Docker Hub. We must refer to the right image location. And, since the ACR instance we created is a private registry, we must ensure that we authenticate to it before trying to pull an image.\n1 docker pull acrbicepex.azurecr.io/bicep:latest Importing Docker Hub images to ACR 1 2 3 4 az acr import \\ --name acrbicepex \\ --source docker.io/library/ravikanth/hello-cloud:latest \\ --image hello-cloud:latest In a later post, we shall see how ACR can be used with other container services in Azure.\n","date":"2022-10-29T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/2022-10-29-publish-container-images-to-azure-container-registry/","series":"Container images","title":"Publish container images to Azure Container Registry"},{"categories":["oci","containers"],"content":"In the previous article, we looked at how OverlayFS is used to work with container images. We learned how the image layers are associated with each other and how the copy-on-write layer gets created when a containers gets created from an image. Container registries such as Docker Hub and Quay offer container images for several popular applications and you can publish your images as well. There are different tools available to build and push images to container registries. Whatever container engine you use \u0026ndash; Docker or Podman \u0026ndash; you can simply pull the image [built using any tool] from the registry and use it. How is this interoperability made possible? The answer is the work Open Container Initiative (OCI) is doing.\nAround 2015, containerization started becoming mainstream, a few companies started creating competing tools. Rkt was one such implementation. Folks at CoreOS created Application Container Image (ACI) format and Application Container Execution (ACE) specifications and started implementing tools. At the same time, Docker was already successful in bringing containers to the masses. So, industry saw an opportunity to eliminate fragmentation in the container ecosystem and worked with Docker, CoreOS, and other companies to create Open Container Initiative (OCI) to develop standards around container formats, runtimes, and registries. Most or all container engines and tools implement OCI specifications.\nIn today\u0026rsquo;s article, we will review OCI image specification and explore the how images are packaged.\nImage Specification The OCI image specification contains an image index, image manifest, a set of file system layers, and a configuration. While it sounds like a lot, it is actually quite simple.\nThe above figure represents the association among different parts of the OCI image specification. This starts with an optional image index.\nImage index This, literally, an index of manifests that point to images for different platforms / architectures.\n1 2 3 4 5 6 7 8 9 10 { \u0026#34;schemaVersion\u0026#34;: 2, \u0026#34;manifests\u0026#34;: [ { \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.oci.image.manifest.v1+json\u0026#34;, \u0026#34;digest\u0026#34;: \u0026#34;sha256:30a76a4a2be556c8ff8cff68a3e6a68656a2e2272329b3d4eac5a90072929f7c\u0026#34;, \u0026#34;size\u0026#34;: 657 } ] } As you see in this example, the image index within an OCI image points to an array of image manifests. You will see multiple manifests if the image is a multi-architecture image. The mediaType property within image index will always be set to application/vnd.oci.image.index.v1+json. For each manifest representing a container image, the mediaType will be set to application/vnd.oci.image.manifest.v1+json.\nNote: OCI image specification can also be used to package non-container image artifact types. We will learn about it another day.\nYou can refer to the image specification for a complete description of each property within the image index. The digest property identifies the manifest that contains the remaining image details such as image filesystem layers and configuration. We will learn about the OCI image layout on disk but before that let us take a look at the manifest format as well.\nImage manifest The image manifest contains references to the image configuration JSON and a set of file system layers used to create the image. The contents of an image manifest are specific to a platform and OS version.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 { \u0026#34;schemaVersion\u0026#34;: 2, \u0026#34;config\u0026#34;: { \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.oci.image.config.v1+json\u0026#34;, \u0026#34;digest\u0026#34;: \u0026#34;sha256:8b895fab1ad99d2ce5a24b59e3029741fa14d80a98e42cbe352ebea2f64b4446\u0026#34;, \u0026#34;size\u0026#34;: 1268 }, \u0026#34;layers\u0026#34;: [ { \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.oci.image.layer.v1.tar+gzip\u0026#34;, \u0026#34;digest\u0026#34;: \u0026#34;sha256:3e6080001d7b2e588ba7bd7c83b4fe5cdc389c5619525db7f24656198f7d44ab\u0026#34;, \u0026#34;size\u0026#34;: 2916043 }, { \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.oci.image.layer.v1.tar+gzip\u0026#34;, \u0026#34;digest\u0026#34;: \u0026#34;sha256:da30198d6af810de0ead95a824b42b1748f62816bcb0486d9b60090c70e02759\u0026#34;, \u0026#34;size\u0026#34;: 114 }, { \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.oci.image.layer.v1.tar+gzip\u0026#34;, \u0026#34;digest\u0026#34;: \u0026#34;sha256:5b5387c8849f2140efd643840a2af9829f0616cba8516bc3b01e67529a80406b\u0026#34;, \u0026#34;size\u0026#34;: 145 } ] } The config element refers to the JSON representation of image metadata, execution parameters for creating a container, and filesystem layers (tar balls) to use when creating the container. As you see, the mediaType will be set to application/vnd.oci.image.config.v1+json.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 { \u0026#34;created\u0026#34;: \u0026#34;2022-10-16T05:24:35.948125525Z\u0026#34;, \u0026#34;architecture\u0026#34;: \u0026#34;amd64\u0026#34;, \u0026#34;os\u0026#34;: \u0026#34;linux\u0026#34;, \u0026#34;config\u0026#34;: { \u0026#34;Env\u0026#34;: [ \u0026#34;PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\u0026#34; ], \u0026#34;Cmd\u0026#34;: [ \u0026#34;/bin/sh\u0026#34; ], \u0026#34;Labels\u0026#34;: { \u0026#34;maintainer\u0026#34;: \u0026#34;ravikanth@ravichaganti.com\u0026#34; } }, \u0026#34;rootfs\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;layers\u0026#34;, \u0026#34;diff_ids\u0026#34;: [ \u0026#34;sha256:994393dc58e7931862558d06e46aa2bb17487044f670f310dffe1d24e4d1eec7\u0026#34;, \u0026#34;sha256:9760f635cf77ac2d1a7f69d98d977eabfcf7977e8e4b51b80222481ff0fb2164\u0026#34;, \u0026#34;sha256:2d15fd73d3ab4c53b646668461b9facddd2e7a5ec0007a47cb99e748472a4fb5\u0026#34; ] }, \u0026#34;history\u0026#34;: [ { \u0026#34;created\u0026#34;: \u0026#34;2022-08-09T17:19:53.274069586Z\u0026#34;, \u0026#34;created_by\u0026#34;: \u0026#34;/bin/sh -c #(nop) ADD file:2a949686d9886ac7c10582a6c29116fd29d3077d02755e87e111870d63607725 in / \u0026#34; }, { \u0026#34;created\u0026#34;: \u0026#34;2022-08-09T17:19:53.47374331Z\u0026#34;, \u0026#34;created_by\u0026#34;: \u0026#34;/bin/sh -c #(nop) CMD [\\\u0026#34;/bin/sh\\\u0026#34;]\u0026#34;, \u0026#34;empty_layer\u0026#34;: true }, { \u0026#34;created\u0026#34;: \u0026#34;2022-10-16T05:24:35.841023842Z\u0026#34;, \u0026#34;created_by\u0026#34;: \u0026#34;LABEL maintainer=ravikanth@ravichaganti.com\u0026#34;, \u0026#34;comment\u0026#34;: \u0026#34;buildkit.dockerfile.v0\u0026#34;, \u0026#34;empty_layer\u0026#34;: true }, { \u0026#34;created\u0026#34;: \u0026#34;2022-10-16T05:24:35.841023842Z\u0026#34;, \u0026#34;created_by\u0026#34;: \u0026#34;RUN /bin/sh -c touch hello.txt # buildkit\u0026#34;, \u0026#34;comment\u0026#34;: \u0026#34;buildkit.dockerfile.v0\u0026#34; }, { \u0026#34;created\u0026#34;: \u0026#34;2022-10-16T05:24:35.948125525Z\u0026#34;, \u0026#34;created_by\u0026#34;: \u0026#34;RUN /bin/sh -c echo \\\u0026#34;this is an update to the hello.txt\\\u0026#34; \u0026gt;\u0026gt; hello.txt # buildkit\u0026#34;, \u0026#34;comment\u0026#34;: \u0026#34;buildkit.dockerfile.v0\u0026#34; } ] } This example looks a lot similar to what you see when running docker image inspect imageName. The diff_ids represent the digest of the layer\u0026rsquo;s uncompressed tar archive. In fact, these Diff IDs correlate to the RootFS layers you see in the docker image inspect output.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 $ cat blobs/sha256/8b895fab1ad99d2ce5a24b59e3029741fa14d80a98e42cbe352ebea2f64b4446 | jq .rootfs.diff_ids [ \u0026#34;sha256:994393dc58e7931862558d06e46aa2bb17487044f670f310dffe1d24e4d1eec7\u0026#34;, \u0026#34;sha256:9760f635cf77ac2d1a7f69d98d977eabfcf7977e8e4b51b80222481ff0fb2164\u0026#34;, \u0026#34;sha256:2d15fd73d3ab4c53b646668461b9facddd2e7a5ec0007a47cb99e748472a4fb5\u0026#34; ] $ docker image inspect ravikanth/hello-world:v1 | jq -r \u0026#39;.[0] | {Data: .RootFS.Layers}\u0026#39; { \u0026#34;Data\u0026#34;: [ \u0026#34;sha256:994393dc58e7931862558d06e46aa2bb17487044f670f310dffe1d24e4d1eec7\u0026#34;, \u0026#34;sha256:9760f635cf77ac2d1a7f69d98d977eabfcf7977e8e4b51b80222481ff0fb2164\u0026#34;, \u0026#34;sha256:2d15fd73d3ab4c53b646668461b9facddd2e7a5ec0007a47cb99e748472a4fb5\u0026#34; ] } The image layer digests you see in the above listing are different from what you see in the image manifest layers array. The layer digests in the image manifest indicate the image layers stored on the file system when you extract an image to a local folder. So, how do we extract an image? There are a few tools that can help extracting OCI images to a local folder. Let us take a look at Skopeo.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 $ sudo apt install skopeo $ skopeo copy docker://ravikanth/hello-world:v1 oci:hello_world $ tree hello-world/ hello-world/ ├── blobs │ └── sha256 │ ├── 30a76a4a2be556c8ff8cff68a3e6a68656a2e2272329b3d4eac5a90072929f7c │ ├── 3e6080001d7b2e588ba7bd7c83b4fe5cdc389c5619525db7f24656198f7d44ab │ ├── 5b5387c8849f2140efd643840a2af9829f0616cba8516bc3b01e67529a80406b │ ├── 8b895fab1ad99d2ce5a24b59e3029741fa14d80a98e42cbe352ebea2f64b4446 │ └── da30198d6af810de0ead95a824b42b1748f62816bcb0486d9b60090c70e02759 ├── index.json └── oci-layout Skopeo is a tool to work with images and image registries. Skopeo can copy images between registries, delete images, extract images, inspect, and sign images. The example above copies an image from Docker Hub to the local file system using the OCI layout.\nThe oci-layout file in the above listing is a JSON file that contains the version of the image layout.\nThe output of the tree command is similar to what you saw in the image representation at the beginning. If you look at the image manifest JSON above, you will notice that 3e6080001d7b2e588ba7bd7c83b4fe5cdc389c5619525db7f24656198f7d44ab, 5b5387c8849f2140efd643840a2af9829f0616cba8516bc3b01e67529a80406b, and da30198d6af810de0ead95a824b42b1748f62816bcb0486d9b60090c70e02759 are the compressed image layers on disk.\n1 2 3 4 5 6 $ file hello-world/blobs/sha256/* hello-world/blobs/sha256/30a76a4a2be556c8ff8cff68a3e6a68656a2e2272329b3d4eac5a90072929f7c: JSON data hello-world/blobs/sha256/3e6080001d7b2e588ba7bd7c83b4fe5cdc389c5619525db7f24656198f7d44ab: gzip compressed data, original size modulo 2^32 5826560 hello-world/blobs/sha256/5b5387c8849f2140efd643840a2af9829f0616cba8516bc3b01e67529a80406b: gzip compressed data, original size modulo 2^32 2048 hello-world/blobs/sha256/8b895fab1ad99d2ce5a24b59e3029741fa14d80a98e42cbe352ebea2f64b4446: JSON data hello-world/blobs/sha256/da30198d6af810de0ead95a824b42b1748f62816bcb0486d9b60090c70e02759: gzip compressed data, original size modulo 2^32 1536 You can, alternatively, extract an image locally available as a regular directory.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 $ skopeo copy docker-daemon:ravikanth/hello-world:v1 dir:hello_world1 Getting image source signatures Copying blob 994393dc58e7 done Copying blob 9760f635cf77 done Copying blob 2d15fd73d3ab done Copying config 9b5cea9d1f done Writing manifest to image destination Storing signatures $ tree hello_world1 hello_world1 ├── 2d15fd73d3ab4c53b646668461b9facddd2e7a5ec0007a47cb99e748472a4fb5 ├── 9760f635cf77ac2d1a7f69d98d977eabfcf7977e8e4b51b80222481ff0fb2164 ├── 994393dc58e7931862558d06e46aa2bb17487044f670f310dffe1d24e4d1eec7 ├── 9b5cea9d1f27984de5b0c7ad4e113a02b3b1a90191d3e337285fbe7a2c458f0a ├── manifest.json └── version Since we are reviewing OCI image specification and we shall stick to the OCI layout.\nWithin the OCI image layout, the blobs folder contains image manifest, configuration, and the layer tarballs. We have looked at the manifest and configuration JSON files already. From the blobs folder, you can extract the layers using the tar command.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 $ mkdir base-layer $ tar -xvf 3e6080001d7b2e588ba7bd7c83b4fe5cdc389c5619525db7f24656198f7d44ab --directory base-layer $ ls -l base-layer/ total 68 drwxr-xr-x 2 rchaganti rchaganti 4096 Aug 9 14:17 bin drwxr-xr-x 2 rchaganti rchaganti 4096 Aug 9 14:17 dev drwxr-xr-x 16 rchaganti rchaganti 4096 Aug 9 14:17 etc drwxr-xr-x 2 rchaganti rchaganti 4096 Aug 9 14:17 home drwxr-xr-x 7 rchaganti rchaganti 4096 Aug 9 14:17 lib drwxr-xr-x 5 rchaganti rchaganti 4096 Aug 9 14:17 media drwxr-xr-x 2 rchaganti rchaganti 4096 Aug 9 14:17 mnt drwxr-xr-x 2 rchaganti rchaganti 4096 Aug 9 14:17 opt dr-xr-xr-x 2 rchaganti rchaganti 4096 Aug 9 14:17 proc drwx------ 2 rchaganti rchaganti 4096 Aug 9 14:17 root drwxr-xr-x 2 rchaganti rchaganti 4096 Aug 9 14:17 run drwxr-xr-x 2 rchaganti rchaganti 4096 Aug 9 14:17 sbin drwxr-xr-x 2 rchaganti rchaganti 4096 Aug 9 14:17 srv drwxr-xr-x 2 rchaganti rchaganti 4096 Aug 9 14:17 sys drwxrwxr-x 2 rchaganti rchaganti 4096 Aug 9 14:17 tmp drwxr-xr-x 7 rchaganti rchaganti 4096 Aug 9 14:17 usr drwxr-xr-x 12 rchaganti rchaganti 4096 Aug 9 14:17 var This is the base layer of the image containing all operating system files.\nWhat we have seen so far should give a good idea around what OCI image specification defines and tools such as Skopeo help us explore this.\nWe understand the image specification but how exactly are these images built to comply with this specification? How do we determine how many layers to be created and what gets into each layer? For that, we need to dig into workflows and tools that are used to build container images. We will explore that in the next article.\n","date":"2022-10-28T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/2022-10-28-understanding-container-images-oci-image-specification/","series":"Container images","title":"Understanding container images - OCI image specification"},{"categories":["oci","containers"],"content":"Containers have been around for a while. There is no need for an introduction to containers anymore but If you are still looking for some background, you can read a series of articles I have written on this blog. Docker simplified working with containers by introducing a way to package your applications and application dependencies as container images. It also provided the tooling that made running container instances from these images easy. This meant the user space tools abstracted all complex details and removed the need to know the inner workings of containers. Like every other technology, knowing the internals of how something is built becomes important when troubleshooting issues or optimizing what the technology does. And, of course, knowing / learning internals is fun too!\nToday\u0026rsquo;s article is a collection of my notes [I made] as I started digging into how container images are built. First, the basics.\nNote: This article uses the Docker engine as an example. The fundamental concepts around managing container images do not change between container engines.\nThe basics Let us step back a bit and understand why there is a need for container images. Before containerization was made famous by Docker Inc., IT shops used virtualization as a way to deploy multiple applications on a single physical server. Virtualization vendors provide tools to do this efficiently and sometimes in an automated manner. However, each virtual machine requires a full operating system and application dependencies. Assuming that a single VM took ~10GB [very optimistic assumption] of space for OS alone, running 100 of them on a physical system will consume ~1TB. If all your VMs are running the same OS, this is a huge waste of disk space to store duplicate files. You had to install application dependencies in each VM separately, even when most applications have common dependencies. These limitations brought a bunch of operational challenges, such as patching/updates, portability, and distribution, as well.\nSome virtualization vendors provided using differencing disks [for application code and dependencies] that shared a single parent disk containing an operating system. This, to some extent, helped, but it came with performance penalties and the same set of operational challenges.\nWhen dotCloud, the company where Docker was born, started playing with Linux constructs to run applications as isolated processes [containers], they developed a method to package application code and dependencies that did not result in the same challenges as virtual machines. This packaging made the distribution of application code easy but solved the portability issues. What worked on a developer workstation worked in the production without a change. This packaging is what we call a container image. A container image is an immutable package of operating system files, application code, and dependencies. Container images consist of reusable layers that are stacked on top of each other. Before you dive into consuming or creating images, it is important to understand the Linux construct used in dealing with container images. It is called the Union File System.\nUnion File System The Union File System (UnionFS) in Linux allows merging the contents of one or more file systems [directories] while keeping the content [physically] separate. Different implementations, such as AUFS and OverlayFS, are meant for union mounts. Each of these different implementations has its pros and cons. To understand the concept of a union file system, let us take OverlayFS as an example.\nOverlayFS deals with layers of content \u0026ndash; one or more lower layers and one upper layer. The lower layers are treated as read-only and the upper layer as read-write. OverlayFS provides a unified view of these layers through a union mount.\nAs you can see in this depiction, the merged or union view takes content from both lower and upper layers. If a file exists in both layers, such as file F3, the file in the upper layer overrides the one in the lower layer(s). Each file or directory operation has a specific result.\nAdding new files When adding a new file, OverlayFS adds it to the upper layer.\nDeleting an existing file When you delete a file or folder from the upper layer, it gets deleted. However, when you delete a file that is in the lower layer(s), a special character device gets created with the same name in the upper layer. The lower layers are read-only; therefore, no files or folders get deleted. The character device in the upper layer indicates that the file or folder should be hidden from the unified view.\nModify an existing file There won\u0026rsquo;t be any special change if you modify a file in the upper layer. However, if you modify a file in the lower layer(s), the file first gets copied to the upper layer, and then the changes are made to this copy.\nTalk is cheap. Let us see this in action.\nOverlayFS in action Before creating a union mount, you must first prepare a few directories.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 $ mkdir lower upper merged work demo $ cd demo $ echo \u0026#34;This is a file 1 in lower layer\u0026#34; \u0026gt; lower/f1 $ echo \u0026#34;This is a file 2 in lower layer\u0026#34; \u0026gt; lower/f2 $ echo \u0026#34;This is a file 3 in lower layer\u0026#34; \u0026gt; lower/f3 $ echo \u0026#34;This is a file 3 in upper layer\u0026#34; \u0026gt; upper/f3 $ echo \u0026#34;This is a file 4 in upper layer\u0026#34; \u0026gt; upper/f4 $ cd .. $ tree demo/ demo/ ├── lower │ ├── f1 │ ├── f2 │ └── f3 ├── merged ├── upper │ ├── f3 │ └── f4 └── work Do not create any files in the merged or work directories. Merged will be the union mount, where the unified view of lower and upper directories will be available. Now, you can use the mount command to perform a union mount.\n1 $ sudo mount overlay -t overlay -o lowerdir=demo/lower/,upperdir=demo/upper/,workdir=demo/work/ demo/merged/ The mount command above mounts the directories, lowerdir and upperdir, as a union mount under the merged folder. The workdir can be considered a temporary scratch space copying files from lower to upper.\n1 2 3 4 5 6 7 8 9 $ ls -l demo/merged/ total 16 -rw-rw-r-- 1 rchaganti rchaganti 32 Oct 18 12:29 f1 -rw-rw-r-- 1 rchaganti rchaganti 32 Oct 18 12:29 f2 -rw-rw-r-- 1 rchaganti rchaganti 32 Oct 18 12:31 f3 -rw-rw-r-- 1 rchaganti rchaganti 32 Oct 18 12:31 f4 $ cat demo/merged/f3 This is a file 3 in upper layer Once the mount command succeeds, the contents of the lower and upper directories will be merged to provide a unified view. You can now try different operations, as depicted in the earlier images.\nNote: When working with the union file system, you must make all changes in the unified mount. In the example, it is the merged directory.\nAdding file 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 $ echo \u0026#34;A new file called f5 in merged view\u0026#34; \u0026gt; demo/merged/f5 $ ls -l demo/merged/ total 20 -rw-rw-r-- 1 rchaganti rchaganti 32 Oct 18 12:29 f1 -rw-rw-r-- 1 rchaganti rchaganti 32 Oct 18 12:29 f2 -rw-rw-r-- 1 rchaganti rchaganti 32 Oct 18 12:31 f3 -rw-rw-r-- 1 rchaganti rchaganti 32 Oct 18 12:31 f4 -rw-rw-r-- 1 rchaganti rchaganti 36 Oct 18 13:34 f5 $ ls -l demo/upper/ total 12 -rw-rw-r-- 1 rchaganti rchaganti 32 Oct 18 12:31 f3 -rw-rw-r-- 1 rchaganti rchaganti 32 Oct 18 12:31 f4 -rw-rw-r-- 1 rchaganti rchaganti 36 Oct 18 13:34 f5 Adding a file to the merged view adds it to the upper layer as that is the read/write layer.\nDeleting a file 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 $ rm demo/merged/f2 $ ls -l demo/merged/ total 16 -rw-rw-r-- 1 rchaganti rchaganti 32 Oct 18 12:29 f1 -rw-rw-r-- 1 rchaganti rchaganti 32 Oct 18 12:31 f3 -rw-rw-r-- 1 rchaganti rchaganti 32 Oct 18 12:31 f4 -rw-rw-r-- 1 rchaganti rchaganti 36 Oct 18 13:34 f5 $ ls -l demo/upper/ total 12 c--------- 2 root root 0, 0 Oct 18 13:36 f2 -rw-rw-r-- 1 rchaganti rchaganti 32 Oct 18 12:31 f3 -rw-rw-r-- 1 rchaganti rchaganti 32 Oct 18 12:31 f4 -rw-rw-r-- 1 rchaganti rchaganti 36 Oct 18 13:34 f5 $ ls -l demo/lower/ total 12 -rw-rw-r-- 1 rchaganti rchaganti 32 Oct 18 12:29 f1 -rw-rw-r-- 1 rchaganti rchaganti 32 Oct 18 12:29 f2 -rw-rw-r-- 1 rchaganti rchaganti 32 Oct 18 12:29 f3 Deleting file f2 results in a special character file in the upper layer. The lower layer, however, remains untouched.\nModify file When you modify a file, depending on where it is \u0026ndash; upper or lower \u0026ndash; a copy-up action takes place. For example, if you modify file f1 in the lower layer, f1 is copied to the upper layer and modified in the upper layer.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 $ cat demo/merged/f1 This is a file 1 in lower layer $ echo \u0026#34;adding some random content\u0026#34; \u0026gt;\u0026gt; demo/merged/f1 $ cat demo/merged/f1 This is a file 1 in lower layer adding some random content $ cat demo/upper/f1 This is a file 1 in lower layer adding some random content $ cat demo/lower/f1 This is a file 1 in lower layer As the above example shows, file f1 in the lower layer is left untouched. A copy of f1 is present in the upper layer and gets modified to add the new line.\nNow that you understand how OverlayFS works, it is time to see how container engines use the union file system to manage container images.\nContainer images Container engines like Docker support different types of storage drivers responsible for combining stacked layers into a single unified view. Similar to what you saw in the earlier example, each layer contains only the differences from the below layer. The immutable nature of these layers makes the container image reusable and allows you to run multiple containers from a single image. Each container gets its thin writable layer to facilitate writing to the container file system or modifying or deleting existing files. The storage driver used by the container engine is responsible for managing the copy-on-write (CoW) layer. The Docker engine defaults to overlay2 as the storage driver.\nLet us dig into how this works. First, let us pull an image from the Docker hub for this exercise.\n1 2 3 4 5 6 7 8 $ docker pull ravikanth/hello-world:v1 v1: Pulling from ravikanth/hello-world 213ec9aee27d: Pull complete 0d351817f207: Pull complete 8b09275c3de5: Pull complete Digest: sha256:9964e0545f1cbd09fc902dda80664ba4b23b5f4bd32b1a0e7ab135f819c5ed6c Status: Downloaded newer image for ravikanth/hello-world:v1 docker.io/ravikanth/hello-world:v1 The image that we just pulled has three layers. The overlay2 driver on the Docker host stores the image layers at /var/lib/docker/overlay2.\n1 2 3 4 5 6 $ sudo ls -l /var/lib/docker/overlay2 total 16 drwx--x--- 4 root root 4096 Oct 18 15:32 cf0b92185c9ba3bdf11d19e53252063def37acf80fe3491597fd5a8914e1a696 drwx--x--- 4 root root 4096 Oct 18 15:32 d63b70e8f22063b1eb4296b70c2e231392e28b104475104e14f7346530e0dd39 drwx--x--- 3 root root 4096 Oct 18 15:32 ec861e1affe1b78197b7cc7f6f4381c6de0e490e04a60dc1ff684a9f7ddeab22 drwx------ 2 root root 4096 Oct 18 15:32 l The directories at this location are uncompressed image layers; therefore, the ID does not match the layer ID in the image. The directory named l contains the symbolic links to the diff directory within each uncompressed layer.\nIf you have multiple other images on the docker host already, you can use the docker image inspect command to find the right uncompressed layer IDs for a specific image.\n1 2 3 4 5 6 7 8 9 $ docker image inspect ravikanth/hello-world:v1 | jq -r \u0026#39;.[0] | {Data: .GraphDriver.Data}\u0026#39; { \u0026#34;Data\u0026#34;: { \u0026#34;LowerDir\u0026#34;: \u0026#34;/var/lib/docker/overlay2/d63b70e8f22063b1eb4296b70c2e231392e28b104475104e14f7346530e0dd39/diff:/var/lib/docker/overlay2/ec861e1affe1b78197b7cc7f6f4381c6de0e490e04a60dc1ff684a9f7ddeab22/diff\u0026#34;, \u0026#34;MergedDir\u0026#34;: \u0026#34;/var/lib/docker/overlay2/cf0b92185c9ba3bdf11d19e53252063def37acf80fe3491597fd5a8914e1a696/merged\u0026#34;, \u0026#34;UpperDir\u0026#34;: \u0026#34;/var/lib/docker/overlay2/cf0b92185c9ba3bdf11d19e53252063def37acf80fe3491597fd5a8914e1a696/diff\u0026#34;, \u0026#34;WorkDir\u0026#34;: \u0026#34;/var/lib/docker/overlay2/cf0b92185c9ba3bdf11d19e53252063def37acf80fe3491597fd5a8914e1a696/work\u0026#34; } } The property names \u0026ndash; LowerDir, UpperdDir, MergeDir, and WorkDir \u0026ndash; in the above output should be familiar by now. The layer IDs listed in the output will correlate to the ls output earlier. If you observe this output, the LowerDir property has more than one layer associated with it as the value.\n1 2 /var/lib/docker/overlay2/d63b70e8f22063b1eb4296b70c2e231392e28b104475104e14f7346530e0dd39/diff /var/lib/docker/overlay2/ec861e1affe1b78197b7cc7f6f4381c6de0e490e04a60dc1ff684a9f7ddeab22/diff These layers are represented in the order of their position in the container image. Therefore, ec861e1affe1b78197b7cc7f6f4381c6de0e490e04a60dc1ff684a9f7ddeab22 is the lowest layer and d63b70e8f22063b1eb4296b70c2e231392e28b104475104e14f7346530e0dd39 is the layer above the lowest layer. From the image pull output, you see that there are three layers in this image. The layer represented by UpperDir is the topmost layer in this image.\nNow that we know the order of the layers, let us move on to see what is there in each of these uncompressed layer directories.\nThe lowest layer contains three entries.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 $ sudo ls -l /var/lib/docker/overlay2/ec861e1affe1b78197b7cc7f6f4381c6de0e490e04a60dc1ff684a9f7ddeab22 total 8 -rw------- 1 root root 0 Oct 18 15:32 committed drwxr-xr-x 19 root root 4096 Oct 18 15:32 diff -rw-r--r-- 1 root root 26 Oct 18 15:32 link $ sudo ls -l /var/lib/docker/overlay2/ec861e1affe1b78197b7cc7f6f4381c6de0e490e04a60dc1ff684a9f7ddeab22/diff total 68 drwxr-xr-x 2 root root 4096 Aug 9 14:17 bin drwxr-xr-x 2 root root 4096 Aug 9 14:17 dev drwxr-xr-x 16 root root 4096 Aug 9 14:17 etc drwxr-xr-x 2 root root 4096 Aug 9 14:17 home drwxr-xr-x 7 root root 4096 Aug 9 14:17 lib drwxr-xr-x 5 root root 4096 Aug 9 14:17 media drwxr-xr-x 2 root root 4096 Aug 9 14:17 mnt drwxr-xr-x 2 root root 4096 Aug 9 14:17 opt dr-xr-xr-x 2 root root 4096 Aug 9 14:17 proc drwx------ 2 root root 4096 Aug 9 14:17 root drwxr-xr-x 2 root root 4096 Aug 9 14:17 run drwxr-xr-x 2 root root 4096 Aug 9 14:17 sbin drwxr-xr-x 2 root root 4096 Aug 9 14:17 srv drwxr-xr-x 2 root root 4096 Aug 9 14:17 sys drwxrwxrwt 2 root root 4096 Aug 9 14:17 tmp drwxr-xr-x 7 root root 4096 Aug 9 14:17 usr drwxr-xr-x 12 root root 4096 Aug 9 14:17 var $ sudo cat /var/lib/docker/overlay2/ec861e1affe1b78197b7cc7f6f4381c6de0e490e04a60dc1ff684a9f7ddeab22/link BOOTVTCSKJUJXUJWX765MW7TMJ $ sudo ls -l /var/lib/docker/overlay2/l/BOOTVTCSKJUJXUJWX765MW7TMJ lrwxrwxrwx 1 root root 72 Oct 18 15:32 /var/lib/docker/overlay2/l/BOOTVTCSKJUJXUJWX765MW7TMJ -\u0026gt; ../ec861e1affe1b78197b7cc7f6f4381c6de0e490e04a60dc1ff684a9f7ddeab22/diff This image is based on alpine:latest and therefore diff directory in the lowest layer contains the OS files. Let us move to the next layer. If you look at the last command in the output, file namedlink in the uncompressed layer is referencing the diff directory in the same layer.\nLet us move to the next layer in the stack.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 $ sudo ls -l /var/lib/docker/overlay2/d63b70e8f22063b1eb4296b70c2e231392e28b104475104e14f7346530e0dd39 total 16 -rw------- 1 root root 0 Oct 18 15:32 committed drwxr-xr-x 2 root root 4096 Oct 18 15:32 diff -rw-r--r-- 1 root root 26 Oct 18 15:32 link -rw-r--r-- 1 root root 28 Oct 18 15:32 lower drwx------ 3 root root 4096 Oct 18 15:32 work $ sudo ls -l /var/lib/docker/overlay2/d63b70e8f22063b1eb4296b70c2e231392e28b104475104e14f7346530e0dd39/diff total 0 -rw-r--r-- 1 root root 0 Oct 16 10:54 hello.txt $ sudo cat /var/lib/docker/overlay2/d63b70e8f22063b1eb4296b70c2e231392e28b104475104e14f7346530e0dd39/diff/hello.txt $ sudo cat /var/lib/docker/overlay2/d63b70e8f22063b1eb4296b70c2e231392e28b104475104e14f7346530e0dd39/link FYTORA5MEBUHE4B4ZY4IPI3P5D $ sudo cat /var/lib/docker/overlay2/d63b70e8f22063b1eb4296b70c2e231392e28b104475104e14f7346530e0dd39/lower l/BOOTVTCSKJUJXUJWX765MW7TMJ All uncompressed layers above the lowest layer contain five entries. The diff directory contains the contents of the layer \u0026ndash; a zero-byte hello.txt file in this case. The file named lower points to the lower layer in the stack. And, in this example, it naturally points to the lower layer diff directory. You can ignore the rest of the entries in this uncompressed layer. In each layer, the link file contains the symbolic link to the layer\u0026rsquo;s diff directory.\nFinally, we have one more layer to look at.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 $ sudo ls -l /var/lib/docker/overlay2/cf0b92185c9ba3bdf11d19e53252063def37acf80fe3491597fd5a8914e1a696 total 16 drwxr-xr-x 2 root root 4096 Oct 18 15:32 diff -rw-r--r-- 1 root root 26 Oct 18 15:32 link -rw-r--r-- 1 root root 57 Oct 18 15:32 lower drwx------ 3 root root 4096 Oct 18 15:32 work $ sudo ls -l /var/lib/docker/overlay2/cf0b92185c9ba3bdf11d19e53252063def37acf80fe3491597fd5a8914e1a696/diff total 4 -rw-r--r-- 1 root root 35 Oct 16 10:54 hello.txt $ sudo cat /var/lib/docker/overlay2/cf0b92185c9ba3bdf11d19e53252063def37acf80fe3491597fd5a8914e1a696/diff/hello.txt this is an update to the hello.txt $ sudo cat /var/lib/docker/overlay2/cf0b92185c9ba3bdf11d19e53252063def37acf80fe3491597fd5a8914e1a696/link S2YWWPZHW75GE3EIQ7ULNMR5RZ $ sudo cat /var/lib/docker/overlay2/cf0b92185c9ba3bdf11d19e53252063def37acf80fe3491597fd5a8914e1a696/lower l/FYTORA5MEBUHE4B4ZY4IPI3P5D:l/BOOTVTCSKJUJXUJWX765MW7TMJ This layer has two layers below it and therefore you see two entries (separated by :) in the file named lower. Similar to the previous layer, the diff directory in the layer contains a file named hello.txt. As this is the upper layer or the topmost layer, when a container is created using this image, you should see the contents of this hello.txt file within the container.\nTo better understand this, you need to visualize this layer structure.\nAs described earlier, container images are immutable. You cannot write anything directly to these layers. The only way to consume these layers is to create a container instance. You can do that using the docker run command.\n1 2 $ docker run -d --name helloworld ravikanth/hello-world:v1 sleep 1000000 cff3f0322cb6765f06575bba9405d8ffeb0fecac237c5f615b906a9546d2a413 Note: Starting this container in the background will help in exploring the copy-on-write operations.\nIn an earlier illustration, you saw how more than one container can be created from a single image and how each gets its own copy-on-write (CoW) read/write layer. So, in theory, this CoW layer must be somehow getting associated with the uncompressed image layers that you explored. This association can be found by running the docker container inspect command.\n1 2 3 4 5 6 7 8 9 $ docker container inspect helloworld | jq -r \u0026#39;.[0] | {Data: .GraphDriver.Data}\u0026#39; { \u0026#34;Data\u0026#34;: { \u0026#34;LowerDir\u0026#34;: \u0026#34;/var/lib/docker/overlay2/38705da4838102c3b9d2ae8aeb49844dac84dae95881df61fcffd1685caf9751-init/diff:/var/lib/docker/overlay2/cf0b92185c9ba3bdf11d19e53252063def37acf80fe3491597fd5a8914e1a696/diff:/var/lib/docker/overlay2/d63b70e8f22063b1eb4296b70c2e231392e28b104475104e14f7346530e0dd39/diff:/var/lib/docker/overlay2/ec861e1affe1b78197b7cc7f6f4381c6de0e490e04a60dc1ff684a9f7ddeab22/diff\u0026#34;, \u0026#34;MergedDir\u0026#34;: \u0026#34;/var/lib/docker/overlay2/38705da4838102c3b9d2ae8aeb49844dac84dae95881df61fcffd1685caf9751/merged\u0026#34;, \u0026#34;UpperDir\u0026#34;: \u0026#34;/var/lib/docker/overlay2/38705da4838102c3b9d2ae8aeb49844dac84dae95881df61fcffd1685caf9751/diff\u0026#34;, \u0026#34;WorkDir\u0026#34;: \u0026#34;/var/lib/docker/overlay2/38705da4838102c3b9d2ae8aeb49844dac84dae95881df61fcffd1685caf9751/work\u0026#34; } } This looks similar to what you have seen in the image information. The LowerDir has four different layers in the below order.\n1 2 3 4 /var/lib/docker/overlay2/38705da4838102c3b9d2ae8aeb49844dac84dae95881df61fcffd1685caf9751-init/diff /var/lib/docker/overlay2/cf0b92185c9ba3bdf11d19e53252063def37acf80fe3491597fd5a8914e1a696/diff /var/lib/docker/overlay2/d63b70e8f22063b1eb4296b70c2e231392e28b104475104e14f7346530e0dd39/diff /var/lib/docker/overlay2/ec861e1affe1b78197b7cc7f6f4381c6de0e490e04a60dc1ff684a9f7ddeab22/diff This is essentially the three image layers you saw earlier and an additional container layer. You may have observed MergeDir and WorkDir in the image information output. However, as images are immutable, these values do not matter much for an image. In the case of a container, these values do matter. Therefore, the /var/lib/docker/overlay2/38705da4838102c3b9d2ae8aeb49844dac84dae95881df61fcffd1685caf9751/merged directory contains the unified view of all image layers.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 $ sudo ls -l /var/lib/docker/overlay2/38705da4838102c3b9d2ae8aeb49844dac84dae95881df61fcffd1685caf9751/merged total 72 drwxr-xr-x 2 root root 4096 Aug 9 14:17 bin drwxr-xr-x 1 root root 4096 Oct 18 18:10 dev drwxr-xr-x 1 root root 4096 Oct 18 18:10 etc -rw-r--r-- 1 root root 35 Oct 16 10:54 hello.txt drwxr-xr-x 2 root root 4096 Aug 9 14:17 home drwxr-xr-x 7 root root 4096 Aug 9 14:17 lib drwxr-xr-x 5 root root 4096 Aug 9 14:17 media drwxr-xr-x 2 root root 4096 Aug 9 14:17 mnt drwxr-xr-x 2 root root 4096 Aug 9 14:17 opt dr-xr-xr-x 2 root root 4096 Aug 9 14:17 proc drwx------ 2 root root 4096 Aug 9 14:17 root drwxr-xr-x 2 root root 4096 Aug 9 14:17 run drwxr-xr-x 2 root root 4096 Aug 9 14:17 sbin drwxr-xr-x 2 root root 4096 Aug 9 14:17 srv drwxr-xr-x 2 root root 4096 Aug 9 14:17 sys drwxrwxrwt 2 root root 4096 Aug 9 14:17 tmp drwxr-xr-x 7 root root 4096 Aug 9 14:17 usr drwxr-xr-x 12 root root 4096 Aug 9 14:17 var $ sudo cat /var/lib/docker/overlay2/38705da4838102c3b9d2ae8aeb49844dac84dae95881df61fcffd1685caf9751/merged/hello.txt this is an update to the hello.txt Now, try writing a new file in the running container.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 $ docker exec helloworld touch /hello-world.txt rchaganti@ubuntu02:~$ docker exec helloworld ls -l total 60 drwxr-xr-x 2 root root 4096 Aug 9 08:47 bin drwxr-xr-x 5 root root 340 Oct 18 12:40 dev drwxr-xr-x 1 root root 4096 Oct 18 12:40 etc -rw-r--r-- 1 root root 0 Oct 18 13:11 hello-world.txt -rw-r--r-- 1 root root 35 Oct 16 05:24 hello.txt drwxr-xr-x 2 root root 4096 Aug 9 08:47 home drwxr-xr-x 7 root root 4096 Aug 9 08:47 lib drwxr-xr-x 5 root root 4096 Aug 9 08:47 media drwxr-xr-x 2 root root 4096 Aug 9 08:47 mnt drwxr-xr-x 2 root root 4096 Aug 9 08:47 opt dr-xr-xr-x 252 root root 0 Oct 18 12:40 proc drwx------ 2 root root 4096 Aug 9 08:47 root drwxr-xr-x 2 root root 4096 Aug 9 08:47 run drwxr-xr-x 2 root root 4096 Aug 9 08:47 sbin drwxr-xr-x 2 root root 4096 Aug 9 08:47 srv dr-xr-xr-x 12 root root 0 Oct 18 12:40 sys drwxrwxrwt 2 root root 4096 Aug 9 08:47 tmp drwxr-xr-x 7 root root 4096 Aug 9 08:47 usr drwxr-xr-x 12 root root 4096 Aug 9 08:47 var You can see that a new file got added inside the container. The view represents the contents of MergedDir. You can check the directory represented by UpperDir to see if it appears there as well.\n1 2 3 $ sudo ls -l /var/lib/docker/overlay2/38705da4838102c3b9d2ae8aeb49844dac84dae95881df61fcffd1685caf9751/diff total 0 -rw-r--r-- 1 root root 0 Oct 18 18:41 hello-world.txt If you follow the same steps you followed to determine the links between multiple image layers, you will see the association between the CoW layer and the lower immutable image layers.\nThis is one messy diagram but it demonstrates the point. The topmost layer is the CoW read/write layer and all layers below are read-only layers. I mentioned earlier that this is a temporary lived read/write layer. What happens if we stop and start the container? What happens if we remove the container?\nTry it yourself. Hint: The file you created within the container survives a container restart.\nOh, BTW, you can use a tool called dive to look into the container images. While that is a great time saver, it is not as much fun as exploring the images manually \u0026ndash; the hard way.\nThis is one long article but I had fun refining the notes I made to write this. This article showed how images are managed by the storage driver. You may have read elsewhere that container images can be used with any container engine. How is that made possible? Let us review that in the next article around Open Container Initiative (OCI) and the OCI image specification.\n","date":"2022-10-18T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/2022-10-18-understanding-container-images-the-fundamentals/","series":"Container images","title":"Understanding container images - The fundamentals"},{"categories":["cuelang","vscode","containers"],"content":"In an earlier article, I wrote a bit about Visual Studio Code development containers. I have been using devcontainers for different development environments extensively. This is helping me keep the environment consistent and helps me rebuild an environment from scratch almost instantly. I use dev containers for all my Go language, Python, Rust, and now CUE language learning and development work. You can find all my dev container definition in a sample repository I created on GitHub.\nSince I wrote the last article, devcontainers feature has evolved and a few things changed in how we define dev containers and dev container features. I realized that I needed to update some of my definitions and how I created dev container features.\nIn this article, I will walk through creating a feature for CUE language first and then show you how to consume that feature in a dev container definition.\nWhat is a dev container feature? To quote a Microsoft article,\nFeatures are self-contained units of installation code, container configuration, and/or settings and extensions designed to enable new development capabilities in your dev container.\nIn essence, you can use dev container features to any add missing tools or languages to your development environment. These features can be authored in any language of your choice available in the dev container. The default choice is a shell script. To start authoring a new feature, you can clone the feature template repository.\nI have a repository created from this template at rchaganti/vsc-devcontainer-features: Visual Studio devcontainer features (github.com)\nAs you see here, this template repository contains src, test, and GitHub workflows folders. All you feature definitions go into the src folder. The template comes with a standard GitHub workflow that builds and publishes the features as OCI artifacts to the GitHub Container Registry (GHCR). You can place each feature in its own subfolder under the src folder. Each feature folder must contain at least two files \u0026ndash; devcontainer-feature.json and install.sh. The devcontainer-feature.json contains the metadata needed to describe the feature and any arguments that you need to pass to the installer script.\ndevcontainer-feature.json For building a feature for the CUE language, here is what I have defined in the JSON.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 { \u0026#34;id\u0026#34;: \u0026#34;cuelang\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1.0.3\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;CUE Lang\u0026#34;, \u0026#34;options\u0026#34;: { \u0026#34;version\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Specify a version of CUE language\u0026#34; } }, \u0026#34;customizations\u0026#34;: { \u0026#34;vscode\u0026#34;: { \u0026#34;extensions\u0026#34;: [ \u0026#34;brody715.vscode-cuelang\u0026#34; ] } } } In this definition, id, version, and name are required fields. Any time you make a change to either the JSON definition or the install.sh script or any other artifact within the feature folder, you must bump up the version in devcontainer-feature.json. The options object can be used to define any arguments that must be passed to the install.sh script. In the above definition, there is only one property \u0026ndash; version. The customizations section is where you can add VS Code setting default for this dev container and any extensions that you need have within the development environment. For the CUE lang, there is no official extension. Therefore, I used what is available from the community in the marketplace.\ninstall.sh Once you have the devcontainer-feature.json ready, you can start implementing the necessary logic within install.sh script. For example, here is what I have in the install.sh for the CUE lang feature.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 #!/bin/bash set -e echo \u0026#34;Activating feature \u0026#39;CUELANG\u0026#39;\u0026#34; # CUE lang version VERSION=${VERSION:-\u0026#34;latest\u0026#34;} CUEURL=\u0026#34;https://github.com/cue-lang/cue/releases/download/v${VERSION}/cue_v${VERSION}_linux_amd64.tar.gz\u0026#34; sudo curl -Lo /tmp/cue.tar.gz ${CUEURL} sudo mkdir /tmp/cue sudo tar xvzf /tmp/cue.tar.gz -C /tmp/cue sudo cp /tmp/cue/cue /usr/local/bin/cue sudo chmod +x /usr/local/bin/cue sudo rm -rf /tmp/cue The version option coming from the dev container definition can be retrieved in the install.sh script. In this script, the CUEURL variable uses the version value passed to the install script to construct the URL needed to download the CUE lang binary.\nGitHub workflow As mentioned earlier, the features can be referenced in different ways in a dev container definition but the most easiest and my favorite way is to publish to / consume from GHCR. The feature template repository comes with a built-in workflow to publish the feature tarballs as OCI artifacts.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 name: \u0026#34;Release dev container features\u0026#34; on: workflow_dispatch: push: branches: - main jobs: deploy: if: ${{ github.ref == \u0026#39;refs/heads/main\u0026#39; }} runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - name: \u0026#34;Publish\u0026#34; uses: devcontainers/action@v1 with: publish-features: \u0026#34;true\u0026#34; base-path-to-features: \u0026#34;./src\u0026#34; env: GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} With this in place, you should be all set. You can commit this feature into a GitHub repository and the workflow should take care of publishing the artifacts to GHCR.\nConsuming a custom feature With the change to publish features as OCI artifacts, there is a change in consuming as well. Here is how you can consume the above CUE lang feature in a dev container definition.\n1 2 3 4 5 6 7 8 9 10 { \u0026#34;image\u0026#34;: \u0026#34;mcr.microsoft.com/vscode/devcontainers/base\u0026#34;, \u0026#34;settings\u0026#34;: {}, \u0026#34;features\u0026#34;: { \u0026#34;ghcr.io/rchaganti/vsc-devcontainer-features/cuelang:latest\u0026#34; : { \u0026#34;version\u0026#34; : \u0026#34;0.4.3\u0026#34; } }, \u0026#34;remoteUser\u0026#34;: \u0026#34;vscode\u0026#34; } This is it! When you spin a dev container using this definition, you can CUE lang installed and ready to use.\nHope you find this useful. Leave any questions / feedback you may have in the comments.\n","date":"2022-10-10T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/2022-10-10-cue-lang-dev-container-feature/","series":null,"title":"CUE language VS Code development container feature"},{"categories":["book","azure","bicep"],"content":"Finally! I published the final chapter in my book Azure Bicep - Zero to Hero today. I started this project more than a year ago with an intention that I will complete it before end of year 2021. However, a few personal life events made me shift focus to my family and that pushed the book timelines to the right. In the last few weeks, I spent good amount of my personal time to explore a few new areas within Bicep and formed the content for the last few chapters.\nSo far, this book has 40 readers. As promised earlier, I have not taken a single penny from this book revenue for my personal use. All proceeds will go to charity once I hit 100 readers (whatever that amount may be).\nHere is the overall table of contents.\nIntroduction Infrastructure As Code Azure Resource Manager Why Bicep Introducing Bicep Summary Getting Started Installing Bicep CLI Authoring Bicep templates First Bicep template Compiling and deploying Bicep templates Summary Working with resources Defining resources Resource dependencies Child resources Extension Resources Reference existing resources Scoped resource deployment Summary Authoring reusable templates Parameters Parameter decorators Output Generating Parameter files Summary Variables, function, and expressions Summary Iterations and Conditions Iterations Sequential deployments Conditional deployments Combining iterations and conditions Summary Creating and Consuming Modules Creating modules Consuming modules Iterations and conditions Deploying at multiple scopes Sharing modules Module aliases Summary Developing Bicep templates in VS Code devcontainers Visual Studio Code Development Containers GitHub Codespaces Summary CI and CD of Bicep templates Customizing linter rules Build and deploy A local pipeline GitHub actions for CI/CD Summary Built-in function primer any function loadJsonContent function getSecret function Summary Hope you find this book helpful. You can leave your feedback as comments on this page!\n","date":"2022-07-09T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/2022-07-09-azure-bicep-zero-to-hero-s-complete/","series":"Azure Bicep","title":"[Book] Azure Bicep - Zero to Hero is complete"},{"categories":["devops","azure","bicep"],"content":"One of the very important aspects of Infrastructure as Code (IaC) is automation. And, it is not just about the configuration tasks that we must automate but it is also the entire validation and deployment / delivery workflow itself. We call this a continuous integration (CI) and continuous delivery / deployment (CD) pipeline. The core idea of a CI/CD pipeline is to accelerate testing and uncover any bugs and fix those bugs before the code reaches production. There are many ways and tools to implement a CI/CD pipeline. We can use GitHub actions or Jenkins or AppVeyor to name just a few. But, as a starting point, I prefer building a local CI/CD pipeline using git hooks. In this article, I will show you how I built a simple test and deploy pipeline for my Azure Bicep experiments.\nGit hooks is not a new concept. These hooks are simply scripts that execute before or after Git events. These Git events include commit, push, pull, and so on. Every Git repository contains a hooks folder under .git folder at the root of the repository. There are bunch of sample scripts already there for us as a reference. We can use that as a starting point or create something of our from scratch.\nYou can clone the repo https://github.com/rchaganti/bicepcicd to understand how I built this local CI/CD pipeline. The rest of the article explains the same.\nFirst, you need a Git repository. You can download and install Git from https://git-scm.com. Once you have the Git binaries installed, you can run the following commands to create a local repository.\n1 2 3 mkdir c:\\temp\\myBicepProject cd C:\\temp\\myBicepProject git init . Once a repository is created, you will see a hidden .git folder and a folder called hooks within that. To initiate a script after a commit is complete, we need to place a post-commit script in the hooks folder. Here is what we will use as a post commit script.\n1 2 3 4 #!/bin/bash message=$(git log -1 --format=%s) exec powershell.exe -NoProfile -ExecutionPolicy Bypass -File \u0026#34;$PWD/.git/hooks/post-commit.ps1\u0026#34; -CommitMessage \u0026#34;\\\u0026#39;$message\\\u0026#39;\u0026#34; This is a simple shell script and calls a PowerShell script that does the real magic. We can save this as post-commit (no extension) within the hooks folder. We will now look at the post-commit.ps1 script.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 [CmdletBinding()] param ( [Parameter()] [String] $CommitMessage ) $action = [regex]::Matches($CommitMessage, \u0026#39;(?\u0026lt;=\\[)[^]]+(?=\\])\u0026#39;).Value $buildPath = Split-Path -Path (Split-Path -Path $PSScriptRoot -Parent) -Parent function buildBicep { [CmdletBinding()] param ( [Parameter(Mandatory)] [String] $BuildPath ) Write-Host -ForegroundColor Green \u0026#34;[Build] Building Bicep template\u0026#34; bicep build \u0026#34;${buildPath}\\main.bicep\u0026#34; if ($LASTEXITCODE -ne 0) { return $false } return $true } if ($action.Count -ge 1) { switch ($action) { \u0026#34;Build\u0026#34; { $buildStatus = buildBicep -BuildPath $buildPath if (!$buildStatus) { Write-Error \u0026#34;Failed to build the Bicep template\u0026#34; exit 1 } break } \u0026#34;Deploy\u0026#34; { $buildStatus = buildBicep -BuildPath $buildPath if (!$buildStatus) { Write-Error \u0026#34;Failed to build the Bicep template\u0026#34; exit 1 } Write-Host \u0026#34;[Deploy] Deploying Bicep template\u0026#34; -ForegroundColor Green $resourceGroup = $action[1] if (!$resourceGroup) { Write-Error \u0026#34;No resource group specified\u0026#34; exit 1 } Write-Host \u0026#34;Starting Bicep template deployment\u0026#34; -ForegroundColor Green az deployment group create --resource-group $resourceGroup --template-file \u0026#34;${buildPath}\\main.bicep\u0026#34; if ($LASTEXITCODE -ne 0) { Write-Error \u0026#34;Failed to deploy the Bicep template\u0026#34; } break } default { Write-Error \u0026#34;Invalid action specified\u0026#34; } } } else { Write-Host \u0026#34;No action specified\u0026#34; -ForegroundColor Red } The above PowerShell script takes one argument \u0026ndash; CommitMessage. Whatever commit message we specify along with the git commit -m command gets passed as an argument to this PowerShell script. For the purpose of providing a choice of task to execute, we can use a fixed format for the commit message.\n1 [Action] [Resource-Group] Commit Message The first part within the [] describes an action. We can set it to Build or Deploy.\nIf the action is set to Deploy, the second part within the [] provides the resource group to which the deployment will be performed. The rest of the commit message can be ignored. So, the post-commit.ps1 script uses the action to identify what needs to be executed. In case of Bicep, the build step includes linting as well as build. Therefore, we can combine that into a validation or linting step and perform deployment only if the linting completes successfully.\nThis script assumes that you have Bicep and Azure CLI installed and you have already completed Azure device login on the system where this git hook runs.\nWe need to save this PowerShell script in the same folder as the post-commit shell script. Now, we can get to the real work. We need a Bicep template and a bicepconfig.json to enforce linting rules. For the purpose of this simple experiment, we will create a template to provision a storage account.\n1 2 3 4 5 6 7 8 9 10 11 12 13 param resLocation string = resourceGroup().location resource stg \u0026#39;Microsoft.Storage/storageAccounts@2021-02-01\u0026#39; = { name: \u0026#39;bicepstg003\u0026#39; location: resLocation kind: \u0026#39;StorageV2\u0026#39; sku: { name: \u0026#39;Standard_LRS\u0026#39; } properties: { accessTier: \u0026#39;Cool\u0026#39; } } This template adheres to all the following linter rules and therefor we should not see any errors during the linting / build process.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 { \u0026#34;analyzers\u0026#34;: { \u0026#34;core\u0026#34;: { \u0026#34;enabled\u0026#34;: true, \u0026#34;verbose\u0026#34;: false, \u0026#34;rules\u0026#34;: { \u0026#34;no-loc-expr-outside-params\u0026#34;: { \u0026#34;level\u0026#34;: \u0026#34;error\u0026#34; }, \u0026#34;no-hardcoded-location\u0026#34;: { \u0026#34;level\u0026#34;: \u0026#34;error\u0026#34; }, \u0026#34;no-unused-params\u0026#34;: { \u0026#34;level\u0026#34;: \u0026#34;error\u0026#34; }, \u0026#34;no-unused-vars\u0026#34;: { \u0026#34;level\u0026#34;: \u0026#34;error\u0026#34; }, \u0026#34;prefer-unquoted-property-names\u0026#34;:{ \u0026#34;level\u0026#34;: \u0026#34;error\u0026#34; }, \u0026#34;explicit-values-for-loc-params\u0026#34;: { \u0026#34;level\u0026#34;: \u0026#34;error\u0026#34; }, \u0026#34;prefer-interpolation\u0026#34;: { \u0026#34;level\u0026#34;: \u0026#34;error\u0026#34; }, \u0026#34;simplify-interpolation\u0026#34;: { \u0026#34;level\u0026#34;: \u0026#34;error\u0026#34; }, \u0026#34;secure-parameter-default\u0026#34;:{ \u0026#34;level\u0026#34;: \u0026#34;error\u0026#34; } } } } } Once you have all these files created, it is time to perform our first commit.\nThis is a simple technique. You can extend this further to publish Bicep templates as template specs or to a module registry.\n","date":"2022-07-09T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/2022-07-09-local-ci-cd-for-your-bicep-templates/","series":"Azure Bicep","title":"Local CI/CD for your Bicep templates"},{"categories":["vscode","containers","azure","bicep"],"content":"Starting with Bicep version 0.5.6, Microsoft added support for consuming from a public module repository hosted on Microsoft Container Registry (MCR). To consume a module from Microsoft’s pubic registry, you need to use the following syntax.\n1 module \u0026lt;module-id\u0026gt; \u0026#39;br/public:\u0026lt;module-group\u0026gt;/\u0026lt;module-name\u0026gt;:\u0026lt;version\u0026gt;\u0026#39; = { ... } br/public indicates that you want to consume the module hosted on MCR. Here is an example of creating a Azure virtual network using the virtual-network module. The version tag is mandatory.\n1 2 3 4 5 6 7 8 9 module samplevNet \u0026#39;br/public:network/virtual-network:1.0.1\u0026#39; = { name: \u0026#39;${uniqueString(deployment().name, \u0026#39;EastUS\u0026#39;)}-vnet\u0026#39; params: { name: \u0026#39;vnet-bicep-sample\u0026#39; addressPrefixes: [ \u0026#39;10.0.0.0/24\u0026#39; ] } } An index of modules available in this public registry can be seen at Azure/bicep-registry-modules: Bicep registry modules (github.com). There is a better way to find out the list of modules from the Microsoft\u0026rsquo;s public registry.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 [CmdletBinding()] param ( ) $mcrUrl = \u0026#39;https://mcr.microsoft.com/v2/_catalog\u0026#39; $allRepos = (Invoke-RestMethod -Uri $mcrUrl -ErrorAction SilentlyContinue).repositories $moduleList = [System.Collections.ArrayList]@() if ($allRepos) { $bicepRepos = $allRepos.Where({$_.StartsWith(\u0026#39;bicep\u0026#39;)}) foreach ($bicepRepo in $bicepRepos) { $tagsUrl = (\u0026#34;https://mcr.microsoft.com/v2/{0}/tags/list\u0026#34; -f $bicepRepo) $tagsObject = Invoke-RestMethod -Uri $tagsUrl $null = $moduleList.Add([PSCustomObject]@{ Name = $tagsObject.Name Tags = $tagsObject.tags }) } } return $moduleList You can save this script to a local folder and invoke it at the command line.\n1 2 3 4 5 6 7 8 9 PS C:\\scratch\u0026gt; .\\get-bicepmodule.ps1 Name Tags ---- ---- bicep/samples/hello-world {1.0.1, 1.0.2} bicep/samples/array-loop {1.0.1} bicep/compute/availability-set {1.0.1} bicep/deployment-scripts/import-acr {1.0.1} bicep/network/virtual-network {1.0.1} There are not many modules in the public registry today. However, you can contribute modules to registry yourself.\n","date":"2022-04-19T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/working-with-azure-bicep-public-modules-registry/","series":"Azure Bicep","title":"Working with Azure Bicep Public modules registry"},{"categories":["vscode","containers","azure","bicep"],"content":"Visual Studio Code development containers are magic! With the help of VS Code Remote - Containers extension and Docker Desktop, you can run a full-featured development environment within a container.\nImage source: Microsoft\nContainers, in general, help you package application code and all the dependencies. Devcontainers take advantage of this and help you share the development environment with other team members, and all have the same base setup for developing your project code. This eliminates any issues related to the development environment since your team will always use the same environment provided by a devcontainer.\nNote: This is not a development containers fundamentals article.\nA devcontainer can be created from a pre-built container image, Dockerfile, or Docker compose file. To start with, you can add the following JSON as devcontainer.json under the .devcontainer folder in your source code workspace.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 { \u0026#34;image\u0026#34;: \u0026#34;mcr.microsoft.com/vscode/devcontainers/base\u0026#34;, \u0026#34;settings\u0026#34;: {}, \u0026#34;extensions\u0026#34;: [ \u0026#34;ms-vscode.azure-account\u0026#34; ], \u0026#34;features\u0026#34;: { \u0026#34;azure-cli\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34; }, \u0026#34;git\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34;, \u0026#34;ppa\u0026#34;: false } }, \u0026#34;remoteUser\u0026#34;: \u0026#34;vscode\u0026#34; } Once this file is added and you have the VS Code Remote - Container extension installed, VS Code prompts you to open the workspace in a devcontainer. If you choose to open in a devcontainer, a container image gets created from what is described in the devcontainer.json.\nI have been using devcontainers to standardize my development environment across systems and in GitHub codespaces. I have a devcontainer associated with almost every project I am working on. If you have read my earlier posts here, you will know that I am authoring a book on Azure Bicep. This book contains several Bicep examples across many chapters to demonstrate usage of Bicep language semantics and concepts. When I build examples for this book, I tend to test the example code across different versions of the Bicep. To help me quickly build the development environment for different versions of Bicep, I started using devcontainers. To simplify the preparation of devcontainer, I built a simple feature for Azure Bicep.\nDevelopment container features Devcontainer features are in preview and help you choose what tools you want to install within a devcontainer.\nImage source: Microsoft\nThe above example shows two features added to the devcontainer \u0026ndash; Azure CLI and Git. The built-in list is missing Bicep CLI. However, it is not too difficult to build one yourself. You can use the template provided by Microsoft and add more features.\nHere is what I did for adding Azure Bicep as a feature.\ndevcontainer-features.json First, you need the meta data that describes your feature. You need to name it devcontainer-features.json.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 { \u0026#34;features\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;azurebicep\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Azure Bicep\u0026#34;, \u0026#34;options\u0026#34;: { \u0026#34;version\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;default\u0026#34;: \u0026#34;latest\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Specify a version of the Azure Bicep CLI\u0026#34; } }, \u0026#34;entrypoint\u0026#34;: \u0026#34;/usr/installbicep.sh\u0026#34; } ] } You can add multiple features to the same JSON and access what you need by using the feature ID. This example shows only Azure Bicep as the feature. It has one parameter (options object). The Azure Bicep feature has version option to choose which version of Bicep you want to install and defaults to latest version. The entrypoint property specifies the script to call when build the devcontainer and extensions property specifies the VS Code extensions to install within the devcontainer.\ninstall.sh Once the devcontainer-features.json is ready, you need an install.sh file to configure the devcontainer with necessary settings \u0026ndash; in this case, installing Azure Bicep CLI.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 #!/bin/bash set -e set -a . ./devcontainer-features.env set +a if [ ! -z ${_BUILD_ARG_AZUREBICEP} ]; then echo \u0026#34;Activating feature \u0026#39;Azure Bicep\u0026#39;\u0026#34; # Azure Bicep CLI version CLIVERSION=${_BUILD_ARG_AZUREBICEP_VERSION:-\u0026#34;latest\u0026#34;} if [ \u0026#34;${CLIVERSION}\u0026#34; = \u0026#34;latest\u0026#34; ]; then CLIURL=\u0026#34;https://github.com/Azure/bicep/releases/latest/download/bicep-linux-x64\u0026#34; else CLIURL=\u0026#34;https://github.com/Azure/bicep/releases/download/${CLIVERSION}/bicep-linux-x64\u0026#34; fi sudo tee /usr/installbicep.sh \u0026gt; /dev/null \\ \u0026lt;\u0026lt; EOF sudo curl -Lo /usr/local/bin/bicep ${CLIURL} sudo chmod +x /usr/local/bin/bicep EOF sudo chmod +x /usr/installbicep.sh fi Similar to the devcontainer-features.json, you can have the installation logic for multiple features. If a feature ID is specified in the devcontainer.json, then the _BUILD_ARG_\u0026lt;FEATURE-ID\u0026gt; will be set in the devcontainer environment. This triggers the logic to create the necessary endpoint mentioned in the feature definition.\nFeature release workflow Finally, you need a workflow that creates a release of your devcontainer feature. You can take this from Microsoft\u0026rsquo;s dev-container-features-template repository. As soon as you create a release tag, this workflow triggers a release and uploads the artifact. This artifact gets downloaded during the devcontainer build process.\nConsuming a devcontainer feature You have already seen an example at the beginning. Adding your own feature to the devcontainer.json is not much different. Here is a complete example from the bicepbookexamples repository where I use this actively.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 { \u0026#34;image\u0026#34;: \u0026#34;mcr.microsoft.com/vscode/devcontainers/base\u0026#34;, \u0026#34;settings\u0026#34;: {}, \u0026#34;extensions\u0026#34;: [ \u0026#34;ms-vscode.azure-account\u0026#34; ], \u0026#34;features\u0026#34;: { \u0026#34;rchaganti/vsc-devcontainer-features/azurebicep\u0026#34; : { \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34; }, \u0026#34;azure-cli\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34; }, \u0026#34;git\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34;, \u0026#34;ppa\u0026#34;: false } }, \u0026#34;remoteUser\u0026#34;: \u0026#34;vscode\u0026#34; } You can open this repository in VS Code and then use the Remote-Containers: Rebuild and Reopen in container command.\nBuilding the devcontainer image may take a few minutes since the features need to get downloaded and installed. Once the container is ready, you can check the presence of all features you specified in the devcontainer.json by running the following commands.\n1 2 3 git --version az version bicep --version One disadvantage of this method (using devcontainer features) is the size of the container image. The more features you add to the devcontainer.json, the larger the size of the image. This is because each feature becomes a layer in the image.\nMy new book Azure Bicep - Zero to Hero covers more about VS Code Development Containers and collaborating as a team.\n","date":"2022-03-20T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/bicep-feature-in-vscode-devcontainer/","series":"Azure Bicep","title":"Azure Bicep as a feature in VS Code development containers"},{"categories":["Azure","Azure Policy"],"content":"In the last part of this series around Azure Policy Guest Configuration, you got an introduction to what is Guest Configuration. In this part, you will learn about assigning Guest Configuration policies and initiatives.\nAzure Policy Guest Configuration (GC) is an extension to Azure Policy and therefore use the same JSON definition structure. For example, here is the (redacted) GC definition for the policy Audit Windows machines that do not have the specified Windows PowerShell execution policy.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 { \u0026#34;properties\u0026#34;: { \u0026#34;displayName\u0026#34;: \u0026#34;Audit Windows machines that do not have the specified Windows PowerShell execution policy\u0026#34;, \u0026#34;policyType\u0026#34;: \u0026#34;BuiltIn\u0026#34;, \u0026#34;mode\u0026#34;: \u0026#34;Indexed\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Requires that prerequisites are deployed to the policy assignment scope. For details, visit https://aka.ms/gcpol. Machines are non-compliant if the Windows PowerShell command Get-ExecutionPolicy returns a value other than what was selected in the policy parameter.\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;category\u0026#34;: \u0026#34;Guest Configuration\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;3.0.0\u0026#34;, \u0026#34;requiredProviders\u0026#34;: [ \u0026#34;Microsoft.GuestConfiguration\u0026#34; ], \u0026#34;guestConfiguration\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;WindowsPowerShellExecutionPolicy\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1.*\u0026#34;, \u0026#34;configurationParameter\u0026#34;: { \u0026#34;ExecutionPolicy\u0026#34;: \u0026#34;[PowerShellExecutionPolicy]PowerShellExecutionPolicy1;ExecutionPolicy\u0026#34; } } }, \u0026#34;parameters\u0026#34;: { \u0026#34;IncludeArcMachines\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;String\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;displayName\u0026#34;: \u0026#34;Include Arc connected servers\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;By selecting this option, you agree to be charged monthly per Arc connected machine.\u0026#34;, \u0026#34;portalReview\u0026#34;: \u0026#34;true\u0026#34; }, \u0026#34;allowedValues\u0026#34;: [ \u0026#34;true\u0026#34;, \u0026#34;false\u0026#34; ], \u0026#34;defaultValue\u0026#34;: \u0026#34;false\u0026#34; }, \u0026#34;ExecutionPolicy\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;String\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;displayName\u0026#34;: \u0026#34;PowerShell Execution Policy\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;The expected PowerShell execution policy.\u0026#34; }, \u0026#34;allowedValues\u0026#34;: [ \u0026#34;AllSigned\u0026#34;, \u0026#34;Bypass\u0026#34;, \u0026#34;Default\u0026#34;, \u0026#34;RemoteSigned\u0026#34;, \u0026#34;Restricted\u0026#34;, \u0026#34;Undefined\u0026#34;, \u0026#34;Unrestricted\u0026#34; ] }, \u0026#34;effect\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;String\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;displayName\u0026#34;: \u0026#34;Effect\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Enable or disable the execution of this policy\u0026#34; }, \u0026#34;allowedValues\u0026#34;: [ \u0026#34;AuditIfNotExists\u0026#34;, \u0026#34;Disabled\u0026#34; ], \u0026#34;defaultValue\u0026#34;: \u0026#34;AuditIfNotExists\u0026#34; } }, \u0026#34;policyRule\u0026#34;: { \u0026#34;if\u0026#34;: { \u0026#34;anyOf\u0026#34;: [ { \u0026#34;allOf\u0026#34;: [ { \u0026#34;field\u0026#34;: \u0026#34;type\u0026#34;, \u0026#34;equals\u0026#34;: \u0026#34;Microsoft.Compute/virtualMachines\u0026#34; } ] } ] }, \u0026#34;then\u0026#34;: { \u0026#34;effect\u0026#34;: \u0026#34;[parameters(\u0026#39;effect\u0026#39;)]\u0026#34;, \u0026#34;details\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;Microsoft.GuestConfiguration/guestConfigurationAssignments\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;[concat(\u0026#39;WindowsPowerShellExecutionPolicy$pid\u0026#39;, uniqueString(policy().assignmentId, policy().definitionReferenceId))]\u0026#34;, \u0026#34;existenceCondition\u0026#34;: { \u0026#34;allOf\u0026#34;: [ { \u0026#34;field\u0026#34;: \u0026#34;Microsoft.GuestConfiguration/guestConfigurationAssignments/complianceStatus\u0026#34;, \u0026#34;equals\u0026#34;: \u0026#34;Compliant\u0026#34; }, { \u0026#34;field\u0026#34;: \u0026#34;Microsoft.GuestConfiguration/guestConfigurationAssignments/parameterHash\u0026#34;, \u0026#34;equals\u0026#34;: \u0026#34;[base64(concat(\u0026#39;[PowerShellExecutionPolicy]PowerShellExecutionPolicy1;ExecutionPolicy\u0026#39;, \u0026#39;=\u0026#39;, parameters(\u0026#39;ExecutionPolicy\u0026#39;)))]\u0026#34; } ] } } } } }, \u0026#34;id\u0026#34;: \u0026#34;/providers/Microsoft.Authorization/policyDefinitions/c648fbbb-591c-4acd-b465-ce9b176ca173\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Authorization/policyDefinitions\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;c648fbbb-591c-4acd-b465-ce9b176ca173\u0026#34; } As you see in this definition, the overall document structure is same as an Azure Policy definition. However, the category in the metadata section at the top identifies this as a GC policy. This is a built-in GC policy definition.\nWorking with GC Policies The following examples in this section use Azure CLI to work with GC policies. The az policy command helps you in working with policy definitions.\n1 az policy definition list --query \u0026#34;[?metadata.category==\u0026#39;Guest Configuration\u0026#39;].{DisplayName:displayName, Mode:mode, PolicyType:policyType, Name:name}\u0026#34; --output json This command filters policy definitions to only GC policies and then returns JSON output containing only a few selected properties. Here is the sample output.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 { \u0026#34;DisplayName\u0026#34;: \u0026#34;[Deprecated]: Show audit results from Windows VMs configurations in \u0026#39;Security Options - Microsoft Network Client\u0026#39;\u0026#34;, \u0026#34;Mode\u0026#34;: \u0026#34;All\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;fcbc55c9-f25a-4e55-a6cb-33acb3be778b\u0026#34;, \u0026#34;PolicyType\u0026#34;: \u0026#34;BuiltIn\u0026#34; }, { \u0026#34;DisplayName\u0026#34;: \u0026#34;[Deprecated]: Show audit results from Linux VMs that do not have the specified applications installed\u0026#34;, \u0026#34;Mode\u0026#34;: \u0026#34;All\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;fee5cb2b-9d9b-410e-afe3-2902d90d0004\u0026#34;, \u0026#34;PolicyType\u0026#34;: \u0026#34;BuiltIn\u0026#34; }, { \u0026#34;DisplayName\u0026#34;: \u0026#34;Azure Stack HCI Best Practices\u0026#34;, \u0026#34;Mode\u0026#34;: \u0026#34;Indexed\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;Azure Stack HCI Best Practices\u0026#34;, \u0026#34;PolicyType\u0026#34;: \u0026#34;Custom\u0026#34; } As you see in the output, you can see there are built-in and custom GC policies. The custom policies are what you as an end user can create and publish to your Azure subscription. Also, there are different modes \u0026ndash; All and Indexed. The policies that are defined as all mode, the policy gets evaluated for all resource groups, subscriptions, and all resource types. When the policy is in indexed mode, it gets evaluated only for resources types that support tags and location.\nTo retrieve a single policy definition, you can use the az policy definition show command.\n1 az policy definition show -n \u0026#34;e068b215-0026-4354-b347-8fb2766f73a2\u0026#34; You can assign a GC policy using the az policy assignment command.\nNote: There is a guestconfig extension that you can use to manage guest configuration assignments. However, I prefer az policy.\n1 az policy assignment create --name \u0026#34;testAssignment\u0026#34; --policy \u0026#34;e068b215-0026-4354-b347-8fb2766f73a2\u0026#34; -g azconf The argument to the --policy parameter is the value of name property you saw in the output of all policy definitions. Once this assignment is complete, you can see it in the portal.\nYou can trigger a policy compliance run using the az policy state command.\n1 az policy state trigger-scan -g azconf This will trigger the compliance run and wait for the task to return. If you do not want to block console until the task is complete, add --no-wait parameter.\nYou can check the policy assignment compliance state using the az policy state summarize command.\nWorking with GC initiatives Initiatives are groups of policy definition. Initiatives help group related policy definitions and assign them at once to a Azure scope. You can list all available GC initiatives by running the following command.\n1 2 az policy set-definition list --query \u0026#34;[?metadata.category==\u0026#39;Guest Configuration\u0026#39;].{DisplayName:displayName, PolicyType:pol icyType, Name:name}\u0026#34; --output json This returns output similar to what is shown below.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 { \u0026#34;DisplayName\u0026#34;: \u0026#34;[Deprecated]: Audit Windows VMs on which the Log Analytics agent is not connected as expected\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;06c5e415-a662-463a-bb85-ede14286b979\u0026#34;, \u0026#34;PolicyType\u0026#34;: \u0026#34;BuiltIn\u0026#34; }, { \u0026#34;DisplayName\u0026#34;: \u0026#34;Audit machines with insecure password security settings\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;095e4ed9-c835-4ab6-9439-b5644362a06c\u0026#34;, \u0026#34;PolicyType\u0026#34;: \u0026#34;BuiltIn\u0026#34; }, { \u0026#34;DisplayName\u0026#34;: \u0026#34;Deploy prerequisites to enable Guest Configuration policies on virtual machines\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;12794019-7a00-42cf-95c2-882eed337cc8\u0026#34;, \u0026#34;PolicyType\u0026#34;: \u0026#34;BuiltIn\u0026#34; } You can now assign an GC initiative using the az policy assignment command and using --policy-set-definition parameter.\n1 az policy assignment create --name \u0026#34;testInitiativeAssignment\u0026#34; --policy-set-definition \u0026#34;095e4ed9-c835-4ab6-9439-b5644362a06c\u0026#34; -g azconf Checking the compliance state of an initiative assignment is similar to how you did that for policies.\nThis is all for today and in the next part of this series, you will learn about writing custom GC policies.\n","date":"2021-08-01T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/azure-policy-guest-configuration-assignments/","series":"Azure Policy Guest Configuration","title":"Azure Policy Guest Configuration - Assignments"},{"categories":["Azure","Bicep","book"],"content":" Many are struggling to make ends meet ever since the pandemic started. Therefore, I decided that all proceeds from this book will go to charity.\nProject Bicep is undoubtedly the best thing that happened in the Azure world, and I have been following the development and using Bicep quite often. I published a series of articles on this blog around Bicep. However, there is always more. This series gave me an idea about a complete book on Bicep. And, today, I am announcing this new book project - Azure Bicep - Zero to Hero. This book is still a work in progress and 30% complete. I will be spending a few more weekends completing the rest of the chapters. You can read it as I complete and provide your feedback. Since this is a self-published book, all updates to the book will be free forever.\nAbout the book Azure Resource Manager (ARM) brought in a new deployment model to Microsoft Azure. Unlike the classic deployment model, ARM looks at groups of resources and manages the life cycle of these resource groups from end to end. ARM simplified how you built and managed services on Azure. Along with the ARM deployment model, Microsoft also introduced a new way of provisioning Azure services through Azure Resource Manager templates. These templates are based on JSON data representation and provided a declarative way to define your Azure infrastructure. ARM templates offer a great way to automate infrastructure provisioning and integrate well into the infrastructure as code practices. However, using JSON data representation for ARM template language makes it too complex to read and write more extensive infrastructure definitions. For users getting started with Azure infrastructure deployments, this can be a nightmare.\nEnter Azure Bicep. Bicep language is a transparent abstraction over ARM template language. Azure Bicep files traspile to ARM templates. Bicep labguage is easy to learn and very simple to read and write. Being a transparent abstraction on top of ARM templates, Bicep supports the same resource types and properties. By building on top of resource specification API as the backend, Bicep enables day-zero support for any new resource introduced as an Azure service.\nThis book focuses on deploying and managing Azure infrastructure with Bicep and covers everything that you need to know right from basics to the advanced usage of Bicep language to create complex Azure infrastructure configurations and implementing continuous pipelines for your Azure infrastructure configurations.\nThis book covers:\nIntroduction to Bicep Getting started with Bicep Working with resources Creating reusable Bicep files Variables and Expressions in Bicep Loops and Conditions in Bicep Creating and consuming Bicep modules Using Deployment scripts and Templates Spec with Bicep Testing and implementing CI/CD pipelines for your Azure infrastructure with Bicep A Bicep built-in function primer Overall, this book takes your Azure Bicep knowledge from zero to hero and helps you master ARM template authoring using Bicep language.\n","date":"2021-06-06T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/book-announcing-azure-bicep-zero-to-hero/","series":"Azure Bicep","title":"[Book] - Annoucing Azure Bicep - Zero to Hero"},{"categories":["Azure","Bicep"],"content":" Bicep v0.4.1 was released yesterday. This release includes a bunch of bug fixes, more snippets, and most importantly, a linter!\nLinters bring in a variety of capabilities to a developer toolkit. These tools can help you perform static code analysis, check compliance against a style guide, find syntax errors, and flag potential optimizations in the code.\nFor a developer, linting support in an IDE is a must. With this Bicep release, Microsoft added support for linting of Bicep files in VS Code and at the command line. To realize the benefits of linting, you must upgrade Bicep CLI and the VS Code extension to v4.0.1 or later. The current set of linter rules are minimal and taken from arm-ttk test cases. Both VS Code extension and Bicep CLI check for all available rules by default and all rules are set at warning level. Based on the level of a rule, you will see errors or warnings or informational messages within the editor.\nWhen possible, you can even apply a quick fix.\nCustomizing linter behavior The best part of this feature is the ability to customize the linter behavior. You can do this by placing biceconfig.json with the right set of properties at the same level as your Bicep file or the closest or top-level of the folder tree. Here is an example. Observe where the bicepconfig.json is and where the Bicep file is.\nUsing this configuration file, you can enable or disable linting, supply rule-specific values, and set the level of rules as well. What\u0026rsquo;s more \u0026ndash; the Bicep VS Code extension even provides intellisense for this configuration file.\nYou get to see all available rules as you start customizing the Bicep linting configuration.\nYou will get help in checking if there are rule-specific values you can provide.\nHere is my customized linter configuration.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 { \u0026#34;analyzers\u0026#34;: { \u0026#34;core\u0026#34;: { \u0026#34;verbose\u0026#34;: false, \u0026#34;enabled\u0026#34;: true, \u0026#34;rules\u0026#34;: { \u0026#34;no-hardcoded-env-urls\u0026#34;: { \u0026#34;level\u0026#34;: \u0026#34;warning\u0026#34; }, \u0026#34;no-unused-params\u0026#34; : { \u0026#34;level\u0026#34;: \u0026#34;error\u0026#34; }, \u0026#34;no-unused-vars\u0026#34; : { \u0026#34;level\u0026#34;: \u0026#34;error\u0026#34; }, \u0026#34;prefer-interpolation\u0026#34; : { \u0026#34;level\u0026#34;: \u0026#34;warning\u0026#34; }, \u0026#34;secure-parameter-default\u0026#34; : { \u0026#34;level\u0026#34;: \u0026#34;error\u0026#34; }, \u0026#34;simplify-interpolation\u0026#34; :{ \u0026#34;level\u0026#34;: \u0026#34;warning\u0026#34; } } } } } If you update the bicepconfig.json while a Bicep file is open in the editor, you won\u0026rsquo;t see the updated linting behavior until you close and re-open the Bicep file.\nIf you want to disable linting, set enabled to false.\nDepending on where you are using the linter, different levels of rule might mean different things. (Source: bicep/linter.md at main · Azure/bicep (github.com))\nlevel Build-time behavior Editor behavior Error Violations appear as Errors in command-line build output, and cause builds to fail. Offending code is underlined with a red squiggle and appears in Problems tab. Warning Violations appear as Warnings in command-line build output, but do not cause builds to fail. Offending code is underlined with a yellow squiggle and appears in Problems tab. Info Violations do not appear in command-line build output. Offending code is underlined with a blue squiggle and appears in Problems tab. Off Suppressed completely. Suppressed completely. 1 2 C:\\sandbox\\bicep\\modules\u0026gt;bicep build storageAccount.bicep C:\\sandbox\\bicep\\modules\\storageAccount.bicep(4,9) : Error simplify-interpolation: Remove unnecessary string interpolation. [https://aka.ms/bicep/linter/simplify-interpolation] With the ability to lint at command line as well, you can integrate these checks as a part of your CI/CD pipelines.\nLinting in GitHub actions As Bicep CLI supports linting at the command-line, it is easy to reflect the state of your Bicep with regards to the linting rules in a CI / CD pipeline. You can use a GitHub action to attempt a bicep build, and errors will fail the pipeline.\nHere is a workflow file that I am using.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 name: ARM Template Build and Test on: push: branches: - main workflow_dispatch: jobs: build_and_test: name: Build ARM JSON Template runs-on: ubuntu-latest steps: # Check out - name: Checkout the repo uses: actions/checkout@v2 # Set up Bicep - name: Setup Bicep uses: anthony-c-martin/setup-bicep@v0.1 # Build ARM JSON template - name: Build Bicep run: bicep build ./main.bicep Overall, I like this new feature. I am looking forward to writing my own rules at some point in time in the future. I do have a wish list around linting.\nSupport linting as a command-line option in Bicep CLI. For example, bicep lint main.bicep. Something like this will be good as part of the CI/CD process where I intend to verify if the Bicep file matches my style guide or not necessarily attempt a compile. Support for structured output (JSON/XML) from the linter. For example, bicep lint main.bicep --output file.json. This structured output will help generate reports when a failure occurs. ","date":"2021-06-02T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/bicep-v0-4-and-linter/","series":"Azure Bicep","title":"Bicep v0.4 and the linter"},{"categories":["Azure","Bicep","Docker","Containers"],"content":" While using the Azure CLI container image for one of my side projects, I started looking for an image that contains Bicep CLI as well. I found Mikolaj Mackowiak\u0026rsquo;s Bicep-cli image. This was what I really needed and it includes both Azure CLI and Bicep CLI. However, just as a fun side project, I wanted to publish my own image as well.\nI started with Alpine base image and started building everything needed for Azure CLI and Bicep CLI. However, as I started building all dependencies, the size of this image grew from 5MB to ~1GB. This is when I switched the base layer to Microsoft\u0026rsquo;s Azure CLI image and just added Bicep as a layer.\nHere is how my Dockerfile looks now.\n1 2 3 4 5 6 FROM mcr.microsoft.com/azure-cli:latest LABEL maintainer=\u0026#34;ravikanth@ravichaganti.com\u0026#34; RUN curl -Lo bicep.bin https://github.com/Azure/bicep/releases/latest/download/bicep-linux-musl-x64 \\ \u0026amp;\u0026amp; chmod +x ./bicep.bin \\ \u0026amp;\u0026amp; mv ./bicep.bin /usr/local/bin/bicep Update (05/21) - Optimized the Dockerfile to reduce number of image layers.\nThis is available in my Bicephelpers GitHub repository and I have a GitHub action that builds this image and pushes to Docker Hub.\nYou can pull this image using the following command line.\n1 docker image pull ravikanth/bicep:latest I will continue to update this whenever Azure CLI or Bicep CLI get updated.\n","date":"2021-05-20T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/bicep-container-image/","series":"Azure Bicep","title":"Bicep Container Image"},{"categories":["Azure","Bicep"],"content":" I wrote a series of articles to introduce Bicep language fundamentals. I really enjoyed working on these articles and learnt quite a lot in that process.\nThe Bicep team released version 0.3.539 of Bicep command line as well as the VS Code extension a few hours ago and it came with a lot of goodness \u0026ndash; bug fixes as well as new shiny things! One of the first things that caught my eye was the Bicep visualizer.\nIf you are looking for a way to upgrade your local version of Bicep command line, you can run the installBicep.ps1 script from my bicephelpers GitHub repo. The VS Code extension should auto-update.\nOnce you have both these updates in place, you can open a Bicep program in VS Code editor and hit CTRL+SHIFT+P to bring up the Bicep visualizer. Here is how it looks. I chose the nested VMs Bicep template for this.\nInstead of opening a new editor window for the visualization, you can choose to open it to the side of the existing editor window.\nThis is a good start. Here is what on my wish list for this.\nAdd support for exporting the generated visualization to a PNG or other format. Navigating to a location in the Bicep program or resource definition when I click on a resource in the visualization Resources represented with the actual Azure icons What is your wish list?\n","date":"2021-05-15T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/bicep-visualizer/","series":"Azure Bicep","title":"Bicep Visualizer"},{"categories":["Azure","Bicep"],"content":" When using ARM JSON templates, you can deploy to any of the four supported scopes \u0026ndash; management groups, tenants, subscriptions, and resource groups. What you can deploy using ARM JSON templates differs between these different target scopes. For example, you can create resource groups at the subscription level only. You identify the target scope for an ARM JSON template using the $schema element in the template. Each scope has a specific schema.\nBicep language too supports specifying scope within the bicep files using the targetScope keyword. This keyword takes one or more values \u0026ndash; resourceGroup, tenant, subscription, and managementGroup.\nWhen the targetScope value is set, Bicep knows what resources to expect within the file.\nHere is an example of using the resource group resource in a subscription scoped Bicep file.\n1 2 3 4 5 6 targetScope = \u0026#39;subscription\u0026#39; resource rg \u0026#39;Microsoft.Resources/resourceGroups@2020-06-01\u0026#39; = { name: \u0026#39;rgTest\u0026#39; location: \u0026#39;WestUS\u0026#39; } This, when compiled, produces the following ARM JSON representation.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 { \u0026#34;$schema\u0026#34;: \u0026#34;https://schema.management.azure.com/schemas/2018-05-01/subscriptionDeploymentTemplate.json#\u0026#34;, \u0026#34;contentVersion\u0026#34;: \u0026#34;1.0.0.0\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;_generator\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;bicep\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;0.3.255.40792\u0026#34;, \u0026#34;templateHash\u0026#34;: \u0026#34;12733481855883894499\u0026#34; } }, \u0026#34;functions\u0026#34;: [], \u0026#34;resources\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Resources/resourceGroups\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;2020-06-01\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;rgTest\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;WestUS\u0026#34; } ] } This is how you deploy a subscription level template using Azure CLI.\n1 az deployment sub create --name rgDeployment --template-file .\\main.bicep --location WestUS You can mix scopes in a Bicep file by explicitly specifying scope property in a resource instance or module. Here is an example.\n1 2 3 4 5 6 7 8 9 10 11 12 13 targetScope = \u0026#39;subscription\u0026#39; resource rgTest \u0026#39;Microsoft.Resources/resourceGroups@2021-01-01\u0026#39; = { name: \u0026#39;rgTest\u0026#39; location: \u0026#39;West US\u0026#39; } module saDeploy \u0026#39;modules/storageAccount.bicep\u0026#39; = { name: \u0026#39;saDeploy\u0026#39; scope: resourceGroup(\u0026#39;rgTest\u0026#39;) params: { saCount: 3 } } In the module specification, the scope property is set to the resource group being created in this template and that creates a dependency on the resource group as well.\nThis is a quick overview of scoped deployment template generation using Bicep. Hope you enjoyed reading this series of articles. This is the final article in this series and I will write more deep dive articles later. If you have suggestions for any specific content around Bicep, feel free to leave a comment.\n","date":"2021-04-20T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/bicep-basics-beyond-basics-scoped-deployment/","series":["Azure Bicep"],"title":"Bicep Language - Beyond Basics - Scoped Deployment"},{"categories":["Azure","Bicep"],"content":" When you go to the Azure portal and deploy a service, you may have seen a prompt where you are asked to select an existing resource group or create a new resource group. And, a few more service deployment scenarios might ask you to select between selecting an existing or creating a new storage account.\nWhat if you want to achieve this in your own ARM templates? You can. In ARM templates, you use the condition element within resource instance definition. Bicep supports this using the if keyword. Here is the general syntax for defining conditional deployment of a resource instance.\n1 2 3 4 5 6 7 resource Identifier \u0026#39;Microsoft.Provider/Type@Version\u0026#39; if (condition) = { name: location: properties: { } } Unlike iterative deployments syntax, you don\u0026rsquo;t have to enclose the resource instance in square brackets. Within the if condition, you can use the comparison or logical operators supported in Bicep. Here is an example of a conditional deployment.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 @allowed([ \u0026#39;new\u0026#39; \u0026#39;existing\u0026#39; ]) param newOrExisting string = \u0026#39;new\u0026#39; param storageAccountName string resource stg \u0026#39;Microsoft.Storage/storageAccounts@2019-06-01\u0026#39; = if (newOrExisting == \u0026#39;new\u0026#39;) { name: storageAccountName location: resourceGroup().location sku: { name: \u0026#39;Standard_LRS\u0026#39; } kind: \u0026#39;StorageV2\u0026#39; properties: { accessTier: \u0026#39;Cool\u0026#39; } } In the above example, the parameter newOrExisting is used to collect input on whether a new storage account needs to be created or not. The value of this parameter gets checked in the if condition within the resource instance definition. Only if the value supplied is new, a new storage account resource gets provisioned. The following ARM JSON is what this example produces.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 { \u0026#34;$schema\u0026#34;: \u0026#34;https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#\u0026#34;, \u0026#34;contentVersion\u0026#34;: \u0026#34;1.0.0.0\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;_generator\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;bicep\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;0.3.255.40792\u0026#34;, \u0026#34;templateHash\u0026#34;: \u0026#34;839898532623422150\u0026#34; } }, \u0026#34;parameters\u0026#34;: { \u0026#34;newOrExisting\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;new\u0026#34;, \u0026#34;allowedValues\u0026#34;: [ \u0026#34;new\u0026#34;, \u0026#34;existing\u0026#34; ] }, \u0026#34;storageAccountName\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; } }, \u0026#34;functions\u0026#34;: [], \u0026#34;resources\u0026#34;: [ { \u0026#34;condition\u0026#34;: \u0026#34;[equals(parameters(\u0026#39;newOrExisting\u0026#39;), \u0026#39;new\u0026#39;)]\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Storage/storageAccounts\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;2019-06-01\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;[parameters(\u0026#39;storageAccountName\u0026#39;)]\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[resourceGroup().location]\u0026#34;, \u0026#34;sku\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Standard_LRS\u0026#34;, \u0026#34;tier\u0026#34;: \u0026#34;Standard\u0026#34; }, \u0026#34;kind\u0026#34;: \u0026#34;StorageV2\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;accessTier\u0026#34;: \u0026#34;Hot\u0026#34; } } ] } Conditional deployments can be used with modules as well. Here is an example of a module for creating a storage account.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 param storageAccountName string resource sa \u0026#39;Microsoft.Storage/storageAccounts@2019-04-01\u0026#39; = { name: storageAccountName location: resourceGroup().location sku: { name: \u0026#39;Standard_LRS\u0026#39; } kind: \u0026#39;StorageV2\u0026#39; properties: { accessTier: \u0026#39;Cool\u0026#39; } } output saId string = resourceId(\u0026#39;Microsoft.Storage/storageAccounts\u0026#39;, storageAccountName) This module gets used in a main Bicep file as shown below.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 @allowed([ \u0026#39;new\u0026#39; \u0026#39;existing\u0026#39; ]) param newOrExisting string = \u0026#39;new\u0026#39; param storageAccountName string module stgModule \u0026#39;modules/storageAccount.bicep\u0026#39; = if (newOrExisting == \u0026#39;new\u0026#39;) { name: \u0026#39;storageDeploy\u0026#39; params: { storageAccountName: storageAccountName } } The way you perform a conditional deployment of a module is same as that of a resource instance.\nCan you combine iterative and conditional deployments in Bicep? Yes, of course. Here is an example.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 @allowed([ \u0026#39;new\u0026#39; \u0026#39;existing\u0026#39; ]) param newOrExisting string = \u0026#39;new\u0026#39; param storageAccountName string module stgModule \u0026#39;modules/storageAccount.bicep\u0026#39; = [for index in range(0,3): if (newOrExisting == \u0026#39;new\u0026#39;) { name: \u0026#39;storageDeploy\u0026#39; params: { storageAccountName: \u0026#39;${storageAccountName}${index}\u0026#39; } }] As shown in the above example, the condition must follow the iteration syntax. This is it for today. In the next part of this series, you will learn about scoped deployments. Stay tuned.\n","date":"2021-04-19T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/bicep-basics-beyond-basics-conditional-deployment/","series":["Azure Bicep"],"title":"Bicep Language - Beyond Basics - Conditional Deployment"},{"categories":["Azure","Bicep"],"content":" Reusability and repeatability are two basics requirements when you want to implement Infrastructure as Code (IaC) practices. You looked at how parameters can be added to Bicep files to make those programs reusable. This is the first step towards modularizing your programs. With the ARM JSON templates, you might have used linked templates that are meant to provide similar functionality. Bicep has support for modularity and helps simplify complex configurations into smaller reusable modules.\nIn this part of the series of articles on Bicep language, you will learn about modularizing Bicep files. You already have the necessary knowledge, parameterizing Bicep files, to start this learning.\nHere is an example from one of the previous parts.\n1 2 3 4 5 6 7 8 9 10 11 12 13 param saCount int = 2 resource sa \u0026#39;Microsoft.Storage/storageAccounts@2019-04-01\u0026#39; = [for index in range(0, saCount): { name: \u0026#39;${index}stg${uniqueString(resourceGroup().id)}\u0026#39; location: resourceGroup().location sku: { name: \u0026#39;Standard_LRS\u0026#39; } kind: \u0026#39;StorageV2\u0026#39; properties: { accessTier: \u0026#39;Hot\u0026#39; } }] Add some output to this using what you learned in the previous part about using output in Bicep.\n1 output saId array = [for i in range(0, saCount): resourceId(\u0026#39;Microsoft.Storage/storageAccounts\u0026#39;, \u0026#39;${i}stg${uniqueString(resourceGroup().id)}\u0026#39;)] Now, save this as storageAccount.bicep. Congratulations. You just created a Bicep module! Now, to the important part. How do you consume this in another Bicep file?\nConsuming a module The module keyword helps in consuming a Bicep module. The syntax for consuming a Bicep module is:\n1 2 3 4 5 6 module \u0026lt;module-identifier\u0026gt; \u0026#39;\u0026lt;path-to-module-file\u0026gt;\u0026#39; = { name: \u0026lt;name-to-be-used-for-nested-deployment\u0026gt; params: { \u0026lt;param-name\u0026gt;: \u0026lt;param-value\u0026gt; } } module-identifier, similar to a resource identifier, will be used to reference the module object in other parts of the Bicep file as needed. The name property within the module will be used as a name for the nested deployment within the generated ARM template. This property is mandatory. And, then the params property will be used to supply a list of parameter values to the module.\nHere is how you can consume the storageAccount module you created earlier.\n1 2 3 4 5 6 7 8 module stgModule \u0026#39;./storageAccount.bicep\u0026#39; = { name: \u0026#39;storageDeploy\u0026#39; params: { saCount: 3 } } output stgResourceId array = stgModule.outputs.saId Save this example as main.bicep. Note that it is not necessary that you always store the module files at the same level as the main Bicep file. You can store all modules in a different folder and use that relative path. For example:\n1 2 3 4 5 6 7 8 module stgModule \u0026#39;./modules/storageAccount.bicep\u0026#39; = { name: \u0026#39;storageDeploy\u0026#39; params: { saCount: 3 } } output stgResourceId array = stgModule.outputs.saId Also, make sure you use the Unix type path. Bicep does not support Windows backslash for cross-platform compatibility reasons. You can compile the main.bicep using bicep build. Here is how the generated ARM JSON will look like.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 { \u0026#34;$schema\u0026#34;: \u0026#34;https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#\u0026#34;, \u0026#34;contentVersion\u0026#34;: \u0026#34;1.0.0.0\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;_generator\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;bicep\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;0.3.255.40792\u0026#34;, \u0026#34;templateHash\u0026#34;: \u0026#34;11285400335859160700\u0026#34; } }, \u0026#34;functions\u0026#34;: [], \u0026#34;resources\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Resources/deployments\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;2019-10-01\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;storageDeploy\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;expressionEvaluationOptions\u0026#34;: { \u0026#34;scope\u0026#34;: \u0026#34;inner\u0026#34; }, \u0026#34;mode\u0026#34;: \u0026#34;Incremental\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;saCount\u0026#34;: { \u0026#34;value\u0026#34;: 3 } }, \u0026#34;template\u0026#34;: { \u0026#34;$schema\u0026#34;: \u0026#34;https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#\u0026#34;, \u0026#34;contentVersion\u0026#34;: \u0026#34;1.0.0.0\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;_generator\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;bicep\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;0.3.255.40792\u0026#34;, \u0026#34;templateHash\u0026#34;: \u0026#34;7655284844148565855\u0026#34; } }, \u0026#34;parameters\u0026#34;: { \u0026#34;saCount\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;int\u0026#34;, \u0026#34;defaultValue\u0026#34;: 2 } }, \u0026#34;functions\u0026#34;: [], \u0026#34;resources\u0026#34;: [ { \u0026#34;copy\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;sa\u0026#34;, \u0026#34;count\u0026#34;: \u0026#34;[length(range(0, parameters(\u0026#39;saCount\u0026#39;)))]\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Storage/storageAccounts\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;2019-04-01\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;[format(\u0026#39;{0}stg{1}\u0026#39;, range(0, parameters(\u0026#39;saCount\u0026#39;))[copyIndex()], uniqueString(resourceGroup().id))]\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[resourceGroup().location]\u0026#34;, \u0026#34;sku\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Standard_LRS\u0026#34; }, \u0026#34;kind\u0026#34;: \u0026#34;StorageV2\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;accessTier\u0026#34;: \u0026#34;Hot\u0026#34; } } ], \u0026#34;outputs\u0026#34;: { \u0026#34;saId\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;array\u0026#34;, \u0026#34;copy\u0026#34;: { \u0026#34;count\u0026#34;: \u0026#34;[length(range(0, parameters(\u0026#39;saCount\u0026#39;)))]\u0026#34;, \u0026#34;input\u0026#34;: \u0026#34;[resourceId(\u0026#39;Microsoft.Storage/storageAccounts\u0026#39;, format(\u0026#39;{0}stg{1}\u0026#39;, range(0, parameters(\u0026#39;saCount\u0026#39;))[copyIndex()], uniqueString(resourceGroup().id)))]\u0026#34; } } } } } } ], \u0026#34;outputs\u0026#34;: { \u0026#34;stgResourceId\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;array\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;[reference(resourceId(\u0026#39;Microsoft.Resources/deployments\u0026#39;, \u0026#39;storageDeploy\u0026#39;), \u0026#39;2019-10-01\u0026#39;).outputs.saId.value]\u0026#34; } } } Module iterations You can use modules with iterations as well. It is straight forward to do this based on what you learned in the earlier part about iterations in Bicep.\n1 2 3 4 5 6 module \u0026lt;module-identifier\u0026gt; \u0026#39;\u0026lt;path-to-module-file\u0026gt;\u0026#39; = [for \u0026lt;item\u0026gt; in \u0026lt;collection\u0026gt;: { name: \u0026lt;name-to-be-used-for-nested-deployment\u0026gt; params: { \u0026lt;parameter-name\u0026gt;: \u0026lt;parameter-value\u0026gt; } }] You can quickly build an example that uses module iteration. Start with a module first. This module will be used to create a virtual network and a subnet within that.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 param vNetName string param addressPrefix string param subnetPrefix string resource vnet \u0026#39;Microsoft.Network/virtualNetworks@2018-11-01\u0026#39; = { name: vNetName location: resourceGroup().location properties: { addressSpace: { addressPrefixes: [ addressPrefix ] } subnets: [ { name: \u0026#39;${vNetName}-subnet\u0026#39; properties: { addressPrefix: subnetPrefix } } ] } } output vnetId string = resourceId(\u0026#39;Microsoft.Network/virtualNetworks\u0026#39;, vNetName) Save this as vnet.bicep.\nTo consume this, create a main bicep file.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 var vNet = [ { vNetName: \u0026#39;testNet\u0026#39; addressPrefix: \u0026#39;10.0.0.0/25\u0026#39; subnetPrefix: \u0026#39;10.0.0.0/27\u0026#39; } { vNetName: \u0026#39;devNet\u0026#39; addressPrefix: \u0026#39;10.0.0.0/25\u0026#39; subnetPrefix: \u0026#39;10.0.0.32/27\u0026#39; } { vNetName: \u0026#39;prodNet\u0026#39; addressPrefix: \u0026#39;10.0.0.0/25\u0026#39; subnetPrefix: \u0026#39;10.0.0.64/27\u0026#39; } ] module net \u0026#39;modules/vnet.bicep\u0026#39; = [for network in vNet: { name: \u0026#39;${network.vNetName}deployment\u0026#39; params: { vNetName: network.vNetName addressPrefix: network.addressPrefix subnetPrefix: network.subnetPrefix } }] output vNetId array = [for network in vNet: { id: resourceId(\u0026#39;Microsoft.Network/virtualNetworks\u0026#39;,network.vNetName) }] The variable vNet will hold a collection of virtual networks that need to be created. Using the collection iteration along with module declaration, you can supply the necessary parameter values needed for the module parameters.\nAs you start building complex templates, you will find modules very helpful. This is it for today. You will learn about conditional deployments in the next part of this series.\n","date":"2021-04-16T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/bicep-basics-beyond-basics-modules/","series":"Azure Bicep","title":"Bicep Language - Beyond Basics - Modules"},{"categories":["Azure","Azure Bicep","Presentations"],"content":"Today I had the opportunity to present live (virtually) at the Global Azure Bootcamp 2021. My session was around Azure Bicep. I really enjoyed preparing for and delivering this session.\nIf you follow me on Twitter or this blog, you know I\u0026rsquo;ve started a new series of articles on Azure Bicep. These articles are essentially notes from my Bicep learning sessions. There are a few more pending in the next few days, but overall, I really enjoyed learning about Bicep. This series and my session at GAB 2021 helped me share my learning with the community.\nSlideShare Presentation This presentation requires your consent to load content from SlideShare (LinkedIn), which may set tracking cookies. Enable SlideShare Presentation ID: L3mIzdQ40zCrFP I published these slides and demo scripts that I used in a GitHub repository.\n","date":"2021-04-16T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/slides-and-demo-scripts-introduction-to-azure-bicep-gab-2021/","series":"Azure Bicep","title":"Slides and Demo Scripts: Global Azure Bootcamp 2021 - Azure Bicep"},{"categories":["Azure","Bicep"],"content":" You use outputs in ARM JSON templates to return value(s) from the deployment. Returning output from a template is not mandatory, but it is useful in scenarios such as deploying templates through a CI/CD pipeline or creating templates as reusable modules. To support this, Bicep language has an outputs element.\nOutput Syntax for adding an output element to Bicep is:\n1 output \u0026lt;output-identifier\u0026gt; \u0026lt;output-type\u0026gt; = \u0026lt;literal\u0026gt; or \u0026lt;expression\u0026gt; You can add any number of outputs in a Bicep file. The output-identifier, along with the literal value or the value from the expression, will be returned after a successful deployment. As you will learn in the next part of this series, Bicep modules make use of outputs from a template deployment. The type of data that you return from output can be an integer, a string, an array, an object, or a boolean value.\nHere is a simple example that returns the storage account Id once the template deployment is complete.\n1 2 3 4 5 6 7 8 9 10 11 12 13 resource sa \u0026#39;Microsoft.Storage/storageAccounts@2019-04-01\u0026#39; = { name: \u0026#39;stg${uniqueString(resourceGroup().id)}\u0026#39; location: resourceGroup().location sku: { name: \u0026#39;Standard_LRS\u0026#39; } kind: \u0026#39;StorageV2\u0026#39; properties: { accessTier: \u0026#39;Hot\u0026#39; } } output sid string = sa.id You can use the resource accessor to assign the value of storage account resource Id to the output. In this example, sa is the resource instance name within the deployment. So, sa.id will retrieve the resource Id of the storage account.\nYou can deploy this template using az deployment group create --template-file=C:\\sandbox\\bicep\\main.bicep --resource-group bicep. The output from this command will include an outputs object.\n1 2 3 4 5 6 \u0026#34;outputs\u0026#34;: { \u0026#34;sid\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;String\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;/subscriptions/5073fd4c-3a1b-4559-8371-21e034f70820/resourceGroups/bicep/providers/Microsoft.Storage/storageAccounts/stg6axmw2qxc2xnq\u0026#34; } } Output Iterations Similar to resources and resource properties, you can use iterations with output as well. There are three types of iterations that you learned about in the previous part of this series. The syntax for using any of those three methods is similar for outputs as well.\n1 2 3 output \u0026lt;output-identifier\u0026gt; \u0026lt;output-type\u0026gt; = [for index in range(\u0026lt;start\u0026gt;, \u0026lt;stop\u0026gt;): \u0026lt;output-properties\u0026gt; ] In the last part of this series, you learned how to use loop index based iteration. The same example can be extended to add output of all storage account resource Ids. Here is how you do that.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 param saCount int = 3 resource sa \u0026#39;Microsoft.Storage/storageAccounts@2019-04-01\u0026#39; = [for index in range(0, saCount): { name: \u0026#39;${index}stg${uniqueString(resourceGroup().id)}\u0026#39; location: resourceGroup().location sku: { name: \u0026#39;Standard_LRS\u0026#39; } kind: \u0026#39;StorageV2\u0026#39; properties: { accessTier: \u0026#39;Hot\u0026#39; } }] output saId array = [for i in range(0, saCount): resourceId(\u0026#39;Microsoft.Storage/storageAccounts\u0026#39;, \u0026#39;${i}stg${uniqueString(resourceGroup().id)}\u0026#39;)] In the ARM template deployment output, you should see:\n1 2 3 4 5 6 7 8 9 10 \u0026#34;outputs\u0026#34;: { \u0026#34;saId\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;Array\u0026#34;, \u0026#34;value\u0026#34;: [ \u0026#34;/subscriptions/5073fd4c-8371-21e034f70820/resourceGroups/bicep/providers/Microsoft.Storage/storageAccounts/0stg6axmw2qxc2xnq\u0026#34;, \u0026#34;/subscriptions/5073fd4c-8371-21e034f70820/resourceGroups/bicep/providers/Microsoft.Storage/storageAccounts/1stg6axmw2qxc2xnq\u0026#34;, \u0026#34;/subscriptions/5073fd4c-8371-21e034f70820/resourceGroups/bicep/providers/Microsoft.Storage/storageAccounts/2stg6axmw2qxc2xnq\u0026#34; ] } } Within the output array, if you want to return multiple properties, you can simply specify that using a property name.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 var sAccounts = [ { name: \u0026#39;sqllondon\u0026#39; accessTier: \u0026#39;Cool\u0026#39; } { name: \u0026#39;sqlchennai\u0026#39; accessTier: \u0026#39;Hot\u0026#39; } { name: \u0026#39;archiveseattle\u0026#39; accessTier: \u0026#39;Cool\u0026#39; } ] resource sa \u0026#39;Microsoft.Storage/storageAccounts@2019-04-01\u0026#39; = [for (sItem, sIndex) in sAccounts: { name: \u0026#39;stg${sItem.name}${sIndex}\u0026#39; location: resourceGroup().location sku: { name: \u0026#39;Standard_LRS\u0026#39; } kind: \u0026#39;StorageV2\u0026#39; properties: { accessTier: sItem.accessTier } }] output storageDetail array = [for (sItem, sIndex) in sAccounts: { id: reference(\u0026#39;stg${sItem.name}${sIndex}\u0026#39;, \u0026#39;2019-04-01\u0026#39;, \u0026#39;Full\u0026#39;).resourceId accessTier: reference(\u0026#39;stg${sItem.name}${sIndex}\u0026#39;).accessTier }] This deployment will return the output similar to:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 \u0026#34;outputs\u0026#34;: { \u0026#34;storageDetail\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;Array\u0026#34;, \u0026#34;value\u0026#34;: [ { \u0026#34;accessTier\u0026#34;: \u0026#34;Cool\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;Microsoft.Storage/storageAccounts/stgsqllondon0\u0026#34; }, { \u0026#34;accessTier\u0026#34;: \u0026#34;Hot\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;Microsoft.Storage/storageAccounts/stgsqlchennai1\u0026#34; }, { \u0026#34;accessTier\u0026#34;: \u0026#34;Cool\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;Microsoft.Storage/storageAccounts/stgarchiveseattle2\u0026#34; } ] } Alright. This is it for today. In the next part of this series, you will learn about authoring reusable modules in Bicep. Stay tuned!\n","date":"2021-04-15T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/bicep-basics-beyond-basics-output/","series":"Azure Bicep","title":"Bicep Language - Beyond Basics - Output"},{"categories":["Azure","Bicep"],"content":" When working ARM templates, you may have come across a situation where you want to provision multiple instances of a resource with similar configuration. For example, multiple storage accounts or virtual machines or multiple data disks attached to a virtual machine. In the JSON template language, the copy element is used for this purpose. Bicep language has a few different ways you can achieve this at different levels like resources, resource properties, and outputs. Variable iterations are not available yet and mostly coming as a part of 0.4 release which is due by May 25th 2021.\nIn this part of the series of articles on Bicep language, you will learn about using iterations in your Bicep files. There are three ways to implement iterations in Bicep.\nUsing a loop index Using an iteration or loop index is the simplest way to create multiple instances of a resource or iterating resource properties. A loop index syntax in Bicep is similar to what you may have seen in other languages.\n1 2 3 [for \u0026lt;index\u0026gt; in range(\u0026lt;start\u0026gt;, \u0026lt;stop\u0026gt;): { \u0026lt;resource-instance\u0026gt; or \u0026lt;resource-properties\u0026gt; }] As you will see next, all you need to do is just assign this either to a resource declaration or resource property.\nResource iteration Here is the general syntax of implementing a index based iteration in Bicep for resources.\n1 2 3 resource \u0026lt;resource-symbolic-name\u0026gt; \u0026#39;\u0026lt;resource-type\u0026gt;@\u0026lt;api-version\u0026gt;\u0026#39; = [for \u0026lt;index\u0026gt; in range(\u0026lt;start\u0026gt;, \u0026lt;stop\u0026gt;): { \u0026lt;resource-instance\u0026gt; }] This should not be difficult to understand since you already know how to declare a resource instance. To add an iteration, all you need to do is add the iterator. In this case a loop index. The range() function here helps in creating a collection of integers from start integer to stop integer. Stop index is excluded. Here is a simple example around creating multiple storage accounts using loop index.\n1 2 3 4 5 6 7 8 9 10 11 12 13 param saCount int = 2 resource sa \u0026#39;Microsoft.Storage/storageAccounts@2019-04-01\u0026#39; = [for index in range(0, saCount): { name: \u0026#39;${index}stg${uniqueString(resourceGroup().id)}\u0026#39; location: resourceGroup().location sku: { name: \u0026#39;Standard_LRS\u0026#39; } kind: \u0026#39;StorageV2\u0026#39; properties: { accessTier: \u0026#39;Hot\u0026#39; } }] In the previous article on expressions in Bicep language, you learned how to use interpolation. The same technique can be used here to access the value stored in index \u0026ndash; ${index}.\nThis is how the resource declaration gets tranpiled into ARM JSON.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 \u0026#34;resources\u0026#34;: [ { \u0026#34;copy\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;sa\u0026#34;, \u0026#34;count\u0026#34;: \u0026#34;[length(range(0, parameters(\u0026#39;saCount\u0026#39;)))]\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Storage/storageAccounts\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;2019-04-01\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;[format(\u0026#39;{0}stg{1}\u0026#39;, range(0, parameters(\u0026#39;saCount\u0026#39;))[copyIndex()], uniqueString(resourceGroup().id))]\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[resourceGroup().location]\u0026#34;, \u0026#34;sku\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Standard_LRS\u0026#34; }, \u0026#34;kind\u0026#34;: \u0026#34;StorageV2\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;accessTier\u0026#34;: \u0026#34;Hot\u0026#34; } } ] Resource property iteration The general syntax for using loop indexes for resource properties is similar to that of resources.\n1 2 3 \u0026lt;property-name\u0026gt;: [for \u0026lt;index\u0026gt; in range(\u0026lt;start\u0026gt;, \u0026lt;stop\u0026gt;): { \u0026lt;resource-properties\u0026gt; }] Here is a quick example of iterating on resource properties. This example demonstrates creating multiple subnets in a Azure Virtual Network.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 var numSubnets = 3 var subnetPrefix = [ \u0026#39;10.10.12.0/24\u0026#39; \u0026#39;10.10.13.0/24\u0026#39; ] resource vnet \u0026#39;Microsoft.Network/virtualNetworks@2018-11-01\u0026#39; = { name: \u0026#39;vnet\u0026#39; location: resourceGroup().location properties: { addressSpace: { addressPrefixes: [ \u0026#39;10.10.10.0/16\u0026#39; ] } subnets: [for sIndex in range(0, numSubnets): { name: \u0026#39;subnet${sIndex}\u0026#39; properties: { addressPrefix: subnetPrefix[sIndex] } }] } } The way this example is built is very similar to how you create multiple storage account instances. For the number of subnets, the numSubnets variable is used and the subnetPrefix array has the prefixes used for each subnet. Using array indexer, you can retrieve the prefix from the subnetPrefix array.\nThose of you with a sharp eye must have noticed that the numSubnets is set to 3 but there are only two elements in the subnetPrefix array. For Bicep, this does not matter. You must make sure that these values are consistent.\nIterating over a collection The second method of creating resource or resource property iterations is to iterate over a collection. Here is the general syntax for this.\n1 2 3 [for \u0026lt;item\u0026gt; in \u0026lt;collection\u0026gt;: { \u0026lt;resource-instance\u0026gt; or \u0026lt;resource-properties\u0026gt; }] Once again, this syntax can be used with either resource declaration or resource properties.\nResource Iteration The general syntax for iterating over a collection to create multiple resource instances is:\n1 2 3 resource \u0026lt;resource-symbolic-name\u0026gt; \u0026#39;\u0026lt;resource-type\u0026gt;@\u0026lt;api-version\u0026gt;\u0026#39; = [for \u0026lt;item\u0026gt; in \u0026lt;collection\u0026gt;: { \u0026lt;resource-instance\u0026gt; }] Here is a full example of creating multiple storage accounts using this method.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 var saNames = [ \u0026#39;sqllondon\u0026#39; \u0026#39;sqlchennai\u0026#39; \u0026#39;archiveseattle\u0026#39; ] resource sa \u0026#39;Microsoft.Storage/storageAccounts@2019-04-01\u0026#39; = [for sName in saNames: { name: concat(sName, uniqueString(resourceGroup().id)) location: resourceGroup().location sku: { name: \u0026#39;Standard_LRS\u0026#39; } kind: \u0026#39;StorageV2\u0026#39; properties: { accessTier: \u0026#39;Hot\u0026#39; } }] In this example, the variable saNames contains a list of storage account names. The iteration in this example loops over the elements in saNames array and uses these values as a part of the storage account name. Here is how the resource declaration gets compiled into ARM JSON representation.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 \u0026#34;resources\u0026#34;: [ { \u0026#34;copy\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;sa\u0026#34;, \u0026#34;count\u0026#34;: \u0026#34;[length(variables(\u0026#39;saNames\u0026#39;))]\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Storage/storageAccounts\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;2019-04-01\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;[concat(variables(\u0026#39;saNames\u0026#39;)[copyIndex()], uniqueString(resourceGroup().id))]\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[resourceGroup().location]\u0026#34;, \u0026#34;sku\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Standard_LRS\u0026#34; }, \u0026#34;kind\u0026#34;: \u0026#34;StorageV2\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;accessTier\u0026#34;: \u0026#34;Hot\u0026#34; } } ] Resource property iteration Here is the general syntax for using collection iteration with resource properties.\n1 2 3 \u0026lt;property-name\u0026gt;: [for \u0026lt;item\u0026gt; in \u0026lt;collection\u0026gt;: { \u0026lt;resource-properties\u0026gt; }] The previous example around the storage account creation can be modified to add iteration for the accessTier property.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 var sAccounts = [ { name: \u0026#39;sqllondon\u0026#39; accessTier: \u0026#39;Cool\u0026#39; } { name: \u0026#39;sqlchennai\u0026#39; accessTier: \u0026#39;Hot\u0026#39; } { name: \u0026#39;archiveseattle\u0026#39; accessTier: \u0026#39;Cool\u0026#39; } ] resource sa \u0026#39;Microsoft.Storage/storageAccounts@2019-04-01\u0026#39; = [for sAcct in sAccounts: { name: concat(sAcct.name, uniqueString(resourceGroup().id)) location: resourceGroup().location sku: { name: \u0026#39;Standard_LRS\u0026#39; } kind: \u0026#39;StorageV2\u0026#39; properties: { accessTier: sAcct.accessTier } }] In this example, the sAccounts variable is a collection of objects. Each object in this collection has two properties \u0026ndash; name and accessTier. We use the same collection for both storage account instance and the accessTier resource property. Within the iteration, you can access the properties using an accessor. sAcct represents the item from sAccounts collection in the current iteration and sAcct.name and sAcct.accessTier represent the name and accessTier values associated with the current item in the collection.\nIterating over elements in an array The final method to use loops in Bicep files is to iterate over elements in an array. In the first method, you looked using the index value from the iteration. In the second method where we used an array of storage account names, we used the element value within each iteration. In this third method, you can access both index and element value. The general syntax for this type of iteration is:\n1 2 3 [for (\u0026lt;item\u0026gt;, \u0026lt;index\u0026gt;) in \u0026lt;collection\u0026gt;: { \u0026lt;resource-instance\u0026gt; or \u0026lt;resource-properties\u0026gt; }] Resource Iteration The syntax for resource iteration using this third method is:\n1 2 3 resource \u0026lt;resource-symbolic-name\u0026gt; \u0026#39;\u0026lt;resource-type\u0026gt;@\u0026lt;api-version\u0026gt;\u0026#39; = [for (\u0026lt;item\u0026gt;, \u0026lt;index\u0026gt;) in \u0026lt;collection\u0026gt;: { \u0026lt;resource-instance\u0026gt; }] Using this method, you can simplify the above example where you created multiple storage accounts and configured a different access tier for each account.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 var sAccounts = [ { name: \u0026#39;sqllondon\u0026#39; accessTier: \u0026#39;Cool\u0026#39; } { name: \u0026#39;sqlchennai\u0026#39; accessTier: \u0026#39;Hot\u0026#39; } { name: \u0026#39;archiveseattle\u0026#39; accessTier: \u0026#39;Cool\u0026#39; } ] resource sa \u0026#39;Microsoft.Storage/storageAccounts@2019-04-01\u0026#39; = [for (sItem, sIndex) in sAccounts: { name: \u0026#39;stg${sItem.name}${sIndex}\u0026#39; location: resourceGroup().location sku: { name: \u0026#39;Standard_LRS\u0026#39; } kind: \u0026#39;StorageV2\u0026#39; properties: { accessTier: sItem.accessTier } }] In this method of iteration, sItem represents the element at the current index and sIndex is the integer representing the current iteration. You can build this and see how it gets transpiled to ARM JSON representation.\nResource property Iteration The syntax for resource property iteration in this method is:\n1 2 3 \u0026lt;property-name\u0026gt;: [for (\u0026lt;item\u0026gt;, \u0026lt;index\u0026gt;) in \u0026lt;collection\u0026gt;: { \u0026lt;resource-properties\u0026gt; }] Here is an example of using this method to create multiple subnets in a Azure virtual network.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 var subnetPrefix = [ \u0026#39;10.10.12.0/24\u0026#39; \u0026#39;10.10.13.0/24\u0026#39; ] resource vnet \u0026#39;Microsoft.Network/virtualNetworks@2018-11-01\u0026#39; = { name: \u0026#39;vnet\u0026#39; location: resourceGroup().location properties: { addressSpace: { addressPrefixes: [ \u0026#39;10.10.10.0/16\u0026#39; ] } subnets: [for (sItem, sIndex) in subnetPrefix: { name: \u0026#39;subnet${sIndex}\u0026#39; properties: { addressPrefix: sItem } }] } } When you choose one of these methods depends on what type of configuration you want to perform. As you start building more complex Bicep files, you will start identifying places to use one of the three methods that you just learned.\nSerial deployment of resources By default, creating multiple resource instances happens in parallel. However, you can configure to do this serially as well. This is where you use the the @batchSize decorator in a resource declaration. This decorator takes an integer as input which represent the number of parallel deployments to perform. Here is a quick example:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 param saCount int = 20 @batchSize(3) resource sa \u0026#39;Microsoft.Storage/storageAccounts@2019-04-01\u0026#39; = [for index in range(0, saCount): { name: \u0026#39;${index}stg${uniqueString(resourceGroup().id)}\u0026#39; location: resourceGroup().location sku: { name: \u0026#39;Standard_LRS\u0026#39; } kind: \u0026#39;StorageV2\u0026#39; properties: { accessTier: \u0026#39;Hot\u0026#39; } }] When you add a batchSize decorator, the resulting ARM representation sets the mode property of the copy element to serial.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 \u0026#34;resources\u0026#34;: [ { \u0026#34;copy\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;sa\u0026#34;, \u0026#34;count\u0026#34;: \u0026#34;[length(range(0, parameters(\u0026#39;saCount\u0026#39;)))]\u0026#34;, \u0026#34;mode\u0026#34;: \u0026#34;serial\u0026#34;, \u0026#34;batchSize\u0026#34;: 3 }, \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Storage/storageAccounts\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;2019-04-01\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;[format(\u0026#39;{0}stg{1}\u0026#39;, range(0, parameters(\u0026#39;saCount\u0026#39;))[copyIndex()], uniqueString(resourceGroup().id))]\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[resourceGroup().location]\u0026#34;, \u0026#34;sku\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Standard_LRS\u0026#34; }, \u0026#34;kind\u0026#34;: \u0026#34;StorageV2\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;accessTier\u0026#34;: \u0026#34;Hot\u0026#34; } } ] So far you have learned about using iteration for resource creation and resource properties. This is it for today. In the next part of this series, you will learn about outputs in Bicep files and how iterations can be used with outputs. Stay tuned.\n","date":"2021-04-14T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/bicep-basics-beyond-basics-iterations/","series":"Azure Bicep","title":"Bicep Language - Beyond Basics - Iterations"},{"categories":["Azure","Bicep"],"content":" Expressions are a critical part of any programming language. For a Domain-Specific Language (DSL) such as Bicep that transpiles to ARM template JSON, expressions add a great value in reducing the complexity in authoring templates. All built-in functions available within ARM JSON template language are available within Bicep as well. Using these functions and the supported language constructs, you can create some powerful expressions in Bicep that significantly reduces the complexity compared to JSON templates. In this article, you will learn about writing a few different ways of writing expressions.\nString Interpolation If you are familiar with languages like PowerShell or Terraform HashiCorp Configuration Language (HCL), you will know that the string interpolation syntax is used in expanding a value (parameter or variable) and then concatenating it to another string. With the ARM JSON templates, the concat() or format() functions are used for this purpose. Bicep simplifies this by using the interpolation syntax.\nTake a look at this example.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 @maxLength(19) param storageAccountName string = \u0026#39;rchagantistg\u0026#39; var resourcePrefix = \u0026#39;hrdept\u0026#39; resource sa \u0026#39;Microsoft.Storage/storageAccounts@2019-06-01\u0026#39; = { name: \u0026#39;${resourcePrefix}${storageAccountName}\u0026#39; location: resourceGroup().location sku: { name: \u0026#39;Standard_LRS\u0026#39; } kind: \u0026#39;StorageV2\u0026#39; properties: { accessTier: \u0026#39;Hot\u0026#39; } } The name property of the storage account resource is assigned a value using string interpolation. This results in the following ARM JSON template representation.\n1 \u0026#34;name\u0026#34;: \u0026#34;[format(\u0026#39;{0}{1}\u0026#39;, variables(\u0026#39;resourcePrefix\u0026#39;), parameters(\u0026#39;storageAccountName\u0026#39;))]\u0026#34; Similarly, you can combine built-in functions within these expressions. For example,\n1 var storageAccountName = \u0026#39;${resourcePrefix}${uniqueString(resourceGroup().id)} In the above example, uniqueString() and resourceGroup() are the built-in functions.\nTernary Operator Bicep language has unary and binary operators as well as a ternary operator. The unary and binary operators are something you must have already used in other languages and not difficult to understand. The ternary operator is something a bit special. Here is the general syntax of using ternary operator in Bicep language.\n1 \u0026lt;condition-to-evaluate\u0026gt; ? \u0026lt;value-if-true\u0026gt; : \u0026lt;value-if-false\u0026gt; The value from ternary operator evaluation can be assigned to a variable or a resource property directly. For example,\n1 var resourcePrefix = resourceGroup().location == \u0026#39;WestUS\u0026#39; ? \u0026#39;wus\u0026#39; : \u0026#39;us\u0026#39; This will result in the following variable assignment in the ARM JSON template.\n1 2 3 \u0026#34;variables\u0026#34;: { \u0026#34;resourcePrefix\u0026#34;: \u0026#34;[if(equals(resourceGroup().location, \u0026#39;WestUS\u0026#39;), \u0026#39;wus\u0026#39;, \u0026#39;us\u0026#39;)]\u0026#34; } String Indexers Bicep supports both string and array indexers. An indexer is simply a way to access an element from a collection of items. As you have learned earlier, Bicep supports variables an parameters of type of object. Object data type is similar to how you represent data in JSON format. For example, say you want to decide the access tier based on the target usage of the storage account. You may represent this as an object in Bicep.\n1 2 3 4 5 6 7 8 var resourcePrefix = { performance : { accessTier: \u0026#39;Hot\u0026#39; } capacity : { accessTier: \u0026#39;Cool\u0026#39; } } To use this, you can add a parameter that collects user input around target usage of the storage account.\n1 param targetUsage string = \u0026#39;capacity\u0026#39; Now, you can use the value from the parameter as a part of string indexer on the variable resourcePrefix.\n1 2 3 properties: { accessTier: resourcePrefix[targetUsage].accessTier } Here is the complete Bicep file for this string indexer example.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 param targetUsage string = \u0026#39;capacity\u0026#39; var resourcePrefix = { performance : { accessTier: \u0026#39;Hot\u0026#39; } capacity : { accessTier: \u0026#39;Cool\u0026#39; } } resource sa \u0026#39;Microsoft.Storage/storageAccounts@2019-06-01\u0026#39; = { name: \u0026#39;${resourcePrefix}${uniqueString(resourceGroup().id)}\u0026#39; location: resourceGroup().location sku: { name: \u0026#39;Standard_LRS\u0026#39; } kind: \u0026#39;StorageV2\u0026#39; properties: { accessTier: resourcePrefix[targetUsage].accessTier } } The accessTier resource property gets compiled to the following representation in ARM JSON template.\n1 2 3 \u0026#34;properties\u0026#34;: { \u0026#34;accessTier\u0026#34;: \u0026#34;[variables(\u0026#39;resourcePrefix\u0026#39;)[parameters(\u0026#39;targetUsage\u0026#39;)].accessTier]\u0026#34; } Property Accessors The property accessors are another variant of expressions and can be used to reference a specific property within an object in Bicep. For example, if you know the target storage usage without needing that as a parameter input, the above expression can be replaced with:\n1 2 3 properties: { accessTier: resourcePrefix.performance.accessTier } This results in the following within ARM JSON template.\n1 2 3 \u0026#34;properties\u0026#34;: { \u0026#34;accessTier\u0026#34;: \u0026#34;[variables(\u0026#39;resourcePrefix\u0026#39;).performance.accessTier]\u0026#34; } Similar to property accessors, Bicep also supports resource accessors. For example, the resource instance name in the above example is sa. So, if you want to access the id property of the storage account resource, you can use sa.id. This type expression is useful within the output element of the Bicep file or within a child resource specification. You will learn more about this later.\nThis is it for today. In the next part of this series, you will learn about iterations in Bicep language and how you can use iterations with different elements within Bicep files. Stay tuned!\n","date":"2021-04-13T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/bicep-basics-beyond-basics-expressions/","series":"Azure Bicep","title":"Bicep Language - Beyond Basics - Expressions"},{"categories":["Azure","Bicep"],"content":" This part of the series will take you beyond Bicep language basics by showing you how to add variables to your Bicep files. If you have worked on ARM JSON templates, you know that variables exist there as well. The primary intention of using variables in either ARM JSON templates or Bicep language is to bring in a balance between free-form vs fixed configurations.\nFor example, as you learned in last part of this series, if you parameterize all resource properties, you will end up with a huge list of input parameters that need to be provided while compiling the Bicep file to ARM JSON template. While this provides flexibility, it may not always have desired outcome. If you want to enforce some standard resource property values across deployments, you must either hard-code those values or constrain what values can be specified if you are collecting the value from a parameter.\nAzure Policy can help with some of what I just described and is a different discussion altogether.\nHaving too many parameters is not desired and hard-coding values in a template makes it difficult to maintain especially when the same value is used for multiple resource instances in a template.\nThis is where variables are used. The basic syntax for declaring variables in Bicep is:\n1 var \u0026lt;variable-name\u0026gt; = \u0026lt;variable-value\u0026gt; The var keyword is used to declare a variable. Variables in Bicep should be assigned at the time of declaration and cannot be reassigned. The name of the variable cannot be same as that of a param, resource, or output identifiers. Variable declaration in Bicep does not need type specification like parameters. The data type of a variable gets inferred from the value that you assign.\nHere is an example that uses a variable in Bicep.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 @description(\u0026#39;Specifies the name of the storage account. Length must be between 3 and 24 characters and all characters in lower-case.\u0026#39;) @metadata({ addedOn: \u0026#39;2021-04-11\u0026#39; }) @minLength(3) @maxLength(24) param storageAccountName string @description(\u0026#39;Specifies the storage account tier. Default value is Hot.\u0026#39;) @allowed([ \u0026#39;Hot\u0026#39; \u0026#39;Cool\u0026#39; ]) param accessTier string = \u0026#39;Hot\u0026#39; var location = resourceGroup().location resource sa \u0026#39;Microsoft.Storage/storageAccounts@2019-06-01\u0026#39; = { name: storageAccountName location: location sku: { name: \u0026#39;Standard_LRS\u0026#39; } kind: \u0026#39;StorageV2\u0026#39; properties: { accessTier: accessTier } } In this example, you will see that the location is now a variable and not collected as input from parameters. The value of the location variable is assigned by evaluating the expression resourceGroup().location. Now, imagine if you have multiple resources that you want to deploy as a part of the template. Instead of specifying the hard-coded location resource property value multiple times, you can simply refer to the variable that you defined.\nThis is a quick one today but as you learn more beyond the basics in a later part of this series, you will see how variables can be assigned values from expressions in Bicep language and how that is a powerful way to reduce the complexity of template authoring in Bicep language. Stay tuned!\n","date":"2021-04-12T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/bicep-basics-beyond-basics-variables/","series":"Azure Bicep","title":"Bicep Language - Beyond Basics - Variables"},{"categories":["Azure","Bicep"],"content":" In the previous part, you learned how you can get started with Bicep language to write your first Bicep file. As you learned, resource keyword along with resource declaration and resource properties are the minimum required in a Bicep file. In today\u0026rsquo;s article, you will see what is beyond those basics.\nParameters One of the goals of Infrastructure as Code (IaC) practice is to create reusable and repeatable automation for deploying your infrastructure. To make this happen, you need to parameterize your infrastructure configuration definitions. In this case, your Bicep files.\nBicep language supports parameter declarations that get compiled into ARM template parameters.\nAt a minimum, this is how you specify a parameter declaration.\n1 param \u0026lt;parameter-identifier\u0026gt; \u0026lt;parameter-type\u0026gt; The param keyword is what gets used to declare parameters in Bicep. The parameter-identifier is what gets referenced when we need the value assigned to the parameter within any resource properties. The parameter-type can be any of the supported data types in Bicep. Bicep has support for several data types like every other programming language. This includes simple types such as int, number, string, bool, null, error, and any and complex types such as arrays and objects. You will learn about some of these types are you build more complex Bicep programs.\nSo, in its simplest form, parameter specification in a Bicep file will look like the below example.\n1 param myFirstParameter string You can assign default values to parameters. For example,\n1 2 param myFirstParameter string = \u0026#39;Azure Bicep\u0026#39; param isPrivate bool = false You can also assign value from an expression as a default value as well.\n1 param myFirstParameter string = resourceGroup().location In Bicep, you can add parameter metadata and constraints using decorators. The general syntax for using decorated parameter declaration is as shown below.\n1 2 @expression param \u0026lt;parameter-identifier\u0026gt; \u0026lt;parameter-type\u0026gt; Description Decorator The @description decorator is used to specify parameter description.\n1 2 @description(\u0026#39;This parameter is used to specify the location of the resource. Default value is WestUS\u0026#39;) param location string = \u0026#39;WestUS\u0026#39; Value Decorator The @minValue and @maxValue decorators are used to define the lower and upper bound values for an integer parameter.\n1 2 3 @minValue(1) @maxValue(10) param numVMs int = 2 Allowed Values Decorator Similar to JSON ARM templates, Bicep language too supports constraining parameter values to a known set. This is done using @allowed parameter decorator.\n1 2 3 4 5 6 @allowed([ \u0026#39;WestUS\u0026#39; \u0026#39;WestUS2\u0026#39; \u0026#39;EastUS\u0026#39; ]) param location string = \u0026#39;WestUS\u0026#39; @allowed decorator expects an array of values. This validation is case-sensitive. So, if you specify WestUs2 instead of WestUS2, Bicep build will fail.\n1 2 PS C:\\sandbox\u0026gt; bicep build .\\main.bicep C:\\sandbox\\main.bicep(20,25) : Error BCP027: The parameter expects a default value of type \u0026#34;\u0026#39;EastUS\u0026#39; | \u0026#39;WestUS\u0026#39; | \u0026#39;WestUS2\u0026#39;\u0026#34; but provided value is of type \u0026#34;\u0026#39;WestUs2\u0026#39;\u0026#34;. Length Decorator Using the @minLength and @maxLength decorator, you can constrain the length of string and array data type parameter values.\n1 2 3 4 5 6 7 @minLength(3) @maxLength(24) param storageAccountName string @minLength(4) @maxLength(6) param vmNames string[] Secure Decorator The @secure decorator specifies that the parameter is a secure string or secure object. When a parameter is marked as secure, its value does not get stored in the deployment history.\n1 2 @secure() param administratorPasswrod string Metadata Decorator The @metadata decorator can be used to specify any other custom properties that describe the parameter or its significance.\n1 2 3 @metadata({ addedOn: \u0026#39;2021-04-11\u0026#39; }) You can combine multiple decorators for any given parameter in the Bicep file. Here is an example with declaration with parameter decorators.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 @description(\u0026#39;Specifies the name of the storage account. Length must be between 3 and 24 characters and all characters in lower-case.\u0026#39;) @metadata({ }) @minLength(3) @maxLength(24) param storageAccountName string @description(\u0026#39;Specifies the storage account tier. Default value is Hot.\u0026#39;) @allowed([ \u0026#39;Hot\u0026#39; \u0026#39;Cool\u0026#39; ]) param accessTier string = \u0026#39;Hot\u0026#39; @description(\u0026#39;Specifies the location of the new storage account. Default location is WestUS2.\u0026#39;) @allowed([ \u0026#39;WestUS\u0026#39; \u0026#39;WestUS2\u0026#39; \u0026#39;EastUS\u0026#39; ]) param location string = \u0026#39;WestUS2\u0026#39; resource sa \u0026#39;Microsoft.Storage/storageAccounts@2019-06-01\u0026#39; = { name: storageAccountName location: location sku: { name: \u0026#39;Standard_LRS\u0026#39; } kind: \u0026#39;StorageV2\u0026#39; properties: { accessTier: accessTier } } Here is how the generated template looks once you compile this using bicep build command.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 { \u0026#34;$schema\u0026#34;: \u0026#34;https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#\u0026#34;, \u0026#34;contentVersion\u0026#34;: \u0026#34;1.0.0.0\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;storageAccountName\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;maxLength\u0026#34;: 24, \u0026#34;minLength\u0026#34;: 3, \u0026#34;metadata\u0026#34;: { \u0026#34;addedOn\u0026#34;: \u0026#34;2021-04-11\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Specifies the name of the storage account. Length must be between 3 and 24 characters and all characters in lower-case.\u0026#34; } }, \u0026#34;accessTier\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;Hot\u0026#34;, \u0026#34;allowedValues\u0026#34;: [ \u0026#34;Hot\u0026#34;, \u0026#34;Cold\u0026#34;, \u0026#34;Archive\u0026#34; ], \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Specifies the storage account tier. Default value is Hot.\u0026#34; } }, \u0026#34;location\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;WestUS2\u0026#34;, \u0026#34;allowedValues\u0026#34;: [ \u0026#34;WestUS\u0026#34;, \u0026#34;WestUS2\u0026#34;, \u0026#34;EastUS\u0026#34; ], \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Specifies the location of the new storage account. Default location is WestUS2.\u0026#34; } } }, \u0026#34;functions\u0026#34;: [], \u0026#34;resources\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Storage/storageAccounts\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;2019-06-01\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;[parameters(\u0026#39;storageAccountName\u0026#39;)]\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[parameters(\u0026#39;location\u0026#39;)]\u0026#34;, \u0026#34;sku\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Standard_LRS\u0026#34; }, \u0026#34;kind\u0026#34;: \u0026#34;StorageV2\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;accessTier\u0026#34;: \u0026#34;Hot\u0026#34; } } ], \u0026#34;metadata\u0026#34;: { \u0026#34;_generator\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;bicep\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;0.3.126.58533\u0026#34;, \u0026#34;templateHash\u0026#34;: \u0026#34;10010485068534492010\u0026#34; } } } Notice in the storageAccountName parameter definition how the contents of metadata decorator get rolled into parameter\u0026rsquo;s metadata attribute in the JSON template.\nAlright. This is all about using parameters in Bicep. You will gain some more knowledge beyond basics in the next post. Stay tuned.\n","date":"2021-04-11T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/bicep-basics-beyond-basics-parameters/","series":"Azure Bicep","title":"Bicep Language - Beyond Basics - Parameters"},{"categories":["Azure","Bicep"],"content":" In the first part of this new series of articles on Azure Bicep, you learnt about what is Azure Bicep and why it is needed. In this part, you will learn about how to get started with Azure Bicep.\nInstalling Bicep To start working with Bicep you will need, at a minimum, the Bicep CLI. There are multiple ways to get Bicep CLI on to your system. On Windows systems, you can use any of the following methods.\nPowerShell Azure CLI (2.20.0 or above) Windows installer Choco package manager - choco install bicep Winget package manager - winget install -e --id Microsoft.Bicep PowerShell Here is a PowerShell script that I use to install Bicep on my development system. This is a modified version of what is available in Bicep documentation page.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 [CmdletBinding()] param () # Variables $bicepFolder = \u0026#34;$env:USERPROFILE\\.bicep\u0026#34; $bicepShell = \u0026#34;%USERPROFILE%\\.bicep\u0026#34; $bicepExe = \u0026#34;$bicepFolder\\bicep.exe\u0026#34; $bicepUrl = \u0026#39;https://github.com/Azure/bicep/releases/latest/download/bicep-win-x64.exe\u0026#39; # Create the install folder Write-Verbose -Message \u0026#34;Creating $bicepFolder\u0026#34; $installDir = New-Item -ItemType Directory -Path $bicepFolder -Force $installDir.Attributes += \u0026#39;Hidden\u0026#39; # Fetch the latest Bicep CLI binary Write-Verbose -Message \u0026#39;Downloading latest Bicep binary\u0026#39; (New-Object Net.WebClient).DownloadFile($bicepUrl, $bicepExe) # Add bicep to your PATH Write-Verbose -Message \u0026#39;Updating PATH variable\u0026#39; $currentPath = (Get-Item -path \u0026#34;HKCU:\\Environment\u0026#34; ).GetValue(\u0026#39;Path\u0026#39;, \u0026#39;\u0026#39;, \u0026#39;DoNotExpandEnvironmentNames\u0026#39;) if (-not $currentPath.Contains($bicepShell)) { setx PATH ($currentPath + \u0026#34;;$bicepShell\u0026#34;) } if (-not $env:path.Contains($bicepFolder)) { $env:path += \u0026#34;;$bicepFolder\u0026#34; } # Verify you can now access the \u0026#39;bicep\u0026#39; command. bicep --version # Done! Azure CLI Azure CLI, version 2.20.0 and above, has a sub command to work with Bicep. So, any command that reference Bicep will auto-trigger Bicep install if it is not already installed. You can manually install as well using the az bicep install command.\nAuthoring Bicep Files I recommend that you use Visual Studio Code for authoring Bicep files. The Azure Bicep extension helps in providing auto-completion and linting for the Bicep language. This extension gets activated when you open or save a .bicep file in VS Code and provides snippets that will get you started. Since Bicep language uses the resource API specification, the Bicep language service behind VS Code will be capable of providing context-aware auto completion.\nIf the resource type is specified, using the Bicep language services, you will get tips around other missing mandatory properties.\nWith this context-aware completion, not just the resource property names but also the possible property values are shown as well.\nOverall, if you are using Bicep for generating ARM templates, this VS Code extension is must.\nBasic Syntax Before you get started with authoring ARM templates as Bicep files, you need to first understand the basic structure of program in Bicep.\nStart with the minimum required elements in the Bicep file.\n1 2 3 4 5 6 7 resource \u0026lt;resource-symbolic-name\u0026gt; \u0026#39;\u0026lt;resource-type\u0026gt;@\u0026lt;api-version\u0026gt;\u0026#39; = { name: \u0026#39;name-of-resource\u0026#39; location: \u0026#39;location-of-resource\u0026#39; properties: { \u0026lt;other-resource-properties\u0026gt; } } Resource keyword The resource keyword is what you can define/declare a resource that you want to provision. The resource-symbolic-name is an identifier given to this instance of the resource. You can, in other parts of the Bicep file, refer to the properties of this resource instance using this symbolic name. This is not the name of the resource as it appears in Azure Portal after deployment.\nFor each resource instance, you must provide the resource type and API version of the resource type to use. This must be provided in the format 'Microsoft.Provider/Type@APIVersion'. For example, Microsoft.Storage/storageAccounts@2019-06-01 is the valid resource type and version specification for Azure storage account resource.\nResource Properties Within the resource properties, you must specify name of the resource and location for resources that require a location. Based on what resource type you intend to provision, there may be other properties that you have to specify. For example, storage account resource type needs kind and sku properties as well. The properties object is then used group all other resource specific properties. For example, for an Azure storage account, you may want to specify an accesstier.\nWith Bicep, these are the minimum required elements in a Bicep file to generate the ARM template. Here is an example.\n1 2 3 4 5 6 7 8 9 10 11 resource sa \u0026#39;Microsoft.Storage/storageAccounts@2019-06-01\u0026#39; = { name: \u0026#39;rchagantisabicep\u0026#39; location: \u0026#39;WestUS\u0026#39; kind: \u0026#39;StorageV2\u0026#39; sku: { name : \u0026#39;Standard_LRS\u0026#39; } properties: { accessTier: \u0026#39;Hot\u0026#39; } } You can generate an ARM template by compiling this Bicep file.\n1 bicep build main.bicep The generated ARM template will be similar to what is shown below.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 { \u0026#34;$schema\u0026#34;: \u0026#34;https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#\u0026#34;, \u0026#34;contentVersion\u0026#34;: \u0026#34;1.0.0.0\u0026#34;, \u0026#34;functions\u0026#34;: [], \u0026#34;resources\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Storage/storageAccounts\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;2019-06-01\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;rchagantisabicep\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;WestUS\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;StorageV2\u0026#34;, \u0026#34;sku\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Standard_LRS\u0026#34; }, \u0026#34;properties\u0026#34;: { \u0026#34;accessTier\u0026#34;: \u0026#34;Hot\u0026#34; } } ], \u0026#34;metadata\u0026#34;: { \u0026#34;_generator\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;bicep\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;0.3.126.58533\u0026#34;, \u0026#34;templateHash\u0026#34;: \u0026#34;15768260597240828360\u0026#34; } } } You can deploy this using your favorite command \u0026ndash; either Azure CLI or Azure PowerShell.\n1 az deployment group create --resource-group bicep --template-file c:\\\\sandbox\\\\main.json In fact, when using Azure CLI (2.20.0 or later) , it is not necessary to even compile the Bicep file. Azure CLI will take care of that. So, the following command will have the same effect as the previous one.\n1 az deployment group create --resource-group bicep --template-file c:\\\\sandbox\\\\main.bicep Alright. This is it for today. Tomorrow, you will learn what is beyond basics!\n","date":"2021-04-06T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/getting-started-with-azure-bicep/","series":"Azure Bicep","title":"Getting Started With Azure Bicep"},{"categories":["Azure","Bicep"],"content":" As I started preparing slides and demos for my session on Azure Bicep at the Global Azure Bootcamp 2021, I made a bunch of notes. As a part of this new series of articles on Azure Bicep, I will start sharing those notes here.\nAzure Bicep is a new Domain-Specific Language (DSL) for declaratively deploying Azure resources. Bicep is not a general purpose programming language but a transparent abstraction for Azure Resource Manager (ARM) Templates. This ensures that the properties that are valid in ARM templates are valid in Bicep as well. Azure Bicep acts as a transpiler for generating ARM templates from Bicep files. But, what is wrong with ARM templates?\nWhy Bicep? ARM templates are JSON documents that offer a declarative way of defining your Azure infrastructure and configuration. These templates specify your intent to provision a service without the need to express how that can be done. ARM template language offers built-in functions and other language constructs such as loops and that help you create more dynamic infrastructure definitions. However, the JSON syntax for ARM templates makes these documents quite verbose and restricts the extensibility since you have to play within what is supported within JSON data representation. As the complexity of the infrastructure grows, your ARM template becomes almost unreadable and maintain. You can, of course, decompose the template into multiple linked templates and but linked templates too have their own limitations. You can use linked templates only from a HTTP location or a Azure blob store. There are alternatives to ARM template deployment. Especially, HashiCorp Terraform or Pulumi SDK. These tools do not use ARM templates but provide alternate ways to define your infrastructure as code.\nTerraform provides a declarative way to define Azure infrastructure using the HashiCorp Configuration Language (HCL). Being a language on its own, HCL supports variables, data types, arithmetic and logical operations, functions, and expressions. With the help of HCL, terraform delivers syntax that is concise and simple to write. The only downside of Terraform is the day zero support for new resources and resource providers.\nPulumi, on the other hand, provides an SDK that you can use in your favorite language and define your infrastructure as imperative scripts or programs. I have not spent much time on Pulumi because of my non-developer background but Pulumi claims that they can provide day zero support for any new resource or resource provider.\nBoth Terraform and Pulumi support multiple cloud deployments. These are not tools that are specifically written for Microsoft Azure. Microsoft could have chosen to adapt one of these but they instead chose to write their own language that makes authoring ARM templates easier. This is an important step since there are many customers who have heavily invested in ARM templates. If Microsoft moves away from ARM templates, this section of customers won\u0026rsquo;t be happy. Creating a transpiler that provides simple and concise declarative way of generating ARM templates will help maintain the backward compatibility while ensuring that those who are just starting with Microsoft Azure do not get freaked out looking at ARM templates.\nWith Bicep, you don\u0026rsquo;t have to worry about learning ARM template language and author thousands of lines of JSON documents. You simply write a Bicep file that declaratively defines the Azure infrastructure, transpile it into an ARM template, and finally use the methods that are already known to you to provision the generated ARM template. Simple. But, how simple is Bicep language? Let us look at an example.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 param storageAccountName string param accessTier string = \u0026#39;Hot\u0026#39; param location string = \u0026#39;WestUS2\u0026#39; resource sa \u0026#39;Microsoft.Storage/storageAccounts@2019-06-01\u0026#39; = { name: storageAccountName location: location sku: { name: \u0026#39;Standard_LRS\u0026#39; } kind: \u0026#39;StorageV2\u0026#39; properties: { accessTier: accessTier } } The 26 lines in the above example is what you need to create a reusable Bicep file that can generate ARM template to provision an Azure storage account. This, when compiled, produces the following ARM template.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 { \u0026#34;$schema\u0026#34;: \u0026#34;https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#\u0026#34;, \u0026#34;contentVersion\u0026#34;: \u0026#34;1.0.0.0\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;storageAccountName\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, \u0026#34;accessTier\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;Hot\u0026#34; }, \u0026#34;location\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;WestUS2\u0026#34; } }, \u0026#34;functions\u0026#34;: [], \u0026#34;resources\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Storage/storageAccounts\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;2019-06-01\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;[parameters(\u0026#39;storageAccountName\u0026#39;)]\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[parameters(\u0026#39;location\u0026#39;)]\u0026#34;, \u0026#34;sku\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Standard_LRS\u0026#34; }, \u0026#34;kind\u0026#34;: \u0026#34;StorageV2\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;accessTier\u0026#34;: \u0026#34;Hot\u0026#34; } } ], \u0026#34;metadata\u0026#34;: { \u0026#34;_generator\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;bicep\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;0.3.126.58533\u0026#34;, \u0026#34;templateHash\u0026#34;: \u0026#34;6796585337478950038\u0026#34; } } } The generated template is almost twice the size of the Bicep file. This ARM template can be deployed by supplying the necessary parameter values as another JSON or at the command line when using Azure CLI or Azure PowerShell.\nThis is a trivial example but consider the flexibility you will have with a language of its own to generate the ARM templates. You don\u0026rsquo;t have to worry about the JSON syntax gotchas or worry about how you can effectively decompose larger ARM templates into smaller linked templates. Bicep provides not just the constructs of a typical programming language but also a way to compose your Azure infrastructure definitions as smaller reusable modules.\nWith Bicep being a transpiler for ARM templates, your existing CI and CD pipelines don\u0026rsquo;t have to change. All you may have to do is add an additional bicep build step that brings the ARM template into the pipeline.\nAlright. This is a quick overview of what Bicep is and why you may want to look at it. In the next part of this series, you will see how you can get started with Bicep. Stay tuned.\n","date":"2021-04-05T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/introduction-to-azure-bicep/","series":"Azure Bicep","title":"Introduction to Azure Bicep"},{"categories":["Azure","Azure Bicep"],"content":" Project Bicep aims to simplify how you author Azure Resource Manager (ARM) templates. Bicep is a new declarative language and a transpiler. This transparent abstraction takes a bicep file and transpiles it into an ARM template JSON. The Bicep CLI provides the necessary capablities to compile bicep files to ARM templates and decompile ARM templates into bicep files.\nBicep language itself is very easy to understand and start using. However, as you start learning it may be helpful to see a few examples and draw some inspiration as you build your won bicep files. Project Bicep respository on GitHub has a good number of examples for varied level of complexity. While looking at these, I thought it may be a good idea to create scripts that can retrieve and save selected example locally.\nThese scripts are a part of larger Bicep helper scripts and tools project I just published on GitHub. Within this repository I have two PowerShell scripts \u0026ndash; getBicepExample.ps1 and saveBicepExample.ps1.\nRetrieve Bicep Examples To retieve a list of all examples available in the Bicep repository, you can run:\n1 .\\getBicepExample.ps1 -Verbose To retrieve a specific level (complexity) of examples:\n1 .\\getBicepExample.ps1 -TemplateLevel 101 To retrieve examples that contains a specific keyword in the description:\n1 .\\getBicepExample.ps1 -TemplateLevel 101 -Keyword ad -Verbose Save Bicep Examples Once you retrieve and identify the necessary bicep example decripton, you can run:\n1 .\\saveBicepExample.ps1 -TemplateDescription \u0026#39;101/aad-domainservices\u0026#39; -Path C:\\sandbox -Verbose This command will save the 101/aad-domainservices/main.bicep to the local folder C:\\sandbox.\nThis is just a quick 10 minutes of work this morning while playing with bicep. I will update these scripts and add more helper scripts and tools soon.\n","date":"2021-03-25T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/retrieve-and-save-project-bicep-examples-from-github/","series":"Azure Bicep","title":"Retrieve and save Project Bicep examples from GitHub"},{"categories":["Azure","Azure Bicep","Speaking"],"content":" Global Azure Bootcamp is happening this year from April 15th to 17th as an online event. And, I will be speaking this year. I submitted a couple of sessions and the Azure Bicep session got accepted.\nSo, what is Azure Bicep? Azure Bicep is the Domain-Specific Language (DSL) and a transparent abstraction built on top of Azure Resource Manager (ARM). The intention behind this project is to simplify authoring ARM templates. Bicep language transpiles into an ARM template JSON which you can deploy using already well-known methods and your existing CI/CD pipelines.\nWhat is this session about? My session at the Global Azure Bootcamp will be about understanding Bicep language and getting started with writing some basic to advanced Bicep files.\nI can\u0026rsquo;t wait to demonstrate what I have been building with Bicep! See you there.\n","date":"2021-03-22T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/speaking-at-global-azure-bootcamp-india-2021/","series":"Azure Bicep","title":"Speaking at Global Azure Bootcamp India 2021"},{"categories":["PowerShell","AzCli"],"content":"Whenever I have to build a new development machine or VM, I usually go about installing a few development tools and command line tools that I use quite often. One such tool is the Azure CLI.\nThere are many alternatives for installing Azure CLI as a part of build process but what I use is this PowerShell script.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 [CmdletBinding()] param ( ) $InformationPreference = \u0026#39;Continue\u0026#39; # Verify that the script is running as administrator if ([bool](([System.Security.Principal.WindowsIdentity]::GetCurrent()).groups -match \u0026#39;S-1-5-32-544\u0026#39;)) { # Download Azure CLI MSI package $uri = \u0026#39;https://aka.ms/installazurecliwindows\u0026#39; $request = Invoke-WebRequest -UseBasicParsing -Uri $uri -MaximumRedirection 0 -ErrorAction Ignore if(($request.StatusCode -ge 300) -and ($request.StatusCode -lt 400)) { $location = $request.Headers.Location $fileName = Split-Path -Path $location -Leaf $cliVersion = $fileName.Split(\u0026#39;-\u0026#39;)[2].trim(\u0026#39;.msi\u0026#39;) $downloadPath = \u0026#34;${env:Temp}\\$fileName\u0026#34; Write-Information -Message (\u0026#34;Downloading Azure CLI version {0} to {1}\u0026#34; -f $cliVersion, $downloadPath) Invoke-WebRequest -UseBasicParsing -Uri $location -OutFile $downloadPath -ErrorAction SilentlyContinue -Verbose:$false if (Test-Path -Path $downloadPath) { Write-Information -Message (\u0026#34;Installing Azure CLI version {0} from {1}\u0026#34; -f $cliVersion, $downloadPath) #Start-Process -FilePath msiexec.exe -ArgumentList \u0026#34;/i $downloadPath /qb /passive\u0026#34; -Wait -ErrorAction Stop -Verbose:$false Write-Information -Message (\u0026#34;Remove Azure CLI installer file from {0}\u0026#34; -f $downloadPath) Remove-Item -Path $downloadPath -Force } } else { throw \u0026#39;Cloud not retrieve the redirected URL\u0026#39; } } else { throw \u0026#39;This must run as administrator at an elevated PowerShell prompt.\u0026#39; } This script downloads the most recent public build and installs it.\n","date":"2021-03-19T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/powershell-script-to-download-and-install-azure-cli/","series":"Azure CLI","title":"PowerShell script to download and install Azure CLI"},{"categories":["PowerShell"],"content":"At the recent Ignite event, Microsoft announced the release candidate 2 update of PowerShell SecretManagement module and the SecretStore extension vault modules. Both modules have been bumped up to version 0.9.1.\nIn the updated release, the secret management module has added support for secret metadata. The extension vault module must implement support for the metadata for this to work. To this extent, the SecretStore module has been updated to support metadata as well.\nThis metadata can be useful in tagging additional non-sensitive details that describe the secret. For example, you can use secret metadata if you want to store the tenant or subscription associated with the secret.\n1 Set-Secret -Name GITHUB_TOKEN -Secret \u0026#39;ThIsIsANAPIKeyFoRGitHUBAuthentICation\u0026#39; -Metadata @{\u0026#39;github_handle\u0026#39;=\u0026#39;rchaganti\u0026#39;} The additional metadata can be retrieved using the Get-SecretInfo command.\n1 Get-SecretInfo -Name GITHUB_TOKEN | fl * You can update existing metadata or add new metadata by using the Set-SecretInfo command.\n1 Set-SecretInfo -Name GITHUB_TOKEN -Metadata @{\u0026#39;TestKey\u0026#39;=\u0026#39;TestValue\u0026#39;;\u0026#39;github_handle\u0026#39;=\u0026#39;rchaganti\u0026#39;} Another good update that I have seen is about more meaningful error messages. For example, when you run Get-Secret command when no vaults are registered, you will see a more friendly error message.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 PS C:\\\u0026gt; Get-Secret -Name GITHUB_TOKEN WARNING: There are currently no extension vaults registered. At least one vault must be registered before SecretManagement can add or retrieve secrets. You can download SecretManagement extension vault modules from PowerShellGallery. https://aka.ms/SecretManagementVaults Get-Secret : The secret GITHUB_TOKEN was not found. At line:1 char:1 + Get-Secret -Name GITHUB_TOKEN + ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ + CategoryInfo : ObjectNotFound: (Microsoft.Power...etSecretCommand:GetSecretCommand) [Get-Secret], ItemNotFound Exception + FullyQualifiedErrorId : GetSecretNotFound,Microsoft.PowerShell.SecretManagement.GetSecretCommand Microsoft announced that the RC2 release of the SecretManagement and SecretStore modules are go live versions and will most probably be promoted to GA release if there are no high-risk bugs.\n","date":"2021-03-11T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/powershell-and-secrets-management-updates/","series":"PowerShell Secret Management","title":"PowerShell and Secret Management - Updates"},{"categories":["PowerShell"],"content":"Update: This article has been updated to show the latest version of the secret store module.\nIn the last part, you have seen an introduction to secret management in PowerShell and towards the end you learned that they are vault extensions that provide the real functionality for storing and managing secrets. In this part of the series, you will learn more about the SecretStore vault extension for the SecretManagement PowerShell module.\nThe SecretStore extension vault is a wrapper around the .NET Crypto API. This vault stores the secrets to the local machine based on the current user account context. You can install this from the PowerShell gallery.\n1 Install-Module -Name Microsoft.PowerShell.SecretStore This extension vault comes with a few commands that are used to configure the vault behavior.\n1 2 3 4 5 6 7 8 9 PS C:\\\u0026gt; Get-Command -Module Microsoft.PowerShell.SecretStore CommandType Name Version Source ----------- ---- ------- ------ Cmdlet Get-SecretStoreConfiguration 0.9.1 Microsoft.PowerShell.SecretStore Cmdlet Reset-SecretStore 0.9.1 Microsoft.PowerShell.SecretStore Cmdlet Set-SecretStoreConfiguration 0.9.1 Microsoft.PowerShell.SecretStore Cmdlet Set-SecretStorePassword 0.9.1 Microsoft.PowerShell.SecretStore Cmdlet Unlock-SecretStore 0.9.1 Microsoft.PowerShell.SecretStore Before you can start using this extension vault, by default, you must first set a secret store password.\n1 Set-SecretStorePassword This command prompts for entering old and new passwords. Simply enter a desired password and hit enter. Once the password is set, you can run the Get-SecretStoreConfiguration command to see the default vault extension configuration.\n1 2 3 4 5 PS C:\\\u0026gt; Get-SecretStoreConfiguration Scope Authentication PasswordTimeout Interaction ----- -------------- --------------- ----------- CurrentUser Password 900 Prompt As you can see, the default scope of the secret store is set CurrentUser and it can be otherwise set to AllUsers. The Authentication property by default requires to supply a password to manage the secrets. This can be disabled by setting the Authentication property to None. Within a session, the supplied password is valid for 900 seconds, by default. This can be changed by setting the PasswordTimeout property. The Interaction property, by default, is set to Prompt and can be set to None to ensure you are not prompted to enter password for any configuration retrieval or changes. When disabling password prompt, you must ensure that you also set Authentication to None to avoid lockdown of secret store. In case you want to set the Interaction to None but still leave the Authentication to Password, you can use the Unlock-SecretStore command to unlock the vault.\nOnce this initial configuration is complete as per your needs, you can register this extension vault with the secret management module.\n1 Register-SecretVault -ModuleName Microsoft.PowerShell.SecretStore -Name SecretStore -DefaultVault -Verbose The above command registers Microsoft.PowerShell.SecretStore module as the extension vault and sets as the default vault. The Get-SecretVault command retrieves all registered extension vaults.\n1 2 3 4 5 PS C:\\\u0026gt; Get-SecretVault Name ModuleName IsDefaultVault ---- ---------- -------------- SecretStore Microsoft.PowerShell.SecretStore True You can start storing secrets using the SecretManagement module commands now.\n1 Set-Secret -Name GITHUB_TOKEN -Secret \u0026#39;ThIsIsANAPIKeyFoRGitHUBAuthentICation\u0026#39; You can get a secret value by using the Get-Secret command. You must use -AsPlainText switch to return the stored secret as readable string.\n1 Get-Secret -Name GITHUB_TOKEN -AsPlainText The Get-SecretInfo returns the metadata information related to the secret.\n1 2 3 4 5 PS C:\\\u0026gt; Get-SecretInfo -Name GITHUB_TOKEN Name Type VaultName ---- ---- --------- GITHUB_TOKEN String SecretStore You can remove a stored secret using the Remove-Secret command.\n1 Remove-Secret -Name GITHUB_TOKEN This is a quick overview of using the SecretStore extension vault with SecretManagement module. You will learn more about other extension vaults and secret management module itself in later parts of this series. Stay tuned.\n","date":"2021-01-15T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/powershell-and-secret-management-using-secret-store-extension-vault/","series":"PowerShell Secret Management","title":"PowerShell and Secret Management - Using Secret Store Extension Vault"},{"categories":["PowerShell"],"content":"Update: This article has been updated to show the latest version of the secret management module.\nWithin the infrastructure automation, you always stumble upon a need to store and retrieve credentials. For example, imagine deploying a database instance and you want the database administrator username and password to be same across based on some IT standard. For this, you either have to hardcode the values in a script or prompt the user running the automation for these values. Storing secrets such as passwords, API keys, etc is a big NO. You should never do that. And, prompting a user will hamper the automation flow and will require manual intervention. This is where secret vaults play a role. You can store all secrets in a vault (local or remote) and then on-demand you can retrieve these values from the vault and consume in your automation. This method is most preferred when delegating automation tasks, running scripts in CI / CD pipelines, and in general anything that requires unattended automation.\nWithin many of my PowerShell scripts, I have used Windows Credential Manager to store and retrieve secrets. PowerShell gallery has a bunch of modules that enable secret management in PowerShell. There are other 3rd party vaults such as Hashicorp Vault. At Ignite 2019, PowerShell team introduced secrets management in PowerShell. This is available on GitHub and PowerShell Gallery as well.\n1 Install-Module -Name Microsoft.PowerShell.SecretManagement This module provides the plumbing needed to register secret vaults and then store/retrieve secrets from the registered vaults.\n1 2 3 4 5 6 7 8 9 10 11 12 13 PS C:\\\u0026gt; Get-Command -Module Microsoft.PowerShell.SecretManagement CommandType Name Version Source ----------- ---- ------- ------ Cmdlet Get-Secret 0.9.1 Microsoft.PowerShell.SecretManagement Cmdlet Get-SecretInfo 0.9.1 Microsoft.PowerShell.SecretManagement Cmdlet Get-SecretVault 0.9.1 Microsoft.PowerShell.SecretManagement Cmdlet Register-SecretVault 0.9.1 Microsoft.PowerShell.SecretManagement Cmdlet Remove-Secret 0.9.1 Microsoft.PowerShell.SecretManagement Cmdlet Set-Secret 0.9.1 Microsoft.PowerShell.SecretManagement Cmdlet Set-SecretInfo 0.9.1 Microsoft.PowerShell.SecretManagement Cmdlet Set-SecretVaultDefault 0.9.1 Microsoft.PowerShell.SecretManagement Cmdlet Test-SecretVault 0.9.1 Microsoft.PowerShell.SecretManagement Cmdlet Unregister-SecretVault 0.9.1 Microsoft.PowerShell.SecretManagement Before you can start managing secrets, you need to register a secret vault. By default, there is no registered vault.\nNote: The earlier alpha version of this module had a built-in vault that was a wrapper around the Windows Credential Manager. It has now been moved out as a vault extension.\nYou can verify that using the Get-SecretVault command. The secret management module vault extensions can be developed to add support for many secret vaults that already exist out there. There are bunch of them already on the PowerShell gallery.\nYou can download the vault extension from PowerShell gallery the same way you download any module.\n1 Install-Module -Name Microsoft.PowerShell.SecretStore Each secret store or the vault extension may have it\u0026rsquo;s own configuration. In the next part of this series of articles on secret management in PowerShell, you will learn how to use the SecretStore vault extension.\nStay tuned!\n","date":"2021-01-10T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/powershell-and-secret-management-introduction/","series":"PowerShell Secret Management","title":"PowerShell and Secret Management - Introduction"},{"categories":["Azure","Azure Policy"],"content":"As your Azure adoption increases and as you on-board various departments and groups within your organization to start using Azure services, it is important that you put in place some level of governance. As cloud architect, you would want control over who can create what type resources, where can they create those resources and how many. You would also want to standardize on the deployments so that the configuration of resources is in compliance with your organization standards. Among many other aspects like this you would want the ability to track your cloud cost more granularly for every service deployed in the cloud and for every business unit or group within your organization. To address this and to implement efficient governance Azure offers features and services such as Policies, Management Groups, Resource Graph, and Blueprints. Here is an excellent depiction of Azure governance architecture from the partner blog.\nOne of the components within Azure governance framework is Azure Policy. Azure Policy helps enforce your organizational IT and business standards for resource consistency, compliance to regulatory requirements, management, security, and cost. Azure offers a set of built-in policy definitions that are readily available for some of the common use cases in your Azure environment. You can, of course, create your own custom policy definitions as well. These policy definitions can be assigned to different scopes within your subscription.\nAn assigned policy is evaluated:\nWhen a resource goes through it\u0026rsquo;s life cycle events such as creation, modification, or deletion When the standard compliance evaluation cycle gets triggered When a assigned policy gets newly assigned or gets updated One of the, trivial but easy to understand, examples of using Azure Policy is to control the size of virtual machines that a user can create. To achieve this, you define a policy and assign it at any applicable scope from a management group to even an individual resource. When it comes to Infrastructure as a Service (IaaS) virtual machines, using Azure policy you can define constraints on what type of VMs, location of VMs, enforcing tags, and so on. You can consider this as the outside configuration of a virtual machine. If you have to audit configuration settings within the OS and / or applications running inside the VM, you can use the Azure Policy Guest Configuration.\nAzure Policy Guest Configuration Azure Policy, through the use of Guest Configuration, can audit settings inside a VM. This is done using the Guest Configuration Extension. The Azure Policy Guest Configuration can be used with both Azure VMs as well as Azure Arc Connected machines. For Azure VMs, you must enable the virtual machine extension and have a system managed identity assigned to the VM. The extension inside the VM uses the system managed identity to read and write to the Guest Configuration service.\nFor Azure Arc connected machines, you must have the Azure Connected Machine agent.\nInside the Azure VM or Azure Arc Connected machine, guest configuration uses the tools specific to the operating system to audit configuration settings. For Windows OS, PowerShell Desired State Configuration (DSC) v2 gets used and for Linux systems, Chef Inspec is used. Guest configuration policies get evaluated every 15 minutes.\nHere is a nice overview session by Michael Greene who is the Program Manager for the Azure Policy Guest Configuration feature.\nYouTube Video This video requires your consent to load content from YouTube, which may set tracking cookies. Enable YouTube Video ID: pySg_YuxnR0 Similar to Azure Policy, Guest Configuration service also has a bunch of built-in definitions and initiatives. If you filter the category to Guest Configuration, you will see a list of policies that can be assigned to Azure VMs and/or Azure Arc Connected machines.\nPolicy definitions when grouped together are called initiatives and Guest Configuration has a few initiatives as well.\nYou will see many of them marked as deprecated. You can either assign to individual policies or initiatives to your machines (Azure or Arc Connected).\nWith Guest Configuration, you can author your own policy definitions and initiatives. At present, these are written as DSC resource modules. You will learn more about this in a later part of this series.\nStay tuned for the next part where you will learn how to assign policies and initiatives to Azure virtual machines.\n","date":"2021-01-06T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/azure-policy-guest-configuration-introduction/","series":"Azure Policy Guest Configuration","title":"Azure Policy Guest Configuration - Introduction"},{"categories":["Golang","Go"],"content":" In the last two parts of this series, you looked at functions and anonymous functions. Both provide a way to create reusable blocks of code and each having its own purpose and use within Go programming. As learned earlier, Go is not a pure object oriented programming language. But, it does support some aspects of OOP. Methods in Go are one way to implement that \u0026ldquo;object-orientedness\u0026rdquo;. This is based on the concept that a type in Go language can have methods associated with it.\n","date":"2020-10-23T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/get-set-go-methods-in-go-language/","series":"Get Set GO","title":"Get set Go - Methods in Go language"},{"categories":["Golang","Go"],"content":" In the last part of the series, you learned about functions in Go language. Functions are first-class citizens in Golang. What this means is that you can not only use function declarations as just reusable code blocks but you can also assign functions to variables, use functions as parameters on other functions, and even return functions from other functions. This is achieved using function literals which are also known as anonymous functions.\nFunction literals Go language specification defines a function literal syntax as follows.\n1 FunctionLit = \u0026#34;func\u0026#34; Signature FunctionBody . Compare this to the function declaration syntax.\n1 FunctionDecl = \u0026#34;func\u0026#34; FunctionName Signature [ FunctionBody ] . The difference is in the name of the function. The function literals or anonymous functions do not have a name or an identifier. The function declaration binds an identifier to a function whereas function literals do not have a name or an identifier (and, therefore called anonymous functions) and can be assigned to a variable, passed to another function, and returned from a function.\nInline execution Here is an example of inline execution of a function literal.\n1 2 3 4 5 6 7 8 9 10 package main import \u0026#34;fmt\u0026#34; func main() { fmt.Println(\u0026#34;Hello from main!\u0026#34;) func() { fmt.Println(\u0026#34;Hello from an anonymous function!\u0026#34;) }() } In the above example, the function literal inside main is the function literal. It has no name or an identifier. And, the parenthesis at the end of the function literal make it execute inline. So, when you run this, the first Println gets called and then the Println inside the anonymous function gets called.\nPassing arguments You can pass arguments to these anonymous functions.\n1 2 3 4 5 6 7 8 9 10 package main import \u0026#34;fmt\u0026#34; func main() { fmt.Println(\u0026#34;Hello from main!\u0026#34;) func(version float64) { fmt.Printf(\u0026#34;Hello from an anonymous function in Go language %.2f!\u0026#34;, version) }(1.15) } This example declares one parameter to the function called version which is of float64 data type. And, at the end of function literal definition, you can pass the value associated with the version parameter within the parenthesis.\nAssigning functions to variables Function literals, as mentioned earlier, can be assigned to variables. It is like any other value that you assign to a variable. Here is an example.\n1 2 3 4 5 6 7 8 9 10 11 12 package main import \u0026#34;fmt\u0026#34; func main() { fmt.Println(\u0026#34;Hello from main!\u0026#34;) f := func(version float64) { fmt.Printf(\u0026#34;Hello from an anonymous function in Go language %.2f!\u0026#34;, version) } f(1.15) } This example is similar the previous one. The only difference is the missing the parenthesis at the end of function literal. Instead, the function gets assigned to a variable and we use the variable \u0026ndash; f(1.15) \u0026ndash; to invoke the function. The value 1.15 gets passed to the anonymous function. So, what do you think the type of variable f will be? You can check that by adding fmt.Printf(\u0026quot;\\n%T\u0026quot;, f) to the above example. This will be the output.\n1 2 3 4 PS C:\\GitHub\\GetSetGo\u0026gt; go run .\\function.go Hello from main! Hello from an anonymous function in Go language 1.15! func(float64) So the variable f is of func() type. This leads us into the custom and user-defined function types.\nCustom function types Similar to defining a struct, you can define a custom function type. The general syntax for defining a custom function type is:\n1 type \u0026lt;func-name\u0026gt; func(\u0026lt;parameter1 parameter-1type, parameter2 parameter2-type\u0026gt;) \u0026lt;return-type\u0026gt; For example, type concat func(fName, lName string) string defines a new custom function type named concat. This defines the signature of the function.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 package main import \u0026#34;fmt\u0026#34; type concat func(fName, lName string) string func main() { var s concat = func(fName, lName string) string { msg := fmt.Sprintf(\u0026#34;%s %s rocks!\u0026#34;, fName, lName) return msg } fmt.Println(s(\u0026#34;Go\u0026#34;, \u0026#34;Language\u0026#34;)) fmt.Printf(\u0026#34;%T\u0026#34;, s) } In the main function, you can use the new function type to create a variable and assign the function to it. The above example declares and initializes variable s for this purpose. You can then use variable s as you have already seen earlier. When you run this example, you will see that the type of the variable s is main.concat which is the custom function type.\nPassing functions as arguments Function literals can be passed as arguments to other functions. You have learned how to create a custom function type in the previous section. The following example demonstrates how to pass that custom function type as an argument to another function. Take a look.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 package main import \u0026#34;fmt\u0026#34; type concat func(fName, lName string) string func wrapper(s concat) { fmt.Println(s(\u0026#34;Go\u0026#34;, \u0026#34;Language\u0026#34;)) } func main() { a := func(f, l string) string { msg := fmt.Sprintf(\u0026#34;%s %s rocks!\u0026#34;, f, l) return msg } wrapper(a) } The function that is getting passed to the wrapper function need not be of a custom function type. The custom type in this example takes string arguments and returns string type. You can define any function signature as an argument and match that signature in the function literal body that you define.\nFunctions as return values In Go language, you can not only pass functions as arguments but you can also return functions from other functions. Here is a variation of the above example to demonstrate this.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 package main import \u0026#34;fmt\u0026#34; func wrapper() func(fName, lName string) string { a := func(f, l string) string { msg := fmt.Sprintf(\u0026#34;%s %s rocks!\u0026#34;, f, l) return msg } return a } func main() { concat := wrapper() fmt.Println(concat(\u0026#34;Go\u0026#34;, \u0026#34;Language\u0026#34;)) } The wrapper function in this update example takes no arguments but returns a function. Within the wrapper function body, you simply declare and initialize a function literal and return that. In the main function, a variable called concat gets initialized to the return value of the wrapper function which is a function. Finally, the concat function literal gets invoked with the string arguments.\nSo, what happened to the custom type that you saw an earlier example? Can you still use that custom type as a return value? Yes, of course. Here is the updated example.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 package main import \u0026#34;fmt\u0026#34; type concat func(fName, lName string) string func wrapper() concat { a := func(f, l string) string { msg := fmt.Sprintf(\u0026#34;%s %s rocks!\u0026#34;, f, l) return msg } return a } func main() { concat := wrapper() fmt.Println(concat(\u0026#34;Go\u0026#34;, \u0026#34;Language\u0026#34;)) } This brings this part to the final concept around function literals called closures.\nClosures Wikipedia describes closures as below.\nIn programming languages, a closure, also lexical closure or function closure, is a technique for implementing lexically scoped name binding in a language with first-class functions. Operationally, a closure is a record storing a function together with an environment. The environment is a mapping associating each free variable of the function (variables that are used locally, but defined in an enclosing scope) with the value or reference to which the name was bound when the closure was created. Unlike a plain function, a closure allows the function to access those captured variables through the closure\u0026rsquo;s copies of their values or references, even when the function is invoked outside their scope.\nThat is a lot to digest but pay attention to the last sentence. If we have to put that in simple words, a closure can access the variables defined outside its scope. Here is an example.\n1 2 3 4 5 6 7 8 9 10 11 12 package main import ( \u0026#34;fmt\u0026#34; ) func main() { lang := \u0026#34;Go Language\u0026#34; func() { fmt.Println(lang, \u0026#34;rocks!\u0026#34;) }() } In the above example, the function literal inside the main function can access the variable lang defined outside its scope. A closure in Go has access to its surrounding state and it gets bound to that. The state of the closure becomes unique when it is created.\nTake a look at this example to understand this.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 package main import ( \u0026#34;fmt\u0026#34; ) func sub() func() int { a := 100 return func() int { a = a - 10 return a } } func main() { a1 := sub() a2 := sub() fmt.Printf(\u0026#34;\\nValue from a1 %d\u0026#34;, a1()) fmt.Printf(\u0026#34;\\nValue from a2 %d\u0026#34;, a2()) fmt.Printf(\u0026#34;\\nValue from a1 again %d\u0026#34;, a1()) fmt.Printf(\u0026#34;\\nValue from a2 again %d\u0026#34;, a2()) } In the sub() function, variable a gets initialized to 100. Also, sub() function returns a function in which variable a gets decremented by 10 and the resulting value gets returned. In this main function, variables a1 and a2 get initialized to the closure returned from sub() function. Now, the interesting part comes when you invoke these closures. Here is the output.\n1 2 3 4 5 6 7 PS C:\\GitHub\\GetSetGo\u0026gt; go run .\\function.go Value from a1 90 Value from a2 90 Value from a1 again 80 Value from a2 again 80 PS C:\\GitHub\\GetSetGo\u0026gt; The first calls to a1 and a2 return the same value. And, then when you call these closures again, they return the same value. What does this mean? This demonstrates that the closures do have a unique and isolated state when they get created. So, when you created a1 and a2, each of them have variable a at 100 and when these closures get invoked, value of a gets decremented by 10. Therefore, each set of invocation of these closures return the same value.\nAs you proceed learning more advanced concepts in Go language, you will start seeing real use cases of closures. Stay tuned.\n","date":"2020-10-21T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/get-set-go-anonymous-functions-in-go-language/","series":"Get Set GO","title":"Get set Go - Anonymous Functions in Go language"},{"categories":["Golang","Go"],"content":" Alright. In this series so far, you learned about some basics of programming in Go language, about data types, conditions, and loops. In all the examples in the earlier parts of the series, there was just the main function which is the entry point into a Go program. All execution of a Go program starts at the main function. The idea of a function in any language is to basically group together a set of instructions and these instructions can now be used repeatedly as needed. As you learned, the main function is a bit special though. Functions promote reusability and readability of your code. In this part of the series, you shall learn about functions in Go language.\nFunctions A function in go, like any other language, is a block of code that takes input, performs a specific task, and generates some output. The general syntax for declaring functions in Go language is shown below.\n1 2 3 func func-name(param1-name param1-type, param2-name param2-type) func-return-type { //statements that perform a specific task } The func keyword, you have already seen this with the main function, is used to declare the function type in Go followed by a name given to the function. Within the parenthesis, you specify any parameters that are needed for the function to perform its task followed by function return type(s). Both parameters and return type are optional. Like the main function, the function body needs to be enclosed in a pair of curly brackets with the open bracket on the same line as the func declaration. The parameters in the function declaration are always in the format of parameter name followed by its type. You can have any number of parameters. Similarly, you can specify a comma separated list of return types as well. However, if you have more than one return type, you must enclose them in ().\nTake a look at this example.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 package main import \u0026#34;fmt\u0026#34; func main() { a, b := 10, 20 c := add(a, b) fmt.Println(\u0026#34;Sum of a and b is\u0026#34;, c) } func add(x int, y int) int { sum := x + y return sum } Well, this is a super trivial elementary type example but you get the point. In the main function, there are two variables a and b. These variables are declared and initialized with some integer values. Now, another variable named c is declared and initialized to the return value from the function add. This is yet another way to initialize variables in Go language. Observe how the function is called. We just use the name of the function with all parameter values enclosed in () \u0026ndash; like add(a, b).\nThe add function is declared with two input parameters \u0026ndash; x and y \u0026ndash; of integer type. Since the variable c in function main gets its value from the add function, this function has an integer return type. Within the function body, the sum variable is declared and initialized to store the value of x+y. And, finally, the return sum statement\nSince both parameters of this function are of integer types, we can simplify the function declaration as follows.\n1 2 3 func add(x, y int) int { //code } Named return values Go functions support named return values. First, see the below example.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 package main import \u0026#34;fmt\u0026#34; func main() { a, b := 10, 20 c := add(a, b) fmt.Println(\u0026#34;Sum of a and b is\u0026#34;, c) } func add(x int, y int) (sum int) { sum = x + y return } In this example which is more or less same as the first one, all that is modified is the add function. For the function return value, there is both name and type enclosed in parenthesis. And, within the function body, you don\u0026rsquo;t have to declare the variable sum anymore since it is already declared as a part of the return value declaration. And, finally, observe the return statement. You don\u0026rsquo;t have specify the name of the variable that you want to return. Instead, simply use the return statement. This tells Go to return values of variables declared as a part of the function declaration. So, what happens if you don\u0026rsquo;t have a variable named sum in the function body? Does Go through an error? Not really. It simply returns a zero value for the type mentioned in the function return value declaration.\nSee this example and the program output to understand this.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 package main import \u0026#34;fmt\u0026#34; func main() { a, b := 10, 20 c := add(a, b) fmt.Println(\u0026#34;Sum of a and b is\u0026#34;, c) } func add(x int, y int) (sum int) { sum1 := x + y fmt.Println(sum1) return } In this modified example, sum is declared as the return value from function add. However, there is no value assigned to sum within the function body. So, the return statement returns the zero value of integer variable sum.\nAnd, here is the output.\n1 2 3 4 PS C:\\GitHub\\GetSetGo\u0026gt; go run .\\function.go 30 Sum of a and b is 0 PS C:\\GitHub\\GetSetGo\u0026gt; As you see in the program output, there is no error and the return value from the add function is 0 which gets assigned to variable c in the main function.\nMultiple return values In Go, a function can return multiple values. The way you can declare such as function is shown here.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 package main import \u0026#34;fmt\u0026#34; func main() { a, b := 10, 20 c, d := math(a, b) fmt.Println(\u0026#34;Sum of a and b is\u0026#34;, c) fmt.Println(\u0026#34;Difference if a and b is\u0026#34;, d) } func math(x int, y int) (sum, diff int) { sum = x + y diff = x - y return } In this example, the add function is renamed to math and one more return value called diff has been added. With this, you have two integer return values on the function. Similar the function parameters, you can specify the type of return values just once if you have more than one value of the same type. Once again, this example uses named return values to so there is no need to specify the variable names along with the return statement.\nIn the main function, variables c and d get their values from the math function. Since sum is declared first in the return values of the function, variable c gets the value of sum and variable d gets the value of diff assigned respectively. If you are interested only in one value returning from the function, you can use the blank identifier (_) you learned in an earlier part of this series.\nVariadic functions In the example above, function add takes exactly two values and adds them up and returns the result to main function. Is there a way you can pass it an arbitrary number of values to get the sum of those values? This is where variadic functions play a role.\nFirst, take a look at this example.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 package main import \u0026#34;fmt\u0026#34; func main() { c := add(10, 20, 30, 40, 50) fmt.Println(\u0026#34;Sum of a and b is\u0026#34;, c) } func add(values ...int) int { sum := 0 for _, i := range values { sum = sum + i } return sum } In this example, the add function\u0026rsquo;s parameter definition is different from what you have seen so far. In the parameter definition \u0026ndash; values ...int \u0026ndash; values is the name of the parameter and int is the data type of that parameter. The three dots ... (Ellipsis) indicate that the parameter values must be rolled into a slice type. Observe that there is no space between the Ellipsis and the data type of the parameter. Essentially, you prefix Ellipsis to the data type of the parameter.\nWithin the add function, once the parameters values are received and rolled into a slice, the rest of the logic using the for loop is similar to what you have learned in an earlier part of the series. When you use Ellipsis in a function parameter definition, the function becomes a variadic function. With this change, it does not matter how many values of the same data type you specify.\nNow, what if you have to pass some more values (as parameters) to this variadic function? This can be done but the Ellipsis can be used only with the final parameter of the function.\nTake a look at this example.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 package main import \u0026#34;fmt\u0026#34; func main() { num := []int{10, 20, 30, 40, 50} c := add(0, num...) fmt.Println(\u0026#34;Sum of a and b is\u0026#34;, c) } func add(x int, values ...int) int { sum := 0 for _, i := range values { sum = sum + i + x } return sum } In this example,the add function is updated to add one more integer parameter called x. As you can see in the function parameter definition, the add function has two parameters \u0026ndash; x and values. Within main function, you now have a slice called num. And, the way add function invoked is also a bit different. Instead of passing the slice directly, Ellipsis is postfixed to the variable name \u0026ndash; num... \u0026ndash; to unroll the slice values into a bunch of integers. These values get rolled into a slice again within the add function.\nThere are several use cases for variadic function. First and foremost, when you do not know in advance how many values you may have to pass to a function. Secondly, using variadic functions, you can avoid creating a temporary slice to just pass a bunch of values to a function.\nPassing by value vs passing by reference Like many other programming languages, Go language too supports pointers which form the basis of passing values to a function by reference. First, take a look at this example.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 package main import \u0026#34;fmt\u0026#34; type person struct { firstName string lastName string age int } func main() { p1 := person{ firstName: \u0026#34;Ravikanth\u0026#34;, lastName: \u0026#34;Chaganti\u0026#34;, age: 39, } //print p1 fmt.Println(p1) //increment age incrementAge(p1) //print p1 again fmt.Println(p1) } func incrementAge(p1 person) { p1.age++ fmt.Println(p1) } In this example, the struct person is a custom type which has three elements \u0026ndash; firstName, lastName, and age. Age is an integer can be incremented using the increment() function. In the main function, p1 is declared and initialized.\nHere is how the output will be when you run this program.\n1 2 3 4 5 PS C:\\GitHub\\GetSetGo\u0026gt; go run .\\function.go {Ravikanth Chaganti 39} {Ravikanth Chaganti 40} {Ravikanth Chaganti 39} PS C:\\GitHub\\GetSetGo\u0026gt; As you see here, the changes done to the age element within p1 are not visible within the main function. The reason is, by default, when you pass a value to a function in Go, a copy of that variable gets created within the function scope. So, essentially, the increment() function gets copy of the p1 struct and then it modifies the age element within that copy which is certainly not visible to the main function. So, how do we pass values by reference so that the calling function sees the changes in values? We need to use pointers. Here is an updated example.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 package main import \u0026#34;fmt\u0026#34; type person struct { firstName string lastName string age int } func main() { p1 := \u0026amp;person{ firstName: \u0026#34;Ravikanth\u0026#34;, lastName: \u0026#34;Chaganti\u0026#34;, age: 39, } fmt.Println(*p1) incrementAge(p1) fmt.Println(*p1) } func incrementAge(p1 *person) { p1.age++ } In this updated example, p1 is a reference to the person struct and by passing this pointer to the incrementAge() function, the value of age element gets incremented and becomes available to the main function. Observe how the incrementAge() function parameter definition is updated to receive the pointer to person struct. And, within the function, you simply reference the age element and increment it.\nWow! This has been a long post. This is a good overview of function basics in Go. In the next part, you will learn about anonymous functions in Go language. Stay tuned.\n","date":"2020-10-12T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/get-set-go-functions-in-go-language/","series":"Get Set GO","title":"Get set Go - Functions in Go language"},{"categories":["Golang","Go"],"content":" In the two previous parts of the series, you learned how to use if, switch, and for loop in Go language. You also learned about using break and continue statements in Go language to alter the control flow. What you have seen in the examples with these statements was an unlabeled way to break or continue the iteration. Go language supports labels that let you transfer control to the place in the (same function) code where the label is defined. There are different ways to use labeled statements.\nGoto Similar to break and continue, the [goto](https://golang.org/ref/spec#Goto_statements) statement in Go language transfers the control to the place in the code where a specific label is defined within the same function. Here is an example.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 package main import \u0026#34;fmt\u0026#34; func main() { num := 1 jumpTo: for num \u0026lt;= 10 { if num%2 == 0 { num++ goto jumpTo } fmt.Printf(\u0026#34;\\n num has value set to %d\u0026#34;, num) num++ } fmt.Printf(\u0026#34;\\nFinal value of num is %d\u0026#34;, num) } The above example\u0026rsquo;s purpose is to print all odd numbers between 1 and 10. In this, the goto statement is used to jump to the start of the loop again. Note the way the label \u0026ndash; jumpTo \u0026ndash; is defined. The general syntax for defining a label in a Go program is identifier: and then with the goto statement, you just use the identifier \u0026ndash; goto jumpTo.\nHere is the output from this program.\n1 2 3 4 5 6 7 8 PS C:\\GitHub\\GetSetGo\u0026gt; go run .\\for.go num has value set to 1 num has value set to 3 num has value set to 5 num has value set to 7 num has value set to 9 Final value of num is 11 The scope of a label is the function where it is defined. With the goto statement, it does not matter where the label is defined within the function. For example, the following program works as well. Of course, without printing all the odd number values.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 package main import \u0026#34;fmt\u0026#34; func main() { num := 1 for num \u0026lt;= 10 { if num%2 == 0 { num++ goto jumpTo } fmt.Printf(\u0026#34;\\n num has value set to %d\u0026#34;, num) num++ } jumpTo: fmt.Printf(\u0026#34;\\nFinal value of num is %d\u0026#34;, num) } This produces output as shown below.\n1 2 3 4 PS C:\\GitHub\\GetSetGo\u0026gt; go run .\\for.go num has value set to 1 Final value of num is 3 Function scope of a label also means that you cannot have the same label identifier declared multiple times even within different code blocks within the same function. Also, using goto has a couple of caveats.\nSince goto can be used to jump forward in the program flow, you cannot jump over a variable declaration. You cannot jump into a new code block. Continue and Break with labels If you look at the continue and break statement syntax from the Go language specification, you will notice that these statements have an optional label.\n1 2 BreakStmt = \u0026#34;break\u0026#34; [ Label ] . ContinueStmt = \u0026#34;continue\u0026#34; [ Label ] . Break statement works only within a for loop, switch or select statement while continue works only in a loop. You have already seen examples of break and continue within a for loop. Similar to the goto statement, you just need to suffix the label identifier to continue and break statements.\nContinue statement without a label, it skips the current iteration of the innermost loop. First, take a look at this example.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 package main import \u0026#34;fmt\u0026#34; func main() { s1 := []int{1, 2, 5, 6, 9, 10} for i := 0; i \u0026lt; 3; i++ { fmt.Printf(\u0026#34;\\n\\ni is %d\u0026#34;, i) for _, j := range s1 { if j%2 == 0 { continue } fmt.Printf(\u0026#34;\\nj is %d\u0026#34;, j) } } } This is straightforward. Within the inner loop, if you encounter an even number, you simply go to the next iteration using continue. Here is the output from this program.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 PS C:\\GitHub\\GetSetGo\u0026gt; go run .\\for.go i is 0 j is 1 j is 5 j is 9 i is 1 j is 1 j is 5 j is 9 i is 2 j is 1 j is 5 j is 9 Now, imagine based on the condition being checked in the inner loop, you may want to simply start again from the outer loop. You can achieve this with a simple change of adding the label to the continue statement.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 package main import \u0026#34;fmt\u0026#34; func main() { s1 := []int{1, 2, 5, 6, 9, 10} jumpTo: for i := 0; i \u0026lt; 3; i++ { fmt.Printf(\u0026#34;\\n\\ni is %d\u0026#34;, i) for _, j := range s1 { if j%2 == 0 { continue jumpTo } fmt.Printf(\u0026#34;\\nj is %d\u0026#34;, j) } } } The only change here is that a label \u0026ndash; called jumpTo \u0026ndash; is added right before the outer for loop. And, the same identifier is added to continue statement inside the innermost loop. You can clearly see the difference in the control flow when you run this program. Here is what you will see.\n1 2 3 4 5 6 7 8 9 10 11 PS C:\\GitHub\\GetSetGo\u0026gt; go run .\\for.go i is 0 j is 1 i is 1 j is 1 i is 2 j is 1 Since the innermost loop reinitializes every time it is reached during execution, you will never go beyond the second element in the slice. The labeled continue becomes very useful in case of nested loops where exiting the current scope based on a condition is needed.\nSimilarly, labels can be used with break statement as well. For this, just replace continue with break in the above example.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 package main import \u0026#34;fmt\u0026#34; func main() { s1 := []int{1, 2, 5, 6, 9, 10} jumpTo: for i := 0; i \u0026lt; 3; i++ { fmt.Printf(\u0026#34;\\n\\ni is %d\u0026#34;, i) for _, j := range s1 { if j%2 == 0 { break jumpTo } fmt.Printf(\u0026#34;\\nj is %d\u0026#34;, j) } } } The following snippet shows the output both with and without the label to illustrate the difference.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 PS C:\\GitHub\\GetSetGo\u0026gt; go run .\\for.go //without break label i is 0 j is 1 i is 1 j is 1 i is 2 j is 1 PS C:\\GitHub\\GetSetGo\u0026gt; go run .\\for.go //with break label i is 0 j is 1 This is it about using labeled statements in Go. When you write some \u0026ldquo;real\u0026rdquo; Go code other than these trivial examples, you will understand the advantages of these labeled statements in control flow. In this next part, you will learn about functions in Go.\n","date":"2020-10-09T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/get-set-go-labels-in-go-language/","series":"Get Set GO","title":"Get set Go - Labels in Go language"},{"categories":["Golang","Go"],"content":" With what you learned about basic data types and structs in this series so far, you are now ready to look at the looping construct in Go language. Go language is simple and to that extent Go has just one looping construct \u0026ndash; For loop.\nFor loop A few programming languages that you may have used in the past may have more than one looping construct \u0026ndash; do - while, do - until, for, and foreach and so on. You use a specific construct based on what you need to achieve and type of conditions that need to be tested. However, Go language has just one looping construct \u0026ndash; the for loop. Although there is just one looping construct, go supports multiple variations of this construct for different use cases.\nThe general syntax of writing a for loop is\n1 2 3 for \u0026lt;initialization\u0026gt;; \u0026lt;condition\u0026gt;; \u0026lt;post\u0026gt; { } In this general syntax, the three different components are given a specific purpose. Initialization is used initialize any variables needed. This runs only once. The condition in the middle gets checked in every iteration and the loop runs until this condition evaluates to false. The post statement gets executed after the iteration executes successfully. After every successful iteration, the condition gets evaluated again and the loop body will continue to execute. All these components are totally optional. This leads into the different for loop variations.\nInfinite loop As you learned earlier, all components within the for loop syntax are optional. So, if you remove all the components mentioned earlier, it results in an infinite loop.\n1 2 3 4 5 6 7 8 9 package main import \u0026#34;fmt\u0026#34; func main() { for { fmt.Println(\u0026#34;This is an infinite loop. You need to break the execution.\u0026#34;) } } The above will continue to run until you interrupt it (CTRL+C on Windows). Infinite loops always need a way to break out of the loop. Here is a better example.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;math/rand\u0026#34; ) func main() { for { num := rand.Intn(100000) if num % 2 == 0 { break } fmt.Printf(\u0026#34;\\nGenerated random number \u0026#39;%d\u0026#39; is an odd number.\u0026#34;, num) } } In this example, within the loop body, the rand.Intn function is used to generate a non-negative random integer between 0 and n and n here is set to 100000. In the next statement, the if condition is used to evaluate if the generated random number is an even number and if so, the loop breaks and exits. if the generate number is an odd number, the number gets printed and the loop continues execution.\nBreak The break statement in a loop is used to break out of the based on a condition like what you have seen in the previous example. Once the loop exits, the program control goes to the next line in the program, if any.\nThe next variant of for loop is the that uses a boolean expression.\nFor loop with Boolean expression In the general syntax of a for loop that you learned in the beginning of this part, there were three components \u0026ndash; initializer, condition, and post statement. In the absence of these three components, you get to define an infinite loop. You can add only the condition component and still create a for loop. Here is an example.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 package main import ( \u0026#34;fmt\u0026#34; ) func main() { num := 1 for num \u0026lt;= 10 { if num%2 == 0 { fmt.Printf(\u0026#34;\\n \u0026#39;%d\u0026#39; is an even number\u0026#34;, num) } else { fmt.Printf(\u0026#34;\\n \u0026#39;%d\u0026#39; is an odd number.\u0026#34;, num) } num++ } } In the above example, for loop is using a boolean expression to determine the number of iterations. In this case, the loop iterates until the variable num is less than or equal to 10. Within the loop body, the post increment of variable num ensures that its value gets incremented. Without this, you will end up with an infinite loop again.\nWhile what you have seen so far \u0026ndash; infinite loop and the loop with boolean expression \u0026ndash; indirectly used all three components you saw in the general for loop syntax. So, why not put all of them together and see another example.\nFor loop with an initializer, condition, and post statement Here is an example of what you have seen in the general syntax.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 package main import ( \u0026#34;fmt\u0026#34; ) func main() { for num := 1; num \u0026lt;= 10; num++ { if num%2 == 0 { fmt.Printf(\u0026#34;\\n \u0026#39;%d\u0026#39; is an even number\u0026#34;, num) } else { fmt.Printf(\u0026#34;\\n \u0026#39;%d\u0026#39; is an odd number.\u0026#34;, num) } } } As you see in the above example, using all components essentially eliminates the need for a separate initialization and post increment. But, the result is same as the earlier example with just boolean expression as a part of for loop.\nFor loop over a range The final variant of for loop is iterate over a range of values. You may have an array or slice of integers or even a map of key value pairs. If you need to iterate over those values, you can use this variant of the for loop in Go language.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 package main import ( \u0026#34;fmt\u0026#34; ) func main() { a := [10]int{1, 2, 3, 4, 5, 6, 7, 8, 9, 10} for i, v := range a { if v%2 == 0 { fmt.Printf(\u0026#34;\\n \u0026#39;%d\u0026#39; at index \u0026#39;%d\u0026#39; is an even number\u0026#34;, v, i) } else { fmt.Printf(\u0026#34;\\n \u0026#39;%d\u0026#39; at index \u0026#39;%d\u0026#39; is an odd number.\u0026#34;, v, i) } } } In the example above, the for loop used shorthand declaration to initialize two values \u0026ndash; i and v. But, you may ask. Isn\u0026rsquo;t that a simple array? Yes, it is. But, when you iterate over an array using range, it returns two values \u0026ndash; index of the element and the value at that index. Therefore, the above example uses two variables \u0026ndash; i and v. Since Go does not allow you to declare a variable and not use it, the print statements in the subsequent code refer to both these variables. But, what if you want to ignore the index value? Simple.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 package main import ( \u0026#34;fmt\u0026#34; ) func main() { a := [10]int{1, 2, 3, 4, 5, 6, 7, 8, 9, 10} for _, v := range a { if v%2 == 0 { fmt.Printf(\u0026#34;\\n \u0026#39;%d\u0026#39; is an even number\u0026#34;, v) } else { fmt.Printf(\u0026#34;\\n \u0026#39;%d\u0026#39; is an odd number.\u0026#34;, v) } } } In this example, instead using two variables, the for loop used an underscore character (_) to ignore the value of index coming from range. This is called a blank identifier in Go.\nYou can use the above method to iterate over a slice or even string as well.\nHere is an example that shows how to iterate over a map.\n1 2 3 4 5 6 7 8 9 10 11 12 package main import ( \u0026#34;fmt\u0026#34; ) func main() { a := map[int]string{0: \u0026#34;Go\u0026#34;, 1: \u0026#34;PowerShell\u0026#34;, 2: \u0026#34;Python\u0026#34;} for k, v := range a { fmt.Printf(\u0026#34;\\n%d ==\u0026gt; %s\u0026#34;, k, v) } } In case of a map value, instead of index, you get the key and value. In the above example, the variables k and v represent key and value coming from the map.\nContinue In one of the previous examples, you looked at how to break out of a loop based on a condition. But, what if you just want to skip to the next iteration instead? This is where the continue keyword is helpful. In the following example, the loop simply continues to the next element in the array on encountering an even number.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 package main import \u0026#34;fmt\u0026#34; func main() { a := []int{1, 2, 3, 4, 5, 6, 7, 8, 9, 10} for _, v := range a { if v%2 == 0 { continue } fmt.Printf(\u0026#34;\\n%d is an odd number.\u0026#34;, v) } } When you run this code, whenever v is an even number, you will see that the print statement gets skipped and the execution goes to the next element in the iteration and therefore printing only odd numbers between 1 and 10. So, whenever execution reaches to the continue statement, the rest of the statements in the loop body get skipped and loop continues to the next iteration.\nNested loops Similar to nested if statements, you can nest for loops too in Go language. Here is a very example.\n1 2 3 4 5 6 7 8 9 10 11 12 package main import \u0026#34;fmt\u0026#34; func main() { //OuterLoop: for x := 0; x \u0026lt; 3; x++ { for y := 0; y \u0026lt; 3; y++ { fmt.Printf(\u0026#34;x=%v, y=%v\\n\u0026#34;, x, y) } } } You can nest any variants of for loop seen in the above examples.\nThis is it for this part of the series. In the next part, you will learn about labels in Go language.\n","date":"2020-10-07T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/get-set-go-loops-in-go-language/","series":"Get Set GO","title":"Get set Go - Loops in Go language"},{"categories":["Golang","Go"],"content":" With what you learned about basic data types and structs in this series so far, you are now ready to look at the branching constructs in Go language. You may have used branching constructs such as if..else if..else and switch statements in other languages. Go language too offers these constructs.\nIf .. else if .. else If construct in any programming language is a branching construct and moves the execution from one place in the code to another based on a condition. Go language is no exception.\nThe general syntax of if statement in Go language is as below.\n1 2 3 4 5 6 7 if condition { // code\t} else if condition { // code } else { // code } For the code in the if and else if blocks to execute, the condition must evaluate to true. If none of the conditions in the if and else if do not evaluate to true, the code in else gets executed. The else if and else blocks are optional. As with other types of command blocks in Go, all opening brackets must be on the same line as if, else if, and else statements.\nTake a look at this example.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 package main import \u0026#34;fmt\u0026#34; func main() { a := 10 if a \u0026gt; 10 { fmt.Println(\u0026#34;Varible a is greater than 10\u0026#34;) } else if a \u0026lt; 10 { fmt.Println(\u0026#34;Varible a is less than 10\u0026#34;) } else { fmt.Println(\u0026#34;Varible a is equal to 10\u0026#34;) } } When you run this program, the Println statement in the else block will execute since the variable a is initialized to a value 10. The first and second conditions will evaluate to false and therefore the control goes into the else block.\nAssignment in a comparison As a part of the if construct you can perform variable declaration and initialization as well.\n1 2 3 4 5 6 7 8 9 package main import \u0026#34;fmt\u0026#34; func main() { if a := 10; a%2 == 0 { fmt.Println(\u0026#34;10 is an even number\u0026#34;) } } What you have seen in the above example, \u0026gt;, \u0026lt;, and ==, are just two of the available comparison operators in Go language.\nComparison Operators There are different comparison operators you can in use in Go language. These operators compare two operands and return a boolean value \u0026ndash; true or false. While using any comparison operator, both operands must be of the same type. For example, you can compare an integer to a float without explicitly casting one of the operand.\nOperator Description Type == Equal to Equality != Not equal to Equality \u0026lt; Less than Ordering \u0026gt; Greater than Ordering \u0026lt;= Less than or equal to Ordering \u0026gt;= Greater than or equal to Ordering The equality operators from the above table apply to types that are comparable. Booleans, integers, strings, arrays, structs, complex numbers, and floating point values are all comparable in addition to a few more types such as channels and interfaces that you will learn in the future. Slices and maps are not comparable types.\nThe second type of operators, ordering operators, can be used only with types that can be ordered. Ordering operators cannot be used with structs, pointers, arrays, complex numbers, booleans, interfaces, and channels. So, that essentially leaves you with integers, strings, and floating point values.\nGo language allows chaining different comparisons using logical operators.\nLogical Operators With the logical operators the operands are always boolean values and return a boolean result.\nOperator Syntax Description \u0026amp;\u0026amp; (AND) if a \u0026amp;\u0026amp; b {} Evaluates to true only if both a and b are true. || (OR) if a || b {} Evaluates to true if any of operands a or b are true. ! (NOT) if !a Evaluates to true if a is false. Take a look at this example that uses the logical operators along with if statement.\n1 2 3 4 5 6 7 8 9 10 11 12 package main import \u0026#34;fmt\u0026#34; func main() { a := 10 if a \u0026gt; 1 \u0026amp;\u0026amp; a \u0026lt; 10 { fmt.Println(\u0026#34;Varible a has a value between 2 and 9\u0026#34;) } else if a \u0026gt;= 10 \u0026amp;\u0026amp; a \u0026lt;= 20 { fmt.Println(\u0026#34;Varible a has a value greater than or equal to 10 and less than or equal to 20\u0026#34;) } } When you have multiple conditions to evaluate and take an action, the typical if construct may become too long. This is where many programming languages, including Go, provide the switch construct.\nSwitch A switch statement evaluates an expression and compares the result against a few possible matches defined within the switch block.\nThe general syntax for writing a switch construct is as follows.\n1 2 3 4 5 6 7 8 switch \u0026lt;expression\u0026gt; { case \u0026lt;expr1\u0026gt;: //code case \u0026lt;expr2\u0026gt;: //code default: //code } As you see in the above syntax, the switch statement is followed by an expression. This can be a simple variable or an expression that evaluates to a value that can be looked up in the possible matches in a switch block. You can have any number of cases to match. These cases must all be unique. The default block gets executed when no other cases match. Default is optional.\nHere is a full example.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 package main import \u0026#34;fmt\u0026#34; func main() { a := 10 switch a { case 20: fmt.Println(\u0026#34;Value of a is 20\u0026#34;) case 30: fmt.Println(\u0026#34;Value of a is 30\u0026#34;) case 10: fmt.Println(\u0026#34;Value of a is 10\u0026#34;) default: fmt.Println(\u0026#34;Value of a did not match any cases above\u0026#34;) } } When you run this program, the third case gets evaluated and \u0026ldquo;Value of a is 10\u0026rdquo; gets printed. The case statement can have multiple expressions as well. For example,\n1 2 3 4 5 6 7 8 9 10 11 12 13 package main import \u0026#34;fmt\u0026#34; func main() { string1 := \u0026#34;o\u0026#34; switch string1 { case \u0026#34;a\u0026#34;, \u0026#34;e\u0026#34;, \u0026#34;i\u0026#34;, \u0026#34;o\u0026#34;, \u0026#34;u\u0026#34;: fmt.Println(\u0026#34;Supplied string is an vowel\u0026#34;) default: fmt.Println(\u0026#34;Supplied string is not an vowel\u0026#34;) } } Unlike other languages, Go does not allow an automatic fall through. If you need a fall through execution of all subsequent case blocks, you need to explicitly specify that using the fallthrough keyword.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 package main import \u0026#34;fmt\u0026#34; func main() { switch a := 56; { case a \u0026lt; 50: fmt.Println(\u0026#34;a is less than 50\u0026#34;) fallthrough case a \u0026lt; 100: fmt.Println(\u0026#34;a is less than 100\u0026#34;) fallthrough case a \u0026lt; 150: fmt.Println(\u0026#34;a is less than 150\u0026#34;) fallthrough case a \u0026lt; 200: fmt.Println(\u0026#34;a is less than 200\u0026#34;) } } In this program, variable a is getting declared and initialized as a part of the switch statement expression. In the case statements, there are checks to see if the value if less than 50, 100, 150, and 200. If the value of variable a is less than 50, all other cases will evaluate to true. In such a scenario, all other case statements will have to be executed as well. This is where fallthrough keyword plays a role.\nWhen you execute the above program, you will see the following output.\n1 2 3 4 PS C:\\GitHub\\GetSetGo\u0026gt; go run .\\switch.go a is less than 100 a is less than 150 a is less than 200 Now, you may ask about how the one of the examples that demonstrated the if statement can be translated to use switch construct. One way is what you already seen in the above example. An alternate way is to use expressionless switch construct. Here is how it is done.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 package main import \u0026#34;fmt\u0026#34; func main() { a := 21 switch { case a \u0026gt; 1 \u0026amp;\u0026amp; a \u0026lt; 10: fmt.Println(\u0026#34;Varible a has a value between 2 and 9\u0026#34;) case a \u0026gt;= 10 \u0026amp;\u0026amp; a \u0026lt;= 20: fmt.Println(\u0026#34;Varible a has a value greater than or equal to 10 and less than or equal to 20\u0026#34;) default: fmt.Println(\u0026#34;Variable a is greater than 20\u0026#34;) } } In the above example, there is no expression associated with the switch statement. Instead, we have expressions matching at the case statement.\nThis brings this part to an end. In the next part of this series, you will learn about loops in Go language.\n","date":"2020-10-05T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/get-set-go-condition-statements-in-go-language/","series":"Get Set GO","title":"Get set Go - Condition statements in Go language"},{"categories":["Golang","Go"],"content":" So far in this series, you learned about different built-in data types such as integers, strings, floats, arrays, slices, maps, and pointers. All of these allow only one type of data. All elements in the array have to of the same time. All keys and values have to be of the same type. What if you have need to combine multiple types and create a custom type for your programs? This is where structs play a role in Go language.\nStructs A struct in Go language is a user-defined data type which is essentially a collection of different types. For example, when describing information about a computer in a data structure, you will describes the properties of a computer such as Model, serial number, amount of physical memory, number of processor cores, size of the hard drive and so on. When you have to create a such a data structure, other programming languages that implement true object oriented programming (OOP) concepts have classes. However, Go is not a true OOP language. Instead, Go offers something closer with the help structs. What an OOP language supports \u0026ndash; such as inheritance and polymorphism \u0026ndash; out of the box can be done in Go by working around a few things but it still won\u0026rsquo;t be true object oriented.\nThe syntax to declare struct user-defined data type is as shown below.\n1 2 3 4 5 type \u0026lt;name-of-custom-type\u0026gt; struct { field1-Name field1-Type field2-Name field2-Type ... } Take a look at this example to understand how you can implement a custom data type using the above syntax.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 package main import \u0026#34;fmt\u0026#34; func main() { type computer struct { model string serialNumber string memoryInGB int numProcCores int diskCapacityInTB int } cmp1 := computer{ model: \u0026#34;SuperComputer10\u0026#34;, serialNumber: \u0026#34;SCOMP123\u0026#34;, memoryInGB: 96, numProcCores: 8, diskCapacityInTB: 5, } fmt.Println(cmp1) } The above example, declared a struct named computer and added five fields to it. Then, we created a struct cmp1 of the type computer by specifying each field name and an associated value separated by a colon (:). If you prefer, you can totally eliminate specifying the field names. For example, cmp1 := {\u0026quot;SuperComputer10\u0026quot;, \u0026quot;SCOMP123\u0026quot;, 96, 8, 5} is totally valid as well but does not have convey how the values are associated to field names except the order in which they are specified.\nFinally, when you print a struct, it just prints the values you specified while creating the struct. You can retrieve a specific field value buy using the reference (.) operator.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 package main import \u0026#34;fmt\u0026#34; func main() { type computer struct { model string serialNumber string memoryInGB int numProcCores int diskCapacityInTB int } cmp1 := computer{ model: \u0026#34;SuperComputer10\u0026#34;, serialNumber: \u0026#34;SCOMP123\u0026#34;, memoryInGB: 96, numProcCores: 8, diskCapacityInTB: 5, } fmt.Println(cmp1.model) fmt.Println(cmp1.memoryInGB) } While it is always possible to assign values to all fields while creating the struct, it is not mandatory. You can always create zero valued struct.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 package main import \u0026#34;fmt\u0026#34; func main() { type computer struct { model string serialNumber string memoryInGB int numProcCores int diskCapacityInTB int } var cmp1 computer fmt.Println(cmp1) cmp1.model = \u0026#34;SuperComputer10\u0026#34; cmp1.serialNumber = \u0026#34;SCOMP123\u0026#34; cmp1.memoryInGB = 96 cmp1.numProcCores = 8 cmp1.diskCapacityInTB = 6 fmt.Println(cmp1) } When you run the above program, the first Println will print the zero values of fields in the struct based on their data types. The second Println will print the assigned values.\nAnonymous structs In the examples so far, we declared a user-defined data type and have given it a name. We then used it to create a struct and initialized it with field values. However, it is possible to create structs without a name defined. Here is how you do that.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 package main import \u0026#34;fmt\u0026#34; func main() { cmp1 := struct { model string serialNumber string memoryInGB int numProcCores int diskCapacityInTB int }{ model: \u0026#34;SuperComputer10\u0026#34;, serialNumber: \u0026#34;SCOMP123\u0026#34;, memoryInGB: 96, numProcCores: 8, diskCapacityInTB: 5, } fmt.Println(cmp1) } What you defined above as a struct, cmp1, is called an anonymous struct variable. And, similar to this, you can defined anonymous struct fields also.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 package main import \u0026#34;fmt\u0026#34; func main() { type computer struct { string int } cmp1 := computer{ string: \u0026#34;SuperComputer10\u0026#34;, int: 8, } fmt.Println(cmp1) } In this example, the user-defined type computer has only two fields \u0026ndash; string and int. Unlike our earlier example that has named fields, you have multiple fields of the same data type when using anonymous fields in a struct.\nNested Structs Go allows adding user-defined struct data types as a field within another struct. Take a look at this example.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 package main import \u0026#34;fmt\u0026#34; func main() { type os struct { version string build int } type computer struct { model string serialNumber string memoryInGB int numProcCores int diskCapacityInTB int operatingSystem os } cmp1 := computer{ model: \u0026#34;SuperComputer10\u0026#34;, serialNumber: \u0026#34;SCOMP123\u0026#34;, memoryInGB: 96, numProcCores: 8, diskCapacityInTB: 6, opeatingSystem: os{ version: \u0026#34;Windows 10\u0026#34;, build: 2009, }, } fmt.Println(cmp1) } In the above example, the struct named os declares a user-defined data type containing version and build as the fields. In the computer type declaration, you can simply added a new field called operatingSystem and assigned it the type os.\nThe nested fields from the struct can be accessed by following the complete field path. For example, cmp1.operatingSystem.version.\nStruct Pointers In a previous part of this series, you learned about pointers where you have only seen integer and string pointers. You can create struct pointers as well. Here is an example.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 package main import \u0026#34;fmt\u0026#34; func main() { type os struct { version string build int } type computer struct { model string serialNumber string memoryInGB int numProcCores int diskCapacityInTB int operatingSystem os } cmp1 := \u0026amp;computer{ model: \u0026#34;SuperComputer10\u0026#34;, serialNumber: \u0026#34;SCOMP123\u0026#34;, memoryInGB: 96, numProcCores: 8, diskCapacityInTB: 6, operatingSystem: os{ version: \u0026#34;Windows 10\u0026#34;, build: 2009, }, } fmt.Println(*cmp1) fmt.Println((*cmp1).operatingSystem.version) } The process of creating a pointer to a struct is similar to the other data types. The only difference is in how you dereference the value of a struct. You can see that in the Println statements towards the end of the program.\nThis is it for today. You will learn more about adding methods to structs and using structs in a more meaningful way than these trivial examples. Stay tuned.\n","date":"2020-10-01T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/get-set-go-structs-in-go-language/","series":"Get Set GO","title":"Get set Go - Structs in Go language"},{"categories":["Golang","Go"],"content":" In the previous part of this series, you learned about pointers in Go language. In this part, you will learn about maps in Go language. Maps are another built-in data type in Go and maps store key-value pairs. This is like dictionaries in other programming languages.\nMaps First, look at the below example.\n1 2 3 4 5 6 7 8 package main import \u0026#34;fmt\u0026#34; func main() { var m1 map[string]int fmt.Println(m1) } In the above example, variable m1 is of map data type and declares an empty map. When you run this, all you will see is map[]. The zero value of map keys will be nil and therefore this will be called a nil map. Maps store key-value pairs. So, if we generalize the syntax from the above example, it will be:\nvar \u0026lt;variable-name\u0026gt; map[\u0026lt;key-data-type\u0026gt;]\u0026lt;value-data-type\u0026gt;\nKeys in a map can be of any comparable type \u0026ndash; Boolean, integer, float, string, complex, and so on.\nIn the key-value pairs that you want to store, all keys have to be of the same data type and all values should also be. In the example above, all keys will have to of string type and all values should be of int type. Also, keys should be unique within a map.\nNow, since you have just created a nil map, how do you add key-value pairs or elements to this map?\nAdding Key Values When you create a map using the example above, it creates a nil map which is equivalent to empty map but no elements can be added to it. If you indeed attempt something like m1[\u0026quot;Go\u0026quot;] = 1 you will see a panic. The following example shows how to initialize an empty map.\n1 2 3 4 5 6 7 8 9 10 package main import \u0026#34;fmt\u0026#34; func main() { m2 := make(map[string]int) m2[\u0026#34;Go Language\u0026#34;] = 1 fmt.Println(m2) } As you see in the example here, you need to use the built-in make function to declare and initialize the map. Once this is done, you can use the simple assignment syntax to add elements to the map.\nUpdating elements in a map is again same as how you added the element using assignment. So, by using m2[\u0026quot;Go Language\u0026quot;] = 10, you will update the value associated with that key.\nWhat if you have lot of items to add to a map? Do you need to perform assignment for each item? No, there is a better way to initialize the map like arrays and slices.\nInitialize during declaration You can add values to a map while declaring the map. Here is the generic syntax.\n\u0026lt;variable-name\u0026gt; := map[\u0026lt;key-type\u0026gt;]\u0026lt;value-type\u0026gt;{ \u0026quot;key\u0026quot;: \u0026quot;value\u0026quot;, \u0026quot;key\u0026quot; : \u0026quot;value\u0026quot;}\n1 2 3 4 5 6 7 8 9 10 11 12 13 package main import \u0026#34;fmt\u0026#34; func main() { m2 := map[string]int{ \u0026#34;Go Language\u0026#34;: 1, \u0026#34;PowerShell\u0026#34;: 2, \u0026#34;Python\u0026#34;: 3, } fmt.Println(\u0026#34;length of map\u0026#34;, len(m2)) fmt.Println(m2) } Simple. Observe the trailing comma after the last item. This is needed since Go treats newline as end of statement. As shown in the example, you can use the built-in len() function to retrieve the length or number of elements in a map.\nIf you want to retrieve a single element value from the map, you can do that simply by referencing the correct key name.\nm2[\u0026quot;PowerShell\u0026quot;] will return it\u0026rsquo;s value 2. When you reference a non-existing key, you will simply get the zero value based on the type of the value specified during declaration. So, in this example, if you try m2[\u0026quot;Rust\u0026quot;], you will get 0 as the value. This is not always desired as the subsequent statements in the program might assume that 0 is a real value.\nSo, how do you check if a key exists in the map or not?\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 package main import \u0026#34;fmt\u0026#34; func main() { m2 := map[string]int{ \u0026#34;Go Language\u0026#34;: 1, \u0026#34;PowerShell\u0026#34;: 2, \u0026#34;Python\u0026#34;: 3, } value, exists := m2[\u0026#34;Rust\u0026#34;] fmt.Println(value, exists) } when you query for a non-existing key within a map, it will return both associated value and a Boolean value representing whether the key exists or not. Therefore, when you want to check if a key exists in a map or not, you should always use the method shown above. If you are only interested in verifying the key existence, you can ignore the value returned by using syntax as shown below.\n_, exists := m2[\u0026quot;Rust\u0026quot;]\nDelete items You can use the built-in delete() function to delete items from a map.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 package main import \u0026#34;fmt\u0026#34; func main() { m2 := map[string]int{ \u0026#34;Go Language\u0026#34;: 1, \u0026#34;PowerShell\u0026#34;: 2, \u0026#34;Python\u0026#34;: 3, } fmt.Println(m2) delete(m2, \u0026#34;Python\u0026#34;) fmt.Println(m2) } Item ordering You need to understand that maps in Go language are unordered. So, the order in which you add or initialize elements may not be the same when you retrieve or iterate over items in the map. Take a look at this example.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 package main import \u0026#34;fmt\u0026#34; func main() { m2 := map[string]int{ \u0026#34;Go Language\u0026#34;: 1, \u0026#34;PowerShell\u0026#34;: 2, \u0026#34;Python\u0026#34;: 3, \u0026#34;Rust\u0026#34;: 4, \u0026#34;C++\u0026#34;: 5, } fmt.Println(m2) } When you run this program, the order of items printed towards the end may not be same as the order in which they were initialized or added to the map.\nSimilar to slices, maps are also reference types. Take a look at this example.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 package main import \u0026#34;fmt\u0026#34; func main() { m2 := map[string]int{ \u0026#34;Go Language\u0026#34;: 1, \u0026#34;PowerShell\u0026#34;: 2, \u0026#34;Python\u0026#34;: 3, } m3 := m2 m3[\u0026#34;Rust\u0026#34;] = 4 fmt.Println(\u0026#34;Map m2\u0026#34;, m2) fmt.Println(\u0026#34;Map m3\u0026#34;, m3) } When you run this program, you will see the same number of items printed for both maps. This is because maps are reference types. When you assign one map to another, all changes done to one map will be made in the other map as well.\nAlright. This is all about basics of maps in Go language. You will learn more about using maps later in this series. Stay tuned.\n","date":"2020-09-30T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/get-set-go-maps-in-go-language/","series":"Get Set GO","title":"Get set Go - Maps in Go language"},{"categories":["Golang","Go"],"content":" In the previous part of this series, you learned about slices in Go language. Slices, unlike arrays, allow flexibility and certainly are the most used compared to arrays in Go language. Towards the end of the article on slices, you learned that slices are essentially references to an underlying array and any changes made to the slice will reflect in the underlying array as well. In this article, you shall learn about another reference type called pointers in Go language.\nPointers Pointers in Go, like other languages, hold the memory address of a value. First, an example.\n1 2 3 4 5 6 7 8 9 10 11 12 package main import \u0026#34;fmt\u0026#34; func main() { a := 10 b := \u0026amp;a fmt.Println(\u0026#34;value of a is\u0026#34;, a) fmt.Println(\u0026#34;value of b is\u0026#34;, b) fmt.Println(\u0026#34;value at b is\u0026#34;, *b) } The first variable declaration and initialization in the above example is already known. It is a simple integer. The line where variable b is declared is special. On the right-hand side, using \u0026amp; prefix tells Go that you want to store the memory location of the value represented by variable a. \u0026amp; is called the addressOf operator. So, variable b gets the memory address of variable a. In the last line, the asterisk (*) dereferences the value at variable b and therefore it is called the dereferencing operator.\nThis should be clear if you run this example.\n1 2 3 4 PS C:\\GitHub\\GetSetGo\u0026gt; go run .\\pointers.go value of a is 10 value of b is 0xc000012090 value at b is 10 The second output shows the value stored in variable b which is the memory address of variable a. And, the last line in the output shows the value at the memory address represented by variable b which is the value of variable a \u0026ndash; 10.\nSince variable b is the address of value represented variable a, you cannot assign another integer value to b directly. Instead, you dereference and then assign a value which changes the value of variable a.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 package main import \u0026#34;fmt\u0026#34; func main() { a := 10 b := \u0026amp;a fmt.Println(\u0026#34;value of a is\u0026#34;, a) fmt.Println(\u0026#34;value of b is\u0026#34;, b) fmt.Println(\u0026#34;value at b is\u0026#34;, *b) *b = 20 fmt.Println(\u0026#34;value of b is\u0026#34;, b) fmt.Println(\u0026#34;value of a is\u0026#34;, a) } If you run this above example, you will see output similar to what is shown below.\n1 2 3 4 5 6 PS C:\\GitHub\\GetSetGo\u0026gt; go run .\\pointers.go value of a is 10 value of b is 0xc000012090 value at b is 10 value of b is 0xc000012090 value of a is 20 If you observe the output, the memory address stored in variable b itself does not change but the value assigned to variable gets updated to 20. This is good but are there other methods we can declare pointers in Go language?\nThe regular variable declaration syntax works for variables as well.\nvar b *int = \u0026amp;a\nThe above method is just an expanded form of what you learned in the first example. In this expanded form, there is explicit declaration of pointer type which is *int in this case or you can call it an integer pointer. You can skip the initialization part and just declare an integer pointer. In such as case the zero value of a pointer will be set to nil and you cannot dereference until you assign a value.\nUsing new function When you use the built-in new function to declare and initialize a variable, you get a pointer to its memory address. Take a look at the below example.\n1 2 3 4 5 6 7 package main func main() { a := new(int) println(a) println(*a) } When you run this, the first println will print the memory address and when you dereference it\u0026rsquo;s value, you will see a zero value.\nThis is just an introduction to pointers in Go language and you will learn more about the use cases of pointers when we discuss functions and other advanced concepts. Stay tuned.\n","date":"2020-09-29T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/get-set-go-pointers-in-go-language/","series":"Get Set GO","title":"Get set Go - Pointers in Go language"},{"categories":["Golang","Go"],"content":" In the previous part of this series, you learned about arrays in Go language. As you have learned, arrays have a fixed size and therefore you cannot resize (grow or shrink) arrays. This limitation can be overcome using slices in Go language. In this part, you will learn about slices and how to use this data type in your Go programs.\nSlices A slice in Go language is a slice of an underlying array! Yes, no pun intended. 😊\nHere is how the Go language specification describes slices. A slice is a descriptor for a contiguous segment of an underlying array and provides access to a numbered sequence of elements from that array.\nThere are different ways to declaring and initializing slices in Go language.\nSlice literal The syntax for declaring a variable as a slice is var s1 = []int{1,2}. This declare a slice of integers and initializes with two elements. This is similar to how you declared array variables. The only difference is that there is no size specification. You can avoid any values in the declaration to create an empty slice of integers. For example, var s2 = []int{}. However, you cannot add elements (to an empty slice or at an index that is out of bound) using the indexer syntax. For example, specifying s2[0]= 100 will result in an error. The indexer method works only when updating an existing element at a specific index in the slice. To add elements to an empty slice, you need use the built-in append() method.\nHere is a quick example that shows both these methods.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 package main import \u0026#34;fmt\u0026#34; func main() { var s1 = []int{1, 2} s1[0] = 10 fmt.Println(s1) var s2 = []int{} s2 = append(s2, 100) s2 = append(s2, 200) fmt.Println(s2) } With slices, unlike arrays, you can append elements. This essentially changes the size of the slice. You can use the len() built-in function to find the current number of elements and the cap() function to find the maximum capacity of the slice. Review the below example to understand the difference between length and capacity in-depth.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 package main import \u0026#34;fmt\u0026#34; func main() { var s1 = []int{1, 2} fmt.Println(\u0026#34;Initial len and cap\u0026#34;, len(s1), cap(s1)) s1 = append(s1, 3) fmt.Println(\u0026#34;len and cap after adding 3rd element\u0026#34;, len(s1), cap(s1)) s1 = append(s1, 4) fmt.Println(\u0026#34;len and cap after adding 4th element\u0026#34;, len(s1), cap(s1)) s1 = append(s1, 5) fmt.Println(\u0026#34;len and cap after adding 5th element\u0026#34;, len(s1), cap(s1)) } When you run this, you will see the following output.\n1 2 3 4 5 PS C:\\GitHub\\GetSetGo\u0026gt; go run .\\slices.go Initial len and cap 2 2 len and cap after adding 3rd element 3 4 len and cap after adding 4th element 4 4 len and cap after adding 5th element 5 8 If you observe the above output, the initial length and capacity of the slice is what you initialized it to \u0026ndash; with two elements. After adding a third element, the capacity changed to 4 and stayed as 4 even after the adding the 4th element. However, the moment the 5th element gets added, the capacity of the slice jumps to 8. So, what is happening here?\nWith the append function, whenever you add an element, if the length exceeds the current capacity of the slice, Go simply doubles the capacity. So, when we added the 3rd element, the capacity doubled to 4 and then we added the 5th element, it doubled to 8.\nUsing Make Slices can also be declared and initialized using the make built-in function. The syntax for using make to create a slice is as follows.\n1 var name = make(slice-type, slice-length, slice-capacity) Here is an example that uses make function.\n1 2 3 4 5 6 7 8 9 10 11 12 package main import \u0026#34;fmt\u0026#34; func main() { var s3 = make([]int, 2, 4) s3[0] = 10 s3[1] = 20 s3 = append(s3, 20) fmt.Println(s3, len(s3), cap(s3)) } In this example, the last integer value that is used to specify the capacity of the slice is optional. By default, the capacity will be same as length. The code in the above example creates a slice with length of 2 and capacity of 4. And, observe how the 3rd element is added. You have to use the append function since adding the 3rd element exceeds the length specified when using make function. This behavior is different from what you seen above with append method on a slice created using slice literal syntax.\nSlicing arrays Yet another method of creating a slice is to slice an existing array! Take a look at this example.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 package main import \u0026#34;fmt\u0026#34; func main() { var languages = [6]string{\u0026#34;Go\u0026#34;, \u0026#34;Python\u0026#34;, \u0026#34;Rust\u0026#34;, \u0026#34;Java\u0026#34;, \u0026#34;C#\u0026#34;, \u0026#34;PowerShell\u0026#34;} fmt.Println(\u0026#34;Elements in the string array are\u0026#34;, languages) var langSlice = languages[0:2] fmt.Println(\u0026#34;\\nElements in the derived slice are\u0026#34;, langSlice) fmt.Println(\u0026#34;\\nSlicing without a start index is similar to using start index 0\u0026#34;, languages[:3]) fmt.Println(\u0026#34;\\nSlicing without an end index gets all elements from start_index till end of the array\u0026#34;, languages[4:]) } In the above example, the languages array has six elements. A slice of these elements \u0026ndash; from index 0 to 2 \u0026ndash; is assigned to the variable langSlice. This variable is of slice data type. In the variable assignment, you have specified the the slice as start_index:end_index in square brackets. In this specification,\nThe end_index in excluded. So, when the indexes specified are 0 and 2, the resulting slice will have elements from index 0 and 1 but not 2. Both start_index and end_index are optional. So, for example, if you specify [:3], the first three elements from the array. If you specify [4:], elements from index 4 till the end of the array get assigned to the slice. Finally, skipping both \u0026ndash; [:] \u0026ndash; will simply return all elements from the array. You learned in an earlier part of this series that slices are reference types. This means that when you update an element in the slice the element in the underlying array gets updated as well.\nTake a look at this example.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 package main import \u0026#34;fmt\u0026#34; func main() { var languages = [6]string{\u0026#34;Go\u0026#34;, \u0026#34;Python\u0026#34;, \u0026#34;Rust\u0026#34;, \u0026#34;Java\u0026#34;, \u0026#34;C#\u0026#34;, \u0026#34;PowerShell\u0026#34;} fmt.Println(\u0026#34;Elements in the string array are\u0026#34;, languages) var langSlice = languages[0:2] fmt.Println(\u0026#34;\\nElements in the derived slice are\u0026#34;, langSlice) langSlice[1] = \u0026#34;C++\u0026#34; fmt.Println(\u0026#34;\\nElements in the updated slice are\u0026#34;, langSlice) fmt.Println(\u0026#34;\\nElements in the underlying array are\u0026#34;, languages) } If you run this above example, you will see output similar to what is shown here.\n1 2 3 4 5 6 7 8 PS C:\\GitHub\\GetSetGo\u0026gt; go run .\\slices.go Elements in the string array are [Go Python Rust Java C# PowerShell] Elements in the derived slice are [Go Python] Elements in the updated slice are [Go C++] Elements in the underlying array are [Go C++ Rust Java C# PowerShell] As you see here, once you update an element in the derived slice, the underlying array also gets updated. This brings us to the discussion around pointers in Go language which is the subject of next part in this series.\nStay tuned.\n","date":"2020-09-28T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/get-set-go-slices-in-go-language/","series":"Get Set GO","title":"Get set Go - Slices in Go language"},{"categories":["Azure","Community","Speaking"],"content":" Azure Community Conference is a 3 day multi-track virtual event scheduled to go live on November 24th 2020. This event is being organized by the Bangalore Azure User Group. The target audience of this virtual event are cloud and web developers working with open source and cloud native technologies on the Microsoft stack. This event will be a free two conference days that includes a free workshop as well. Registration for this event is open.\nThis event already has got a super-star line up of speakers including Janakiram MSV, James Montemagno, Dhananjay Kumar, Lars Klint, and Neependra Khare to name just a few. I will be speaking as well on one of my most favorite topics in the Infrastructure as Code (IaC) space. If you want to speak, the call of papers is still open.\nWhether you are an expert or still a learner (a life long learner like me), go ahead and register for this event. I am sure there is loads of learning coming our way!\n","date":"2020-09-21T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/azure-community-conference-2020/","series":null,"title":"Azure Community Conference - 2020"},{"categories":["Golang","Go"],"content":" In the earlier part of this series, you looked at types in Go language and learned about the basic data types \u0026ndash; int, float, and strings. You also learned about other categories of data types such as aggregate types, reference types, and interface types. In the next subsequent parts, you will dive into each of these different types.\nAggregate Types Aggregate types in Go language include arrays and structs. These data types are formed by combining basic data types. Learning structs will require knowledge of a few more things about Go language and Go types. You will learn about that soon. In today\u0026rsquo;s part, you will learn about arrays.\nArrays An array in Go language is a fixed-length sequence of homogeneous elements in the memory. There is emphasis on two words in the previous sentence \u0026ndash; fixed-length and homogeneous. The length of the array is decided at the time of declaration. You cannot change it runtime and therefore fixed-length. And, an array can only be used store elements of the same data type and that is the homogeneous part in the definition. The number of elements in the array is called the length of the array and it will be an integer that is either zero or more than zero.\nThe method to define/declare an array in Go is similar to that of a variable.\n1 var \u0026lt;variable_name\u0026gt; [SIZE] \u0026lt;variable_type\u0026gt; As you see in the above syntax, size is a part of the array declaration. So, for example, var names [4] string will declare an array of fixed-length 4 to store values of string data type. The size must be an integer greater than zero. You can access the array elements using the index operator and the index always starts at zero and goes up to index of last element - 1. The built-in len function can be used to find the length of the array. Therefore, the index of the last element can be derived using len(arrayName) - 1.\nTake a look at this example.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 package main import \u0026#34;fmt\u0026#34; func main() { var names [4]string // elements get initialized to a zero value based on type fmt.Println(\u0026#34;value at index 0 is\u0026#34;, names[0]) // set a value at an index names[0] = \u0026#34;Go Language\u0026#34; fmt.Println(\u0026#34;value at index 0 is\u0026#34;, names[0]) // get length of the array var l = len(names) fmt.Println(\u0026#34;Length of names array is\u0026#34;, l) fmt.Println(\u0026#34;value at last index is\u0026#34;, names[l-1]) } As shown in the above example, using the index operator gives you the value at that index and you can assign / update a value at a given array index. Using this method of assigning values can be tedious and error-prone if you have a larger array. You can initialize arrays in a way similar to how variables of basic types are initialized during declaration or using shorthand form.\n1 var variable_name = [SIZE]\u0026lt;variable_type\u0026gt;{item1, item2, item3, ...itemN} var names = [4]string{\u0026quot;Go\u0026quot;, \u0026quot;Python\u0026quot;, \u0026quot;PowerShell\u0026quot;, \u0026quot;Rust\u0026quot;} will declare and initialize the names array. The number of values you specify within the curly brackets should be equal to the size of the array specified within square brackets. You can avoid specifying the size of the array too \u0026ndash; var languages = [4]string{\u0026quot;Go\u0026quot;, \u0026quot;Python\u0026quot;, \u0026quot;PowerShell\u0026quot;, \u0026quot;Rust\u0026quot;}. The number of values specified in the initialization will be used to set the size of the array.\nAnd, finally, within a function, you can always use the shorthand declaration. For example, languages := [4]string{\u0026quot;Go\u0026quot;, \u0026quot;Python\u0026quot;, \u0026quot;PowerShell\u0026quot;, \u0026quot;Rust\u0026quot;}\nHere is an example.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 package main import \u0026#34;fmt\u0026#34; func main() { languages := []string{\u0026#34;Go\u0026#34;, \u0026#34;Python\u0026#34;, \u0026#34;PowerShell\u0026#34;, \u0026#34;Rust\u0026#34;} fmt.Println(\u0026#34;All elements in the array are\u0026#34;, languages) fmt.Println(\u0026#34;value at index 0 is\u0026#34;, languages[0]) // get length of the array var l = len(languages) fmt.Println(\u0026#34;Length of languages array is\u0026#34;, l) fmt.Println(\u0026#34;value at last index is\u0026#34;, languages[l-1]) } Whatever you have seen so far are single-dimension arrays. You can create multi-dimension arrays too in Go language. The syntax for that is not too different from what you have already tried. You just have to add multiple size values.\n1 var \u0026lt;variable-name\u0026gt; = [SIZE1][SIZE2][SIZEn]\u0026lt;variable-type\u0026gt;{{values1},{values2},{valuesn}} for example, var languages = [2][2]string{{\u0026quot;Go\u0026quot;,\u0026quot;PowerShell\u0026quot;},{\u0026quot;English\u0026quot;, \u0026quot;Spanish\u0026quot;}} will declare a two-dimensional array. Notice the curly brackets around the value specification in the array declaration. You can access the array elements in a multi-dimensional array by specifying multiple indices. For example, languages[0][0] will result in the value Go retrieved from the array. If you specify only one index, Go will present the entire set of elements at the dimension that matches the index.\nHere is an example.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 package main import \u0026#34;fmt\u0026#34; func main() { languages := [2][2]string{{\u0026#34;Go\u0026#34;, \u0026#34;Python\u0026#34;}, {\u0026#34;English\u0026#34;, \u0026#34;PowerShell\u0026#34;}} fmt.Println(\u0026#34;value at index 0 is\u0026#34;, languages[0][0]) // get length of the array var l = len(languages) fmt.Println(\u0026#34;Length of languages array is\u0026#34;, l) fmt.Println(\u0026#34;value at last index is\u0026#34;, languages[l-1]) } In the example above, the length of the languages array is 2. It is the number of dimensions and not the total number of elements.\nSo far, you have learned about arrays in Go language. To summarize,\nArrays in Go are fixed-length and can contain elements of the same data type Arrays in Go can either be single or multi dimensional. Arrays in Go can neither be resized nor a sub-array can be retrieved The last point can be addressed using slices in Go language. And, that is the next part of this series.\n","date":"2020-08-31T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/get-set-go-arrays-in-go-language/","series":"Get Set GO","title":"Get set Go - Arrays in Go language"},{"categories":["Golang","Go"],"content":"After looking at variables in the previous part of this series, you have a fair understanding of using different types of variable declarations and using the variable values in your program. Each of these variables you created and used have an associated type. The type of a variable dictates what that variable is allowed store. So, what are different data types in Go language? You will learn that today!\nData types in Golang Go language has several built-in data types and these types can be classified into three different categories \u0026ndash; basic data types, aggregate data types, reference types, and interface types.\nCategory Included types Description Basic data types Numerals, Strings, and Booleans Data types that form the basis or included in other data types Aggregate data types Arrays and Structs Formed by combining the basic and simple data types Reference types Slices, Functions, Pointers, and Channels Refer indirectly to state or program variables Interface types Abstractions around behavior of other types In this part, you will learn about basic data types and you will learn about other types as you proceed in this series.\nBasic data types In the last chapter, some of the examples used variables of integer, string, and boolean data types. To recap, var \u0026lt;variable-name\u0026gt; \u0026lt;data-type\u0026gt; = \u0026lt;value | expression\u0026gt; is how you declare variables. You learned that the variable declaration can either use static type declaration or the type can be determined based on the value assigned to the variable. The three types mentioned here fall in the category of basic data types. The following sections dive into each of these basic data types.\nNumerals Go language supports different types to represent numbers. You can at a high-level categorize these into integers, floating point numbers, and complex numbers. Each of these types represent the size of the value that can be stored within a variable of that type. And, these types can be both signed and unsigned.\nType Description Possible values uint8 unsigned 8-bit integers 0 to 255 uint16 unsigned 16-bit integers 0 to 65535 uint32 unsigned 32-bit integers 0 to 4294967295 uint64 unsigned 64-bit integers 0 to 18446744073709551615 int8 signed 8-bit integers -128 to 127 int16 signed 16-bit integers -32768 to 32767 int32 signed 32-bit integers -2147483648 to 2147483647 int64 signed 64-bit integers -9223372036854775808 to 9223372036854775807 float32 32-bit floating-point numbers float64 64-bit floating-point numbers complex64 complex numbers with float32 real and imaginary parts complex128 complex numbers with float64 real and imaginary parts byte alias for uint8 rune alias for int32 Well, this is a huge list and what you choose to use in your program really depends on your need. Within integers, unsigned integers (uint) contain only the positive numbers while signed (int) contains both positive and negative numbers. Integers come in different sizes \u0026ndash; 8, 16, 32, and 64 bits. int is probably the most commonly used numeric type.\nA byte type is same as unit8. You will see a good number of examples using byte type in this series of articles. The type rune is same as int32.\nFloating point numbers contain a decimal component. For example 3.14 is a floating point number. Floating point numbers come in 32 and 64 bit sizes. The complex64 and complex128 are essentially floating point types with the imaginary part.\nLike every other programming language, Go too supports a wide array of arithmetic operators to work with different data types. For example, +, -, *, /, and %. The first four operators apply to all types if numerals while the last one \u0026ndash; remainder operator (%) \u0026ndash; applies only to integers.\nHere is an example that sh ows all these numeric data types.\n1 2 3 4 5 6 7 8 9 10 11 package main import \u0026#34;fmt\u0026#34; func main() { fmt.Println(1 + 10) fmt.Println(2.0 * 10) fmt.Println(3.3 / 3.1) fmt.Println(4 - 5) fmt.Println(5 % 5) } Using complex numbers in Go language is a bit different from how you initialize and use ints and floats.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 package main import \u0026#34;fmt\u0026#34; func main() { // using constructor to create a comples number c1 := complex(56, 3) // gets real part realPart := real(c1) // gets imaginary part imgPart := imag(c1) fmt.Println(c1) fmt.Println(realPart) fmt.Println(imgPart) // second method to creating complex numbers c2 := 10 + 6i fmt.Println(real(c2)) fmt.Println(imag(c2)) //complex number arithmetic fmt.Println(c1 + c2) } In the above example, the constructor function complex() creates a complex number. The two arguments to this function are the real and imaginary parts of the number. The real() function gets the real part of a complex number while the imaginary part can be retrieved using imag() function. Finally, the last line shows the complex number arithmetic.\nStrings Strings are sequences of characters. Each character is represented by a byte value. Go language supports unicode characters as well as a part of the strings. So, one of the examples you saw earlier \u0026ndash; \u0026ldquo;I ❤ Golang!\u0026rdquo; \u0026ndash; is a totally valid string in Golang. String literals should always be enclosed either in double-quotes or back ticks.\n1 2 3 4 5 6 7 8 package main import \u0026#34;fmt\u0026#34; func main() { fmt.Println(\u0026#34;I ❤ Golang!\u0026#34;) fmt.Println(`Go language is simple to learn!`) } In other languages \u0026ndash; if you are familiar with PowerShell like I do \u0026ndash; you might have used single-quotes as well to represent strings. While single-quotes are not permitted, using back ticks is in a way similar to single-quotes. For example, in Go, the escape sequences such as /n (newline) and /t (tab) have no meaning when the string is enclosed in back ticks. Update the example above to add one of these escape sequences and try out the program again. Do you see something similar where the escape sequence at the end of string enclosed in back ticks does not get replaced to its meaning?\nAs mentioned earlier, strings are sequence of characters and get indexed from 0. So, for example, \u0026quot;Golang\u0026quot;[3] refers to byte representation of character \u0026ldquo;a\u0026rdquo;. So, if you use fmt.Println(\u0026quot;Golang\u0026quot;[3]), you will see 97 printed on the console. So, how can you print the character \u0026ldquo;a\u0026rdquo; instead of its byte representation? For this, you use format verbs. You will learn more about string formatting in a later article but if you are keen on trying this, you can use the following statement to print the character instead of its byte value.\n1 fmt.Printf(\u0026#34;%q\u0026#34;, \u0026#34;Golang\u0026#34;[3]) Note that in the above example, Printf function is used instead of the Println.\nThe length of a string is equivalent to the number of bytes in the string. You can derive the length of a string in Go using the predeclared function called len.\n1 fmt.Println(len(\u0026#34;Golang\u0026#34;)) Finally, you can concatenate strings using the + operator.\n1 fmt.Println(\u0026#34;I \u0026#34; + \u0026#34;❤ \u0026#34; + \u0026#34;Golang!\u0026#34;) Booleans Booleans represent a true or false value. true and false are predeclared constants in Go and represent the boolean truth values. The predeclared boolean type is bool. Booleans are represented using special one bit integer values. With boolean values, you use logical operators such as \u0026amp;\u0026amp; (and), || (or), and ! (not).\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;reflect\u0026#34; ) func main() { // using constructor to create a comples number var isTypeNum bool a := 10 isTypeNum = (reflect.TypeOf(a).String() == \u0026#34;int\u0026#34;) fmt.Println(\u0026#34;Is variable a of type int?:\u0026#34;, isTypeNum) } The statement var isTypeNum bool creates a variable named isTypeNum of type boolean. The expression (reflect.TypeOf(a).String() == \u0026quot;int\u0026quot;) uses the TypeOf() function in the reflect package to retrieve the type of variable a, convert it to string and then compare it to the string \u0026ldquo;int\u0026rdquo; using the == operator. This comparison (you will learn about comparison operators when looking at control flow statements) results in a boolean value \u0026ndash; true or false and that gets assigned to the variable isTypeNum as value.\nThis is quick overview of basic data types in Go language. It is time to practice what you learned today.\nExercises Create a program that declares two variables \u0026ndash; an integer and a floating pointing type. Perform multiplication of these values and print the result using Println function. Post your solution here. Create a program that uses shorthand declaration to declare variables of types int, string, and bool and assigns the values 10, \u0026lsquo;hello\u0026rsquo;, and true respectively. What happens when you use the statement fmt.Println(i == s). Publish your answer here. What did you learn from this exercise? ","date":"2020-08-13T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/get-set-go-types-in-go-language/","series":"Get Set GO","title":"Get set Go - Types in Go language"},{"categories":["Golang","Go"],"content":" In the last part of this series, you looked at writing your first program and understood the program structure and understood the keywords such as package, import, and func. It was a good start. In this part, you will learn how to declare and use variables in Go programs.\nNaming convention in Go Before you start looking at variables in Golang, you must first understand some rules for naming different program entities \u0026ndash; variables, types, statements, packages, and constants \u0026ndash; in Go.\nNames or identifiers for any of these entities must start either with a letter or an underscore. You can use Unicode letters as well. These names or identifiers can have any number of additional letters or underscores or digits. The case of a name matters in Go language. For example, firstName is different from Firstname or FirstName. You learned in the last part that Go language has 25 keywords. These keyword names are reserved and may not be used as names or identifiers for any of the program entities. There are also a few predeclared identifiers in Go language. You may use the names of predeclared identifiers for your program entities but make sure you do not confuse yourself or create confusion for others reading your code.\nWhile there is no limit on the number of characters in a name or an identifier, Gophers tend to use shorter names for program entities. It may sound strange for people coming from other programming language background but many Go programmers use single letter names and it is highly recommended. The recommendation here is to use shorter names or identifiers for local scope and longer names for names or identifiers that have a larger visibility. You will learn more about scopes in a different part of this article.\nFinally, names or identifiers starting with an uppercase letter are used only when you want to make the program entity visible outside the current package. For example, if you revisit the code of your first program, the function that we used to print text on the console was Println in the fmt package. Otherwise, the general recommendation is to begin the name or identifier with a lowercase letter or underscore.\nAlright, with this background, you can now get started with variables.\nVariables A variable is essentially a storage location in memory to store a specific value of a given type. Variable declaration names the storage location for easier identification. The general syntax for declaring variables in Go language is as shown below.\n1 var name_of_variable type = value_or_expression For example, var s string declares an identifier s to store a value of type string. This declaration also allocates the storage needed for storing the value. Assigning type or assigning a value at the time of variable declaration is optional. In Go language, there is no concept of uninitialized variables. When no value is assigned during declaration, the variable gets initialized to a default or zero value based on the type. For string type, it will be an empty string. The zero value assignment ensures that a variable always hold a valid value of the type specified.\nWhen you specify the type during declaration, it defines the static type of the variable. If a type name is not specified during the declaration, a dynamic type gets determined based on the value assigned.\nYou cannot skip both type and value assignment during variable declaration.\nLet us see this in action.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;reflect\u0026#34; ) func main() { // declare a string variable without assigning a value var s string // print variable value fmt.Println(\u0026#34;Value of variable s is\u0026#34;, s) //declare a variable without type var fName = \u0026#34;Go\u0026#34; fmt.Println(\u0026#34;type of variable fName is\u0026#34;, reflect.TypeOf(fName)) } When you run this Go program, you will see the following output.\nAs seen in the output, the dynamic type of variable fName is set to string. After this, within this program, you cannot assign a value of another type to fName.\nGo language allows you to declare multiple variables at the same time and these variable can be of different types as well. For example,\n1 2 3 4 5 // multiple variables initialized to zero values var n1, n2, n3, n4 int // muliple variables with no type declaration but initializer values var n, s, f, b = 4, \u0026#34;golang\u0026#34;, \u0026#34;3.14\u0026#34;, true In the above examples, the first one declares multiple integer type variables. In the second declaration, the values get on the right-hand side get assigned to the variables in the order specified. Also, you must match the number of values provided on the right to number of identifiers specified on the left. The variable initializers (on the right) can be literal values like what is specified above or can be expressions as well.\nYou can, similar to the import statement, wrap variable declaration in (). For example, the above two declarations can be changed to use the following syntax.\n1 2 3 4 var ( n1, n2, n3, n4 int n, s, f, b = 4, \u0026#34;golang\u0026#34;, \u0026#34;3.14\u0026#34;, true ) This is yet another way and usage depends on the readability requirements and preferences in your code.\nShort variable declaration Within Go functions, you can use a short variable declaration and syntax for that is:\n1 variable-name := value-or-expression Remember that this can be used only within functions as non-declaration statements are not allowed at the package level. Here is a quick example of using short variable declaration method.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;reflect\u0026#34; ) func main() { //simple short variable declaration s := \u0026#34;Go Language\u0026#34; fmt.Println(\u0026#34;Value of variable S is\u0026#34;, s) fmt.Println(\u0026#34;Type of variable s is\u0026#34;, reflect.TypeOf(s)) //short variable declaration for multiple variables n, b := 10, false fmt.Println(\u0026#34;Values of n and b are\u0026#34;, n, b) } Unlike = (assignment operator), := is a declaration. Even in this case too, the initializer can either be a literal value or an expression such as a function call. You will see several examples this method of variable declaration as you go forward in this series.\nAlright, that is a quick overview of declaring and using variables in Go language. It is time to practice what you learned.\nExercises In the first exercise, update the hello world program you create in the last part to add a variable fName and initialize it with your name as the value. Finally, print the statement Hello, fName to the screen. Post your solution here. Create a program that declares two integer variables i and j and assigns values 10 and 30. Print the values of these variables. Add an expression to swap the values of the variables using assignment operator. Finally, print the values of the variable values after swapping. Post your solution here. ","date":"2020-08-11T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/get-set-go-variables-in-go-language/","series":"Get Set GO","title":"Get set Go - Variables in Go language"},{"categories":["Azure Resource Manager","Azure","ARM Template","Key Vault"],"content":" In this series so far, you have seen how to get started with ARM templates, parameterize them by adding parameters, and optimize them using variables, expressions, and user-defined functions. It is now time to move on to more advanced topics and begin building the template for the remaining components of the architecture shown above. In the architecture shown above, you will implement an ARM template that provisions virtual machines, requiring the administrator and domain-join credentials as inputs. Also, the configuration scripts used to configure the guest OS may need to access an internal storage account blob, which requires the storage connection strings and access keys. Storing these secrets in plain text is not recommended. Also, as an architect, you may want to standardize on passwords for local administrator accounts and avoid sharing domain-join credentials with any user when provisioning an ARM template. This needs a more centralized credential and secret store. Azure Vault provides this capability. Today, you will learn how to handle secrets such as passwords, access keys, certificates, and more in an ARM template.\nAzure Key Vault Azure Key Vault is a service that provides a centralized secret store. You can use the Key Vault for cryptographic keys, API keys, passwords, connection strings, and certificates. For the cryptographic keys, you can use a Hardware Security Module (HSM) as well. Once a key vault is provisioned, you can add your secrets and retrieve them in an ARM template for use with other resource types such as virtual machines.\nCreating a key vault To create a key vault, you need few mandatory input parameters such as tenantId, objectId of the user or service principal or the security group in Azure Active Directory (AAD), key and secret permissions. The specified tenant will be used for authenticating requests to the key vault and the object Id of the AAD user will be used to provide the necessary permissions to interact with the Key Vault.\nHere is a simple starter template that you can use.\nGitHub Gist This code snippet requires your consent to load content from GitHub, which may set cookies. Enable GitHub Gist rchaganti/d7e35878c6687da07ae5fa5dfb7d54c2 In this starter template, you see that the value of tenantId defaults to the value of the tenantId property from the subscription() standard template function. The value to the objectId property is coming from a template parameter. You can retrieve the object ID of an AAD user using az ad user show command.\n1 az ad user show --id Ravikanth@azure.com --query objectId You can deploy the template above to create a Key Vault by clicking the Deploy To Azure button below.\nIf you prefer the Azure CLI, you can run the following commands to deploy this template.\nGitHub Gist This code snippet requires your consent to load content from GitHub, which may set cookies. Enable GitHub Gist rchaganti/d7e35878c6687da07ae5fa5dfb7d54c2 In the template above, as part of the access policies [line 22], for the key, secrets, and certificates permissions, you have used all as the value. As it literally means, this permission level provides full permission set to the service principal specified using objectId property. This is not recommended in a production Key Vault. You must also secure the key vault and restrict access to only what is needed. The allowed values for the key and secret permissions are listed in the resource provider reference.\nTo this end, the template above can be modified to add a few more parameters to specify permission values.\nGitHub Gist This code snippet requires your consent to load content from GitHub, which may set cookies. Enable GitHub Gist rchaganti/d7e35878c6687da07ae5fa5dfb7d54c2 In this updated template, three additional parameters are added to gather key, secret, and certificate permissions. While these parameters have default values, you can provide an updated set of permissions as an array during deployment. If you notice, there are additional resource properties as well added to the Key Vault resource definition.\nenabledForDeployment specifies whether Azure virtual machines can retrieve certificates from the key vault.\nenabledForTemplateDeployment specifies whether Azure Resource Manager is allowed to retrieve secrets from the vault or not.\nFor the architecture you are building, both properties must be set to true.\nYou can deploy this template by clicking the Deploy to Azure button below.\nNow, with this updated template, you have a functional key vault that can store secrets used by other resource configurations. So, how do you use ARM templates to store secrets in a key vault?\nAdding secrets to the vault Once you have a key vault provisioned, you can add the Microsoft.KeyVault/vaults/secrets resource to the template to add secrets to the vault. The following resource definition will do that job.\n1 2 3 4 5 6 7 8 9 10 { \u0026#34;type\u0026#34;: \u0026#34;Microsoft.KeyVault/vaults/secrets\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;[concat(variables(\u0026#39;keyVaultName\u0026#39;), \u0026#39;/\u0026#39;, parameters(\u0026#39;secretName\u0026#39;))]\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;2018-02-14\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[resourceGroup().location]\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;value\u0026#34;: \u0026#34;[parameters(\u0026#39;secretValue\u0026#39;)]\u0026#34;, \u0026#34;contentType\u0026#34; : \u0026#34;string\u0026#34; } } In this resource definition, the resource type is Microsoft.KeyVault/vaults/secrets. The secretName you want to use will be used as the resource name, and the value will be set in the resource properties.\nHere is the full template to provision a key vault and add a secret to it.\nGitHub Gist This code snippet requires your consent to load content from GitHub, which may set cookies. Enable GitHub Gist rchaganti/d7e35878c6687da07ae5fa5dfb7d54c2 This updated template added two additional parameters: secretName and secretValue. secretValue is a secure string. Try deploying the template using the deploy to Azure button below.\nRetrieve vault secret Finally, when you need to use the secret as another resource property value, you can do so using a property definition, as shown below.\n1 2 3 4 5 6 7 8 \u0026#34;adminPassword\u0026#34;: { \u0026#34;reference\u0026#34;: { \u0026#34;keyVault\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;/subscriptions/\u0026lt;SubscriptionID\u0026gt;/resourceGroups/mykeyvaultdeploymentrg/providers/Microsoft.KeyVault/vaults/\u0026lt;KeyVaultName\u0026gt;\u0026#34; }, \u0026#34;secretName\u0026#34;: \u0026#34;vmAdminPassword\u0026#34; } } You will learn more about this pattern in the later articles of this series when you attempt creating virtual machines using ARM template.\nSummary In this part, you learned how to create an Azure Key Vault, add secrets to the vault, and retrieve the secrets. This knowledge will come handy when you attempt creating virtual machines that require a predefined administrator password. In the next part, you will learn about resource dependencies in ARM templates.\n","date":"2020-08-07T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/azure-resource-manager-using-secrets-in-arm-templates/","series":"Azure Resource Manager In 30 Days","title":"Azure Resource Manager - Using secrets in ARM templates"},{"categories":["Golang","Go"],"content":" With the background provided in the introduction part of this series, you must now be equipped with the necessary tools for the job \u0026ndash; learning Go! So, in this part, you will write your first program in Go language. You will not only learn about the very minimal structure needed for a Go program but you will also learn about organizing your Go programs and why it matters. Get set Go!\nYour first program The most popular first program among many programmers or programming languages is the hello world program. Here is how you can write a hello world program in Go language.\nGitHub Gist This code snippet requires your consent to load content from GitHub, which may set cookies. Enable GitHub Gist rchaganti/023b2d3ad10b4c53e997d58387d1ee2d This is it. The above code presents the basic structure of Go programs.\nAll Go programs are packages and therefore every Go program starts with a package declaration. A Go package can be equated to a library or a module in other languages such as C or Python. In this example, [line1] the package name is main. main package in Go is a special package since any Go program execution starts with main function of the main package. You will learn more about using and building packages in-depth in a later part of this series.\nThe import keyword follows [line 7] the package declaration. This keyword is used to import other packages to use within the current package. In this example, you are importing the fmt package. Note that the fmt string is enclosed within double-quotes and it should be double-quotes only. If you are coming from other programming languages, you may start using single-quotes for representing strings but that is not valid in Go programming. The method of importing packages shown in the example works only for a single package. You can add multiple such import statements one after another or use a better method as shown below.\n1 2 3 4 import ( \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; ) Go does not let you import packages that you are not using within the current program. If you add an import statement for a package but do not use it in your code, you will receive an error during compile time that a package has been imported but not used. This is by design.\nNext thing that you see in the example is the func keyword to declare the main function. Like many other programming languages, functions in Go are the basic building blocks and the name of the function identifies a sequence of steps that you want to perform. In this example, you just have one step that is to print a message using Println function within the fmt package. You access functions from imported packages using the \u0026lt;packageName\u0026gt;.\u0026lt;functionName\u0026gt; syntax and therefore the example above uses fmt.Println().\nGo functions, similar to other languages, have input (arguments) and output (return values). However, main function in Go uses neither \u0026ndash; it neither takes any arguments nor returns any values. The Println function in the fmt package takes a string argument and prints the same to the console when executed. You will learn more about the functions in-depth in a later part of this series.\nAlso, notice that the opening and closing curly brackets in the function declaration. In Go programs, for any code block, the opening bracket has to be on the same line as the the keyword or code block identifier \u0026ndash; in this case the func keyword and the name of the function. There is no choice in the function declaration since a new line character is treated as the end of a statement. On a lighter note, this totally eliminates any my-bracket-style-is-better-than-yours kind of discussion.\nFinally, to the other semantics I skipped. The /* */ [line 3-5] and // [line 9 and end of line 10] indicate comments in Go language. No surprises there. The first method \u0026ndash; /* */ \u0026ndash; is a multi-line or block comment where as the second one using // is a single line comment. Anything that you enclose within comments will be ignored at compile time.\nExecuting Go programs You can execute your Go programs using the go run command.\n1 go run helloworld.go When working with Go language, go and it\u0026rsquo;s subcommands is all you need to compile your programs. In the above example, you are using the run subcommand. When this command is executed, the Go program code gets compiled and translated to machine code which then runs and prints the message that we specified as an argument to the Println function.\nWhat if you want to just build the binary and not run it? You can use the go build subcommand.\n1 go build hellworld.go This command generates helloworld.exe in the current working directory. You can now execute the generated binary to see the output from the program.\nWith the go build subcommand, it is optional to specify the .go filename. If you don\u0026rsquo;t specify a file name, go will try to find the Go source code package with main function and build it.\nIf you look at the screen capture (Figure 1 - Command Output) of program output above, the go run command did not use the full file name \u0026ndash; helloworld.go. Instead, it was just helloworld. It still worked! But, how? If you too thought about it, the answer follows. Read on.\nGo workspaces The command shown in the above output worked because the helloworld source code exists in a known Go workspace. In the introduction, you have learned about the go env command. In the output of this command, you see an environment variable called GOPATH. This variable represents the path where your Go program source code is. By default, this will be set to %USERPROFILE%\\go on Windows and $HOME/go on Linux. You can change this path using the go env subcommand with -w flag.\n1 go env -w GOPATH=C:/GitHub/GetSetGo Go workspace is essentially a location on your disk. The workspace folder should contain a folder named src for all your Go program source code. The other folder that gets created automatically is the bin folder for storing all binary files generated using go install subcommand. Here is how the folder structure looks like on my system.\nWith this folder structure, when you run go build helloworld or go run helloworld or go install helloworld command, go will try to find if there is a match that exists at a path specified by either GOROOT or GOPATH environment variables. A typical Go workspace contains many source control repositories \u0026ndash; one for each package you are working on. So, in the example above, you can consider the folder helloworld under src to be a source control repository. It is a best practice among Go programmers to use single workspace for all their Go source code.\nThe Go workspace is the folder structure that gets used when you use the go get subcommand to download Go packages. For example, if you are using VS Code with the Go extension to develop your Go programs, you might have seen VS Code prompting you to install necessary Go tools to enable the VS Code editor features such as Intellisense, linting, formatting, and so on. So, if you have used the Go: Install/Update Tools in VS Code, all tools get downloaded to pkg folder within the Go workspace and then the packages get compiled to binaries which get installed to the bin folder.\nFor now, it is not mandatory (but good) to use a Go workspace to try out the examples in this or the next few upcoming parts. You will revisit this in a later part discussing Go packages. In the next part of this series, you will get introduced to variables and types in Go language.\nExercises Now that you have got introduced to basic Go program syntax, write a program that prints \u0026ldquo;I ❤ Golang!\u0026rdquo;. You don\u0026rsquo;t have to use anything more than what you learned today. Post your solution to this @ https://gist.github.com/rchaganti/725cc041a30ee90e19d4713a1514b432. Update the GOPATH environment variable to a different location than the default path and create folder structure similar to what is shown in Figure 2. Try go build, go run, and go install commands and understand the difference. ","date":"2020-08-04T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/get-set-go-first-program/","series":"Get Set GO","title":"Get set Go - First program"},{"categories":["Golang","Go"],"content":" Go programming language isn\u0026rsquo;t new. It existed for more than 13 years now. The idea of Go language was first conceived in 2007 at Google by Ken Thompson, Robert Griesemer, and Rob Pike. Go was developed in response to some of the challenges the development teams at Google were facing internally. Some of these issues included ridiculously longer build times, baggage that legacy languages imposed on modern infrastructure / systems development, and uncontrolled dependencies among many other pain points. Go was designed to make it easy and efficient to program modern multicore systems, web service backends, and command line tools. The design considerations such as rigorous dependency management and efficiency at scale make Go one of the best modern programming languages.\nGo was publicly announced in 2009. Version 1.0 of Go language was released in March 2012 and the current version (at this time of writing) is 1.14. Today, Go is a part of many major online services and infrastructure tools that you use every day! Google, of course! Docker \u0026ndash; the most popular container technology \u0026ndash; is written in Go. The most recent entrant in the CLI game, the GitHub CLI, is written in Go. Every tool that Hashicorp developed so far was in Go. Blockchain platform like Ethereum uses Go. This static page that you are reading right now was generated using Hugo which is written in Go. I can Go (pun intended) on with this list and there are many more very well known names that use Go as their primary language for all their product development. So, what is Go?\nWhat is Go? Go is an open source, general-purpose, and modern programming language with its roots in systems programming. If you are familiar with C programing language, you will find a few similarities between Go and C. Apart from C, Go language design was inspired from Pascal, Smalltalk, Modula, Oberon, and Algol. Go is a modern language and offers features such as garbage collection and better memory management.\nGo is a compiled language. Unlike other compiled languages, Go programs can be compiled directly to machine code and compile very fast. Faster compiler times are a part of Go\u0026rsquo;s design. Go programs can be compiled for cross-platform execution which includes Windows, Linux, OS X, Solaris, Plan9 and many other operating systems. Go programs get compiled to a single executable which eliminates the dependency nightmares from other languages.\nGo is statically typed strong language. Go does not allow type coercion unlike weakly typed languages such as JavaScript. Like other statically typed languages, Go types are checked before run-time. This allows any errors related to types surface right during compile time itself.\nGo\u0026rsquo;s package system combines the best of libraries, namespaces, and modules. Every Go file is a package. The package system is designed to identify the package import using the package path than the name. There is no need to make package names unique in the Go package system.\nGo has a simple and clean syntax. Go\u0026rsquo;s grammar is modest in size compared to other languages such as C, C++, and Java. There are just 25 keywords in Go.\nGo offers concurrency that is important and well suited for developing modern computing application that run on multicore systems. Go implements a variant of Communicating Sequential Processes (CSP) to enable support for concurrency that is simple and robust.\nOverall, Go is powerful, simple to learn and use and that is what makes you productive. Now, you may ask, this is all good but why learn go?\nWhy Learn Go? YouTube Video This video requires your consent to load content from YouTube, which may set tracking cookies. Enable YouTube Video ID: FTl0tl9BGdc This interview is 8 years old and Go language has progressed quite a bit and that is evident not only from the investments that big companies are making in Go but also from the developer surveys such as the surveys done by Stack Overflow and HackerRank. In the Stack Overflow survey, Go stood as 5th most loved language and 3rd most wanted language to develop new applications. Go featured as the 3rd most highest paid programming skill. In the HackerRank survey, Go \u0026ndash; for 3 consecutive years \u0026ndash; was at the top of the list of new languages developers want to learn.\nApart from all this, Go is fun. I started learning this really as yet another programming language in my skillset but got hooked to this as I progressed in my journey. I started looking at some of the most popular tools implemented in Go and it was amazing to see how clean the code is and how readable it is. When compared to languages like C, C++, and Java, navigating Go code is so simple. The learning curve to get started with Go and do anything productive is very small.\nGo language is the most popular choice for building web services and command line tools. This is where my interest as well and therefore learning Go is really paying off.\nSo, what are YOU waiting for? Start here and start today.\nGet started with Go To get started with Go, you need not install anything on your system. Really. You can just use the Go Playground to start with the basics.\nWith Go playground, you can write basic Go programs, execute, and see the output. You can also share the code that you write in the playground by clicking on the share button. This generates a unique link to access and share your code.\nGo by example \u0026ndash; an online Go language tutorial site \u0026ndash; has examples that you can open in Go playground and try out. As you get started, this is certainly one place that you don\u0026rsquo;t want to miss.\nFor those of you who prefer having Go installed locally for all learning, you need to download and install the latest release of Go language from https://golang.org/dl/ and follow the install instructions to prepare the environment for getting started with Go.\nNote: I am writing these articles on my Windows 10 system and therefore you will see references to Windows paths and configuration. Otherwise, the content and examples can be used on any supported platform.\nOn Windows, you can either get the zip archive of a Go language release, extract it to location of your choice on the local disk, and set up the required environment variables. At a minimum, you must configure the system PATH variable to include path to the bin folder. In the second but preferred method, you can download the MSI file which installs, by default, at C:\\Go and configures needed environment variables for you.\nYou can verify your Go binary installation and environment variable configuration by executing the go env or go version commands. You will have to re-open the command console before running this command.\nYou should see output similar to what is shown below.\nYou can install and run multiple versions of Go language binaries side by side. The go get command, which you learn later in this series, can help get different versions of Go binaries.\n1 go get golang.org/dl/go1.15rc1 The above command downloads unstable (under development) version of Go. Once this is installed, you can use go1.15rc1 instead of go to explore the new features that are still in development or verify bug fixes.\nFor writing Go programs, a simple notepad would be good enough but that won\u0026rsquo;t make you productive. A good Integrated Development Environment (IDE) is needed for developing Go code with ease. There are many editors such as Visual Studio Code, JetBrains GoLand, Eclipse with GoClipse plugin, and many others. So far, I have used Visual Studio Code only and therefore it is my only preference. The Go language extension for VS code is a must. With this extension, VS Code gets support for IntelliSense, signature help, formatting, linting, build, and more importantly debugging features. As you proceed in this series, you will see mention different VS Code Go extension features and how those features can make you more productive when writing Go programs.\nOk. Enough talk. What are some resources that you can use to get started with Go language?\nLearning resources There are many existing resources online for learning Go language. The following is a list of resources I used or often refer to. Depending what mode of learning you like, here are my recommendations. The Go language specification is not for an end-to-end reading but acts more like a reference. You will see references to the language specification throughout this series of articles.\nResource Link Category Go language documentation https://golang.org/doc/ Documentation Go language specification https://golang.org/ref/spec Documentation Go by example https://gobyexample.com/ Online tutorial Exercism https://exercism.io/tracks/go Code practice The Go Programming Language https://cutt.ly/Ydfkm9w Book Introducing Go https://cutt.ly/VdfkoPp Book Go In Action https://cutt.ly/Mdfj8Bs Book Learn How To Code: Google\u0026rsquo;s Go (golang) Programming Language (Udemy) https://www.udemy.com/course/learn-how-to-code/ Video Go Core Language (Pluralsight path) https://www.pluralsight.com/paths/go-core-language Video Go Fundamentals (Nigel Poulton) https://www.pluralsight.com/courses/go-fundamentals Video Go Programming Language (Reddit) https://www.reddit.com/r/golang/ Community Forum / Discussions There are a lot of companies that use Go language and there are quite a few of them in open source. Looking at the Go code in these open source repositories is another great exercise and gives you insights into how the language is used in the real-world. As a part of this series, you will see references to code from some of these open source projects while explaining a few concepts.\nFinally, I am no expert in Go language and therefore these are the notes from a novice. These notes are how I learned (or still learning) Go language. At this point in time, I don\u0026rsquo;t have a definite number of articles that I want to publish as a part of this series. I will go right from very basics to applied Go language. The applied Go language part of this series will deal with some sample utilities that I developed to strengthen my understanding of Go and walk-through how I went about developing those utilities and what I learned from that exercise. At the end of each part, starting next one, I will post a few exercises that you may want to try out and share your code with the others. Each of these exercises will be available as a Gist and you can comment there to share your solution to the exercise with other readers of this series.\nJoin me in this journey. Let us learn together. Stay tuned!\n","date":"2020-08-03T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/get-set-go-introduction-to-go-lang/","series":"Get Set GO","title":"Get set Go - Introduction to Go language"},{"categories":["Azure Resource Manager","Azure","ARM Template"],"content":" Expressions in ARM template language are a way to reduce complexity in the templates. Using parameters and variables in ARM templates always requires the use of expressions. Parameter and variable values are combined with standard (built-in) template functions (STF) to create complex expressions to implement the business requirements and implement known configurations. Updated ARM template syntax introduced the ability to implement user-defined functions for complicated expressions and use repeatedly in your templates. In this part, you will learn more about standard template functions and learn how to implement user-defined functions.\nStandard template functions There are different categories of standard template functions. To work with arrays, array functions can be used. For example, contains() function can be used to check whether an array / object / string contains a value / key / substring. This function returns true if the item is found an false otherwise. Here is an example.\nGitHub Gist This code snippet requires your consent to load content from GitHub, which may set cookies. Enable GitHub Gist rchaganti/d7e35878c6687da07ae5fa5dfb7d54c2 In this example, the parameter locations has a default value so you do not have to pass any parameter values while evaluating this template using the method you used in the last part. Within the outputs element, there are two output values defined within which the contains() function is used. One function checks for a value that exists on the parameter value (therefore returns true) and the second one checks for a value that does not exist in the parameter value and therefore returns false.\nIn the last part, you tried a few examples of STF already. At a high level, you can classify the STF into two categories \u0026ndash; compile-time and run-time. The examples that we have seen so far are compile-time functions. The compile-time functions evaluate before the deployment begins. The run-time functions evaluate once the deployment begins. One example of a run-time function is the listKeys() function. Using this template function, here is how you retrieve the storage account key for a given storage account.\n1 2 3 4 5 6 7 8 9 10 11 [ listKeys( resourceId( \u0026#39;Microsoft.Storage/storageAccounts\u0026#39;, concat( \u0026#39;sacct\u0026#39;, \u0026#39;-\u0026#39;, uniqueString( resourceGroup().id ) ) ), \u0026#39;2019-06-01\u0026#39; ).keys[0].value ] In the above example, two arguments are passed to the listKeys() function. First one is the resourceId() function to retrieve the unique ID of the storage account that you provisioned. Second parameter is the API version used to deploy the resource. The final part of the expression .keys[0].value is the array reference to retrieve the value of primary storage key.\nHere is another example of a run-time function. This is called reference(). This function returns the runtime state of a resource.\n1 2 3 4 5 6 7 8 [ reference( concat(\u0026#39;Microsoft.Storage/storageAccounts/\u0026#39;, concat( \u0026#39;sacct\u0026#39;, \u0026#39;-\u0026#39;, uniqueString(resourceGroup().id) ), \u0026#39;2019-06-01\u0026#39; ) ).primaryEndpoints.blob ] Similar to the expression that used listKeys() function, this one too uses a prefix string, a unique string generated using the resource group ID, and the API version string to retrieve the storage blob URI. This blob URI is retrieved using .primaryEndpoints.blob.\nIn the above example, concat('sacct', '-', uniqueString(resourceGroup().id)) sub-expression is used to generate a unique name for the storage account. What if you want to use a similar expression for multiple resource names in the ARM template to ensure that your ARM template uses more known configuration? One way to do that is to repeat the expression multiple times with different prefixes as needed. The second and a better way is to create and use an user-defined functions (UDF) where you get the ability to parameterize expressions and reuse the same in your ARM templates like you use built-in functions.\nUser-defined functions With user-defined functions (UDF), you can create your own functions by combining multiple standard template functions into a complicated expression. The following syntax describes how the user-defined functions can be added to ARM templates.\nGitHub Gist This code snippet requires your consent to load content from GitHub, which may set cookies. Enable GitHub Gist rchaganti/d7e35878c6687da07ae5fa5dfb7d54c2 A UDF must be a part of a namespace to avoid name conflicts with STF. All your expressions get added to the members object. You need to give your custom function a unique name within the namespace and add parameters and output as needed. Here is a quick example that wraps the above expression into a custom function.\nGitHub Gist This code snippet requires your consent to load content from GitHub, which may set cookies. Enable GitHub Gist rchaganti/d7e35878c6687da07ae5fa5dfb7d54c2 In the above example, one parameters \u0026ndash; prefix \u0026ndash; allows you to use the expression with varying input. The output value within the UDF is set to the transformed expression that will use the parameter and return a unique resource name. How do you use this UDF in your template? Simple, like any other built-in function in an expression.\n1 2 3 4 5 6 7 \u0026#34;variables\u0026#34;: { \u0026#34;virtualNetworkAddressPrefix\u0026#34; : \u0026#34;10.0.0.0/16\u0026#34;, \u0026#34;virtualNetworkName\u0026#34; : \u0026#34;[armin30.uniqueResourceName(\u0026#39;vnet\u0026#39;)]\u0026#34;, \u0026#34;virtualNetworkSubnetName\u0026#34; : \u0026#34;[armin30.uniqueResourceName(\u0026#39;subnet\u0026#39;)]\u0026#34;, \u0026#34;storageAccountName\u0026#34; : \u0026#34;[armin30.uniqueResourceName(\u0026#39;sacct\u0026#39;)]\u0026#34;, \u0026#34;virtualNetworkSubnetAddressPrefix\u0026#34; : \u0026#34;10.0.1.0/24\u0026#34; } A UDF is accessed using the namespace.functionName notation with a list of comma-separated parameter values. This example assign output value from armin30.uniqueResourceName() function to variables. You can use the UDF directly in resource properties as well.\nLimitations of UDF There are few limitations when using user-defined functions.\nYou cannot access parameters and variables defined in the ARM template\u0026rsquo;s parameters and variables elements. You can only access parameters defined within the user-defined function and these UDF parameters cannot have any default values. There is no nested UDF support. The expressions that use reference and list* methods cannot be used within UDF. Finally, here is the updated template for the scenario you are working on.\nGitHub Gist This code snippet requires your consent to load content from GitHub, which may set cookies. Enable GitHub Gist rchaganti/d7e35878c6687da07ae5fa5dfb7d54c2 You can deploy this template by clicking on the deploy to Azure button.\nOnce the deployment is complete, you can see the deployed resources in the portal or in the console if you used Azure CLI. Note the random strings with the prefixes that you specified as input to the armin30.uniqueResourceName UDF.\nSummary With the help of parameters, variables, expressions, and user-defined functions you get the ability to simplify the ARM template authoring process and make the templates more readable. There are many standard template functions that help you simplify the ARM template resource definitions. When you have to repeat and reuse some of the complicated expressions in an ARM template, you can wrap the expression as a user-defined function and use the UDF in the resource definition. In the next part of the series, you will learn how to use secrets in an ARM template.\n","date":"2020-07-31T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/azure-resource-manager-using-functions-in-arm-templates/","series":"Azure Resource Manager In 30 Days","title":"Azure Resource Manager - Using functions in ARM templates"},{"categories":["Azure Resource Manager","Azure","ARM Template"],"content":" So far in this series, you learned how to use VS Code and ARM Tools extension to get started with the ARM template development, how to perform a subscription scoped deployment to create a resource group and then looked at resource group scoped deployment to create a storage account and a virtual network resources. You learned about parameterizing the ARM template so that it becomes reusable. Equipped with that knowledge, you can now start digging into expressions and variables in the ARM templates.\nExpressions You have already used expressions in the template you built in the last part. For example, [parameters('storageAccountSku')] is an expression that you used. Within an ARM template, expressions are a way to dynamically determine values of resource properties and other artifacts. Expressions are invoked within square brackets and are calculated at deployment time. When creating an expression, you can combine multiple template functions. For example, in the previous template that provisioned a storage account and a virtual network, the storage account name has to be unique. Now, either you can let the end user figure out a unique string for this resource property by trial and error or you can use an expression such as [concat('sacct', '-', uniqueString(resourceGroup().id))] to generate a random string to use as a name for the storage account you want provision. Remember that, within the same resource group, the value generated by this expression will always be same since the seed to the uniqueString() function will always be same. So, when you deploy a template that uses this expression multiple times, no action will be taken if the storage account already exists with the name.\nThere are several categories of standard (built-in) template functions that you can use to simplify template authoring and deployment experience. These functions include comparison and logical functions, numeric and string functions, array, object, and date functions, and resource and deployment value functions. For example, the concat() function that you saw in the earlier example is a part of string functions category. It takes multiple strings and returns a concatenated string. A concat() function exists in the array functions category as well and works on arrays and returns a concatenated array. The uniqueString() function is in the string functions category and generates a hash based on the value passed as its argument. The resourceGroup() function is a part of the resource functions category and returns the properties of the resource group within which the deployment is in progress.\nLet us see another example of using expressions in ARM templates. In the template that was develope d in the last part, the storageAccountName parameter was used the capture the input for the name property of the storage account resource. In this parameter definition, minLength and maxLength elements were used to ensure the string length is between 3 and 24 characters. However, the name of the storage account name should always be in lower case. There is no element in parameter definition to describe this validation. This can be achieved by using the toLower() function from the string functions category to transform the user provided input value into a lower case string. Here is how it can be done.\nGitHub Gist This code snippet requires your consent to load content from GitHub, which may set cookies. Enable GitHub Gist rchaganti/d7e35878c6687da07ae5fa5dfb7d54c2 These are just a few examples of how you can use standard template functions in ARM templates as a part of different expressions. I recommend that you read through the documentation to understand how to use these functions in an expression and explore how you can simplify the template deployment experience. But, how do you evaluate an expression that you want to use in an ARM template?\nEvaluating Expressions Waiting for a real resource template deployment may be not be a great idea and a way to evaluate expression output without initiating a real deployment would be beneficial. This is totally possible with the outputs element in an ARM template.\nAs you learned in the template basics, the outputs element in a template is used to return values from after a deployment is complete. This is especially useful in automated deployment processes. Except for a few that are runtime functions such as reference(), this method can be used with most of the standard template functions and expressions that you create. In general, the following snippet shows the syntax of outputs element.\nGitHub Gist This code snippet requires your consent to load content from GitHub, which may set cookies. Enable GitHub Gist rchaganti/d7e35878c6687da07ae5fa5dfb7d54c2 This is a simplified syntax of outputs element. There are other properties you can use but for now, this is good enough.\nElement Name Description Is Required output-name Name of the output value Yes type type of the output value Yes value Expression that should be evaluated and returned as output value No To try this method of using outputs element , you can create an ARM template with no resource definitions as shown below.\nGitHub Gist This code snippet requires your consent to load content from GitHub, which may set cookies. Enable GitHub Gist rchaganti/d7e35878c6687da07ae5fa5dfb7d54c2 In the outputs element definition, the name of the output value has been set to lowercaseStorageAccountName and the output value is set to [toLower(parameters('StorageAccountName'))]. Although there is just one output from this example template, there can be any number of outputs from a given ARM template. The above ARM template can be deployed using of the known deployment tools to see the value the expression evaluates to. Let us see an example using Azure CLI.\nGitHub Gist This code snippet requires your consent to load content from GitHub, which may set cookies. Enable GitHub Gist rchaganti/d7e35878c6687da07ae5fa5dfb7d54c2 In the command above, inline parameter specification has been used and the value for the storageAccountName has been set to a string containing mixed case characters. The expression used in the output value element should return this string with all characters in lower case.\nWhen you deploy using Azure CLI or Azure PowerShell, the command output contains the deployment output as well when the template deployment completes successfully. The default output format in Azure CLI is JSON. So, when the command completes successfully, you will see the JSON string returned to the command console.\nThis JSON output can be queried using JMESPath. All you have to do is append --query properties.outputs.\u0026lt;output_value_name\u0026gt;.value to the above Azure CLI deployment command.\nOutput values from the template\u0026rsquo;s outputs element can be seen in the Azure portal as well if you navigate to the resource group and check the deployment history.\nVariables Variables are a way to help reduce complexity in an ARM template. In the template that deployed a storage account and a virtual network, we used seven parameters to collect the input required for the template deployment. This provides flexibility in choosing desired values for the deployment. While this flexibility is good, you may want to use fixed address prefixes for virtual network and subnets that you deploy as a part of the template. One way to achieve this is to hard code these values in resource properties. Hard coding values results in repetition and any unintentional mistakes that lead to errors in deployment. Also, there may be certain resource property values that you want to auto-generate based on the value of a specific parameter. This is where variables in ARM template syntax play a role.\nHere is the simplified syntax for adding variables in an ARM template.\nGitHub Gist This code snippet requires your consent to load content from GitHub, which may set cookies. Enable GitHub Gist rchaganti/d7e35878c6687da07ae5fa5dfb7d54c2 Simple variable declaration is straightforward. You just need to add a key-value pair for the variable name and it\u0026rsquo;s value. Here is how you may simplify the earlier template.\nGitHub Gist This code snippet requires your consent to load content from GitHub, which may set cookies. Enable GitHub Gist rchaganti/d7e35878c6687da07ae5fa5dfb7d54c2 This variable definition eliminates the need for three parameters - virtualNetworkAddressPrefix, virtualNetworkSubnetName, and virtualNetworkSubnetAddressPrefix.\nAccessing variable values Within the resource definitions, you can access the values of variables using the variables() function in an expression. Here is the updated template that uses variables for a few resource properties.\nGitHub Gist This code snippet requires your consent to load content from GitHub, which may set cookies. Enable GitHub Gist rchaganti/d7e35878c6687da07ae5fa5dfb7d54c2 You can deploy this template by clicking on the deploy to Azure button.\nIn the portal template deployment experience, you will see that the number of input parameters have reduced but the final result stays same. You can try deploying this template via the Azure CLI using the same set of commands you tried in the last part.\nFree-form vs known configuration So far in this series, you have learned about are parameters, variables, and learn to use them in ARM templates. Parameters enable the flexibility to gather input from user deploying the template. You can parameterize a complete template and let the user provide every value needed for the deployment as an input parameter. This is called free-form configuration. However, using free-form configuration is not always scalable. The number of parameters that a user need to input might be overwhelming. Also, you don’t want your end user deploying this template to mess-up with critical resource properties such as virtual network and subnet address prefixes. This can have negative effects on the overall infrastructure deployment. As you have seen in the updated template, you might always want to use fixed values for certain parameters where there is scope for errors. Some of this cannot be controlled unless you use known configurations. Known configurations help standardize resource configurations. Use variables in the template for resource settings that require no user input or you need control over. By using variables, you can simply change the value of the variable if at all you need to modify the template instead of trying to find where all the value is used.\nSummary Expressions syntax in ARM templates enable combining multiple standard functions provided by the ARM template language. Variables when combined with expression syntax can simplify the overall template authoring experience. Choosing between free-form vs known configurations for the ARM template, you can create a template that is both flexible and implements known configurations to control how the deployed infrastructure gets configured. With this knowledge around parameters, variables, and expressions, in the next part of this series, you will learn more about some of the standard functions and learn how to implement user-defined functions.\n","date":"2020-07-30T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/azure-resource-manager-in-30-days-using-expressions-and-variables-in-arm-templates/","series":"Azure Resource Manager In 30 Days","title":"Azure Resource Manager - Using expressions and variables in ARM templates"},{"categories":["Azure Resource Manager","Azure","ARM Template"],"content":" In the last part of this series, you learned the template syntax and authored an ARM template that provisions the resource group needed for the above architecture. This deployment was performed at the subscription level using Azure CLI. In this part, you will create another template for performing additional resource \u0026ndash; storage account and virtual network \u0026ndash; provisioning at the resource group level. You will, then, parameterize this template to understand how to add parameters to ARM templates.\nResource group deployment Deployments scoped at the resource group level are the most common deployments. In this method, you provision resources within a resource group in your subscription. Within the VS Code editor, typing arm! brings up the snippet for a resource group scoped ARM template.\nGitHub Gist This code snippet requires your consent to load content from GitHub, which may set cookies. Enable GitHub Gist rchaganti/d7e35878c6687da07ae5fa5dfb7d54c2 You can see in the above snippet that the $schema value is different from what was used for a subscription scoped deployment. Within this template, you can either right-click in the editor window and select Insert Item in the context menu and select Resource or click + next to resources in the ARM template outline to add any additional resources you need.\nEither of these options will present you with a list of available resource types. To add a storage account resource type, select Storage Account from the list of resources. You may go ahead and add a virtual network resource as well. This is how the updated template will look like.\nGitHub Gist This code snippet requires your consent to load content from GitHub, which may set cookies. Enable GitHub Gist rchaganti/d7e35878c6687da07ae5fa5dfb7d54c2 As you see here, the insert resources method adds the resource definitions with pre-defined default values. You may not be able deploy this as-is since some of the resource names (storage account especially) might be taken already. You may enter some unique names for the resources and try template deployment using the deploy to Azure button below.\nWhen this template opens in the Azure portal, you will be prompted to select either an existing resource group or create a new resource group. Since you already created a resource group in the last part of this series, you can select the same. For the resources (storage account and virtual network), the selected resource group\u0026rsquo;s location will be used. This is specified in the template using the built-in resourceGroup() function. You will learn more about these functions and expressions in a later part of the series. For now, just remember that resourceGroup().location will give the location of the resource group selected for the resources\u0026rsquo; deployment.\nParameters in ARM templates The template that you just built provides a good foundation to start experimenting with parameters in an ARM template. In the template above, there are several resource properties that can be parameterized so that the template becomes reusable between multiple deployments by providing the necessary input parameters. So, if you have to identify a few parameters within each resource, you may come with a list similar to the one below.\nResource Parameter Names storageAccounts storageAccountName, storageAccountSku, storageAccountTier virtualNetworks virtualNetworkName, virtualNetworkAddressPrefix, VirtualNetworkSubnetName, virtualNetworkSubnetAddressPrefix With this list handy, take a look at the syntax for adding parameters.\nGitHub Gist This code snippet requires your consent to load content from GitHub, which may set cookies. Enable GitHub Gist rchaganti/d7e35878c6687da07ae5fa5dfb7d54c2 Within all possible elements described above for defining a parameter, only name and type are mandatory. Rest all elements provide a way to control the input values. For the type element, the allowed values are string, int, bool, securestring, object, secureObject, and array. The type of a parameter depends on the resource property that we choose to parameterize.\nThe allowedValues element in parameters can be used to define a set of valid values for the selected resource property. For example, Azure supports two type of storage tiers \u0026ndash; standard and premium \u0026ndash; for any storage SKU. Any other value for this property will be invalid. When an end user does not select any value out of the allowed values for a property, you can default to a known value using the defaultValue element.\nThe maxLength and minLength are used for string data types. Within the above template, these elements can be used for the name property of the resources to define the minimum and maximum allowed length of the string input. For example, an Azure storage account name should be between 3 to 24 characters in length. These elements can be used with array type as well.\nThe minValue and maxValue elements are used for parameters of int type. In the parameter selection that we did, there is no need for using these elements yet. You will see the use of these elements in a later part of this series.\nWith this understanding, you can define the parameters for this template as below.\nGitHub Gist This code snippet requires your consent to load content from GitHub, which may set cookies. Enable GitHub Gist rchaganti/d7e35878c6687da07ae5fa5dfb7d54c2 The parameter definitions above provide a clear idea about how to use the allowedValues and defaultValue elements in a parameter definition. Now, how do you use these parameter values in the template for resource properties?\nUsing parameter values The built-in parameters() function can be used to retrieve the value provided by the end user as a part of template deployment. This function takes the parameter name as the argument and retrieves the value. For example, [parameters('storageAccountSku')]. You may now update the template above to change the static values to use the parameters() function.\nGitHub Gist This code snippet requires your consent to load content from GitHub, which may set cookies. Enable GitHub Gist rchaganti/d7e35878c6687da07ae5fa5dfb7d54c2 Deploying parameterized templates You can try deploying the parameterized template by clicking on the below deploy to Azure button.\nThis will bring up the input blade for this template deployment.\nTake a look at how the array value for the virtual network address prefix is provided. The * next to a parameter name indicates a mandatory parameter or a parameter that has no default value. If you hover the ! icon next to the parameter name to see (balloon tip) what the parameter is about. This text is you added in the parameter metadata element as description.\nThe parameter \u0026lsquo;Virtual Network Address Prefix\u0026rsquo; accepts values of array type and it can be seen from the example above how array values are provided. The array values must be enclosed in square brackets.\nNow that you have tried the portal experience of deploying templates, take a look at how you can do that at the command line using Azure CLI. With Azure CLI, there are many ways of providing template parameter input. Inline parameter values is one such method.\nGitHub Gist This code snippet requires your consent to load content from GitHub, which may set cookies. Enable GitHub Gist rchaganti/d7e35878c6687da07ae5fa5dfb7d54c2 In the above command, a resource group for the template deployment gets created first. The az deployment group create command performs the template deployment by using the template URL as the argument to --template-uri parameter and --parameters is used to supply all template input parameter values. The URL specified as argument to the --template-uri parameter must be accessible from Azure. As an alternate to the --template-uri parameter, you can use the --template-file to specify a local path to the template JSON file.\nAs you see in the above Azure CLI command, all mandatory parameters (parameters with no default values) should be provided as key-value pairs separated by space. This can be quite daunting and error-prone if you have a lot of template parameters. You can also provide a JSON string as an argument to --parameters. Also, when using Azure CLI, you must know the names of the parameters to be supplied as input. This is where the parameter files will come in very handy. You can either create one manually or use ARM Tools VS Code extension to generate one for you.\nTo use VS Code extension to generate the parameter file, right-click anywhere in the template editor window and click Select/Create Parameter File. In the command window, select New and then All Parameters. This will prompt for a name and location for the parameter JSON file. Here is how the generated parameter JSON will look like.\nGitHub Gist This code snippet requires your consent to load content from GitHub, which may set cookies. Enable GitHub Gist rchaganti/d7e35878c6687da07ae5fa5dfb7d54c2 You can fill-in the required values and use the following Azure CLI command to start the deployment.\nGitHub Gist This code snippet requires your consent to load content from GitHub, which may set cookies. Enable GitHub Gist rchaganti/d7e35878c6687da07ae5fa5dfb7d54c2 This command specification is way simpler than using inline parameters or JSON string.\nApart from the inline key-value pair specification, the @d4-paramJson.json (@{path}) specification references a file in the local folder path. This is most preferred in case of CI \u0026amp; CD of ARM templates in which the parameter JSON file can be auto-generated during build processes. Also, you can use a mix of inline parameters and parameter JSON as well. You can explore more such examples using az deployment group create --help.\nSummary In this part, you looked at how to add resources to an ARM template and learned how to parameterize the template for reusability. Parameters in ARM templates support multiple data types and learned how to use string and array data types in this article. You will learn about using other data types in the later parts of this series. You learned how to use Azure portal and Azure CLI for template deployments. You looked at using inline parameter key-value specification and looked at creating and using parameter JSON files as well. In the next part, you will learn how to build expressions and variables in ARM templates.\n","date":"2020-07-24T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/azure-resource-manager-adding-parameters-to-arm-templates/","series":"Azure Resource Manager In 30 Days","title":"Azure Resource Manager - Adding parameters to ARM templates"},{"categories":["Azure Resource Manager","Azure","ARM Template"],"content":"In the last two parts of the series, you have seen a brief introduction to Azure Resource Manager and the services that complement ARM to extend its functionality or support the services offered by ARM. You also looked at an overview of ARM templates and the tools available to design, author, and deploy these templates.\nStarting this part of the series for the next few days, you will start learning about authoring ARM templates right from the basics and to the advanced template syntax and techniques with a goal to build an ARM template that helps provision the following architecture.\nThis is not a complex architecture but gives you enough scope to implement what you learn through this series of articles. Apart from this architecture, I recommend that you start looking at the quickstart templates repository and read the templates there to understand the syntax and how some of those templates are written and implement best practices. Throughout this series, you will see examples from quickstart templates repository to explain best practices in template authoring.\nI strongly recommend using VS Code for ARM template authoring and the next section provides a quick overview of this before you dive into ARM template basics.\nVS Code and ARM tools extension If you have already installed VS Code and the ARM tools extension, you may want to customize the extension settings for improved authoring experience and ease. By default, VS Code editor will auto-detect template files if the JSON or JSONC file contains the appropriate schema and changes the editor language to Azure Resource Manager template. You can see this in the status bar of the VS Code editor.\nHowever, this method requires that you at least have a value for the $schema element. You can also use file association setting to detect a file in the editor as an ARM template. This can be done by adding the following in the extension settings. With this setting in place, you can even create an empty file with an extension azrm.json and the editor auto-detects and sets the editor language as ARM template.\nGitHub Gist This code snippet requires your consent to load content from GitHub, which may set cookies. Enable GitHub Gist rchaganti/d7e35878c6687da07ae5fa5dfb7d54c2 Once the editor language gets set to Azure Resource Manager Template, you can access over 70 snippets that come with the extension. For example, typing arm in the editor window, brings up the snippets that you can select and insert into the editor.\nThroughout this series, you will see different features of this VS Code extension. Now, let us get started with the basics of the ARM template language.\nBasics As you have seen already, an ARM template is JSON file with additional template functions and expressions that the Azure Resource Manager understands. Here is a basic skeleton structure of an ARM template.\nGitHub Gist This code snippet requires your consent to load content from GitHub, which may set cookies. Enable GitHub Gist rchaganti/d7e35878c6687da07ae5fa5dfb7d54c2 In the following sections, you will see quick overview of the elements in an ARM template.\n$schema This element identifies the version of template language to be used. The value of this element changes based on the type of editor and the scope of the deployment. There are four different scopes at which you can deploy an ARM template.\nSubscription Scope When you need to deploy resources at the subscription level, for example create resource groups or provision policies and resource-based access controls, you need to use subscription level deployments. For this type of deployments, the value of $schema should be https://schema.management.azure.com/schemas/2018-05-01/subscriptionDeploymentTemplate.json#\nManagement Group Scope Management groups in Azure are used to organize subscriptions for better governance. When you need to provision resources such as policies at the management group level, you need to the use the management group level schema. The value of this element for management group level deployments should be https://schema.management.azure.com/schemas/2019-08-01/managementGroupDeploymentTemplate.json#.\nTenant Scope For provisioning resources across multiple Azure AD tenants, you need the tenant scoped deployments. For tenant level deployments the value of $schema element should be set to https://schema.management.azure.com/schemas/2019-08-01/tenantDeploymentTemplate.json#.\nResource Group Scope This is the typical type of deployment you will normally come across. When using this deployment scope, you describe resources that need to be deployed within a given resource group in a subscription. The value of $schema for this type of deployments in VS Code should be https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#. For Visual Studio, this value should be https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#.\nYou will learn more about scoped deployments in a later article in this series.\nContentVersion This element identifies the version of the template content. This value is required but can be set to anything you want. If you use source control for ARM templates, you can increment the value of this element in your template at every commit. You will learn more about this in a later article discussing CI \u0026amp; CD of ARM templates.\nParameters This element accepts a set of parameters that act as input to the ARM template deployment using which you can introduce reusability of ARM templates. If you are familiar with any programming language, similar to a function or a method parameters, ARM template parameters too support different data types, valid values, default values, and different other characteristics such as min and max values (for numeric types), and min and max length (for string types). This is not a mandatory element but highly recommended to parameterize the template.\nFunctions ARM template syntax support built-in functions and expressions. These functions range from comparison functions to numeric, date, string, and so on. In a later update to the ARM template syntax, support for custom user-defined functions was added. You can now define some complex expressions that you want to use in the template as user-defined functions. This is not a mandatory element.\nVariables Variables, similar to functions, can help reduce the complexity of expressions in the ARM template. This is not a mandatory element. In an ARM template, variables are predominantly inside expressions. Between parameters and variables, you need to always a strike balance (free-form vs known configuration) to create templates are easy to deploy.\nResources This element is where you define what Azure resource you need to deploy or update. An ARM template must have at least one resource. The type of valid resources supported in a template depends on the value of $schema element. In the next section, you will see how to author your first ARM template and you will see more about resource specification in an ARM template.\nOutputs Using this element, you can specify the values returned by the template after the deployment is complete. This element is not mandatory. This can be very useful when using nested templates and you will learn more about it soon in this series.\nFirst ARM template Alright! After that quick overview of the template elements, this section will get you started with your first ARM template. For this purpose, you will perform a subscription scoped deployment to provision a new resource group. Within VS Code, if you want a quick snippet that supports subscription level deployments, you can type arm!s and press enter. You can now add the following snippet under the resource element.\nGitHub Gist This code snippet requires your consent to load content from GitHub, which may set cookies. Enable GitHub Gist rchaganti/d7e35878c6687da07ae5fa5dfb7d54c2 A typical resource specification will be very similar to what is shown above. The following table provides an overview of the resource group object properties that you can use in a template.\nProperty Name Description Is Mandatory? type This property identifies the type of a resource that you want to provision. For a resource group, this will be Microsoft.Resources/resourceGroups. Yes name Using this property, you can specify a name for the resource being provisioned. Yes apiVersion This property identifies the API version to be used for provisioning the resource. For a resource group resource, this, at present, should be 2020-06-01. Yes location This property specifies where there resource should be created. Some resources may not be supported in all regions or enabled in your subscription. Yes properties Specifies additional properties needed for the resource provisioning. Yes tags Specifies a list of tags (key-value pairs) associated with the resource. No For now, you don\u0026rsquo;t have to worry about the parameters, variables, functions, and outputs in the template. You can simply leave them blank. With the addition of resource group snippet, here is how your first template should look like.\nGitHub Gist This code snippet requires your consent to load content from GitHub, which may set cookies. Enable GitHub Gist rchaganti/d7e35878c6687da07ae5fa5dfb7d54c2 This is ready for deployment. If you want to use Azure portal for template deployments, the subscription level deployment, at present, is available only in the preview portal. The below deploy to Azure button will take you directly to the preview portal.\nIf you want to perform this deployment using Azure CLI, you can use the below command to perform this subscription level deployment to create the resource group.\nIf you have not already done so, you must first authenticate using az login command.\nGitHub Gist This code snippet requires your consent to load content from GitHub, which may set cookies. Enable GitHub Gist rchaganti/d7e35878c6687da07ae5fa5dfb7d54c2 In the above command, do not confuse the --name and --location parameters to what you have in the ARM template. These two parameters indicate the name of the deployment and location where the deployment meta data should be stored. The sub subcommand within az deployment indicates that you are doing a subscription level deployment.\nSummary In this part of the article, you looked at basics of ARM template syntax, understood different elements in the template and their purpose, saw a quick overview of scoped deployments, and finally performed a subscription level deployment to create a resource group in your subscription and used Azure CLI for this purpose. In this next part of this series, you will learn how to create reusable ARM templates by adding parameters.\n","date":"2020-07-23T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/azure-resource-manager-arm-template-basics/","series":"Azure Resource Manager In 30 Days","title":"Azure Resource Manager - ARM template basics"},{"categories":["GitHub","Hub","CLI"],"content":"As I started writing more in markdown and using GitHub pages, my work with git and GitHub increased quite a bit. At times, I find myself fighting with git conflicts and merges. I am not an expert in git command-line and in a complex scenario, I figure my way out looking for some help.\nYou may have already seen in my articles that I use GitHub CLI a lot to work with my GitHub repositories, pull requests, and issues. I recently started using another CLI tool from GitHub called hub. Hub is an extension to git command-line and makes working with GitHub from the command-line a breeze.\nInstall Hub CLI You can download Hub CLI from https://github.com/github/hub/releases/. I am using Windows OS and therefore I downloaded the 64-bit zip archive package for Windows. Once the download is complete, extract it to a temporary location and run install.bat at the command prompt. If the install is successful, you will see a message that hub.exe installed successfully.\nThe hub.exe binary gets copied to %LOCALAPPDATA%\\GitHubCLI\\Bin. This gets added to the user path variable and if you close and reopen the console, you should be able to start using hub.\nCreate repositories The first thing you may want to do is to indicate https for all hub communication with GitHub.\n1 git config --global hub.protocol https Once this is done, you can create a GitHub repository.\nFrom an empty folder You can either create a GitHub repository from an empty folder or from folder that already has your files. Let us look at the first method now.\n1 2 3 4 mkdir TestRepo cd TestRepo/ hub init hub create -poc -d \u0026#39;Test repository for hub.exe demo\u0026#39; -h \u0026#39;https://www.ravichaganti.com\u0026#39; The hub init command is same as git init. It will initialize a git repository in the empty folder. The hub create command will create the GitHub repository from the local git repo. The -p flag indicates that you want to create a private repository, -o indicates that the newly create repository URL be opened in a browser, and -c indicates that the created repository URL be copied to clipboard. The -d and -h flags should be self-explanatory.\nhub create will prompt for your GitHub credentials. A personal access token gets created and will be given limited scopes.\nOnce this repository is created, you can add the files to this folder and push upstream to GitHub.\n1 2 3 4 echo \u0026#34;#This is a test repository\u0026#34; \u0026gt; README hub add . hub commit -m \u0026#34;initial commit\u0026#34; hub push -u origin HEAD From an existing folder To create a repository from folder from that contains your code, you need to set the location to that folder and run the following commands.\n1 2 3 4 5 6 cd TestRepo/ hub init hub add . hub commit -m \u0026#39;New repository from an existing folder\u0026#39; hub create hub push -u origin HEAD The above commands will create a new GitHub repository from an existing folder. The name of the folder gets used as the name of the repository.\nDelete repositories The personal access token that gets created when you first authenticate using the hub CLI. However, this token will not be given the delete_repo scope permission. Therefore, to be able to delete a repository using hub CLI, you need to enable the delete_repo scope for the personal access token.\nOnce you enable the required scope, you can use the hub delete \u0026lt;repoName\u0026gt; command.\nThe -y flag command with this command will skip the prompt.\nWorking with gists You can use hub gist subcommand to work with gists. This command supports listing gists, getting contents of a gist and, creating new gists.\nShow gists To list gists from your account, you can use the hub gist show command. This requires a mandatory ID parameter which you can obtain from the gist URL from gists.github.com.\n1 hub gist show 54ccde11aa58f7ecd443e6ff3d1cf319 You can get the contents of a gist by specifying the filename.\n1 hub gist show 54ccde11aa58f7ecd443e6ff3d1cf319 arrays.go Create gists Creating gists can be done using the hub gist create command.\n1 echo Write-Host \u0026#39;This is a test gist\u0026#39; | hub gist create This command creates a secret gist and the URL gets shown in the output. In this method, the gist gets created as a .txt file.\n1 hub gist create test1.ps1 test2.ps1 -c The above command will create the gist and add the files test1.ps1 and test2.ps1 from the local folder. the -c flag will copy the created gist URL to clipboard.\nYou can use the --public flag to create a public gist and -o flag to open the newly create gist in a browser.\nThis is it for today. In a future article, we will revisit some more hub CLI commands.\n","date":"2020-07-23T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/github-hub-cli-an-extension-to-git-command-line/","series":null,"title":"GitHub Hub CLI - An extension to git command-line"},{"categories":["Azure Resource Manager","Azure","ARM Template"],"content":"The first part of this series of articles on Azure Resource Manager (ARM) provided a quick overview of ARM. You looked a quick introduction to ARM templates as well and looked at an overview of different template deployment methods. In this part of the series, we will look at different methods to author ARM templates and an in-depth look at preferred deployment methods. You can choose any of the methods mentioned here but this series of articles will use one preferred method each for authoring and deploying templates. You will read more about those methods and the reasons behind that choice as well.\nAuthoring Tools ARM templates, as mentioned in the last part, are JSON documents. These files can be authored in notepad as well but there are better ways to deal with this task. You will learn about three different methods that I have used and my observations and preferences around these methods. Let us start with the template editor that Azure Portal offers.\nTemplate Editor in Azure Portal The web-based template editor in the Azure portal is one way but I see it as a better way to edit existing templates than writing one from scratch. You can import a quick start template and use that as a starting point for building your own template.\nOnce you click on Quickstart template [1], you will be shown a list of templates available from the GitHub repository. The selected template then gets loaded into the editor for further updates or deployment. I selected the 101-vm-simple-windows template from the list.\nAt this point, you can click on Add resource [2] to add another resource to this template. This will bring up a limited set of additional resources you can add to the loaded template. You may select a Windows Virtual Machine.\nThis brings up a few input parameters needed for the new VM instance. Once you enter these values and click OK, the template in the editor window gets updated to add the new VM resource instance. You can review the updated template and click on Save to move ahead to deploy this template.\nAt this point, you will presented with a list of input parameters needed for this template deployment. You can enter these values and click Purchase to start the template validation and deploy it if there are no validation errors. However, it is not always this straightforward. I ended up updating a few language expressions to make this working. If you have tried this option, you will know that the editing experience is limited. It supports adding only a few types of additional resources to the template. And, of course, this isn\u0026rsquo;t a great way to edit or build anything other than a simple template. You need a better way than this web-based editor.\nVisual Studio Visual Studio provides a rich integrated development environment (IDE) for developing various types of applications. You can use some of the starter templates packaged along with Visual Studio and build upon those existing templates to add your own resources or start from a blank template and build everything from scratch. For this quick overview of using Visual Studio for template authoring and deployment, I will use Visual Studio 2019 Community Edition.\nWhen you install and open Visual Studio, you will see different options to get started. Select Create a new project.\nIn the next window, filter down the options [1] to Azure and Cloud. In the filtered project templates, select Azure Resource Group [2] and click Next.\nEnter the relevant input for creating a Visual Studio project and click Create.\nAs you see in the above screenshot, you can select one of the canned starter templates or start with a blank template. Since we have learned the basics of template authoring yet, scroll down in the template list and select Windows Virtual Machine and click OK. This creates the Visual Studio solution.\nOnce the solution gets created, click on WindowsVirtualMachine.json [1] in the Solution Explorer pane. This opens the ARM template. On the left-hand side, you can see the JSON Outline [2] pane which shows parameters, variables, and resources defined in the template.\nSimilar to the Azure Portal template editing experience, you can add resources to this existing template by right-clicking on resources node in the JSON outline and selecting Add New Resource.\nFrom a set of limited resource types, you can select the resource type you want to add, provide the necessary input parameter values and click Add. The selected resource gets added to the template. You can now customize the template and start a deployment. Before you can deploy the template, you will have to provide the values for the template parameters. This can be done by opening the WindowsVirtualMachine.parameters.json file and enter the relevant values.\nBefore starting a deployment of this template, you can validate the template. For that, right click on the solution name in the Solution Explorer and select Validate.\nYou will be prompted to enter the password for the admin user to continue validation. If the validation completes with no errors, you can again right-click on the solution name in the Solution Explorer and click Deploy to provision the ARM resources defined in the template.\nOverall experience with authoring templates in Visual Studio is good. This method provides intellisense for different resource properties based on the template schema version.\nThe editor experience in Visual Studio also provides insights into what might be wrong with certain resource property values and / or what might be missing therefore providing better editing experience.\nOverall, compared to the web editor in Azure portal, authoring experience in Visual Studio is way better. However, this is a heavy solution. On a decently configured system, it takes close to a minute to just open Visual Studio. Also, as you may have already seen, the starter templates do not support selecting the Azure quickstart templates from the GitHub repository. Within the template editor, you can auto-complete the resource property names once the resource type property is populated. However, there are no quick snippets that you can use directly. This, for me, kills productivity. There may be a few external marketplace extensions but there is no built-in support.\nVisual Studio Code This is my favorite tool. No doubt. There are several reasons as you will read in this section. This tweet poll just endorses my choice.\ntweet 1280058881322872833\nThroughout this series, you will see using VS Code only for authoring ARM templates. Therefore, I will skip any details similar to Visual Studio based template authoring experience you just read earlier. Now, moving on to why I like VS Code for ARM template authoring, first of all, VS Code is an open source, lightweight, and faster alternative to Visual Studio. The extension support within VS Code is unmatchable. With the official Azure extensions and a bunch of other community extensions, you can be super productive when working with the complex ARM template files. Before you get started with authoring an ARM template in VS Code, you must first install the right set of extensions that will help you in the authoring process. The following list is the minimum you will need to follow the articles in this series. Alternatively, you can install the Azure Tools extension pack that will install following three plus a bunch of other extensions that you may never need.\nAzure Account The Azure account VS Code extension allows you to sign into Azure and access your Azure subscriptions. This is necessary for other Azure extensions to function. Once you install this extension, you get a set of commands to sign in, sign out, select subscriptions, create Azure accounts, and open Azure Cloud Shell within VS Code.\nAzure Resource Manager (ARM) Tools The ARM Tools extension is a must if you want to be productive authoring ARM templates in VS Code. This extension provides the full ARM template language support and provides a bunch of snippets to speed up the authoring process. You will see and use this extension a lot in the next parts of this article series and therefore I will not spend any more time on this here.\nAzure Policy Azure Policy extension can be used for authoring and managing policy definitions and assignments. This is in preview like a few other VS Code extensions. This is a good one to install as well as you follow this series of articles on Azure Resource Manager and want to try out Azure Policy related examples.\nAzure CLI Tools Finally, the Azure CLI Tools extension is what will help you when you author Azure CLI scripts to perform template deployments. This is an optional one if all you need is really the ability to run Azure CLI commands to validate and deploy templates within the VS Code terminal.\nDeployment Tools There are many ways to deploy ARM templates. This includes using Azure portal, Azure CLI, Azure PowerShell cmdlets, ARM REST API, and Python or Go SDK.\nAzure Portal Azure Portal allows you to author templates in an online editor or upload templates for deployments or deploy from a collection of quick start templates. We looked at the an example of this in the previous section already. Using the template editor in the Azure portal, you can author and deploy or you can simply upload a template for deployment. Many places online (including the Azure Quickstart template repository), you see a Deploy to Azure button that directly takes to you the custom deployment page. You can use this method to deploy a template stored in an online public repository to perform resource group level deployments. Here is how it appears in a webpage.\nClicking on the above button brings up the custom deployment blade asking for the input parameters needed for the template deployment.\nAzure CLI Azure CLI offers a method to deploy ARM templates as well. You can use the az deployment command to perform template deployment.\nGitHub Gist This code snippet requires your consent to load content from GitHub, which may set cookies. Enable GitHub Gist rchaganti/d7e35878c6687da07ae5fa5dfb7d54c2 With the az deployment command, we can either use an online template file or local file.\nAzure PowerShell Similar to Azure CLI, you can use Azure PowerShell module as well to deploy ARM templates.\nGitHub Gist This code snippet requires your consent to load content from GitHub, which may set cookies. Enable GitHub Gist rchaganti/d7e35878c6687da07ae5fa5dfb7d54c2 If you do not want to install any of these tools (Az CLI and Az PowerShell module) on your local system, you can always use Azure Cloud Shell within VS Code or launch Cloud Shell in the browser to start using any of these tools. Within this series of articles, you will occasionally see a button like the one displayed below. You can click on it to open Azure Cloud Shell and start performing command execution.\nARM REST API ARM REST API is another way to do template deployment but I consider this a bit low-level and requires good amount of scripting / programming experience.\nFor most part of this series of articles on ARM, you will see Azure CLI examples. You will occasionally see a few Azure PowerShell examples. I will write about other deployment methods such as Terraform and Pulumi in a different series of articles. But, before you go there, it is good to get a good grasp of ARM fundamentals. Next few articles in this series of articles will dive into the ARM template authoring and deployment. For now, let us move on to discuss some features and services that work hand in hand with ARM.\nTemplate Design and visualization Tools As a cloud consultant or architect, you have to create the cloud architecture proposals before you go into the implementation phase. There are a bunch of tools \u0026ndash; online and offline \u0026ndash; that can help you create the architecture diagrams for your cloud infrastructure proposals. In this final section of this post, you will get to know about a few tools that I have used or using to create these architecture diagrams.\nYou can always use good old friend Microsoft Visio if you have a license for the product and I trust this a lot and use it. However, I will introduce other tools that you may find useful.\nARM template visualizer The ARM template visualizer is not really a designer like Visio or any other tool you may see in this section but a good way to visualize an ARM template on a canvas. If you navigate to the ARM quickstart template GitHub repository, you will see a button like what is shown below.\nAnd, when you on click on this button, it takes you to an ARM template visualizer where you can see the template in a visual form.\nBy default, the template visualization [1] opens up and you can click on \u0026lt;/\u0026gt; [2] to see the ARM template JSON. You can edit the contents of the template and return back to the canvas to see the updated visualization. Going to the File menu, you can open a local template or download the template that is visualized.\nDraw.io Draw.io is a web-based tool that can help you design and save Azure architecture diagrams. There is a VS Code extension as well for this and you can now create the diagrams right within VS Code.\nCloudskew Cloudskew is another web-based tool that provides a very nice canvas to create cloud architecture diagrams. This is also a free tool that offers several built-in cloud icon support.\nThere may be many other tools for ARM template design and visualization that I may not have tried yet. But, like I mentioned earlier, my trusted source is still good old Visio and I continue to use it.\nSo far in this article, we looked at different authoring tools, deployment methods, and finally a few options to design and visualize these ARM templates. This should get you equipped with the right tools to get started with template authoring. In the next part of this series, we will look at the basics of template authoring. Stay tuned.\n","date":"2020-07-17T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/azure-resource-manager-tools-for-arm-template-design-authoring-and-deployment/","series":"Azure Resource Manager In 30 Days","title":"Azure Resource Manager - Tools for ARM template design, authoring, and deployment"},{"categories":["Azure Resource Manager","Azure"],"content":"A while ago, I had written a series of articles introducing and diving deep into authoring Azure Resource Manager (ARM) templates. A few things have changed in authoring ARM templates and new features got added in the recent past. I thought it is probably a better time to revisit and do it all over again. At the same time, I thought why just limit to just authoring ARM templates. Over the years, I made notes around ARM and learned quite a bit. Starting today, I will share those notes with you here.\nIn this new and improved series, I will start from the very basics and build upon that in each article. The goal is to ensure I create a set of articles that can be referenced in a zero to hero approach to understand ARM and authoring ARM templates. So, without further ado, let us get started and let us start with the very basics.\nWhat is Azure Resource Manager (ARM)? Microsoft Azure, during the early days, had a management layer called Azure Service Manager (ASM) that was responsible for deploying and managing services. These services that you deployed using ASM were all independent entities without any grouping whatsoever. There was no way to define the dependencies either. So, if you were to implement a three-tier application as a set of cloud services, each service in the three-tier application had to be deployed individually and managed individually. When you have to delete this application, you had to do it by deleting each service individually. The same applies to updates as well. Monitoring and billing was a nightmare. This approach was not scalable. So, during Build 2014, Microsoft announced a new deployment and management service called Azure Resource Manager (ARM).\nHere is a super simplified view of the ARM that provides an overview of the constructs.\nWith ARM, all your resources having a common life cycle (create/update/delete) can now be grouped together under a resource group and their life cycle management can be done together. A resource group becomes the unit of management. Each resource in Azure is supplied by a service known as a resource provider. The resources in Azure can be deployed in many ways but the focus of this series will be around the declarative deployment templates through which you can provision the resources in a consistent and idempotent manner.\nLet us dig a bit into the terminology you read in the above paragraph.\nAzure Resources Resources are what you provision in Azure cloud. For example, as shown in the above image, web apps, virtual machines, databases, and so on. Each resource will have certain properties that you can configure. What resources you can provision depends on what is allowed and / or enabled in your subscription. We will look at this in a later article.\nAzure Resource Groups Resource group is a container for all resources that share a common life cycle. A resource should always exist in a resource group and can exist only in one group. It is not necessary that resources that have inter-dependencies are provisioned in the same resource group. These resources within a resource group can exist in different regions too. Instead, the life cycle of the resources is used as a way to group the resources together.\nAzure Resource Providers The resource providers enable the resource and implement the management of the resources. Each resource that you can provision in Azure will have an associated resource provider. ARM binds all this together to provide a single management pane irrespective of what type of resource you are provisioning.\nThere are several methods to interact with ARM for all your resource creation and management needs. These methods include Azure PowerShell module, Azure CLI, Azure SDKs for Golang, Python, and other languages. You can, of course, use the REST API directly to provision and manage resources. Through all these methods, you basically use individual commands or write scripts or you own applications to provision and manage resources. This approach is more imperative in nature. You write a script to implement what you need and at the same time you define how that needs to be done. In an imperative method, you perform checks to see if the resource that you are trying to provision already exists or not and provision only if it does not exist. You perform all error handling yourself in the imperative approach. And, if you were to create a lot of resources that do not necessarily depend on each other, you can write some parallel jobs to perform simultaneous execution of resource creation. But, the onus is on you to write this code and make sure it works.\nAzure Resource Manager Templates ARM templates help eliminate the need for the tedious and error-prone development of automation scripts using any of the methods you read about earlier. These templates are JSON files that provide a declarative syntax to provision Azure resources. Using the declarative syntax of ARM templates, you define what infrastructure you need to provision and not how. With the increasing focus on Agile infrastructure practices and DevOps, this declarative template becomes a part of your application code and lives in the same source control repositories as your application. Any changes to the infrastructure configuration go through the same source control and application development procedures such as automation validations, continuous integration and delivery / deployment. This is what we describe as Infrastructure as Code (IaC).\nThe ARM templates also offer a way to repeatedly deploy your infrastructure in Azure cloud. This is done in an idempotent way. Idempotency means that the result of a provisioning task (state of the resources defined in the template) would be same irrespective of how many times you run the task.\nWith ARM templates, unlike imperative scripting, you don\u0026rsquo;t have to worry about simultaneous resource creation or resource provisioning dependencies or error handling. ARM can infer these dependencies from what is defined in the template and also perform parallel execution when possible. This ensures reliable deployment of resources defined in the template.\nIn summary, ARM templates offer a declarative method to provision your infrastructure resources in a reliable and idempotent manner. There are several other aspects of ARM templates such as validation, extensions, and modularity / composability. You will read more about these characteristics in the upcoming parts of this series with relevant examples.\nApart from ARM templates, there are 3rd party providers such as Terraform and Pulumi for provisioning Azure infrastructure. These methods provide the same characteristics as ARM templates. Each of these methods have their pros and cons. This series of articles will only use ARM templates as a way to provision infrastructure.\nIf you are interested in exploring what ARM templates look like or looking for a quick start for a specific resource type or scenario, you may look up Azure Quickstart templates GitHub repository.\nWriting and reading ARM templates can be very complex and may get boring as well! At Build 2020, Microsoft announced language revision that intends to provide a Domain Specific Language (DSL) abstraction that complies to a JSON template underneath and makes it easy for you to author complex ARM templates.\nYouTube Video This video requires your consent to load content from YouTube, which may set tracking cookies. Enable YouTube Video ID: UaVCNpD3pvg?t=986 As this preview becomes available, I will write more about this new method here.\nAfter you author the ARM templates, you can deploy these templates using a variety of methods. You will read more about these methods in the next part of this series.\nAzure Deployment Manager As your organization grows and you start using Azure with multiple subscriptions and may be in multiple regions too, it becomes complex to perform an ordered and monitored rollout of Azure deployments. This is where Azure Deployment Manager (ADM) comes into play. ADM extends ARM by enabling new features called Service Topologies and Rollouts. ADM makes it easy to define complex deployments that span multiple regions and subscriptions and allows repeated deployments of these topologies using Rollouts. You will learn about this in-depth in later parts of this series.\nAzure Governance While knowing how to author and deploy ARM templates is an important part of working with Azure, it is equally important to know how to govern those resources and services deployed in Azure. As cloud architect, you would want control over who can create what type resources, where can they create those resources and how many. You would also want to standardize on the deployments so that the configuration of resources is in compliance with your organization standards. Among many other aspects like this you would want the ability to track your cloud cost more granularly for every service deployed in the cloud and for every business unit or group within your organization. To address this and to implement efficient governance Azure offers features and services such as Policies, Management Groups, Resource Graph, and Blueprints. Here is an excellent depiction of Azure governance architecture from the partner blog.\nAzure Policy helps enforce your organizational IT and business standards for resource consistency, compliance to regulatory requirements, management, security, and cost. Azure offers a set of built-in policy definitions that are readily available for some of the common use cases in your Azure environment. You can, of course, create your own custom policy definitions as well.\nAzure Management Groups allow organizing subscriptions into containers called management groups and then perform governance on these management groups. The subscriptions under the management groups inherit the governance conditions (policies) applied at the management group level.\nAzure Resource Graph offers an efficient and performant way to query resources across multiple environments and subscriptions. This service can be used to extend management of Azure resources by enabling the ability to query resources with complex filtering, grouping, and sorting by resource properties. To use Azure Graph, you need to understand the Kusto Query Language (KQL). In this series of articles, you will see a basic overview of KQL and using Azure CLI to query the Resource Graph.\nAzure Blueprints enable streamlining of resource deployments by packaging ARM templates, policies, and role assignments. Blueprints are a declarative way to orchestrate these templates and other related artifacts. Blueprints enable efficient tracking and auditing of deployments. The knowledge you gain around ARM templates can be directly used with Azure Blueprints.\nWith this quick overview of Azure governance features and service, you have an understanding of Azure Resource Manager and services that complement ARM. This brings us to the end of today\u0026rsquo;s article. The next few parts of this series will focus on authoring ARM templates. As described earlier, the ARM templates are JSON files and can be authored in notepad as well. But, there are some very good authoring tools and we will take a look at that in the next part of this series. Stay tuned!\n","date":"2020-07-16T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/azure-resource-manager-introduction/","series":"Azure Resource Manager In 30 Days","title":"Azure Resource Manager - Introduction"},{"categories":["PowerShell","GitHub"],"content":" Git.io \u0026ndash; service by GitHub \u0026ndash; can be used generate the short URLs for any GitHub.com based URL. You can navigate to Git.io and paste a GitHub URL into the input box and generate a short URL. Note that this works only with GitHub.com URLs.\nI have been using gists a lot lately and the URL to gists is usually very long and includes guids. For these gist URLs, I have been using the git.io service and wrapped that in a quick PowerShell module to help me generate these URLs. Today, I published it on GitHub and PowerShell Gallery for you to use it as well.\nUsage It is straightforward to use this module. There are only two commands.\nGet-GitIoShortUrl This command is used to get the expanded URL from the short URL.\nGet-GitIoShortUrl -ShortUrl 'https://git.io/psdev'\nNew-GitIoShortUrl This command can create a new short URL for any GitHub URL.\nNew-GitIoShortUrl -Url 'https://github.com/rchaganti/DSCResources' -Code 'psdsc'\nIn the above example, -Code specifies an optional shortcode to be used for create the short URL. So, if the shortcode is not already taken the generated short URL will be https://git.io/psdsc.\nIn the absence of -Code parameter, a random shortcode gets generated.\nIf you generated a short URL for a given GitHub URL without -Code, trying this command again with the same URL and with -Code will not have any impact. Once a short URL gets generated, the git.io service will always return the same short URL.\nBTW, if you are using GistPad VS Code extension, you may want to upvote this feature request to add an option to copy the GitHub shorturl for the gists.\n","date":"2020-07-15T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/powershell-commands-to-generate-git-io-shorturls/","series":null,"title":"PowerShell commands to generate Git.io shorturls"},{"categories":["Hugo"],"content":"One of the to-do list items I had for this static blog site was to figure out a way to add page views counter to every article. This is especially important for PowerShell Magazine articles as we move that to a static site as well. I tried looking for a few solutions but found mostly things that would require playing with JavaScript.\nWhile looking at different GitHub profile pages this morning, I came across a visitor counter on Tyler\u0026rsquo;s page. This piqued my interest and looked at how he was generating it. It was made possible through a service hosted on glitch.me. There is a GitHub repo that contains the source for this. At first, it seemed like a thing only for GitHub readme markdown but looking at the docs, I realized that I can embed that in simple webpages too.\nSo, I updated single.html in my Hugo theme to add the following code. Single.html is the template for the article content in Hugo.\nGitHub Gist This code snippet requires your consent to load content from GitHub, which may set cookies. Enable GitHub Gist rchaganti/056a02f289d7d5415c17d27082ee753b With Hugo framework, this becomes relatively easy since the variables provided within the page context while building the static pages will help us build the necessary URL for tracking the visitors to the page. Depending on what static page generator framework you use, you may have to change the code above.\n","date":"2020-07-10T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/adding-visitor-counter-to-statically-generated-web-pages/","series":null,"title":"Adding visitor counter to statically generated web pages"},{"categories":["Hugo"],"content":"In the past I wrote here about the Az Deploy button shortcode and the channel9 video embed shortcode for Hugo generated static pages. I have been creating some simple shortcodes for my own use on this blog as I start writing on different technologies again. A couple of more shortcodes I added recently are for the ARM template visualize button and launching Azure Cloud Shell.\nToday I decided to publish these shortcodes as a collection on GitHub. Instead of writing about each and every shortcode \u0026ndash; unless it has significant value to others as well \u0026ndash; this article will get updated every time I publish a new shortcode.\nAzDeploy If you ever read the Microsoft documentation around ARM templates or visited the ARM templates quickstart repository, you might have noticed a deploy to Azure button. Clicking on the button will take you to the Azure Portal to a page that provides a blade to input parameter values to deploy the ARM template. This shortcode enables the same functionality on a statically generated page. Here is how you use it.\nGitHub Gist This code snippet requires your consent to load content from GitHub, which may set cookies. Enable GitHub Gist rchaganti/056a02f289d7d5415c17d27082ee753b The link to JSON file should be a publicly accessible raw URL. The above shortcode when gets compiled to a static page will generate a button as shown below.\nArmVisualize Along with the deploy to Azure button, you may have also seen a Visualize button that will show a visual representation of the ARM template. This shortcode helps implement the same functionality in a Hugo generated static page. Here is how you use it.\nGitHub Gist This code snippet requires your consent to load content from GitHub, which may set cookies. Enable GitHub Gist rchaganti/056a02f289d7d5415c17d27082ee753b The link to JSON file should be a publicly accessible raw URL.\nThis shortcode displays a button as shown below.\nAzCloudShell Many Azure related documents have a button that is used to launch Azure cloud shell in a browser. This shortcode can be used to add the same functionality in a static web page.\nGitHub Gist This code snippet requires your consent to load content from GitHub, which may set cookies. Enable GitHub Gist rchaganti/056a02f289d7d5415c17d27082ee753b You can specify to open Bash or PowerShell by specifying that as the target argument. If you skip target parameter, the last opened shell will be launched. Here is how the Launch cloud shell button will appear.\nChannel9 Hugo has built-in shortcodes for YouTube and Vimeo. However, I refer to a few channel9 videos that I want to embed in my articles. So, I decided to write a shortcode that helps me embed Channel 9 videos. Here is how you use it.\nGitHub Gist This code snippet requires your consent to load content from GitHub, which may set cookies. Enable GitHub Gist rchaganti/056a02f289d7d5415c17d27082ee753b Here is how the embed will appear.\nChannel 9 Video This video requires your consent to load content from Microsoft Channel 9, which may set tracking cookies. Enable Channel 9 Video: https://channel9.msdn.com/Events/Build/2020/KEY01 ","date":"2020-07-08T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/my-custom-hugo-shortcode-collection/","series":null,"title":"My custom Hugo shortcode collection"},{"categories":["Hugo","GitHub","GitHub Actions"],"content":"Ever since I moved to this site/blog to static pages using Hugo and GitHub pages, I have been finding ways to optimize the publishing experience. If you ever used Wordpress, the plugins that are available to customize the site look and feel and functionality will spoil you. They make your life easy by extending base functionality and you can achieve pretty much anything you want as long as there is a plugin for that and you will always find one. Moving from such an environment to a static page generator makes you feel that you have to do all the heavy lifting.\nFor example, tweeting an article once it is published is super simple when using Wordpress. You can simply integrate with Jetpack or use an external plugin to do that job for you. However, with static pages hosted on GitHub pages, your options are limited. If your static site generator generates an RSS feed, you can use a service like IFTTT to retrieve changes to the feed and then send out a tweet announcing a new article. In the absence of an RSS feed, you must manually tweet the article.\nAs a part of this move to Hugo and GitHub pages, I wanted more control over publishing articles on this blog. I started writing a couple of longer series of articles. These articles will be written in advance but published one after another. So, I will have a lot of articles in draft state until the time I want to publish them. Now, you may say that I may be over engineering this but this becomes an important aspect as I move PowerShell Magazine to a static site as well. PowerShell Magazine has multiple authors contributing and we need a way to ensure that the content submitted by these contributors is reviewed and allowed to publish only after the review and subsequent updates are complete. We cannot let anyone commit to a master or a live branch. So, figuring out these details before PowerShell Magazine goes live as a static site will help on-board other contributors quickly. Essentially, my site is a playground for all that! :)\nSo, here is the GitHub flow that I implemented as of today. I am going to optimize this further based on what I need for PowerShell Magazine.\nYou will need GitHub CLI try out commands in this article.\nTo implement this flow, I have two different GitHub repositories \u0026ndash; blog and rchaganti.github.io. Blog repository is a private repository that contains all the markdown files for the articles on this blog and rchaganti.github.io is a public repository to which the generated static pages will be pushed to.\nBlog repository has two branches \u0026ndash; draft and live. Draft branch is where I author all the articles and keep them ready for publishing. Live branch has the content that should be live on the public website. Whenever there is content that needs to be pushed into the live branch, I submit a pull request.\nGitHub Gist This code snippet requires your consent to load content from GitHub, which may set cookies. Enable GitHub Gist rchaganti/056a02f289d7d5415c17d27082ee753b I use the blog article title as the argument to --title parameter and the permalink to the new article as the argument to --body parameter. I use these values specifically since I want to be able to send a tweet once the static page is generated and public site is updated. You will see how it gets used in the workflow file.\nHere is how (example from an earlier test) the created pull request will appear.\nAt this point in time, since I am the only author on my site, I can simply merge the pull request. In case of a multi-author blog or external contributions, we can go through the review workflow after the PR is submitted.\nGitHub Gist This code snippet requires your consent to load content from GitHub, which may set cookies. Enable GitHub Gist rchaganti/056a02f289d7d5415c17d27082ee753b The above command will merge the PR. Now, at this point, we can trigger a GitHub workflow to use Hugo to generate the static pages, push to an external public repository, and finally tweet that change. Here is the workflow YAML from my repository.\nGitHub Gist This code snippet requires your consent to load content from GitHub, which may set cookies. Enable GitHub Gist rchaganti/056a02f289d7d5415c17d27082ee753b The above workflow gets triggered only when a pull request is associated with the live branch and the PR is closed. I have chosen a ubuntu runner to ensure all actions that I specified can run. And, this is a multi-step workflow.\nIn the first step, live branch of the blog repository gets checked out. In step 2, I am using peaceiris/actions-hugo@v2 action from the marketplace to setup Hugo and I am using extended version (specified using extended: true) since I have custom CSS that needs to be compiled.\nStep 3 builds the static pages using Hugo command and step 4 publishes those static pages to an external public repository. This step uses peaceiris/actions-gh-pages@v3 from the actions marketplace. This action a few options. If you do not specify an external repository, it will simply create a new branch in the same repository that was checked out earlier and publish your static pages there. It will also auto-configure the cname based on what is configured in the static pages. I did not want this behavior since my Blog repository is a private repository and do not want a public facing site in that repository. For this action to be able to access the external repository and commit the new build of static pages, it needs the GitHub personal token.\nFinally, the last step sends out a tweet using the ethomson/send-tweet-action@v1 action. This action requires Twitter API token and secrets. You can create your own app on Twitter developer portal. If everything else goes fine, this last step will send the tweet out using the pull request title and body arguments you have seen earlier.\nHere is a test tweet that was sent out at the end of an earlier test run.\ntweet 1279720259801530368\nNow, there is one thing I still need to implement is the conditional execution of the GitHub action. For example, I may update an already published article for whatever reasons but may not want a tweet to go out after this update. I can implement this based on the pull request labels. And, I want to be able to schedule article publishing which may be possible using cron syntax within the workflows. More on these later.\n","date":"2020-07-06T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/github-workflow-for-streamlined-publishing-of-static-pages/","series":null,"title":"GitHub workflow for streamlined publishing of static pages"},{"categories":["Community","MVP"],"content":"July 1st of every year is a hit-refresh or F5 day for most of us in the Microsoft MVP community. This year is no different.\nI will be a Cloud \u0026amp; Data Center Management (CDM) MVP for another year. This is my 12th award and still excited the same as my first year in the MVP award program. The last MVP year (2019-2020) was good with PowerShell Conference Asia moving to India and being super successful with over 230 attendees. For the upcoming MVP year (2020-2021), I have a good number of things lined up on this blog already. You will start seeing those contributions starting this month. I am in the process (very slow) of moving PowerShell Magazine as well to a static site so that it becomes easy to maintain and author more articles. Stay tuned for these updates.\n","date":"2020-07-01T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/micosoft-mvp-for-another-year/","series":null,"title":"Microsoft MVP for another year"},{"categories":["GitHub"],"content":"After I moved to a static site hosted on GitHub Pages, I was looking at different options to share the code snippets in a better way. With the Hugo generated static pages, there are a few options for code sharing in an article.\nYou can use the simple pre-formatted text in markdown. This is usually done by enclosing code snippet in ```. This appears (in Hugo Coder theme I am using) as a big black block and has no way to choose the right syntax highlighting based on what programming language the snippet is written in. There are no line numbers or no highlighting of lines in a code snippet and so on.\nThe second method involves Hugo\u0026rsquo;s default Highlight shortcode. This is a good method that supports line numbers, language type, and line highlights. But, the only shortcoming is that I have to now store the code snippet locally in the same static page. So, if I need to update the code snippet, I end up updating the static page itself.\nSo, wanted to explore something better and then I found the Gist shortcode in Hugo. Using this shortcode, you can embed a GitHub Gist on a static page. This seemed like a good fit for me since I get the line numbers, language based syntax highlighting, and nothing saved locally in the static page. However, for adding the code snippets as Gists, I first need to upload the Gist and then use it in my article. For this, I need to navigate to the GitHub site and then return once I am done uploading Gists. This is where I found GistPad VS Code extension quite helpful. With this extension in place, I can now create/update/delete Gists right inside VS Code where I am writing all the markdown content for the articles. So, I never have to leave VS Code. Here is how it appears in VS Code.\nAs you see in the screenshot, I can group all Gists that belong a specific article or category and then simply use them here in an article. With this integration, I simply write all code as a Gist and then use the following syntax to make it appear here.\nGitHub Gist This code snippet requires your consent to load content from GitHub, which may set cookies. Enable GitHub Gist rchaganti/056a02f289d7d5415c17d27082ee753b Simple. I will be using this method of sharing code snippets within an upcoming article series. Stay tuned.\n","date":"2020-06-16T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/how-i-use-gists-to-store-and-display-code-snippets-in-blog-posts/","series":null,"title":"How I use GitHub Gists to store and display code snippets in blog posts"},{"categories":["Hugo"],"content":"I have been trying a few things with my new blogging platform here and in the preparation for a new series of articles, I wanted the ability to embed Channel9 videos. This is Hugo platform! So, it was not a big deal. I just had to create another shortcode like the one I created for Deploy to Azure button.\nHere is the code for the shortcode.\nGitHub Gist This code snippet requires your consent to load content from GitHub, which may set cookies. Enable GitHub Gist rchaganti/056a02f289d7d5415c17d27082ee753b You can save this as channel9.html under the layouts/shortcode folder of your site content. And, then use this shortcode as shown here.\nGitHub Gist This code snippet requires your consent to load content from GitHub, which may set cookies. Enable GitHub Gist rchaganti/056a02f289d7d5415c17d27082ee753b You can use either of the methods shown above. The first one uses an named parameter while the second one avoids named parameter specification. Here is how the embed appears on the static page.\nChannel 9 Video This video requires your consent to load content from Microsoft Channel 9, which may set tracking cookies. Enable Channel 9 Video: https://channel9.msdn.com/Events/Build/2020/KEY01 It is good to be writing here again! Can\u0026rsquo;t wait to start publishing the new series of articles that I am writing now. Stay tuned!\n","date":"2020-06-14T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/embedding-channel9-videos-in-hugo-static-pages/","series":null,"title":"Embedding Channel9 Videos in Hugo Static Pages"},{"categories":["Hugo"],"content":"As I moved to this new implementation of the blog using Hugo and GitHub pages, I decided to move older article series around authoring Azure Resource Manager templates. This series was one of the most visited ones on this blog. This series provided an incremental way to learn how to author ARM templates and deploy these ARM templates. In each of these articles, I added a \u0026ldquo;Deploy to Azure Button\u0026rdquo; that you could just click and open the template directly in Azure Portal ready for deployment. The code behind this button usually looks like this.\nGitHub Gist This code snippet requires your consent to load content from GitHub, which may set cookies. Enable GitHub Gist rchaganti/056a02f289d7d5415c17d27082ee753b As I moved to static page generation using Hugo, I looked around to see if there was an easy way to generate the above HTML snippet. I came across the feature called shortcodes in Hugo. This allows you to embed shortcodes for YouTube videos, images, and so on. Several articles that I moved to this new site use shortcodes. And, it is not too difficult to write your own shortcode. So, I decided to write one for AzDeploy button.\nFirst, you need a shortcode template.\nGitHub Gist This code snippet requires your consent to load content from GitHub, which may set cookies. Enable GitHub Gist rchaganti/056a02f289d7d5415c17d27082ee753b I will spare an explanation of this and you can read the documentation of custom shortcodes. But, this is no rocket science. The way you use this shortcode in markdown is simple. You can use either of the methods shown below.\nGitHub Gist This code snippet requires your consent to load content from GitHub, which may set cookies. Enable GitHub Gist rchaganti/056a02f289d7d5415c17d27082ee753b This is it really. If you put that above code in the markdown and generate the static page, you will see the Deploy to Azure button appear like this.\nSimple and straightforward. I really fell in love with Hugo and static site generation. This has a lot of ways to customize and way faster than any other framework I used so far.\n","date":"2020-05-31T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/adding-azdeploy-button-in-static-pages-using-hugo/","series":null,"title":"Hugo shortcode for Deploy to Azure Button in a Static Page"},{"categories":["Hugo","GitHub"],"content":" It has been a while since I wrote something here. I was using a Digital Ocean droplet to run this blog using self-hosted version of Wordpress. Overall, it was good experience but there are issues with Wordpress installs. While it allows greater levels of customization, when all you need is just sharing your ideas, it becomes rather heavy. Also, the recent vulnerabilities made me think about it again. While I was having this thought process, I came across a few sites hosted on GitHub pages using Jekyll and simple static pages. I really wanted to move but moving from Wordpress to static site isn\u0026rsquo;t an easy option. There are Wordpress plugins available to export to markdown format and then host the same on GitHub pages. This is done using Hugo. While, on paper, it looks very easy job, it is actually not. The exported markdown files won\u0026rsquo;t be compatible with your selected Hugo theme. As I was wondering about this whole process, Chen came to the rescue and gave me a head start. After reviewing what he sent and doing some more research, I decided to build the site from scratch.\nSo, welcome to the new home. In the process of migrating to Hugo+GitHub Pages, I decided to drop some old content from the blog and export what was most visited through last few years. I have moved three most important series of articles I wrote here.\nAuthoring Azure Resource Manager Templates PowerShell Remoting WMI Query Language (WQL) via PowerShell I have archived most of the other content and will try to bring it in slowly when I get some free time. Believe me. It is a lot of work bringing in old content into the markdown format. The images need to be moved, links need to be corrected, and code needs to be formatted again based on what type of syntax highlighter you were using.\nAnyway, I just fell in love with this new look and the way it makes my life easy. I have been writing in markdown for a while and I feel so comfortable doing it. I hope this move helps me write more and share my thoughts.\nI am also in the process of moving PowerShell Magazine to a static site but that is much more complex than a simple blog like mine. But, yes, I certainly want to make it easy for our contributing authors.\nAre you using a static site as well? What is your experience?\n","date":"2020-05-31T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/moved-to-static-site-using-hugo-and-github-pages/","series":null,"title":"Moved Blog to a Static Site Using Hugo and GitHub Pages"},{"categories":["Azure CLI","Azure","Bangalore IT Pro","Presentations"],"content":"I had the opportunity over the weekend to speak at the Global Azure Bootcamp organized by local Microsoft user groups. This event was held at ABB office in Bangalore and I must say that they have done a splendid job. It was a very well organized event. I had only one session and it was about Azure CLI 2.0 Tips and Tricks. I mentioned earlier that I am using CLI mostly these days to work with Azure resources and in this session I shared some tips and tricks around making it easier to work with Azure resource management using Azure CLI 2.0.\nSlideShare Presentation This presentation requires your consent to load content from SlideShare (LinkedIn), which may set tracking cookies. Enable SlideShare Presentation ID: exlTOlYe4c0N2o The demo scripts I used for this session can be downloaded from\nhttps://github.com/rchaganti/Presentations/tree/master/GAB2019-BLR\n","date":"2019-04-29T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/slides-and-demo-scripts-from-azure-cli-2-0-tips-and-tricks-session-at-globalazurebootcamp/","series":null,"title":"Slides: Global Azure Bootcamp - Azure CLI 2.0"},{"categories":["Bangalore IT Pro","Bangalore UG Meet","Presentations"],"content":"At today’s Bangalore IT Pro and PowerShell Bangalore User Group (PSBUG) event, I had the opportunity to showcase the PSKoans module by Joel Sallow. This module has a set of Pester tests that you have to fix as you proceed in your efforts towards mastering PowerShell.\nI have found this method engaging and decided that I must show this to the community.\nSlideShare Presentation This presentation requires your consent to load content from SlideShare (LinkedIn), which may set tracking cookies. Enable SlideShare Presentation ID: dReJxeJsDhvCRz After this session on PSKoans, I did live coding and streamed it live on Twitch (and did not realize that it was without audio :((). I fixed a function in my PSWindowsAdminCenter module and added a new function.\n","date":"2019-02-16T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/session-slides-tests-driven-powershell-learning-using-pskoans/","series":null,"title":"Slides: BITPro UG Meet - Tests Driven PowerShell Learning using PSKoans"},{"categories":["Azure PowerShell","Azure CLI","Azure","Presentations"],"content":"I had the opportunity to speak at the inaugural edition of Experts Live India 2019 over the weekend. It was a great experience. The organizers did a great job even when there were teething facilities issues that plagued the event right from the start.\nMy first session was on Azure at the command line. In this session, I walked through both Az CLI 2.0 and Az PowerShell modules and explained the differences to help the audience choose the right tool for their job. The slides do not contain a huge deal of information as I preferred showing the audience a good demo to understand the differences.\nSlideShare Presentation This presentation requires your consent to load content from SlideShare (LinkedIn), which may set tracking cookies. Enable SlideShare Presentation ID: gKG58Jis0XZUzj The demo scripts are available at https://github.com/rchaganti/Presentations/tree/master/ExpertsLiveIN2019/AzCommandline.\n","date":"2019-01-20T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/session-slides-experts-live-india-azure-at-the-command-line/","series":null,"title":"Slides: Experts Live India – Azure at the command line"},{"categories":["PowerShell","Presentations"],"content":"I had the opportunity to speak at the inaugural edition of Experts Live India 2019 over the weekend. It was a great experience. The organizers did a great job even when there were teething facilities issues that plagued the event right from the start.\nI had planned only for one session at the event but as some international speakers could not make it to the event, I was asked to take up one more session and I chose to show the audience SHiPS module. Hierarchical data can be read as a file system in PowerShell using SHiPS open source module. In this session, I walked the audience through what are providers and what is the advantage of implementing PowerShel providers using the SHiPS module.\nSlideShare Presentation This presentation requires your consent to load content from SlideShare (LinkedIn), which may set tracking cookies. Enable SlideShare Presentation ID: LIDAL5Jq1Unbx2 Once again, the slides do not contain great deal of information since I wanted to make this more of a demo based session. The demo scripts are available at https://github.com/rchaganti/Presentations/tree/master/ExpertsLiveIN2019/PSDataAsFileSystem.\n","date":"2019-01-20T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/session-slides-experts-live-india-powershell-data-as-file-system/","series":null,"title":"Slides: Experts Live India – PowerShell Data as File System"},{"categories":["DevOps","Azure","Presentations"],"content":"I had the privilege to join the Miracle team at Visakhapatnam today for an Azure MVP bootcamp. I spoke about Microsoft Azure Cloud and DevOps. This event, I must say, it a very well organized and orchestrated. I have been to conferences and events in India and outside India and never seen this level of care of speakers and attendees anywhere.\nI started my day with a good morning walk along the beach and just loved it. I wish I get to do this often.\nFinally, in my session, there were 100+ attendees and it was very interactive. Here are the slides from that session.\nSlideShare Presentation This presentation requires your consent to load content from SlideShare (LinkedIn), which may set tracking cookies. Enable SlideShare Presentation ID: 9I392jmqcxwHLF ","date":"2016-11-12T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/slides-from-azure-mvp-bootcamp-microsoft-azure-cloud-and-devops/","series":null,"title":"Slides: Azure MVP Bootcamp – Microsoft Azure Cloud and DevOps"},{"categories":["PSConf","Presentations"],"content":"Yet another successful conference this year. All kudos to Matt, Milton, Sebastian, and Ben for making this year’s PowerShell Conference Asia such a super hit.\nI had a great time speaking about Developing Infrastructure Code for CI \u0026amp; CD and Using PowerShell DSC with AWS Cloud. I have had the opportunity to do an impromptu session with Deepak \u0026amp; Matt on the release pipeline. Overall, I am really happy with the way all my sessions went through and the response from attendees.\nSlideShare Presentation This presentation requires your consent to load content from SlideShare (LinkedIn), which may set tracking cookies. Enable SlideShare Presentation ID: 7JuYV40SYl1yyK SlideShare Presentation This presentation requires your consent to load content from SlideShare (LinkedIn), which may set tracking cookies. Enable SlideShare Presentation ID: IMgRBSo8AZLtKH Slides and demos scripts for my sessions are available in my Github repository: https://github.com/rchaganti/Presentations/tree/master/psconfAsia2016\n","date":"2016-10-24T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/powershell-conference-asia-2016-slides-and-demo-scripts/","series":null,"title":"Slides: PowerShell Conference Asia 2016"},{"categories":["PSConf","Europe","Presentations"],"content":"I returned home from PowerShell Conference EU this morning. Wow, what an amazing event was that! Seriously, right from the opening event to each and every session, this was truly the global PowerShell conference with attendees from more than twelve countries and speakers from four continents.\nI had two sessions at this conference and these were very well received. I uploaded the slides and demo scripts to a PSConfEU Github repository. Feel free to use them as needed. For those who prefer, Slideshare instead, here are the slide decks. The sessions recordings should be online in a couple of weeks. I will updated this post.\nOperations Validation for Infrastructure as Code SlideShare Presentation This presentation requires your consent to load content from SlideShare (LinkedIn), which may set tracking cookies. Enable SlideShare Presentation ID: iqw6Jz1bEGTVew Demo Scripts\nDuring this session, I introduced a new concept called infrastructure blueprints that essentially package PowerShell DSC configurations with integration and operations validation tests in written in Pester. You can check out an example of this in my Github repository. This is a work in progress and I expect to make several changes over next few weeks.\nPowerShell DSC Partial Configurations and Cross-node Synchronization SlideShare Presentation This presentation requires your consent to load content from SlideShare (LinkedIn), which may set tracking cookies. Enable SlideShare Presentation ID: RQoVoRVWyHkwB Demo Scripts\n","date":"2016-04-25T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/session-slides-and-demo-scripts-from-powershell-conference-eu-2016/","series":null,"title":"Slides: PowerShell Conference EU 2016"},{"categories":["Azure Resource Manager","Azure","ARM Templates"],"content":"When testing ARM templates, I typically use the custom template deployment option in the Azure Portal. This UI option just got better! This deployment option now supports lists the parameters, variables, and resources used in template in a nice treeview navigation.\nTo start a new deployment, you can click on New -\u0026gt; Template Deployment.\nThis opens up the Custom Deployment blade. Click on Edit Template.\nIn the Edit Template blade, copy/paste your template JSON in the editor.\nIn a few seconds, you will see the treeview built for the template. You can expand each node to see the individual elements within each category. Selecting a specific element within a node navigates to the definition for that selection within the JSON template.\nYou can click Save and then proceed to the deployment by completing the remaining steps in the custom deployment.\nThe second enhancement is the option to pull a quick start template from the Github repository and deploy it!\nIn the Load a quickstart template blade, you can select a template listed in the dropdown.\nClicking OK in this blade opens the template in the Edit Template blade where you can make changes as needed and save it.\nThese are two nice surprises this morning. Anymore waiting? 🙂\n","date":"2016-03-29T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/custom-template-deployment-improvements-in-azure-portal/","series":"ARM Templates","title":"Custom Template Deployment Improvements in Azure Portal"},{"categories":["Azure Resource Manager","Azure","ARM Templates"],"content":"This subject won\u0026rsquo;t really need a post of its own but I will do it anyway. In the previous part of this ARM series, I showed how we can use the domain join extension instead of DSC to join a VM to an existing AD domain. This resulted is huge savings in time taken to deploy the entire scenario. When I was talking to my good friend, fellow PowerShell MVP and all-things-Azure guru, Ben Gelens, he was quick to point that the delays when using DSC extension here could be due to the install of WMF 5.0. Indeed!\nWhen you use Azure DSC extension, it by default installs latest available version of WMF 5.0 (production preview at the time of this writing) which requires a reboot! Therefore, using DSC extension in ARM templates for down-level OS such as Windows Server 2012 or 2012 R2, there will be delays. But, there is a workaround. We can force DSC extension to use WMF 4.0. Here is how we do that.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 { \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Compute/virtualMachines/extensions\u0026#34;, \u0026#34;copy\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;vmDomainLoop\u0026#34;, \u0026#34;count\u0026#34;: \u0026#34;[variables(\u0026#39;selectedDeployment\u0026#39;).instancesCount]\u0026#34; }, \u0026#34;name\u0026#34;: \u0026#34;[concat(\u0026#39;vm\u0026#39;,copyIndex(1),\u0026#39;/domainjoin\u0026#39;)]\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;2015-05-01-preview\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[resourceGroup().location]\u0026#34;, \u0026#34;dependsOn\u0026#34;: [ \u0026#34;vmLoop\u0026#34; ], \u0026#34;properties\u0026#34;: { \u0026#34;publisher\u0026#34;: \u0026#34;Microsoft.Powershell\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;DSC\u0026#34;, \u0026#34;typeHandlerVersion\u0026#34;: \u0026#34;2.8\u0026#34;, \u0026#34;settings\u0026#34;: { \u0026#34;ModulesUrl\u0026#34;: \u0026#34;[concat(parameters(\u0026#39;assetLocation\u0026#39;),\u0026#39;/Configuration.zip\u0026#39;)]\u0026#34;, \u0026#34;WmfVersion\u0026#34;: \u0026#34;4.0\u0026#34;, \u0026#34;ConfigurationFunction\u0026#34;: \u0026#34;Configuration.ps1\\\\DomainJoin\u0026#34;, \u0026#34;Properties\u0026#34;: { \u0026#34;DomainName\u0026#34;: \u0026#34;[parameters(\u0026#39;adDomainName\u0026#39;)]\u0026#34;, \u0026#34;AdminCreds\u0026#34;: { \u0026#34;UserName\u0026#34;: \u0026#34;[parameters(\u0026#39;adminUsername\u0026#39;)]\u0026#34;, \u0026#34;Password\u0026#34;: \u0026#34;PrivateSettingsRef:adminPassword\u0026#34; } } }, \u0026#34;protectedSettings\u0026#34;: { \u0026#34;Items\u0026#34;: { \u0026#34;adminPassword\u0026#34;: \u0026#34;[parameters(\u0026#39;adminPassword\u0026#39;)]\u0026#34; } } } } Observe line number 19. We use the WmfVersion property and set it to 4.0. This forces DSC extension to use WMF 4.0 instead of installing latest WMF 5.0 bits. Remember to use this method, for down-level OS where WMF / PS 5.0 isn’t available by default and where you don’t need any features provided by WMF / PS 5.0, to make sure deployment times are optimized.\nNote: If you need to do this outside an ARM template, you can do so by creating the JSON needed for this and then use the Azure PowerShell cmdlets.\nYou can click the Deploy to Azure button below to deploy the sample scenario detailed in this article series.\nBy forcing DSC extension to use WMF 4.0, the complete sample scenario (with one AD VM and three VMs joining the domain) was completed in 33 mins!\n","date":"2016-02-15T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/building-azure-resource-manager-templates-forcing-wmf-4-0-when-using-dsc-extension/","series":"ARM Templates","title":"Building Azure Resource Manager Templates – Forcing WMF 4.0 when using DSC Extension"},{"categories":["Azure Resource Manager","Azure","ARM Templates"],"content":"The scenario that we used to understand and build ARM templates contained a domain controller VM along with one or more VMs that joined the domain service hosted by the DC VM.\nTo make sure the VMs join the domain, we used PowerShell DSC configuration. One of the biggest quirks, at least what I faced, with DSC extension with ARM templates is that it takes little longer to complete. For example, the complete scenario deployment took almost 48 minutes to deploy. I am not making up that number. Here is the proof.\nNow, 48 minutes may not sound that worse but imagine deploying tens of VMs that need to join the domain using the DSC configuration as we saw in the earlier example in this series.\nThis is where the new JsonADDomainExtension helps! Instead of using DSC configuration to add VMs to a AD domain, we will now use this VM extension. Within the [earlier template that deployed this scenario][1], we will remove the domainJoin resource definition and replace that with JsonADDomainExtension.\nHere is how that new resource definition looks.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 \u0026#34;resources\u0026#34;: [ { \u0026#34;apiVersion\u0026#34;: \u0026#34;2015-06-15\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Compute/virtualMachines/extensions\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;[concat(concat(\u0026#39;vm\u0026#39;,copyIndex(1)),\u0026#39;/joindomain\u0026#39;)]\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[resourceGroup().location]\u0026#34;, \u0026#34;dependsOn\u0026#34;: [ \u0026#34;[concat(\u0026#39;Microsoft.Compute/virtualMachines/\u0026#39;, concat(\u0026#39;vm\u0026#39;,copyIndex(1)))]\u0026#34; ], \u0026#34;properties\u0026#34;: { \u0026#34;publisher\u0026#34;: \u0026#34;Microsoft.Compute\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;JsonADDomainExtension\u0026#34;, \u0026#34;typeHandlerVersion\u0026#34;: \u0026#34;1.0\u0026#34;, \u0026#34;settings\u0026#34;: { \u0026#34;Name\u0026#34;: \u0026#34;[parameters(\u0026#39;adDomainName\u0026#39;)]\u0026#34;, \u0026#34;User\u0026#34;: \u0026#34;[concat(parameters(\u0026#39;adDomainName\u0026#39;), \u0026#39;\\\\\u0026#39;, parameters(\u0026#39;adminUserName\u0026#39;))]\u0026#34;, \u0026#34;Restart\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;Options\u0026#34;: \u0026#34;3\u0026#34;, \u0026#34;OUPath\u0026#34;: \u0026#34;[parameters(\u0026#39;OUPath\u0026#39;)]\u0026#34; }, \u0026#34;protectedsettings\u0026#34;: { \u0026#34;Password\u0026#34;: \u0026#34;[parameters(\u0026#39;adminPassword\u0026#39;)]\u0026#34; } } } ] In this extension settings, I am re-using a few parameters such as adDomainName, adminUserName, and adminPassword. I added a new parameter called OUPath. This specifies the organization unit for the VM computer account and it is not mandatory to specify this. Let’s take a quick look at the properties of this resource.\nProperty Name Description Name Name of the Active Directory Domain to join User Administrator account name to authenticate Restart Specifies if the VM should restart after domain join. Possible values: true or false Options Domain join options. Default option is 3.Refer to NetJoin options on MSDN. OUPath Organization Unit for the VM computer account. It is not mandatory to specify this value.Example specification: OU=testOU; DC=domain; DC=Domain; DC=com The complete template that uses this new extension is rather lengthy. So, click on the below Deploy to Azure button to deploy this template.\nAs compared to the DSC way of joining a domain, the new domain join extension method took only 31 minutes. This is it for now! Try this template and let me know what you think.\n","date":"2016-02-13T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/building-azure-resource-manager-templates-using-domain-join-extension/","series":"ARM Templates","title":"Building Azure Resource Manager Templates – Using Domain Join Extension"},{"categories":["Azure Resource Manager","Azure","ARM Templates"],"content":"In this series so far, we looked at building ARM templates by example. The focus was not really on the resource types or how to use resource definitions. Instead, our focus was on learning the basics of template language. To that extent, we have a scenario that we want to deploy and we are incrementally building the template for it.\nWhile building an ARM template for this, we looked at how to use parameters and variables. We looked at using copy object to create multiple instances of a resource type without really writing the resource definition multiple times. We went on to find out how we can define dependencies between different resource types so they are orchestrated in the right order. We looked at how we can decompose the template into purpose-specific external templates and how to link them together. While learning these concepts, we created a template that almost built the scenario we started with.\nWe will now add the remaining VMs based on the environmentType selected by the user. So, based on the VM instance count we need, we have to create network interfaces and virtual machines. We have already provisioned the inbound NAT rules needed for the RDP access to these virtual machine consoles post deployment.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 { \u0026#34;apiVersion\u0026#34;: \u0026#34;2015-06-15\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Network/loadBalancers/inboundNatRules\u0026#34;, \u0026#34;copy\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;lbRdpNatLoop\u0026#34;, \u0026#34;count\u0026#34;: \u0026#34;[variables(\u0026#39;selectedDeployment\u0026#39;).instancesCount]\u0026#34; }, \u0026#34;name\u0026#34;: \u0026#34;[concat(\u0026#39;/loadBalancer/\u0026#39;,\u0026#39;VM\u0026#39;, copyIndex(1),\u0026#39;-RDP\u0026#39;)]\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[resourceGroup().location]\u0026#34;, \u0026#34;dependsOn\u0026#34;: [ \u0026#34;Microsoft.Network/loadBalancers/loadBalancer\u0026#34; ], \u0026#34;properties\u0026#34;: { \u0026#34;frontendIPConfiguration\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;[concat(resourceId(\u0026#39;Microsoft.Network/loadBalancers\u0026#39;,\u0026#39;loadBalancer\u0026#39;),\u0026#39;/frontendIPConfigurations/LBFrontEnd\u0026#39;)]\u0026#34; }, \u0026#34;protocol\u0026#34;: \u0026#34;tcp\u0026#34;, \u0026#34;frontendPort\u0026#34;: \u0026#34;[add(3389, copyIndex(1))]\u0026#34;, \u0026#34;backendPort\u0026#34;: 3389, \u0026#34;enableFloatingIP\u0026#34;: false } } Look at line number 8. We are building the name of the inbound NAT rule using copyIndex(). This gives us the ability to differentiate between NAT rules for different VMs. We now have to create the equal number of VM network interfaces based on the instance count selected. Here is how we do it.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 { \u0026#34;apiVersion\u0026#34;: \u0026#34;2015-05-01-preview\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Network/networkInterfaces\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;[concat(\u0026#39;vm\u0026#39;, copyIndex(1), \u0026#39;-nif\u0026#39;)]\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[resourceGroup().location]\u0026#34;, \u0026#34;copy\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;vmNetworkLoop\u0026#34;, \u0026#34;count\u0026#34;: \u0026#34;[variables(\u0026#39;selectedDeployment\u0026#39;).instancesCount]\u0026#34; }, \u0026#34;dependsOn\u0026#34;: [ \u0026#34;Microsoft.Network/loadBalancers/loadBalancer\u0026#34;, \u0026#34;[concat(\u0026#39;Microsoft.Network/virtualNetworks/\u0026#39;, parameters(\u0026#39;vNetName\u0026#39;))]\u0026#34;, \u0026#34;Microsoft.Resources/deployments/updatevnetdns\u0026#34;, \u0026#34;lbRdpNatLoop\u0026#34; ], \u0026#34;properties\u0026#34;: { \u0026#34;ipConfigurations\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;[concat(\u0026#39;vm\u0026#39;, copyIndex(1), \u0026#39;-ipconfig\u0026#39;)]\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;privateIPAllocationMethod\u0026#34;: \u0026#34;dynamic\u0026#34;, \u0026#34;subnet\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;[variables(\u0026#39;vNetSubnet1ID\u0026#39;)]\u0026#34; }, \u0026#34;loadBalancerBackendAddressPools\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;[concat(resourceId(\u0026#39;Microsoft.Network/loadBalancers\u0026#39;,\u0026#39;loadBalancer\u0026#39;),\u0026#39;/backendAddressPools/LBBackEnd\u0026#39;)]\u0026#34; } ], \u0026#34;loadBalancerInboundNatRules\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;[concat(resourceId(\u0026#39;Microsoft.Network/loadBalancers\u0026#39;,\u0026#39;loadBalancer\u0026#39;),concat(\u0026#39;/inboundNatRules/VM\u0026#39;,copyIndex(1),\u0026#39;-RDP\u0026#39;))]\u0026#34; } ] } } ] } } Let us discuss this definition a bit. We need to ensure that the network interfaces are bound to the load balancer. Without this, the RDP endpoints that we created within the NAT rules will not apply to the VMs. To achieve this, we defined dependency (line number 14) on the NAT rules resource definition identified using lbRdpNatLoop. So, this is how we depend on the copy loop within the ARM template language. Line number 32 attaches the right RDP NAT rule for the VM by using the copyIndex() function again.\nNow that we have the network interfaces required (based on the instance count), we can create the virtual machines needed based on the envrionmentType (Development or Test or Production). Based on this choice, there may be more than one virtual machine that gets created. Where there is more than one VM behind a load balancer, we would need an availability set attached to those VMs. So, let us add the resource definition for the availability set.\n1 2 3 4 5 6 7 { \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Compute/availabilitySets\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;availabilitySet\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;2015-06-15\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[resourceGroup().location]\u0026#34;, \u0026#34;properties\u0026#34;: {} } We can now go ahead create the virtual machines we need.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 { \u0026#34;apiVersion\u0026#34;: \u0026#34;2015-05-01-preview\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Compute/virtualMachines\u0026#34;, \u0026#34;copy\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;vmLoop\u0026#34;, \u0026#34;count\u0026#34;: \u0026#34;[variables(\u0026#39;selectedDeployment\u0026#39;).instancesCount]\u0026#34; }, \u0026#34;name\u0026#34;: \u0026#34;[concat(\u0026#39;vm\u0026#39;,copyIndex(1))]\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[resourceGroup().location]\u0026#34;, \u0026#34;dependsOn\u0026#34;: [ \u0026#34;[concat(\u0026#39;Microsoft.Compute/virtualMachines/\u0026#39;, parameters(\u0026#39;dcVMName\u0026#39;))]\u0026#34;, \u0026#34;[concat(\u0026#39;Microsoft.Storage/storageAccounts/\u0026#39;, parameters(\u0026#39;storageAccountName\u0026#39;))]\u0026#34;, \u0026#34;vmNetworkLoop\u0026#34; ], \u0026#34;properties\u0026#34;: { \u0026#34;availabilitySet\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;[resourceId(\u0026#39;Microsoft.Compute/availabilitySets\u0026#39;,\u0026#39;availabilitySet\u0026#39;)]\u0026#34; }, \u0026#34;hardwareProfile\u0026#34;: { \u0026#34;vmSize\u0026#34;: \u0026#34;[parameters(\u0026#39;vmSize\u0026#39;)]\u0026#34; }, \u0026#34;osProfile\u0026#34;: { \u0026#34;computername\u0026#34;: \u0026#34;[concat(\u0026#39;vm\u0026#39;,copyIndex(1))]\u0026#34;, \u0026#34;adminUsername\u0026#34;: \u0026#34;[parameters(\u0026#39;adminUsername\u0026#39;)]\u0026#34;, \u0026#34;adminPassword\u0026#34;: \u0026#34;[parameters(\u0026#39;adminPassword\u0026#39;)]\u0026#34; }, \u0026#34;storageProfile\u0026#34;: { \u0026#34;imageReference\u0026#34;: { \u0026#34;publisher\u0026#34;: \u0026#34;[variables(\u0026#39;imagePublisher\u0026#39;)]\u0026#34;, \u0026#34;offer\u0026#34;: \u0026#34;[variables(\u0026#39;imageOffer\u0026#39;)]\u0026#34;, \u0026#34;sku\u0026#34;: \u0026#34;[parameters(\u0026#39;windowsOSVersion\u0026#39;)]\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34; }, \u0026#34;osDisk\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;osdisk\u0026#34;, \u0026#34;vhd\u0026#34;: { \u0026#34;uri\u0026#34;: \u0026#34;[concat(\u0026#39;http://\u0026#39;,parameters(\u0026#39;storageAccountName\u0026#39;),\u0026#39;.blob.core.windows.net/vhds/\u0026#39;, concat(\u0026#39;vm\u0026#39;,copyIndex(1),\u0026#39;-osdisk.vhd\u0026#39;))]\u0026#34; }, \u0026#34;caching\u0026#34;: \u0026#34;ReadWrite\u0026#34;, \u0026#34;createOption\u0026#34;: \u0026#34;FromImage\u0026#34; } }, \u0026#34;networkProfile\u0026#34;: { \u0026#34;networkInterfaces\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;[resourceId(\u0026#39;Microsoft.Network/networkInterfaces\u0026#39;,concat(\u0026#39;vm\u0026#39;,CopyIndex(1),\u0026#39;-nif\u0026#39;))]\u0026#34; } ] } } } We, once again, use a copy object in this definition to create as many virtual machines as we need based on the derived instance count. We also attach (line number 46) the network interfaces created in an earlier step. Once these VMs are created, we need to join them to the domain we created using DSC extension earlier. This is done using DSC again!\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 { \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Compute/virtualMachines/extensions\u0026#34;, \u0026#34;copy\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;vmDomainLoop\u0026#34;, \u0026#34;count\u0026#34;: \u0026#34;[variables(\u0026#39;selectedDeployment\u0026#39;).instancesCount]\u0026#34; }, \u0026#34;name\u0026#34;: \u0026#34;[concat(\u0026#39;vm\u0026#39;,copyIndex(1),\u0026#39;/domainjoin\u0026#39;)]\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;2015-05-01-preview\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[resourceGroup().location]\u0026#34;, \u0026#34;dependsOn\u0026#34;: [ \u0026#34;vmLoop\u0026#34; ], \u0026#34;properties\u0026#34;: { \u0026#34;publisher\u0026#34;: \u0026#34;Microsoft.Powershell\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;DSC\u0026#34;, \u0026#34;typeHandlerVersion\u0026#34;: \u0026#34;2.8\u0026#34;, \u0026#34;settings\u0026#34;: { \u0026#34;ModulesUrl\u0026#34;: \u0026#34;[concat(parameters(\u0026#39;assetLocation\u0026#39;),\u0026#39;/Configuration.zip\u0026#39;)]\u0026#34;, \u0026#34;ConfigurationFunction\u0026#34;: \u0026#34;Configuration.ps1\\\\DomainJoin\u0026#34;, \u0026#34;Properties\u0026#34;: { \u0026#34;DomainName\u0026#34;: \u0026#34;[parameters(\u0026#39;adDomainName\u0026#39;)]\u0026#34;, \u0026#34;AdminCreds\u0026#34;: { \u0026#34;UserName\u0026#34;: \u0026#34;[parameters(\u0026#39;adminUsername\u0026#39;)]\u0026#34;, \u0026#34;Password\u0026#34;: \u0026#34;PrivateSettingsRef:adminPassword\u0026#34; } } }, \u0026#34;protectedSettings\u0026#34;: { \u0026#34;Items\u0026#34;: { \u0026#34;adminPassword\u0026#34;: \u0026#34;[parameters(\u0026#39;adminPassword\u0026#39;)]\u0026#34; } } } } Line numbers 18 and 19 got the real magic that we need to join an existing domain. I packaged all DSC configurations I need for the DC creation and domain join into the configuration.zip file. We give this path to the DSC extension and tell this extension to use the DomainJoin configuration from the Configuration.ps1 file from the zip package.\nThis completes the resource definitions needed for the scenario we have. The complete ARM template is rather lengthy. So, I will not put the entire contents here but you can find it at https://github.com/rchaganti/armseries/blob/master/arm-series_Storage-VirtualNet-LinkedTemplate-Complete.json.\nYou can optionally click on the below button to deploy the complete template right away.\nOnce you deploy this, you can verify that we have the configuration done in the right away by looking at the load balancer NAT rules and how they are mapped to the VMs in the backendpool.\nThis completes our learning of the basics of ARM template language. Hope you have learned enough to go get started with writing ARM templates. I will continue this series writing about the best practices, tips and tricks around ARM template authoring. Stay tuned for more.\n","date":"2015-12-18T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/building-azure-resource-manager-templates-putting-it-all-together/","series":"ARM Templates","title":"Building Azure Resource Manager Templates – Putting it all together"},{"categories":["Azure Resource Manager","Azure","ARM Templates"],"content":"One of the ARM template authoring best practices is to decompose the JSON template, if applicable, into multiple target-specific templates. Think of this as creating re-usable code. You can leverage the re-usable parts of your code within multiple aspects of your application or the deployment.\nFor linking different external templates within the main template, we need to define the Microsoft.Resources/deployments resource instance. Before we proceed let us look at the scenario for which we are building an ARM template.\nSo far in this series, we have looked at building an ARM template that deploys the following components of this scenario:\nA storage account A virtual network A public IP address A load balancer Virtual network interfaces for the DC and other VMs based on the environment type. Finally, a VM with DNS and Directory Services running in it. By default, the Azure based IaaS deployments use the Azure DNS. If you have deployed the template that we built in the previous part of this series, you will notice that the virtual network us configured to use Azure DNS.\nSince we deployed a VM that runs our own DNS and directory services, we now want to use the custom DNS in the Azure deployment so that other VMs in the deployment can join the domain that we created using the PowerShell DSC configuration.\nFor this, we will use an external template and call it inside our main template. What we essentially need to do here is to update the existing virtual network to use the custom DNS. Here is how it looks.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 { \u0026#34;$schema\u0026#34;: \u0026#34;https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#\u0026#34;, \u0026#34;contentVersion\u0026#34;: \u0026#34;1.0.0.0\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;virtualNetworkName\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;The name of the Virtual Network to Create\u0026#34; } }, \u0026#34;virtualNetworkAddressRange\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;The address range of the new VNET in CIDR format\u0026#34; }, \u0026#34;defaultValue\u0026#34;: \u0026#34;10.0.0.0/16\u0026#34; }, \u0026#34;subnetName\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;The name of the subnet created in the new VNET\u0026#34; } }, \u0026#34;subnetRange\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;The address range of the subnet created in the new VNET\u0026#34; }, \u0026#34;defaultValue\u0026#34;: \u0026#34;10.0.0.0/24\u0026#34; }, \u0026#34;DNSServerAddress\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;array\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;The DNS address(es) of the DNS Server(s) used by the VNET\u0026#34; } } }, \u0026#34;resources\u0026#34;: [ { \u0026#34;apiVersion\u0026#34;: \u0026#34;2015-05-01-preview\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;[parameters(\u0026#39;virtualNetworkName\u0026#39;)]\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Network/virtualNetworks\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[resourceGroup().location]\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;addressSpace\u0026#34;: { \u0026#34;addressPrefixes\u0026#34;: [ \u0026#34;[parameters(\u0026#39;virtualNetworkAddressRange\u0026#39;)]\u0026#34; ] }, \u0026#34;dhcpOptions\u0026#34;: { \u0026#34;dnsServers\u0026#34;: \u0026#34;[parameters(\u0026#39;DNSServerAddress\u0026#39;)]\u0026#34; }, \u0026#34;subnets\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;[parameters(\u0026#39;subnetName\u0026#39;)]\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;addressPrefix\u0026#34;: \u0026#34;[parameters(\u0026#39;subnetRange\u0026#39;)]\u0026#34; } } ] } } ] } The above template is just another ARM JSON template. It has the same syntax. We have defined parameters that are needed for updating the virtual network to use the custom DNS that we just configured. let us save this as vnet-with-dns-server.json and store it at a location that is accessible to the ARM deployment engine. I chose to store it in a public Github repository.\nNow, we need to link this template to the main ARM template that we have been authoring. Here is how it is done in the main template.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 { \u0026#34;apiVersion\u0026#34;: \u0026#34;2015-01-01\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Resources/deployments\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;updatevnetdns\u0026#34;, \u0026#34;dependsOn\u0026#34;: [ \u0026#34;[concat(\u0026#39;Microsoft.Compute/virtualMachines/\u0026#39;, parameters(\u0026#39;dcVMName\u0026#39;),\u0026#39;/extensions/createadforest\u0026#39;)]\u0026#34; ], \u0026#34;properties\u0026#34;: { \u0026#34;mode\u0026#34;: \u0026#34;Incremental\u0026#34;, \u0026#34;templateLink\u0026#34;: { \u0026#34;uri\u0026#34;: \u0026#34;[concat(parameters(\u0026#39;assetLocation\u0026#39;),\u0026#39;/vnet-with-dns-server.json\u0026#39;)]\u0026#34;, \u0026#34;contentVersion\u0026#34;: \u0026#34;1.0.0.0\u0026#34; }, \u0026#34;parameters\u0026#34;: { \u0026#34;virtualNetworkName\u0026#34;: { \u0026#34;value\u0026#34;: \u0026#34;[parameters(\u0026#39;vNetName\u0026#39;)]\u0026#34; }, \u0026#34;virtualNetworkAddressRange\u0026#34;: { \u0026#34;value\u0026#34;: \u0026#34;[variables(\u0026#39;vNetPrefix\u0026#39;)]\u0026#34; }, \u0026#34;subnetName\u0026#34;: { \u0026#34;value\u0026#34;: \u0026#34;[variables(\u0026#39;vNetSubnet1Name\u0026#39;)]\u0026#34; }, \u0026#34;subnetRange\u0026#34;: { \u0026#34;value\u0026#34;: \u0026#34;[variables(\u0026#39;vNetSubnet1Prefix\u0026#39;)]\u0026#34; }, \u0026#34;DNSServerAddress\u0026#34;: { \u0026#34;value\u0026#34;: [ \u0026#34;[variables(\u0026#39;dnsServerPrivateIp\u0026#39;)]\u0026#34; ] } } } } Within this resource instance, we defined a dependency (line number 6) on the PowerShell DSC extension that creates the AD forest. We don’t want the linked template to execute until the DNS service is created in the DC VM. Line number 11 defines the link our external template that will configure the custom DNS settings. _vnet-with-dns-server.json _has a few parameters defined for collecting required custom DNS configuration.\nWithin the main template, we need to pass the parameter values to the external template. This is done using the parameters element within the Microsoft.Resources/deployments resource instance. For the parameter values, we use either what is already gathered from the user using the main template or what is defined in the variables element of the main template. This is how state can be shared between multiple templates. If we need to return some data from the linked template to the main template, we can do so using the outputs element within the linked template. In our scenario, we don’t need any information from the linked template and therefore we will not use the outputs element.\nAlso, notice line number 9. The mode property within the properties element is set to incremental. This is needed because we already have some of the components within the template deployed. By setting the deployment mode to incremental, we tell the deployment engine to add the new resources or update existing resource configuration within the resource group without deleting any existing resources in the group. In our example, we are updating the configuration of an existing virtual network to use the custom DNS. so, the incremental deployment mode is necessary here.\nOnce you deploy this updated template, you will see that the virtual network within the resource group gets set to use the custom DNS deployed using the PowerShell DSC extension.\nIn the next part of this series, we will look at adding more virtual machines to the deployment based on the environmentType selected within the template parameters. By the end of next part, we will completely functional ARM template that deploys our scenario end to end.\n","date":"2015-12-14T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/building-azure-resource-manager-templates-using-linked-templates/","series":"ARM Templates","title":"Building Azure Resource Manager Templates – Using Linked Templates"},{"categories":["Azure Resource Manager","Azure","ARM Templates"],"content":"We will continue learning about building ARM templates by looking at how we can define dependencies between resources. To recap, here is the scenario we are working on.\nIn the earlier parts of this series, we created the storage account, virtual network, a public IP, a load balancer, and added all inbound NAT rules required for the virtual machine RDP access. If you notice, we have components that depend on others. For example, the inbound NAT rules depend on the load balancer. Similarly, VMs depend on network interfaces which in turn depend on the virtual network. In the absence of dependencies, ARM will attempt to deploy these resources in parallel which may result in errors. So, within the resource template, we need to define these dependencies so that ARM can make decisions about the deployment sequence. There are multiple ways of doing this.\nUsing DependsOn If you have noticed in the earlier parts of this series, we have used DependsOn property within the resource element.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 { \u0026#34;apiVersion\u0026#34;: \u0026#34;2015-06-15\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Network/loadBalancers/inboundNatRules\u0026#34;, \u0026#34;copy\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;lbRdpNatLoop\u0026#34;, \u0026#34;count\u0026#34;: \u0026#34;[variables(\u0026#39;selectedDeployment\u0026#39;).instancesCount]\u0026#34; }, \u0026#34;name\u0026#34;: \u0026#34;[concat(\u0026#39;/loadbalancer/\u0026#39;,\u0026#39;VM-\u0026#39;, copyIndex(1),\u0026#39;-RDP\u0026#39;)]\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[resourceGroup().location]\u0026#34;, \u0026#34;dependsOn\u0026#34;: [ \u0026#34;Microsoft.Network/loadBalancers/loadBalancer\u0026#34; ], \u0026#34;properties\u0026#34;: { \u0026#34;frontendIPConfiguration\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;[resourceId(\u0026#39;Microsoft.Network/loadBalancers/frontendIPConfigurations\u0026#39;,\u0026#39;loadBalancer\u0026#39;,variables(\u0026#39;lbFrontend\u0026#39;))]\u0026#34; }, \u0026#34;protocol\u0026#34;: \u0026#34;tcp\u0026#34;, \u0026#34;frontendPort\u0026#34;: \u0026#34;[add(3389, copyIndex(1))]\u0026#34;, \u0026#34;backendPort\u0026#34;: 3389, \u0026#34;enableFloatingIP\u0026#34;: false } } If you look at line number 11, we added DependsOn property to define that the inboundNatRules depend on the load balancer configuration. This is straightforward and very easy to define. You can use template language functions such as ResourceId() or concat() as well within the value of DependsOn and dynamically build these dependencies. You can provide more than one value here as a comma-separated list.\n1 2 3 4 \u0026#34;dependsOn\u0026#34;: [ \u0026#34;Microsoft.Network/loadBalancers/loadBalancer\u0026#34;, \u0026#34;Microsoft.Network/virtualNetworks/vNet1\u0026#34; ] Using References The second method is to define reference to the dependent resource. Let us create a virtual network interface the AD VM in our scenario. This depends on the virtual network resource that we already created.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 { \u0026#34;apiVersion\u0026#34;: \u0026#34;2015-05-01-preview\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Network/networkInterfaces\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;dcvmnif\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[resourceGroup().location]\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;ipConfigurations\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;[concat(reference(parameters(\u0026#39;vNetName\u0026#39;)).subnets[0].name,parameters(\u0026#39;dcVmName\u0026#39;),\u0026#39;-nifconfig\u0026#39;)]\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;privateIPAllocationMethod\u0026#34;: \u0026#34;Static\u0026#34;, \u0026#34;privateIPAddress\u0026#34;: \u0026#34;[variables(\u0026#39;dnsServerPrivateIp\u0026#39;)]\u0026#34;, \u0026#34;subnet\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;[variables(\u0026#39;vNetSubnet1ID\u0026#39;)]\u0026#34; } } } ] } } Observe line number 9. We used the reference() function to get a reference to the virtual network object we created and get the subnet name from there and use it to construct the IPConfiguration name. When ARM engine finds this reference, it creates an implicit dependency on the virtual network that we referenced. There is no need here to specify DependsOn property.\nNote that we cannot use the reference() function as a part of the resource instance name. This is because the ARM engine must know the names of the resource instances before the deployment can start.\nChild or Nested Resources The 3rd way to define dependencies is to use child resources. Let us see an example before we discuss this further.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 { \u0026#34;apiVersion\u0026#34;: \u0026#34;2015-06-15\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Compute/virtualMachines\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;[Parameters(\u0026#39;dcVMName\u0026#39;)]\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[resourceGroup().location]\u0026#34;, \u0026#34;dependsOn\u0026#34;: [ \u0026#34;[resourceId(\u0026#39;Microsoft.Storage/storageAccounts\u0026#39;,parameters(\u0026#39;storageAccount\u0026#39;))]\u0026#34;, \u0026#34;[resourceId(\u0026#39;Microsoft.Network/networkInterfaces\u0026#39;,concat(parameters(\u0026#39;dcVMName\u0026#39;),\u0026#39;-nif\u0026#39;))]\u0026#34; ], \u0026#34;properties\u0026#34;: { \u0026#34;hardwareProfile\u0026#34;: { \u0026#34;vmSize\u0026#34;: \u0026#34;[parameters(\u0026#39;vmSize\u0026#39;)]\u0026#34; }, \u0026#34;osProfile\u0026#34;: { \u0026#34;computerName\u0026#34;: \u0026#34;[parameters(\u0026#39;dcVMName\u0026#39;)]\u0026#34;, \u0026#34;adminUsername\u0026#34;: \u0026#34;[parameters(\u0026#39;adminUsername\u0026#39;)]\u0026#34;, \u0026#34;adminPassword\u0026#34;: \u0026#34;[parameters(\u0026#39;adminPassword\u0026#39;)]\u0026#34; }, \u0026#34;storageProfile\u0026#34;: { \u0026#34;imageReference\u0026#34;: { \u0026#34;publisher\u0026#34;: \u0026#34;[variables(\u0026#39;imagePublisher\u0026#39;)]\u0026#34;, \u0026#34;offer\u0026#34;: \u0026#34;[variables(\u0026#39;imageOffer\u0026#39;)]\u0026#34;, \u0026#34;sku\u0026#34;: \u0026#34;[parameters(\u0026#39;windowsOSVersion\u0026#39;)]\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34; }, \u0026#34;osDisk\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;[concat(parameters(\u0026#39;dcVMName\u0026#39;), \u0026#39;-osdisk.vhd\u0026#39;)]\u0026#34;, \u0026#34;vhd\u0026#34;: { \u0026#34;uri\u0026#34;: \u0026#34;[concat(\u0026#39;http://\u0026#39;,parameters(\u0026#39;storageAccount\u0026#39;),\u0026#39;.blob.core.windows.net/vhds/\u0026#39;, parameters(\u0026#39;dcVMName\u0026#39;), \u0026#39;-osdisk.vhd\u0026#39;)]\u0026#34; }, \u0026#34;caching\u0026#34;: \u0026#34;ReadWrite\u0026#34;, \u0026#34;createOption\u0026#34;: \u0026#34;FromImage\u0026#34; }, \u0026#34;dataDisks\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;[concat(parameters(\u0026#39;dcVMName\u0026#39;), \u0026#39;-data-disk1.vhd\u0026#39;)]\u0026#34;, \u0026#34;vhd\u0026#34;: { \u0026#34;uri\u0026#34;: \u0026#34;[concat(\u0026#39;http://\u0026#39;,parameters(\u0026#39;storageAccount\u0026#39;),\u0026#39;.blob.core.windows.net/vhds/\u0026#39;, parameters(\u0026#39;dcVMName\u0026#39;), \u0026#39;-data-disk1.vhd\u0026#39;)]\u0026#34; }, \u0026#34;caching\u0026#34;: \u0026#34;None\u0026#34;, \u0026#34;createOption\u0026#34;: \u0026#34;Empty\u0026#34;, \u0026#34;diskSizeGB\u0026#34;: \u0026#34;1000\u0026#34;, \u0026#34;lun\u0026#34;: 0 } ] }, \u0026#34;networkProfile\u0026#34;: { \u0026#34;networkInterfaces\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;[resourceId(\u0026#39;Microsoft.Network/networkInterfaces\u0026#39;,concat(parameters(\u0026#39;dcVMName\u0026#39;),\u0026#39;-nif\u0026#39;))]\u0026#34; } ] }, \u0026#34;diagnosticsProfile\u0026#34;: { \u0026#34;bootDiagnostics\u0026#34;: { \u0026#34;enabled\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;storageUri\u0026#34;: \u0026#34;[concat(\u0026#39;http://\u0026#39;,parameters(\u0026#39;storageAccount\u0026#39;),\u0026#39;.blob.core.windows.net\u0026#39;)]\u0026#34; } } }, \u0026#34;resources\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;extensions\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;createadforest\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;2015-05-01-preview\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[resourceGroup().location]\u0026#34;, \u0026#34;dependsOn\u0026#34;: [ \u0026#34;[concat(\u0026#39;Microsoft.Compute/virtualMachines/\u0026#39;,parameters(\u0026#39;dcVMName\u0026#39;))]\u0026#34; ], \u0026#34;properties\u0026#34;: { \u0026#34;publisher\u0026#34;: \u0026#34;Microsoft.Powershell\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;DSC\u0026#34;, \u0026#34;typeHandlerVersion\u0026#34;: \u0026#34;2.8\u0026#34;, \u0026#34;settings\u0026#34;: { \u0026#34;ModulesUrl\u0026#34;: \u0026#34;[concat(parameters(\u0026#39;assetLocation\u0026#39;),\u0026#39;/Configuration.zip\u0026#39;)]\u0026#34;, \u0026#34;ConfigurationFunction\u0026#34;: \u0026#34;CreateADPDC.ps1\\\\CreateADPDC\u0026#34;, \u0026#34;Properties\u0026#34;: { \u0026#34;DomainName\u0026#34;: \u0026#34;[parameters(\u0026#39;adDomainName\u0026#39;)]\u0026#34;, \u0026#34;AdminCreds\u0026#34;: { \u0026#34;UserName\u0026#34;: \u0026#34;[parameters(\u0026#39;adminUsername\u0026#39;)]\u0026#34;, \u0026#34;Password\u0026#34;: \u0026#34;PrivateSettingsRef:AdminPassword\u0026#34; } } }, \u0026#34;protectedSettings\u0026#34;: { \u0026#34;Items\u0026#34;: { \u0026#34;AdminPassword\u0026#34;: \u0026#34;[parameters(\u0026#39;adminPassword\u0026#39;)]\u0026#34; } } } } ] } This example is quite long. It is the virtual machine resource instance that we need for the domain controller. Apart from all the virtual machine properties such as storage profile and OS profile, we have something interesting at line number 61. We have another resources element within the VM resource definition. As with the top-level resources element, this is also a JSON array and contains the resource definition for the VM DSC extension. It is obvious that VM DSC extension without a VM is meaningless. By defining this as a child or nested resource inside the VM resource definition, we are creating an implicit dependency between the VM and the VM DSC extension. When ARM engine looks at this in the template, it schedules VM creation before the DSC extension. BTW, this VM DSC extension has the DSC configuration required to build the domain controller in this deployment.\nAlways make sure you that you create enough flexibility within the template for someone else to take this and deploy the template in their own environment.To this extent, I have used a parameter called assetLocation (line 75) in the DSC extension properties. You can call this whatever you want. This parameter specifies where all the template assets such as DSC configuration scripts and any additional template files are stored.\nSo far in this part, we have seen three different ways of defining dependencies between resources. If you check the template that we built so far for this scenario, you will observe that I have added a few more variables and parameters needed for the DC VM and its configuration.\nGo ahead and deploy this.\nIn the next part, we will look at the nested template deployments.\n","date":"2015-12-08T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/building-azure-resource-manager-templates-defining-resource-dependencies/","series":"ARM Templates","title":"Building Azure Resource Manager Templates – Defining Resource Dependencies"},{"categories":["Azure Resource Manager","Azure","ARM Templates"],"content":"If you are following this series, by now you know how to use parameters and variables in ARM template language. We used that knowledge to create a template that creates relevant things to like storage account and virtual network. Let us revisit the scenario.\nIn this part, we will extend the template to add publicIP, load balancer, and RDP endpoints for the virtual machines. Before we do that, let us review what we need. To achieve what we want in this, we will use copy object in the template.\nWhat is Copy Object? In our template, we already have the storage account and virtual network configuration defined. What we now need is a load balancer with a public IP so that we can access this deployment from the Internet. Also, we need the inbound NAT rules for enabling RDP access to the VMs in this deployment. But before we create the RDP rules for each VM, we need to know how many VM instances we are deploying. As shown in the scenario diagram, we need to be able to deploy this template within development, test, or production environment. Depending on the selected environment, we will have one AD DC VM along with 1 (development) or 2 (test) or 3 (production) VMs that join the AD domain. Therefore, we need a way to capture the environment type as an input parameter. Also, based on the environment type selected by the user, we need to decide on the number of VM instances required for the deployment and then create only relevant number of inbound NAT rules within the load balancer.\nThere are multiple ways to achieve this.\nAdd multiple resource definitions; one for each resource instance we need. This does not give the flexibility to dynamically add or remove instances based on the VM count we need. This method is not efficient. Use some kind of iterator, like other programming languages, within a resource definition and tell ARM how many resource instances we need. This can be achieved using copy object technique in ARM templates. Before we go to the copy object discussion, let us first add more parameters and variables that are needed for this template. These additional parameters will also help us with defining the iteration count within the copy object.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 \u0026#34;parameters\u0026#34;: { \u0026#34;storageAccount\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Unique name for the storage account.\u0026#34; } }, \u0026#34;storageAccountType\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;Standard_LRS\u0026#34;, \u0026#34;allowedValues\u0026#34;: [ \u0026#34;Standard_LRS\u0026#34;, \u0026#34;Standard_GRS\u0026#34;, \u0026#34;Standard_ZRS\u0026#34; ], \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Account type based on the storage redundancy requirements.\u0026#34; } }, \u0026#34;vNetName\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Name for the virtual network.\u0026#34; } }, \u0026#34;DnsName\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Unique public DNS prefix for the deployment. The fqdn will look something like \u0026#39;\u0026amp;lt;dnsname\u0026amp;gt;.westus.cloudapp.azure.com\u0026#39;.\u0026#34; } }, \u0026#34;environmentType\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;allowedValues\u0026#34;: [ \u0026#34;Development\u0026#34;, \u0026#34;Test\u0026#34;, \u0026#34;Production\u0026#34; ], \u0026#34;defaultValue\u0026#34;: \u0026#34;Development\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Type of environment where this deployment should occur. This has an impact on the number of VMs to be depoloyed.\u0026#34; } } } Within the new parameters, we have the dnsName parameter that will be for external access. The environmentType parameter is used to capture whether the user is deploying this template for dev, test, or production. However, based on this selection, we need a way to find out the number of VMs we need in the domain. That is, we need know the instance count for each environment. Remember our discussion on free-form vs known configurations? We don’t want to leave such choices to the end user. So, the place where this decision can be made will be in the variables element.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 \u0026#34;variables\u0026#34;: { \u0026#34;vNetPrefix\u0026#34;: \u0026#34;10.0.0.0/16\u0026#34;, \u0026#34;vNetSubnet1Name\u0026#34;: \u0026#34;[concat(resourceGroup().Location, \u0026#39;-\u0026#39;, parameters(\u0026#39;vNetName\u0026#39;), \u0026#39;-\u0026#39;, \u0026#39;Subnet\u0026#39;)]\u0026#34;, \u0026#34;vNetSubnet1Prefix\u0026#34;: \u0026#34;10.0.0.0/24\u0026#34;, \u0026#34;vNetSubnet1ID\u0026#34;: \u0026#34;[concat(resourceId(\u0026#39;Microsoft.Network/virtualNetworks\u0026#39;,parameters(\u0026#39;vNetName\u0026#39;)),\u0026#39;/subnets/\u0026#39;,variables(\u0026#39;vNetSubnet1Name\u0026#39;))]\u0026#34;, \u0026#34;dnsServerPrivateIp\u0026#34;: \u0026#34;10.0.0.8\u0026#34;, \u0026#34;deploymentSize\u0026#34;: { \u0026#34;Development\u0026#34;: { \u0026#34;instancesCount\u0026#34;: 1 }, \u0026#34;Test\u0026#34;: { \u0026#34;instancesCount\u0026#34;: 2 }, \u0026#34;Production\u0026#34;: { \u0026#34;instancesCount\u0026#34;: 3 } }, \u0026#34;selectedDeployment\u0026#34;: \u0026#34;[variables(\u0026#39;deploymentSize\u0026#39;)[parameters(\u0026#39;environmentType\u0026#39;)]]\u0026#34; } Within the variables element, we defined new variables for making it easier to select the environment type. The deploymentSize variable defines a JSON object. Within this, we are associating instancesCount to every deployment type.\n1 2 3 4 5 6 7 8 9 10 11 \u0026#34;deploymentSize\u0026#34;: { \u0026#34;Development\u0026#34;: { \u0026#34;instancesCount\u0026#34;: 1 }, \u0026#34;Test\u0026#34;: { \u0026#34;instancesCount\u0026#34;: 2 }, \u0026#34;Production\u0026#34;: { \u0026#34;instancesCount\u0026#34;: 3 } } We use the selectedDeployment variable to retrieve the value of deploymentSize variable. This is done by associating the value of environmentType parameter and looking up for that in the deploymentSize variable.\n1 \u0026#34;selectedDeployment\u0026#34;: \u0026#34;[variables(\u0026#39;deploymentSize\u0026#39;)[parameters(\u0026#39;environmentType\u0026#39;)]]\u0026#34; This works because deploymentSize is a JSON object similar to a dictionary. We can index into it using one of the key names. So, in this case, the key names will match the value passed to the environmentType parameter and indexing into that will give us the right object within _deploymentSize. _Once we have the selectedDeployment variable populated, we can access the instancesCount value by dot-referencing the property name. For example,\n1 [variables(\u0026#39;selectedDeployment\u0026#39;).instancesCount]\u0026#34; This should not be alien to us. We use variables() function and pass the selectedDeployment variable name to it. The resulting object has the instancesCount property which can then be retrieved using dot-referencing. Now that we figured this out, let us go back to the discussion around multiple instance creation within a template.\nWorking with Copy Object As stated earlier, the reason we need instancesCount value is because we need to tell ARM how many times it has to iterate for creating the resource for which we need multiple instances. This is done within the copy object syntax.\n1 2 3 4 \u0026#34;copy\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;copy loop name\u0026#34;, \u0026#34;count\u0026#34;: \u0026#34;integer value\u0026#34; } The name property within the copy element defines a name for the iterator which can later be used for defining dependencies. The _count _property defines the number of times the iterator has to run. In our case, we will set the value of count property to the instancesCount value we retrieve from the selectedDeployment.\nWe place this inside the resource definition. In our scenario, we will use the copy object within Microsoft.Network/loadBalancers/inboundNatRules resource type. Let us first see the new resources that we need to enable public IP and load balancer configuration. We will then review the resource definition to understand how we use the copy object.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 \u0026#34;resources\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;[parameters(\u0026#39;storageAccountName\u0026#39;)]\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Storage/storageAccounts\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[ResourceGroup().Location]\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;2015-05-01-preview\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;accountType\u0026#34;: \u0026#34;[parameters(\u0026#39;storageAccountType\u0026#39;)]\u0026#34; } }, { \u0026#34;name\u0026#34;: \u0026#34;[parameters(\u0026#39;vNetName\u0026#39;)]\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Network/virtualNetworks\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[ResourceGroup().Location]\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;2015-05-01-preview\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;addressSpace\u0026#34;: { \u0026#34;addressPrefixes\u0026#34;: [ \u0026#34;[variables(\u0026#39;vNetPrefix\u0026#39;)]\u0026#34; ] }, \u0026#34;subnets\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;[variables(\u0026#39;vNetSubnet1Name\u0026#39;)]\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;addressPrefix\u0026#34;: \u0026#34;[variables(\u0026#39;vNetSubnet1Prefix\u0026#39;)]\u0026#34; } } ] } }, { \u0026#34;apiVersion\u0026#34;: \u0026#34;2015-05-01-preview\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Network/publicIPAddresses\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;publicIp\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[resourceGroup().location]\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;publicIPAllocationMethod\u0026#34;: \u0026#34;Dynamic\u0026#34;, \u0026#34;dnsSettings\u0026#34;: { \u0026#34;domainNameLabel\u0026#34;: \u0026#34;[parameters(\u0026#39;DnsName\u0026#39;)]\u0026#34; } } }, { \u0026#34;apiVersion\u0026#34;: \u0026#34;2015-05-01-preview\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;loadBalancer\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Network/loadBalancers\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[resourceGroup().location]\u0026#34;, \u0026#34;dependsOn\u0026#34;: [ \u0026#34;Microsoft.Network/publicIPAddresses/publicIp\u0026#34; ], \u0026#34;properties\u0026#34;: { \u0026#34;frontendIPConfigurations\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;[variables(\u0026#39;lbFrontend\u0026#39;)]\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;publicIPAddress\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;[resourceId(\u0026#39;Microsoft.Network/publicIPAddresses\u0026#39;,\u0026#39;publicIp\u0026#39;)]\u0026#34; } } } ], \u0026#34;backendAddressPools\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;[variables(\u0026#39;lbBackendAddrPool\u0026#39;)]\u0026#34; } ] } }, { \u0026#34;apiVersion\u0026#34;: \u0026#34;2015-06-15\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Network/loadBalancers/inboundNatRules\u0026#34;, \u0026#34;copy\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;lbRdpNatLoop\u0026#34;, \u0026#34;count\u0026#34;: \u0026#34;[variables(\u0026#39;selectedDeployment\u0026#39;).instancesCount]\u0026#34; }, \u0026#34;name\u0026#34;: \u0026#34;[concat(\u0026#39;/loadbalancer/\u0026#39;,\u0026#39;VM-\u0026#39;, copyIndex(1),\u0026#39;-RDP\u0026#39;)]\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[resourceGroup().location]\u0026#34;, \u0026#34;dependsOn\u0026#34;: [ \u0026#34;[concat(\u0026#39;Microsoft.Network/loadBalancers/loadBalancer\u0026#39;)]\u0026#34; ], \u0026#34;properties\u0026#34;: { \u0026#34;frontendIPConfiguration\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;[resourceId(\u0026#39;Microsoft.Network/loadBalancers/frontendIPConfigurations\u0026#39;,\u0026#39;loadBalancer\u0026#39;,variables(\u0026#39;lbFrontend\u0026#39;))]\u0026#34; }, \u0026#34;protocol\u0026#34;: \u0026#34;tcp\u0026#34;, \u0026#34;frontendPort\u0026#34;: \u0026#34;[add(3389, copyIndex(1))]\u0026#34;, \u0026#34;backendPort\u0026#34;: 3389, \u0026#34;enableFloatingIP\u0026#34;: false } } ] In this updated resource collection, we have added three resource types. A public IP address to assign it to the load balancer, a load balancer that will act as a container for the VM endpoints, and finally, RDP endpoints based on the number of VM instances we need.\nObserve the highlighted lines in the resource collection. The copy object that we used within the inboundNATRules resource type tells ARM how many instances we need to create based on the VM count we need.\n1 2 3 4 \u0026#34;copy\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;lbRdpNatLoop\u0026#34;, \u0026#34;count\u0026#34;: \u0026#34;[variables(\u0026#39;selectedDeployment\u0026#39;).instancesCount]\u0026#34; }, Within an ARM template, for each resource type and its instances, the name property should carry a unique value. Without this, you will see an error during deployment that multiple instances cannot use the same name. So, if you look at line number 77, we are dynamically constructing the value for name.\n1 \u0026#34;name\u0026#34;: \u0026#34;[concat(\u0026#39;/loadbalancer/\u0026#39;,\u0026#39;VM-\u0026#39;, copyIndex(1),\u0026#39;-RDP\u0026#39;)]\u0026#34; As you see here, we are concatenating ‘/loadbalancer/’ with ‘VM-‘ and then using the copyIndex() function and finally add ‘-RDP’. The copyIndex() function gives us the iteration number. So, if we are creating three VM endpoints within this resource definition, we get iteration values 0, 1, and 2. The index always starts from 0. However, if we want to offset it to different value than zero, we can specify the value inside the copyIndex() function. In my example, I am using 1 so that the values the template receives will be 1, 2, and 3. Since we are using the iteration number within the name value, we always get an unique identifier.\nAnother function that we are using within this template update is the add() function. This function, as its name suggests, adds integers. I am using this function to dynamically derive the frontend port number for the VM RDP endpoint.\n1 \u0026#34;frontendPort\u0026#34;: \u0026#34;[add(3389, copyIndex(1))]\u0026#34; We are doing this since all VM RDP endpoints should be accessible through the load balancer. By using add function, we are adding the iteration value to the RDP port (3389) to derive a unique value.\nNow that we have completed our discussion on the copy object and its usage, let us move on to see the ARM template update that creates storage account, virtual network, public IP, load balancer, and finally the VM RDP endpoints. We are not creating an RDP endpoint for the DC VM.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 { \u0026#34;$schema\u0026#34;: \u0026#34;https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#\u0026#34;, \u0026#34;contentVersion\u0026#34;: \u0026#34;1.0.0.0\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;storageAccountName\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Unique name for the storage account.\u0026#34; } }, \u0026#34;storageAccountType\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;Standard_LRS\u0026#34;, \u0026#34;allowedValues\u0026#34;: [ \u0026#34;Standard_LRS\u0026#34;, \u0026#34;Standard_GRS\u0026#34;, \u0026#34;Standard_ZRS\u0026#34;, \u0026#34;Premium_LRS\u0026#34;, \u0026#34;Standard_RAGRS\u0026#34; ], \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Account type based on the storage redundancy requirements.\u0026#34; } }, \u0026#34;vNetName\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Name for the virtual network.\u0026#34; } }, \u0026#34;DnsName\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Unique public DNS prefix for the deployment. The fqdn will look something like \u0026#39;\u0026amp;lt;dnsname\u0026amp;gt;.westus.cloudapp.azure.com\u0026#39;.\u0026#34; } }, \u0026#34;environmentType\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;allowedValues\u0026#34;: [ \u0026#34;Development\u0026#34;, \u0026#34;Test\u0026#34;, \u0026#34;Production\u0026#34; ], \u0026#34;defaultValue\u0026#34;: \u0026#34;Development\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Type of environment where this deployment should occur. This has an impact on the number of VMs to be depoloyed.\u0026#34; } } }, \u0026#34;variables\u0026#34;: { \u0026#34;vNetPrefix\u0026#34;: \u0026#34;10.0.0.0/16\u0026#34;, \u0026#34;vNetSubnet1Name\u0026#34;: \u0026#34;[concat(resourceGroup().Location, \u0026#39;-\u0026#39;, parameters(\u0026#39;vNetName\u0026#39;), \u0026#39;-\u0026#39;, \u0026#39;Subnet\u0026#39;)]\u0026#34;, \u0026#34;vNetSubnet1Prefix\u0026#34;: \u0026#34;10.0.0.0/24\u0026#34;, \u0026#34;vNetSubnet1ID\u0026#34;: \u0026#34;[concat(resourceId(\u0026#39;Microsoft.Network/virtualNetworks\u0026#39;,parameters(\u0026#39;vNetName\u0026#39;)),\u0026#39;/subnets/\u0026#39;,variables(\u0026#39;vNetSubnet1Name\u0026#39;))]\u0026#34;, \u0026#34;dnsServerPrivateIp\u0026#34;: \u0026#34;10.0.0.8\u0026#34;, \u0026#34;lbFrontend\u0026#34;: \u0026#34;lbFrontEnd\u0026#34;, \u0026#34;lbBackendAddrPool\u0026#34;: \u0026#34;lbBackEndPool\u0026#34;, \u0026#34;deploymentSize\u0026#34;: { \u0026#34;Development\u0026#34;: { \u0026#34;instancesCount\u0026#34;: 1 }, \u0026#34;Test\u0026#34;: { \u0026#34;instancesCount\u0026#34;: 2 }, \u0026#34;Production\u0026#34;: { \u0026#34;instancesCount\u0026#34;: 3 } }, \u0026#34;selectedDeployment\u0026#34;: \u0026#34;[variables(\u0026#39;deploymentSize\u0026#39;)[parameters(\u0026#39;environmentType\u0026#39;)]]\u0026#34; }, \u0026#34;resources\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;[parameters(\u0026#39;storageAccountName\u0026#39;)]\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Storage/storageAccounts\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[ResourceGroup().Location]\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;2015-05-01-preview\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;accountType\u0026#34;: \u0026#34;[parameters(\u0026#39;storageAccountType\u0026#39;)]\u0026#34; } }, { \u0026#34;name\u0026#34;: \u0026#34;[parameters(\u0026#39;vNetName\u0026#39;)]\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Network/virtualNetworks\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[ResourceGroup().Location]\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;2015-05-01-preview\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;addressSpace\u0026#34;: { \u0026#34;addressPrefixes\u0026#34;: [ \u0026#34;[variables(\u0026#39;vNetPrefix\u0026#39;)]\u0026#34; ] }, \u0026#34;subnets\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;[variables(\u0026#39;vNetSubnet1Name\u0026#39;)]\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;addressPrefix\u0026#34;: \u0026#34;[variables(\u0026#39;vNetSubnet1Prefix\u0026#39;)]\u0026#34; } } ] } }, { \u0026#34;apiVersion\u0026#34;: \u0026#34;2015-05-01-preview\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Network/publicIPAddresses\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;publicIp\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[resourceGroup().location]\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;publicIPAllocationMethod\u0026#34;: \u0026#34;Dynamic\u0026#34;, \u0026#34;dnsSettings\u0026#34;: { \u0026#34;domainNameLabel\u0026#34;: \u0026#34;[parameters(\u0026#39;DnsName\u0026#39;)]\u0026#34; } } }, { \u0026#34;apiVersion\u0026#34;: \u0026#34;2015-05-01-preview\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;loadBalancer\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Network/loadBalancers\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[resourceGroup().location]\u0026#34;, \u0026#34;dependsOn\u0026#34;: [ \u0026#34;Microsoft.Network/publicIPAddresses/publicIp\u0026#34; ], \u0026#34;properties\u0026#34;: { \u0026#34;frontendIPConfigurations\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;[variables(\u0026#39;lbFrontend\u0026#39;)]\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;publicIPAddress\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;[resourceId(\u0026#39;Microsoft.Network/publicIPAddresses\u0026#39;,\u0026#39;publicIp\u0026#39;)]\u0026#34; } } } ], \u0026#34;backendAddressPools\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;[variables(\u0026#39;lbBackendAddrPool\u0026#39;)]\u0026#34; } ] } }, { \u0026#34;apiVersion\u0026#34;: \u0026#34;2015-06-15\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Network/loadBalancers/inboundNatRules\u0026#34;, \u0026#34;copy\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;lbRdpNatLoop\u0026#34;, \u0026#34;count\u0026#34;: \u0026#34;[variables(\u0026#39;selectedDeployment\u0026#39;).instancesCount]\u0026#34; }, \u0026#34;name\u0026#34;: \u0026#34;[concat(\u0026#39;/loadbalancer/\u0026#39;,\u0026#39;VM-\u0026#39;, copyIndex(1),\u0026#39;-RDP\u0026#39;)]\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[resourceGroup().location]\u0026#34;, \u0026#34;dependsOn\u0026#34;: [ \u0026#34;[concat(\u0026#39;Microsoft.Network/loadBalancers/loadBalancer\u0026#39;)]\u0026#34; ], \u0026#34;properties\u0026#34;: { \u0026#34;frontendIPConfiguration\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;[resourceId(\u0026#39;Microsoft.Network/loadBalancers/frontendIPConfigurations\u0026#39;,\u0026#39;loadBalancer\u0026#39;,variables(\u0026#39;lbFrontend\u0026#39;))]\u0026#34; }, \u0026#34;protocol\u0026#34;: \u0026#34;tcp\u0026#34;, \u0026#34;frontendPort\u0026#34;: \u0026#34;[add(3389, copyIndex(1))]\u0026#34;, \u0026#34;backendPort\u0026#34;: 3389, \u0026#34;enableFloatingIP\u0026#34;: false } } ] } You can go ahead and deploy this update by clicking on the button below. This brings up the portal and prompts for the parameter values we need for the deployment. Within the parameters, you will see the environmentType dropdown with three possible values. I have selected Production as the input here and I expect to see three RDP endpoints created at the end of template deployment. Let us see if this worked.\nAs you see here, the endpoints with respective frontend port numbers are created. We will have to associate these endpoints to VM network interfaces and we will take that up in a later part. Stay tuned.\n","date":"2015-11-17T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/building-azure-resource-manager-templates-using-copy-object/","series":"ARM Templates","title":"Building Azure Resource Manager Templates – Using Copy Object"},{"categories":["Azure Resource Manager","Azure","ARM Templates"],"content":"If you have been following this series on ARM templates, in the last part, we started with a sample scenario that we are using to build an ARM template. Here it is again.\nIn the last part, we completed creation of storage account required for backing the OS and data disks for virtual machines in the deployment. The next step in our scenario is to create the virtual network for the VM connectivity. A virtual network and any subnets required must exist before the creation of virtual machines. So, in today’s article, we will see how we can use variables in the ARM template language while incrementally building an ARM template for our scenario and add virtual network. Towards the end, we will review some best practices guidance with regards to using parameters and variables in developing ARM templates.\nWorking with Variables Variables in ARM template language can be used to simplify the template. For example, in our template, we will be using the subnet name at multiple places. There is no point repeating the same string everywhere for this. This may also lead to human errors such as unintentional misspelling of the subnet name. This type of errors will result in failure during the deployment. Instead, we can assign subnet name as a value to a variable and refer that variable within the template as needed. Also, variables in ARM template language will let us expand or construct other variable values. We will see examples of this in the template.\nHere is the generic syntax for variables in ARM template language.\n1 2 3 4 5 6 \u0026#34;variables\u0026#34;: { \u0026#34;\u0026lt;variable-name\u0026gt;\u0026#34;: \u0026#34;\u0026lt;variable-value\u0026gt;\u0026#34;, \u0026#34;\u0026lt;variable-name\u0026gt;\u0026#34;: { \u0026lt;variable-complex-type-value\u0026gt; } } In general, variables is a JSON object which contains pairs of variable names and values. The variable value can be a complex type such as another JSON object. We will see these as examples as we add the variables needed to complete virtual network configuration.\n1 2 3 4 5 6 7 \u0026#34;variables\u0026#34;: { \u0026#34;vNetPrefix\u0026#34;: \u0026#34;10.0.0.0/16\u0026#34;, \u0026#34;vNetSubnet1Name\u0026#34;: \u0026#34;[concat(resourceGroup().Location, \u0026#39;-\u0026#39;, parameters(\u0026#39;vNetName\u0026#39;), \u0026#39;-\u0026#39;, \u0026#39;Subnet\u0026#39;)]\u0026#34;, \u0026#34;vNetSubnet1Prefix\u0026#34;: \u0026#34;10.0.0.0/24\u0026#34;, \u0026#34;vNetSubnet1ID\u0026#34;: \u0026#34;[concat(resourceId(\u0026#39;Microsoft.Network/virtualNetworks\u0026#39;,parameters(\u0026#39;vNetName\u0026#39;)),\u0026#39;/subnets/\u0026#39;,variables(\u0026#39;vNetSubnet1Name\u0026#39;))]\u0026#34;, \u0026#34;dnsServerPrivateIp\u0026#34;: \u0026#34;10.0.0.8\u0026#34; } For the virtual network required for our scenario, we defined a variable called vNetPrefix and set a value “10.0.0.0/16”. This is the overall virtual network address scope. Within this vNet, we intend to create one subnet with address prefix “10.0.0.0/24” and this is represented using the variable _vNetSubnet1Prefix. _I have added another variable named dnsServerPrivateIp to reserve an IP address for the AD domain controller VM. This is because we don’t want a DHCP assigned IP for the AD DC.\nYou will also see that we are using another function within this variables element called concat().\n1 \u0026#34;vNetSubnet1Name\u0026#34;: \u0026#34;[concat(resourceGroup().Location, \u0026#39;-\u0026#39;, parameters(\u0026#39;vNetName\u0026#39;), \u0026#39;-\u0026#39;, \u0026#39;Subnet\u0026#39;)]\u0026#34; This function can take ‘n’ strings and concatenate them together. In the example above, we are retrieving the resource group location and then concatenate that with ‘-‘ and the value of the vNetName parameter. This variable value is then used to derive the value of the variable vNetSubnet1ID.\n1 \u0026#34;vNetSubnet1ID\u0026#34;: \u0026#34;[concat(resourceId(\u0026#39;Microsoft.Network/virtualNetworks\u0026#39;,parameters(\u0026#39;vNetName\u0026#39;)),\u0026#39;/subnets/\u0026#39;,variables(\u0026#39;vNetSubnet1Name\u0026#39;))]\u0026#34; As I’d mentioned earlier, variables values can be generated dynamically based on other values. If you observe the above code snippet, we are constructing the vNetSubnet1ID by retrieving the resource ID for the virtual network that we are creating. The resourceId() function returns the unique identifier of a resource and it won’t be available until the vNet is created. This shows us that the values of variables can be derived from other related configuration settings. We have used the concat() function again here to concatenate the resource Id value of the vNet with ‘/subnets/’ and the value of the vNetSubnet1Name variable that we derived in last step. Similar to parameters() function, the variables() function gives us the value of the variable name specified.\n1 \u0026#34;[variables(\u0026#39;vNetSubnet1Name\u0026#39;)]\u0026#34; The variables() function can be used with the variables element as well as the resource definitions and outputs element. Like any other expression in template language, anytime you want to use this function, it must be enclosed in square brackets ([]).\nWe will see more about creating variables with complex value types in a later part but fundamentals that you learned here are applicable even to complex types. Now that we have the variables needed for the vNet creation, let us go ahead and add the virtual network resource definition to our template.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 { \u0026#34;$schema\u0026#34;: \u0026#34;https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#\u0026#34;, \u0026#34;contentVersion\u0026#34;: \u0026#34;1.0.0.0\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;storageAccountName\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;MyARMDemo\u0026#34;, \u0026#34;minLength\u0026#34;: 3, \u0026#34;maxLength\u0026#34;: 24, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Unique name for the storage account.\u0026#34; } }, \u0026#34;storageAccountType\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;Standard_LRS\u0026#34;, \u0026#34;allowedValues\u0026#34;: [ \u0026#34;Standard_LRS\u0026#34;, \u0026#34;Standard_GRS\u0026#34;, \u0026#34;Standard_ZRS\u0026#34;, \u0026#34;Premium_LRS\u0026#34;, \u0026#34;Standard_RAGRS\u0026#34; ], \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Account type based on the storage redundancy requirements.\u0026#34; } }, \u0026#34;vNetName\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;myARMDemo\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Unique name of the virtual network for this deployment\u0026#34; } } }, \u0026#34;variables\u0026#34;: { \u0026#34;vNetPrefix\u0026#34;: \u0026#34;10.0.0.0/16\u0026#34;, \u0026#34;vNetSubnet1Name\u0026#34;: \u0026#34;[concat(resourceGroup().Location, \u0026#39;-\u0026#39;, parameters(\u0026#39;vNetName\u0026#39;), \u0026#39;-\u0026#39;, \u0026#39;Subnet\u0026#39;)]\u0026#34;, \u0026#34;vNetSubnet1Prefix\u0026#34;: \u0026#34;10.0.0.0/24\u0026#34;, \u0026#34;vNetSubnet1ID\u0026#34;: \u0026#34;[concat(resourceId(\u0026#39;Microsoft.Network/virtualNetworks\u0026#39;,parameters(\u0026#39;vNetName\u0026#39;)),\u0026#39;/subnets/\u0026#39;,variables(\u0026#39;vNetSubnet1Name\u0026#39;))]\u0026#34;, \u0026#34;dnsServerPrivateIp\u0026#34;: \u0026#34;10.0.0.8\u0026#34; }, \u0026#34;resources\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;[parameters(\u0026#39;storageAccountName\u0026#39;)]\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Storage/storageAccounts\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[ResourceGroup().Location]\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;2015-05-01-preview\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;accountType\u0026#34;: \u0026#34;[parameters(\u0026#39;storageAccountType\u0026#39;)]\u0026#34; } }, { \u0026#34;name\u0026#34;: \u0026#34;[parameters(\u0026#39;vNetName\u0026#39;)]\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Network/virtualNetworks\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[ResourceGroup().Location]\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;2015-05-01-preview\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;addressSpace\u0026#34;: { \u0026#34;addressPrefixes\u0026#34;: [ \u0026#34;[variables(\u0026#39;vNetPrefix\u0026#39;)]\u0026#34; ] }, \u0026#34;subnets\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;[variables(\u0026#39;vNetSubnet1Name\u0026#39;)]\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;addressPrefix\u0026#34;: \u0026#34;[variables(\u0026#39;vNetSubnet1Prefix\u0026#39;)]\u0026#34; } } ] } } ] } I added a vNetName parameter to the parameters element so that user deploying this template can provide that as input. We can go ahead and test if this is template is valid or not.\n1 Test-AzureRmResourceGroupDeployment -ResourceGroupName myARMDemo -TemplateFile .\\arm-series_Storage-VirtualNet.json -storageAccountName myARMDemo -storageAccountType Standard_LRS -vNetName myARMDemo -Verbose Once the validation is complete, we can go ahead and deploy the same to ensure it is deploy-able. To test this immediately, click on the deploy to Azure button below.\nFree-form vs Known Configuration We have so far looked at parameters, variables, and used them in the template that we are building. Parameters give us the flexibility to gather input from user deploying the template. We can completely parameterize the template and let the user provide every value needed for the deployment as an input parameter. This is called free-form configuration. However, using free-form configuration is not always scalable. The number of parameters that a user need to input might be overwhelming. Also, you don’t want your end user deploying this template to mess-up with the vNet and subnet address prefixes. This can have negative effects on the overall IaaS deployment. You might always want to prefix the subnet name with the resource group name to ensure there is uniformity across the resource names that you use. Some of this cannot be controlled unless you use known configurations. Known configurations help standardize resource configurations. Use variables in the template for resource settings that require no user input or you need control over. By using variables, you can simply change the value of the variable if at all you need to modify the template instead of trying to find where all the value is used.\nWe will review more such best practices guidance and see using known configurations throughout this series. This is it for today. Stay tuned for more!\n","date":"2015-11-15T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/building-azure-resource-manager-templates-using-variables/","series":"ARM Templates","title":"Building Azure Resource Manager Templates – Using Variables"},{"categories":["Azure Resource Manager","Azure","ARM Templates"],"content":"In the earlier parts of this series, we briefly looked at different methods of deploying ARM templates and then basics of ARM template language. Starting today’s article, we will see our leaning in action. To get going with our leaning, we will start building the ARM template for the following scenario.\nWe will incrementally build this scenario by learning different aspects of ARM template language. We won’t be covering all resource types in Azure but we will cover every aspect of the ARM template language and functions and expressions used in the template language. We will also review best practices in building these ARM templates as we proceed forward in this series.\nSo, what is there in the scenario that we are building?\nA resource group to host different resource types in our scenario. A storage account for hosting the VM OS and data disks. A virtual network for VM network connectivity. A public IP to connect to this deployment from external world. A load balancer that has the RDP and PowerShell remoting endpoints for the non-DC virtual machines in this scenario. Network interfaces for the AD DC and other VMs in our scenario. An Active Directory Domain Controller VM with DNS services running in it. Up to three VMs (depending on the environment type) that join the AD DS. This scenario is not a complex deployment by any means but can be useful in building an ARM template that uses most or all of the artifacts in the template language. We will do this over a series of articles. But, at the end of each part, we will have a deploy-able template that we will test.\nLet us see how we can go about this.\nCreate a Resource Group Like I mentioned earlier, we need a resource group for deploying the components in our scenario using ARM templates. So, let us create one. You can do this using various means but I will stick to Azure PowerShell for this.\nNote: I will be using Azure PowerShell 1.0 cmdlets. You can get these using either PackageManagement cmdlets or Web PI or just get the MSI package from Github.\n1 2 Login-AzureRmAccount New-AzureRmResourceGroup -Name MyARMDemo -Location \u0026#39;West US\u0026#39; Note: An Azure resource group can contain resources from different regions.\nAdding Parameters Now that we have a resource group created, let us start creating our ARM template for the scenario. First thing we need is a storage account for our resources. Storage accounts in Azure have a couple of properties — name, type, and location — that we can configure. We should ideally collect the values for name and type from the user deploying this template. This is where we will use parameters in the ARM template language. Let us first start by defining that.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 { \u0026#34;$schema\u0026#34;: \u0026#34;https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#\u0026#34;, \u0026#34;contentVersion\u0026#34;: \u0026#34;1.0.0.0\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;storageAccountName\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;myARMDemo\u0026#34;, \u0026#34;minLength\u0026#34;: 3, \u0026#34;maxLength\u0026#34;: 24, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Unique name for the storage account.\u0026#34; } }, \u0026#34;storageAccountType\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;Standard_LRS\u0026#34;, \u0026#34;allowedValues\u0026#34;: [ \u0026#34;Standard_LRS\u0026#34;, \u0026#34;Standard_GRS\u0026#34;, \u0026#34;Standard_ZRS\u0026#34;, \u0026#34;Premium_LRS\u0026#34;, \u0026#34;Standard_RAGRS\u0026#34; ], \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Account type based on the storage redundancy requirements.\u0026#34; } } } } In the above JSON template, we added parameters sub-element to the first two mandatory elements which are $schema and contentVersion. Within the parameters element, we have two parameters needed for creating storage account. Here is the generic syntax for adding parameters.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 \u0026#34;parameters\u0026#34;: { \u0026#34;\u0026lt;parameterName\u0026gt;\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;\u0026lt;type-of-parameter-value\u0026gt;\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;\u0026lt;optional-default-value-of-parameter\u0026gt;\u0026#34;, \u0026#34;allowedValues\u0026#34;: [ \u0026#34;\u0026lt;optional-array-of-allowed-values\u0026gt;\u0026#34; ], \u0026#34;minValue\u0026#34;: \u0026lt;optional-minimum-value-for-int-parameters\u0026gt;, \u0026#34;maxValue\u0026#34;: \u0026lt;optional-maximum-value-for-int-parameters\u0026gt;, \u0026#34;minLength\u0026#34;: \u0026lt;optional-minimum-length-for-string-secureString-array-parameters\u0026gt;, \u0026#34;maxLength\u0026#34;: \u0026lt;optional-maximum-length-for-string-secureString-array-parameters\u0026gt;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;\u0026lt;optional-description-of-the parameter\u0026gt;\u0026#34; } } } We will be using more or less every property within the parameters element within the scenario we are developing. So, don’t worry if you don’t immediately see all of them used right away. The storageAccountName parameter is of string type and we set the defaultValue to ‘myARMDemo’. So, when a user deploys this template it is not mandatory to supply the parameter value. The value of type property should be a valid JSON type and one of the below mentioned types.\nstring or secureString int bool object or secureObject array A storage account name must be at least 3 characters long and 24 characters at the most. The minLength and maxLength properties within the storageAccountName parameter define these constraints. The metadata property of the parameter is used to provide the help text to the end user deploying this template. You can put whatever string that best describes the parameter.\nFor the storageAccountType property, we have set allowedValues property to ensure we restrict what the end user can provide as a value. This is required since there only a set of valid values for the storage account type in Azure. Note that allowedValues property is a JSON array and is represented using square brackets. Since we are supplying allowedValues, when a user deploys this template, they get to select one of the values either using dropdown or tab-complete when using PowerShell. For eample, in Azure PowerShell, this is achieved using the New-AzureRmResourceGroupDeployment cmdlet.\nWe would eventually add many more parameters by the time we create the final template. The basics that we discussed here should apply to all parameters that we add in future.\nAdding Resources To be able to deploy the template, we must have at least one resource type added to the resources collection element. Let us go ahead and add the storage resource type to create a storage account.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 { \u0026#34;$schema\u0026#34;: \u0026#34;https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#\u0026#34;, \u0026#34;contentVersion\u0026#34;: \u0026#34;1.0.0.0\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;storageAccountName\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;MyARMDemo\u0026#34;, \u0026#34;minLength\u0026#34;: 3, \u0026#34;maxLength\u0026#34;: 24, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Unique name for the storage account.\u0026#34; } }, \u0026#34;storageAccountType\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;Standard_LRS\u0026#34;, \u0026#34;allowedValues\u0026#34;: [ \u0026#34;Standard_LRS\u0026#34;, \u0026#34;Standard_GRS\u0026#34;, \u0026#34;Standard_ZRS\u0026#34;, \u0026#34;Premium_LRS\u0026#34;, \u0026#34;Standard_RAGRS\u0026#34; ], \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Account type based on the storage redundancy requirements.\u0026#34; } } }, \u0026#34;resources\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;[parameters(\u0026#39;storageAccountName\u0026#39;)]\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Storage/storageAccounts\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[ResourceGroup().Location]\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;2015-05-01-preview\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;accountType\u0026#34;: \u0026#34;[parameters(\u0026#39;storageAccountType\u0026#39;)]\u0026#34; } } ] } As we learned earlier, resources element in the ARM template is a JSON array. It is a collection of comma-separated JSON objects where each JSON object represents an instance of a resource type. For a resource type, we need to configure the name property to identify the resource instance within the group. Note the way we are retrieving the value of storageAccountName parameter and assigning it to the name property of storage account resource. We use the parameters() function. We supply the name of the parameter to this function to retrieve its value. Also note that the functions and expressions within the JSON syntax must be enclosed within square brackets.\n1 \u0026#34;name\u0026#34;: \u0026#34;[parameters(\u0026#39;storageAccountName\u0026#39;)]\u0026#34; The type property is used to define the resource provider (Microsoft.Storage) and resource type (storageAccounts) within the provider. The value of location property identifies the region where we want to deploy the resource type. In the case of our ARM template, we are using the resourceGroup() function to define the location for our storage account. This function returns three attributes — id, name, and location — for the resource group within which the resource type is getting deployed. We use dot-reference to retrieve the value of location attribute.\n1 \u0026#34;location\u0026#34;: \u0026#34;[ResourceGroup().Location]\u0026#34; The properties element within the resource definition is a JSON object that identifies all the resource specific properties. We need to configure the storage account type setting and this is collected using the storageAccountType parameter. Similar to how we assigned value to the name property, we use the parameters() function to get the value of storageAccountType.\n1 2 3 { \u0026#34;accountType\u0026#34;: \u0026#34;[parameters(\u0026#39;storageAccountType\u0026#39;)]\u0026#34; } With this, we have a ARM template that can be used to deploy a storage account. But, how do we know whether this is valid or not. We can either use the REST API for this or PowerShell cmdlets.\nTest-AzureRmResourceGroupDeployment -ResourceGroupName myARMDemo -TemplateFile .\\2-0_Template_Storage.json -storageAccountName myARMDemo -storageAccountType Standard_LRS -Verbose\rOnce we validate the template, we can use one of the methods we discussed in the beginning of this series to deploy this template. One of the methods we discussed is the click to deploy button. So, go ahead and click on the below button and proceed to the Azure Portal (you need a valid Azure subscription).\nYou should see Azure Portal starting a new deployment and prompting you for parameter values.\nif you mouse over the small ‘i’ icon next to the parameter name, you will see the contents of the metadata property for the parameter displayed in a tooltip.\nOnce you fill in all required values in the wizard and click Create, the storage account gets deployed within the resource group.\nThis is it. We have completed the first part of the ARM template development. In the next part, we will add some more resource types to the template and also learn some more artifacts in the ARM template language.\n","date":"2015-11-10T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/building-azure-resource-manager-templates-using-parameters/","series":"ARM Templates","title":"Building Azure Resource Manager Templates – Using Parameters"},{"categories":["Azure","Azure Resource Manager","Presentations"],"content":" I had a great time at Future Unleashed 2015 talking about Authoring Azure Resource Manager Templates. This was a one-day technical event where there were 11 tracks covering different aspects from Cloud to Data to Analytics.\nThe breakout session setup was a little weird. There were no walls for the breakout session room (or whatever you call it). It was just a big expo hall with 11 tracks spread across like exhibitor booths. All the attendees had to wear a headset to hear the speaker.\nThis is where my session had happened. It took a few mins for me get adjusted to this but overall, it was a very good experience. My session was very interactive and the attendees showed lot of interest in learning how to build ARM templates. I used a step-by-step approach in my demo to show the ARM template authoring and discussed how to use some artifacts in the ARM template language. A 45-minutes session is certainly not enough to show everything but I certainly hope that my attendees got a good overview of the authoring process.\nSlideShare Presentation This presentation requires your consent to load content from SlideShare (LinkedIn), which may set tracking cookies. Enable SlideShare Presentation ID: tpPQ7wyyll44Q9 At some point, this month, we should have the session recording available too. I will post the demo scripts along with the session recording. Meanwhile, if you are looking for some in-depth content around learning ARM template authoring, take a look at my ARM template series.\n","date":"2015-11-09T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/slides-from-future-unleashed-2015-authoring-azure-resource-manager-templates/","series":null,"title":"Slides: Future Unleashed – Authoring ARM Templates"},{"categories":["Azure Resource Manager","Azure","ARM Templates"],"content":"As a part of this series, we will go through the language semantics of writing ARM templates. We will explore each and every aspect of the ARM template language and go through multiple examples to understand the concepts better. We won’t focus much on the resource types and resource definitions until we complete the discussion around the language semantics and artifacts.\nLet us get started.\nTools for the job JSON templates for ARM can be edited in notepad too. But, there are better tools that can help us with this.\nVisual Studio Visual Studio, along with Azure SDK, provides an easy way to get started with the template creation. You get started by creating a Resource Group project and just keep adding resources to the project and VS automatically generates the JSON for you. You can then customize it the way you want. More than the auto-generation of JSON, what I like is the intellisense capability that shows me the properties I can use within a template for a resource type.\nFor more information on how to get started with using Visual Studio to create ARM templates, read this Azure team blog post.\nVisual Studio Code Visual Studio Code is my favorite. It loads up much faster than the full-blown Visual Studio and it is very easy to use. However, at the time of this writing, there is no ARM template authoring support like what VS has. However, it has some minimal auto-completion support based on what the editor understands from the schema.\nSublime Text Sublime Text has a Azure Resource Manager package that gives a bunch of snippets for quick ARM template authoring. For example, after you activate ARM package, you can type arm:t and press tab to add a skeleton of the ARM template.\nSublime does provide some level of property auto-completion but it is still buggy. But, if your first choice is sublime, you have ARM template authoring support to some extent.\nThere may be many other editors or IDEs with JSON support. Visual Studio is the first-class citizen as far as ARM template support is concerned and I hope VS code gets a similar level of template authoring support soon. Now that we know what we need to start template authoring, let us start with the language discussion.\nIntroducing ARM template language I have mentioned this several times already but will say it one more time before I start. ARM templates are based on JSON syntax. So, to be able to appreciate and understand the way we write ARM templates, you need to know how JSON synatx works. I am not going to write about the JSON syntax here but I have an article that I published on PowerShell Magazine and that should give you a good overview.\nHere is the basic structure of an ARM template.\n1 2 3 4 5 6 7 8 { \u0026#34;$schema\u0026#34;: \u0026#34;http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#\u0026#34;, \u0026#34;contentVersion\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;parameters\u0026#34;: { }, \u0026#34;variables\u0026#34;: { }, \u0026#34;resources\u0026#34;: [ ], \u0026#34;outputs\u0026#34;: { } } $schema This element is mandatory and the value must be the location of JSON schema file for the ARM template language. The JSON schema is what is used to find the right set of properties for resources etc. You can find the latest schema URL listed at https://github.com/Azure/azure-resource-manager-schemas/. When you start a new resource group project using Visual Studio, this property gets filled for you.\nContentVersion This is another mandatory element and you can use this to specify the version of the template you are authoring. As a part of the ARM authoring process, I suggest that you use the source control and build process to auto-update this element value instead of manually entering a version number.\nParameters Think of parameters like parameters in any other programming language. In any programming language, parameters are used to provide flexibility within the function/method and not hard code any values that are really user-specific. ARM parameters are no exception. By defining parameters, you get the capability to collect user input for resource properties prior to starting the deployment.Parameters are what we use to draw a line between the free-form and known configurations. We will discuss this in-depth later. There are several properties within the parameters element. These properties can be used to set the allowed values or default value for a parameter or even set the minimum and maximum values for a parameter. Instead of just describing what those are, we will see real examples with those properties in the upcoming parts of this series.\nVariables Variables, once again like any other programming language, helps us simplify the template language. For example, you can use variables element to store values for different resource properties and re-use them wherever applicable instead of writing the same value again and again. Also, within the ARM template language, variables can be used to construct values at runtime. You can use the user provided input in terms of parameters and construct the value for a specific resource property. Variables in ARM template can have standard type values as well as complex types. We will see examples of all this as we move forward in this series.\nResources Resource is a mandatory element and defines a collection of resources that you plan to deploy as a part of the template deployment. For each resource type that you define within this collection, there are certain standard sub-elements such as name, type, apiVersion, location, properties and so on. The properties element is used to describe the resource specific properties and the valid values for this are dependent on the resource type. Within this series, we will see a different set of resource types and use them within the templates that we build. However, we will not get into the details of each and every template. You can always refer to JSON schema and online documentation to find more about the properties for any given resource type.\nOutputs The outputs element is used to return data and objects from a deployment. For example, once a web application deployment is complete, you may want to return the URL at which the application is hosted. Also, outputs element is what we use to share state and data between multiple templates in a nested template deployment.\nFunctions and Expressions Apart from the six elements I described above, the ARM template syntax provides support for different functions and expressions to simplify the deployment. For example, when you want to construct resource property values at runtime, you may need support for concatenating strings. As a part of our exploration here, we will look different functions and expressions that we can use within the ARM templates.\nThis brings us to the end of today’s article. In the remaining parts going forward, we will take an example scenario and start building a template for that. In the process of doing that, we will see how different elements and artifacts such as functions and expressions within the template can be used. Before we start with that, I recommend that you get yourself familiarized with a tool that can be used to build these templates. Stay tuned for more.\n","date":"2015-11-06T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/building-azure-resource-manager-templates-the-basics/","series":"ARM Templates","title":"Building Azure Resource Manager Templates – The Basics"},{"categories":["Azure Resource Manager","Azure","ARM Templates"],"content":"Azure Resource Manager isn’t a new thing! It was announced during Build 2014. ARM is certainly the preferred way, with more and more services getting ARM support, to deploy Azure services. Microsoft also announced that the upcoming Azure Stack release will feature Azure Resource Manager and the template deployment. This means that learning how to use ARM and write you own templates is an essential skill not just for public cloud but also for the private and hybrid cloud administrators.\nThere is lot of Microsoft and community content out there that describes why ARM is important and how you can use the templates to deploy multi-tier and multi-service cloud deployments and manage all of them as a single entity. I am not going to repeat all that here. My focus, here, will be a thorough coverage of template language and design patterns. Through this series, you will get a complete and in-depth coverage of what you need to know to build world-class ARM templates. ARM uses JSON formatted templates to perform these cloud deployments. You can see a bunch of these sample templates in the Azure quick start templates Github repo that you can use as a starting point to create your own template or understand how to use the template language and best practices. This series of posts is about sharing my learning – the way I learned authoring ARM templates.\nBefore you we go ahead start our discussion of ARM template language, let us first quickly review different methods of deploying these templates. This is not a detailed walk-through of these different methods since the focus of this article series is not on the deployment but building templates. After you write a custom ARM template, you can choose between any of these methods to deploy those templates.\nNew Template Deployment (Preview Portal) The Azure Preview portal has an option to supply the ARM template contents (JSON) and then use a wizard-based method to start the deployment.\nTo start this, as shown in the picture, click on the + icon and scroll down to click on Template Deployment. The wizard that starts after this pretty much self-explanatory. As with any GUI and click-here methods, this is a manual method. I do not prefer this method.\nAzure PowerShell Azure PowerShell (the following example uses 1.0 preview) is my preferred option. You can just supply a template file and template parameter file to the New-AzureRmResourceGroupDeployment cmdlet.\n1 2 3 Login-AzureRmAccount New-AzureRmResourceGroup -Name ExampleResourceGroup -Location \u0026#34;West US\u0026#34; New-AzureRmResourceGroupDeployment -Name ExampleDeployment -ResourceGroupName ExampleResourceGroup -TemplateFile C:\\ARMTemplates\\SingleVMDeploy.json -TemplateParameterFile C:\\ARMTemplates\\SingleVMDeploy-parameter.json This method, like any other PowerShell-based method, can be completely automated and can be made a part of your CI/CD pipeline.\nIf you prefer, you can do the same with Azure CLI as well.\nARM REST API The template deployment API in the ARM REST API provides a way to deploy custom ARM templates programatically using any language that supports REST interfaces. For example, you can use the following API request to create a template deployment. You can supply the JSON template along with all parameter information in the same request or as a link to a blob in your storage account.\nhttps://management.azure.com/subscriptions/{subscription-id}/resourcegroups/{resource-group-name}/providers/microsoft.resources/deployments/{deployment-name}?api-version={api-version}\nThis method, like the Azure PowerShell or CLI method, can be easily integrated into an existing CI/CD pipeline.\nClick to Deploy Method Finally, there is a click to deploy method that Azure Preview portal gets integrated into. For example, most or all of the sample templates within the Azure quick start templates gallery contain the Deploy to Azure button.\nClicking on the above button will take you to the new template deployment option in the preview portal where you can add the parameter values and start the deployment. Similar to this, you can embed these buttons on any webpage and redirect the user to Azure Preview portal to start the deployment.\nVisual Studio For developers creating ARM custom templates, it might be an easier option to both author and test deployment using Visual Studio. VS provides necessary tooling to do this. VS provides the options to add new resources through a wizard and then later use the VS editor to customize and deploy those templates. Note that VS does not have all the resource types available in the wizard interface. For some of these missing resource types, you might still have to refer to the schema and resource definitions.\nVS is my preferred (and only option at the moment) to start ARM template development. So, in this series, going forward, we will use VS for most of our scenarios but do the deployments using Azure PowerShell.\nThis brings us to the end of today’s article. We have not yet seen any basics of authoring Azure Resource Manager templates. We will start discussing the basics and go in-depth starting the next part in this series. Before that, familiarize yourself with at least one of the custom template deployment methods.\n","date":"2015-11-02T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/building-azure-resource-manager-templates-an-introduction/","series":"ARM Templates","title":"Building Azure Resource Manager Templates – An Introduction"},{"categories":["Containers"],"content":"In the earlier article, I explained the hurdles in the traditional and virtualized ways of implementing workloads. Also, we looked how the DevOps challenges are demanding changes the continuous delivery and integration processes. Towards the end, I’d mentioned that Containers are an answer.\nIn today’s article, we will see what are containers and some history behind containerization. Let us get started. In the subsequent parts, we will dig into each building block used within container technologies and understand how to use them.\nNote: If you are not a Linux user or never even installed Linux, lot of content that follows will sound alien. If you need to get a quick training on Linux, go ahead to edX and take the Linux Foundation course.\nContainers are light weight isolated operating system environments running on a host. Unlike virtual machines, containers\ndon’t need additional hardware capabilities such as Intel-VT and so on. don’t need emulated BIOS or completely virtualized hardware Instead, containers are processes running on a host operating system that provide allocation and assignment of resources such as CPU, memory, block IO, and network bandwidth and do not (or cannot) interfere with rest of the system’s resources or process.\nTake a look at the following figure.\nThe figure above contrasts the way containers implemented with how VMs are built. Remember that containers are complementary to virtual machines. They are not a complete replacement for virtual machines. You may end up creating virtual machines for complete isolation and then run containers inside those VMs.\n[pullquote]The above representation is an over-simplified architecture of containers. In fact, there are no technologies shown in the picture that really are the building blocks for creating containers. We will dig into that in a bit.[/pullquote]\nEach container is a process within the host operating system. The applications running inside the container assume exclusive access to the operating system. In reality, those applications run inside an isolated process environment. This is similar to how chroot works. Historically speaking, the concept of containerization itself is not new. Solaris Zones and Containers did something similar for a long time. Windows OS had something called Dynamic Hardware Partitioning (not on x86 systems and something that never got popular for the same reason) in Windows Server 2008. The container technology that we are going to discuss is based on Linux OS and has some history associated with it. Let us review the history a bit and then dive into the technology that is used to implement these containers.\nHistory Google has been using the container technology to run their own services for a long time. In fact, they create more than 2 billion containers a week. While Google started using containerization in the year 2004, they formally donated the cgroups project to Linux kernel in the year 2007.\n[pullquote]cgroups (abbreviated from control groups) is a Linux kernel feature that limits, accounts for and isolates the resource usage (CPU, memory, disk I/O, network, etc.) of a collection of processes.[/pullquote] The release of cgroups lead to creation of Linux Containers (LXC). Apart from cgroups, LXC uses kernel namespaces, apparmor and SE Linux profiles, Chroots, CGroups, and other kernel capabilities such as Seccomp policies. LXC falls somewhere in between the VMs and a chroot.\nLMCTFY Let-me-contain-that-for-you (LMCTFY) is an open source version of Google’s container stack. LMCTFY works at the same level as LXC and therefore it is not recommended to use it along side other container technologies such as LXC. Most or all of Google’s online services run inside the containers deployed using their own container stack and they have been doing this since 2004! So, it goes without saying that they have mastered this art containerization and certainly have a solid technology in LMCTFY.\nCreating resource isolation is only one part of the story. We still need a better way to orchestrate the whole container creation and then manage the container life cycle.\nDocker Orchestration of the container and managing life cycle is important and this is where tools such as Docker come into picture.\nYouTube Video This video requires your consent to load content from YouTube, which may set tracking cookies. Enable YouTube Video ID: ZzQfxoMFH0U Docker, until release 0.9, used LXC as the default execution environment. Starting with Docker 0.9, LXC was replaced with libcontainer – a library developed to access Linux kernel’s container APIs directly. With libcontainer, Docker can now perform container management without relying on other execution environments such as LXC. Although libcontainer is the new default execution environment, Docker still supports LXC. In fact, the execution driver API was added to ensure that other execution environments can be used when necessary provided they have a execution driver.\nDocker revolutionized and brought the concepts of Linux containers to masses. In the subsequent parts of this series, we will dive into LXC, Docker, and the kernel components that help the overall containerization.\nKubernetes While tools such as Docker provide the container life cycle management, the orchestration of multi-tier containers and container clusters is still not easy. This is where Google once again took the lead and developed Kubernetes.\nKubernetes is an open source system for managing containerized applications across multiple hosts, providing basic mechanisms for deployment, maintenance, and scaling of applications.\nThere are many other tools in the Docker eco-system. We will look at them when we start discussing Docker in-depth.\nRocket Docker has a rival in Rocket. Rocket is a new container runtime, designed for composability, security, and speed.\nFrom their FAQ, Rocket is an alternative to the Docker runtime, designed for server environments with the most rigorous security and production requirements. Rocket is oriented around the App Container specification, a new set of simple and open specifications for a portable container format.\nThis is an interesting development and at the moment with Linux-only focus unlike Docker.\nOthers There are other container solutions such as OpenVZ and Warden. I will not go into the details as I have not worked on any of these. I will try and pull some information on these if I get to experiment with them a bit.\nFuture We looked at the history of container technology itself and the container solutions such as Docker. Docker has certainly brought Linux containers to lime-light. The future holds a lot. Many of the cloud providers have vouched for Docker integration and we can already see that in action with Microsoft Azure, Amazon Web Services, and Google Cloud Platform. Microsoft announced that the Docker engine will soon come to the Windows Server (both on-premises and in the cloud).\nWith the exciting times ahead, it is time for both IT professionals and developers to start looking at the containers. Stay tuned for more in-depth information.\nFurther Reading Cgroups [Kernel Documentation]\nNamespaces [Kernel Documentation]\nResource management: Linux kernel Namespaces and cgroups by Rami Rosen\nJailing your apps using Linux namespaces\nGetting started with LXC\n","date":"2015-02-06T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/containers-what-are-they-and-their-history-part-2/","series":"Containers","title":"Containers – What are they? And, their history! – Part 2"},{"categories":["Cloud","Azure","Presentations"],"content":"I had the opportunity to do the keynote at the Microsoft Reboot Camp Bangalore. This is a community driven event with support from Microsoft. The theme for the event was cloud computing and Microsoft Cloud offerings.\nIn my keynote, I talked about what cloud computing means to the IT professionals and how they should get ready for the future.\nHere are the slides from my keynote.\nSlideShare Presentation This presentation requires your consent to load content from SlideShare (LinkedIn), which may set tracking cookies. Enable SlideShare Presentation ID: w4XP9qWqOyvpZP ","date":"2015-01-31T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/slides-from-reboot-camp-bangalore-keynote-cloud-computing-for-it-professionals/","series":null,"title":"Slides: Reboot Camp Bangalore - Cloud Computing for IT Professionals"},{"categories":["Containers"],"content":"Containers? No, not the shipping containers but the much-hyped container technology in the computer industry. I am sure even if you have not used any of the existing container technologies, you must have at least read or heard about them.\nI am not the first one to write about containers and this is certainly not an in-depth overview of container technologies. I will eventually get there and show you how you can deploy different container technologies. This series of articles is about my own thoughts and a documentation for my own reference. In the process, I wish to help my readers as well.\nLet us get started. In this article, I will first describe what is the need for containers and how they are different from the virtual machines that we have been using all this while.\nInfrastructure Challenges Before we start looking at containers, let us dig into some of the challenges in the current infrastructure models. This will form the foundation for moving to containers and you will be able to appreciate the value containers bring.\nTraditional Infrastructure Not so long ago, most of the data centers had only physical servers. When planning for application workloads, IT managers typically looked at physical servers that are configured to support the resources required for the workload. We had to plan ahead to support future growth of application usage and resource requirements. This was usually achieved by over-sizing these physical servers. And, mostly, these physical servers were used to run only a single application workload. There is technically nothing stopping us from running multiple applications on the same physical server but that was simply not a best practice as we cannot completely limit the resources a process can create and accomplish full isolation between applications. These factors led to different set of problems and challenges.\nOver-sizing physical servers led to lot of unused resources. For example, a web server may not need all the processing power and memory available in a physical server. But, sizing a web server just for its average utilization will lead to performance issues during peak conditions. Another challenge was related scaling out compute power. Going back to the web server example, when you need to scale-out your web farm to support additional load, lets says during holiday season, you had to plan for it in advance and keep the servers ready. This is mainly because provisioning new physical servers takes time.\nThere were also other concerns over space and power utilization when using physical servers in a data center. Then came virtualization and addressed most of the challenges associated with physical server deployments.\nVirtualized Infrastructure Virtualized infrastructures, popularized by VMware, solved most or all of the problems that IT organizations encountered with physical servers. Virtualization enables agile workload deployment, workload mobility, efficient resource utilization using resource pools, and on-demand scale out using automation.\nVirtualization is great. It enables full resource utilization through consolidation and brings in all other benefits I mentioned above. I can take two or more workloads with low resource requirements and consolidate them on a big fat physical server. For the guest OS inside these virtual machines, there is usually no difference. This is because the hypervisor is emulating the hardware that is required for the guest OS to run. But, this ease of use and full functionality comes at a cost. Virtual machines are fat. They have an overhead in terms of the hypervisor itself and the emulation that is needed for the guest OS to be fully functional. So, even if all you want to run is a simple application that is isolated from all other workloads on the same machine, you still have to pay for the cost of running a full OS inside a virtual machine. The cost here is not just dollars but the overhead in running that operating system too. Remember, every OS needs to patched and maintained. All this adds to the complexity of managing the infrastructure and therefore cost too. This needs to be mitigated in a better way. We will see the answer in a while.\nDevOps Challenges The overhead associated with running virtual machines alone isn’t the problem. The VMs are used for running workloads and these workloads or applications need continuous updates as developers add more features or fix existing bugs. With the Agile development and release small and release often requirements, the coordination between developers and operations becomes a bigger challenge. The solution to this challenge is what we know as DevOps.\nDevOps is not a single tool or software but rather a set of practices combined with different tools.\nMost of the issues reported in either QA or pre-production phases of release are due to the differences in environment. That is, the developer environment may be tweaked to make a feature work but if the same changes are not propagated down to the QA, pre-production, and production environments, there is a higher chance of failure when the software updates are deployed. So, there is a need for consistent continuous delivery and integration. This automation may not be a challenge when there are just a handful of systems in each environment. However, that is not the case. Many deployments are large (think how Facebook or Google are running their software on millions of servers) and a simple automation isn\u0026rsquo;t good enough. Also, the continuous delivery and integration challenges only multiply with the need for supporting disparate platforms and system architectures. When working at a large scale, we need to ensure that the application can be scaled out with a lot less effort than creating a VM and then configuring it for the application. The time to value is super critical in this scenario. Some of these challenges can be solved by configuration management platforms such as Puppet, Chef, or even Windows PowerShell Desired State Configuration. We will talk more about this in a later post.\nSo, how do we overcome these challenges posed by the infrastructure, continuous delivery and integration? Containers have an answer.\nIn this article, we looked at the need and the primary drivers for the technology behind containerization. In the next part, I will introduce containers and history of container technologies.\n","date":"2015-01-20T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/containers-what-are-they-why-do-you-need-one-part-1/","series":"Containers","title":"Containers – What are they? Why do you need one? – Part 1"},{"categories":["Cloud","Azure","Presentations"],"content":"Microsoft Azure offers several services each categorized into one of the four major categories – Compute, Data, App, and Network Services. This session takes you through an overview of the Microsoft Azure Compute Services.\nSlideShare Presentation This presentation requires your consent to load content from SlideShare (LinkedIn), which may set tracking cookies. Enable SlideShare Presentation ID: 62eQ9o9FToFmEq Introduction to Microsoft Azure Compute from Ravikanth Chaganti\r","date":"2014-08-24T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/session-slides-community-day-2014-introduction-to-microsoft-azure-compute/","series":null,"title":"Slides: Community Day 2014 – Introduction to Microsoft Azure Compute"},{"categories":["Hyper-V","Bangalore IT Pro","Presentations"],"content":"Here are the session slides for my second talk on Hyper-V. In this session, I had introduced Hyper-V Replica and demonstrated how to manage Hyper-V replica in Windows Server 2012 and Server 2012 R2.\nSlideShare Presentation This presentation requires your consent to load content from SlideShare (LinkedIn), which may set tracking cookies. Enable SlideShare Presentation ID: uw9WkRyuz6OlId ","date":"2014-03-19T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/reboot-it-bangalore-2014-session-slides-hyper-v-replica/","series":null,"title":"Slides: Reboot IT, Bangalore - Hyper-V Replica"},{"categories":["Hyper-V","Bangalore IT Pro","Presentations"],"content":"Here are the slides I used for my first session at the Reboot IT conference in Bangalore. We had over 130 attendees and the entire event was very well received.\nSlideShare Presentation This presentation requires your consent to load content from SlideShare (LinkedIn), which may set tracking cookies. Enable SlideShare Presentation ID: 3PSBj4JU4sFGIL Whats new in Windows Server 2012 R2 Hyper-V from Ravikanth Chaganti\r","date":"2014-03-19T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/reboot-it-bangalore-2014-session-slides-whats-new-in-server-2012-r2-hyper-v/","series":null,"title":"Slides: Reboot IT, Bangalore - Whats new in Server 2012 R2 Hyper-V"},{"categories":["PowerShell","Bangalore IT Pro","Presentations"],"content":"I just completed my online session on PowerShell 4.0, First Look for the Singapore PowerShell User Group. It went on pretty well. Here are the slides I used for this session.\nSlideShare Presentation This presentation requires your consent to load content from SlideShare (LinkedIn), which may set tracking cookies. Enable SlideShare Presentation ID: 313hyQgdFB504w And, here is the session recording!\nYouTube Video This video requires your consent to load content from YouTube, which may set tracking cookies. Enable YouTube Video ID: MGQk5y4g2QA ","date":"2013-08-02T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/powershell-4-0-slides-and-session-recording-from-powershell-saturday-singapore/","series":null,"title":"Slides: PowerShell Saturday - Windows PowerShell 4.0"},{"categories":["PowerShell","Bangalore IT Pro","Presentations"],"content":" SlideShare Presentation This presentation requires your consent to load content from SlideShare (LinkedIn), which may set tracking cookies. Enable SlideShare Presentation ID: y8zcQbeseSuep9 ","date":"2013-08-02T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/vtd-session-windows-powershell-3-0--a-first-look--session-recording/","series":null,"title":"Slides: Virtual Tech Days - Windows PowerShell 3.0 – A first look"},{"categories":["DSC","Bangalore IT Pro","Presentations"],"content":"The Microsoft MVP community in India delivered a Virtual Tech Conference (VTC) online yesterday and I had the opportunity to talk about one my favorite topics in PowerShell 4.0 – Desired State Configuration. I used this opportunity to build a custom DSC resource for managing hosts file using DSC. It was a good learning experience for me. And, I see a lot more possibilities with this. Here are the slides I used for this session:\nSlideShare Presentation This presentation requires your consent to load content from SlideShare (LinkedIn), which may set tracking cookies. Enable SlideShare Presentation ID: GydGnjhCBAPRq9 Click here [download id=”26″] to download the demo scripts I used for this session! Remember that the HostsFile resource I built is still not polished and it was written in a few minutes and I have hard-coded a few things such as how we check / build host entries. But, nonetheless, this can serve as a good starting point if you plan to build custom DSC resources.\n","date":"2013-08-02T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/slides-and-demo-scripts-from-my-desired-state-configuration-session-at-vtc-online-india/","series":null,"title":"Slides: VTC Online - Desired State Configuration"},{"categories":["PowerShell","WQL","WMI"],"content":"While I was preparing a few demo scripts for a Bangalore IT Pro UG meet session, I tumbled upon on WMI event class Win32_VolumeChangeEvent. This one is interesting. It is derived from Win32_DeviceChangeEvent class and gives us the ability to monitor local drive events directly.For example, you can get a notification when a local drive or mount point gets removed or added. The following table shows a list of event types we can monitor.\nNote: This class may not be there on Windows XP. I have not verified this fact.\nValue Meaning 1 Configuration Changed 2 Device Arrival 3 Device Removal 4 Docking Let us see a few examples:\nAdding a new local drive We can monitor a local drive addition using the following query:\n1 $query = \u0026#34;SELECT * FROM Win32_VolumeChangeEvent WHERE EventType=2\u0026#34; Using this, you can monitor removable drives such as external hard drives and flash drives.\nRemoval of a local drive To monitor the removal of local drive events, we can use the following query:\n1 $query = \u0026#34;SELECT * FROM Win32_VolumeChangeEvent WHERE EventType=3\u0026#34; Registering for the above events 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 #Query for finding all device arrival events $query = \u0026#34;SELECT * FROM Win32_VolumeChangeEvent WHERE EventType=2\u0026#34; #Register an event subscription Register-WmiEvent -Query $query -Action { $volumeName = (Get-WMIObject -Class Win32_LogicalDisk -Filter \u0026#34;DeviceID=\u0026#39;$($Event.SourceEventArgs.NewEvent.DriveName)\u0026#39;\u0026#34;).VolumeName Write-Host \u0026#34;$($Event.SourceEventArgs.NewEvent.DriveName) ($($volumeName)) was added\u0026#34; } | Out-Null #Query for finding all device Removal events $query = \u0026#34;SELECT * FROM Win32_VolumeChangeEvent WHERE EventType=3\u0026#34; #Register an event subscription Register-WmiEvent -Query $query -Action { Write-Host \u0026#34;$($Event.SourceEventArgs.NewEvent.DriveName) was removed\u0026#34; } | Out-Null Once we have these event registrations done, we can test this by connecting or disconnecting a local drive such as a USB flash drive.\nNow, let us see a “real-world” application of these events and how we can use that to create a simple script to backup some files automatically whenever a USB flash drive gets added. Here is the script to do that:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 Function Backup-ScriptFolder { Param ([string]$backupDrive,[string]$scriptFolder) $backupFolder = \u0026#34;$($backupDrive)\\Backup-$(Get-Date -Format MM-dd-yyyy-hh-mm)\u0026#34; try { New-Item -type directory -Path $backupFolder -Force } catch { $_ return } try { copy-item $scriptFolder -destination $backupFolder -recurse -Verbose } catch { $_ } } #Query for finding all device arrival events $query = \u0026#34;SELECT * FROM Win32_VolumeChangeEvent WHERE EventType=2\u0026#34; #Register an event subscription Register-WmiEvent -Query $query -Action { $volumeName = (Get-WMIObject -Class Win32_LogicalDisk -Filter \u0026#34;DeviceID=\u0026#39;$($Event.SourceEventArgs.NewEvent.DriveName)\u0026#39;\u0026#34;).VolumeName #Write-Host \u0026#34;$($Event.SourceEventArgs.NewEvent.DriveName) ($($volumeName)) was added\u0026#34; if ($volumeName -eq \u0026#34;BACKUPDRIVE\u0026#34;) { Write-Host \u0026#34;Starting file copy..\u0026#34; Backup-ScriptFolder -backupdrive $Event.SourceEventArgs.NewEvent.DriveName -ScriptFolder \u0026#34;C:\\Dropbox\\Scripts\u0026#34; } } | Out-Null If you see the above code, I have a simple function Backup-ScriptFolder that is called within the events -Action script block. I am calling this function only when the newly added local drive has the volume name “BACKUPDRIVE”. This is to make sure I don’t make multiple copies of my backup on unnecessary drives.\nNow, when I add a new USB flash drive or any external hard drive with a volume name “BACKUPDRIVE”, all files from the specified folder just get copied to the newly added drive under a new folder. In the Backup-ScriptFolder function, -BackupDrive is the newly added drive letter and -ScriptFolder is the folder you want backup.\nThis is a very basic implementation. You can easily extend it by adding a pretty progress bar and other features.\nNote: Register-WMIEvent creates a temporary event consumer. Hence, the event notifications won’t be available if we close the PowerShell host. If you want to have a permanent event registration, use PowerEvents module to make your life easy.\nHere is a video that shows this script in action! YouTube Video This video requires your consent to load content from YouTube, which may set tracking cookies. Enable YouTube Video ID: X0EO-tFhsWc ","date":"2011-08-21T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/monitoring-volume-change-events-in-powershell-using-wmi/","series":"WQL via PowerShell","title":"Monitoring Volume Change Events in PowerShell using WMI"},{"categories":["PowerShell","Bangalore IT Pro","Presentations"],"content":"I did a session at @BangaloreITPro this afternoon and the topic was “PowerShell WMI Eventing for IT Pros”. This session’s focus was to give an introduction to WMI events and how to use them in system administration. Here are the slides and demo scripts I used.\nSlideShare Presentation This presentation requires your consent to load content from SlideShare (LinkedIn), which may set tracking cookies. Enable SlideShare Presentation ID: M0T4zDdJrgbj ","date":"2011-08-20T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/bitpro-ug-meet-powershell-and-wmi-eventing-slides-demo-scripts/","series":null,"title":"Slides: BITPro UG Meet - PowerShell and WMI Eventing"},{"categories":["PowerShell","WQL","WMI"],"content":"During a few load test iterations on a SharePoint farm, I started seeing some SQL exceptions in the application log of SharePoint servers. If you are familiar with SharePoint platform, you may have seen these events such as event ID 3355. This event complains that the SharePoint server cannot connect to SQL server. This need not really mean that the DB server is offline. So, to find out the real reason behind these event logs, I needed to start some trace activities whenever event ID 3355 gets logged.\nInitially, I was looking for eventtriggers.exe which is meant for attaching a script or task to an event log. However, I could neither find this on Windows Server 2008 R2 nor an external download. So, I wanted to look at other options I had. I found that there are multiple ways to achieve this.\nAttach to script or task to the event in Windows Event Viewer You can find this option in event viewer. This link is available in the actions pane of event viewer upon selecting an event.\nHowever, this method is available only if the event log entry you are interested exists in the log. Which meant that, this wizard cannot be used to specify the event ID I wanted to monitor without actually pre-selecting the event entry. In my case, I cleared the even log after some tests and did not have the entry for ID 3355 in the application log. So, this method was not an option for me to attach a script to this specific event ID. By the way, this method create a scheduled task. This will appear under Event Viewer tasks in Task Scheduler.\nUpdate: We can use Task Scheduler to create an event trigger. If you go to task scheduler and click on ‘Create Task’, you can select an event as the trigger and specify an event ID there. Take a look at the image below:\nI did not use this technique. Instead, chose to use PowerShell to do this for me.\nCreate a WMI event subscription using Register-WMIEvent or Management Event Watcher .NET class in PowerShell The System.Management.ManagementEventWatcher .NET class can be used to create an event watcher. This takes a WMI query and starts listening for any specified events. For example,\n1 2 3 4 5 6 $watcher = new-object system.management.ManagementEventWatcher $watcher.query = \u0026#34;Select * From __InstanceCreationEvent Where TargetInstance ISA \u0026#39;Win32_NTLogEvent\u0026#39; AND TargetInstance.LogFile=\u0026#39;Application\u0026#39; AND targetInstance.EventCode=3355\u0026#34; $watcher.WaitForNextEvent() Or, Register-WMIEvent — available in PowerShell v2 — can do the same job.\n1 2 3 4 5 $query = \u0026#34;Select * From __InstanceCreationEvent Where TargetInstance ISA \u0026#39;Win32_NTLogEvent\u0026#39; AND TargetInstance.LogFile=\u0026#39;Application\u0026#39; AND TargetInstance.EventCode=3355\u0026#34; Register-WmiEvent -Query $query -Action { Write-Host \u0026#34;Event Arrived or something useful here\u0026#34; } The WMI query here simple. We just want to monitor Application log for any event with event code 3355. Both the above methods have a drawback. They are only temporary event consumers. A temporary event consumer exists only as long as the host is alive. The moment we close the PowerShell host, we lose these event subscriptions. However, I did not know when this even will happen again. So, I need the WMI event subscription to survive system reboots.\nCreate a permanent WMI event consumer in PowerShell So, the best choice for me here was to create a permanent event consumer that can survive system reboots as well. I can create a WMI permanent consumer of command-line type in PowerShell.\nNote You need to have the knowledge of WMI Query language to create WMI event consumers (permanent or temporary) in PowerShell. If you are new to WMI or WMI query language, refer to my eGuide on ‘WMI Query Language via PowerShell‘ available at http://139.59.40.198/blog/?page_id=2134\nAs I explained in the last chapter of my WQL eguide, creating a WMI event consumer in PowerShell requires lot of typing. So, I chose a shortcut and used the PowerEvents module. This module provides an easy way to create WMI event consumers using cmdlets. So, as a solution to my event log monitoring problem, I used the following 4 lines of PowerShell code.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 Import-Module PowerEvents #Create a WMI Query for the event log entry $query = \u0026#34;SELECT * FROM __InstanceCreationEvent WHERE TargetInstance ISA \u0026#39;Win32_NTLogEvent\u0026#39; AND TargetInstance.LogFile=\u0026#39;Application\u0026#39; AND TargetInstance.EventCode=3355\u0026#34; #Create an event filter $filter = New-WmiEventFilter -Name \u0026#34;Event Filter for Event ID 3355\u0026#34; -Query $query #Create an event consumer $consumer = New-WmiEventConsumer -Verbose -Name \u0026#34;Event Consumer for Event ID 3355\u0026#34; -ConsumerType CommandLine ` -CommandLineTemplate \u0026#34;cmd.exe /c `\u0026#34;C:\\debug\\event.cmd`\u0026#34;\u0026#34; #Bind the filter and consumer together New-WmiFilterToConsumerBinding -Filter $filter -Consumer $consumer -Verbose The above code is really self-explanatory. Whenever there is an event ID with a code 3355, this permanent WMI event consumer will trigger C:\\debug\\event.cmd which in turn starts a trace or some kind of logging.\nThis is it for today. Hope this is helpful.\n","date":"2011-06-24T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/attaching-scripts-or-tasks-to-windows-event-log-entries-using-powershell-and-wmi/","series":"WQL via PowerShell","title":"Attaching scripts or tasks to Windows event log entries using PowerShell and WMI"},{"categories":null,"content":"This eBook has been updated since the initial release.\nYou can download this ebook for free at https://www.ravichaganti.com/ebooks/AlaymansguidetoPowerShell2remotingv2.pdf.\nHere are the additions in the update.\nChapter 2 Enable remoting for only a specific network adapter Remoting in an Enterprise\nChapter 3 Run script files on remote computers\nChapter 9 Added a note on Domain controller credential delegation\n​\tUpdated a note on Windows XP/2003 support for CredSSP\nAppendix A Added some more FAQ\nAppendix B Remoting in an Enterprise by Jan Egil Ring\nErrata Nothing as of today. Please leave a comment here if you find errors in the guide.\r","date":"2011-06-24T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/laymans-guide-to-powershell-2-0-remoting/","series":null,"title":"ebook: Layman's Guide to PowerShell 2.0 remoting"},{"categories":null,"content":"This eBook has been updated since the initial release.\nYou can download this ebook at no cost at https://www.ravichaganti.com/ebooks/WMIQueryLanguageviaPowerShellv20.pdf.\nHere are the additions in the update.\nChapter 2 Enable remoting for only a specific network adapter Remoting in an Enterprise\nChapter 3 Run script files on remote computers\nChapter 9 Added a note on Domain controller credential delegation\n​\tUpdated a note on Windows XP/2003 support for CredSSP\nAppendix A Added some more FAQ\nAppendix B Remoting in an Enterprise by Jan Egil Ring\nErrata Nothing as of today. Please leave a comment here if you find errors in the guide.\r","date":"2011-06-24T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/ebook-wmi-query-language-wql-via-powershell/","series":null,"title":"ebook: WMI query language (WQL) via PowerShell"},{"categories":["PowerShell","WQL","WMI"],"content":"Update: Based on feedback from @ShayLevy, we don’t have to send the background job object to Event action as a variable. It is already available as $event.Sender or just $sender. You can see that in the image below. I just picked up a wrong example to explain the -MessageData parameter. Although, the article originally showed a background job as an example, it was just to show how any object can be passed to event action. For example, if all you want to access is a background job object which you are monitoring using Register-ObjectEvent, you can access the job name using $event.Sender.Name or $sender.Name, job Id using $event.Sender.Id or $sender.Id, and so on.\nTo avoid any confusion, I removed the initial example of background job and updated it with a more generic example.\nWhile working with @LaerteSQLDBA on a SQL SMO related script, I was asked a question on how to pass variables or arguments and access them within an event action script block. The solution is simple. We just use the -MessageData parameter of Register-ObjectEvent. So, all event subscriptions get the object and can be accessed using $event.MessageData and this is what Get-Help for -MessageData property of Register-ObjectEvent tells us.\nLet us say, You have a an object which you need to access inside an event registration for some special purpose. And, say that the event scope is different from the object scope.\n1 $foo = \u0026#34;Ravi\u0026#34; Now, when you need to access this variable $foo in the event action or event script block,\n1 2 3 Register-ObjectEvent -InputObject $PSISE.Options -EventName PropertyChanged -MessageData $foo -SourceIdentifier jobEvent -Action { Write-Host \u0026#34;Job Name: $(event.MessageData)\u0026#34; } | Out-Null If you observe the event registration, I supplied $foo as an argument to to -MessageData. This makes sure that we have the value of $foo is assigned to $event.MessageData in the event action script block.\nSo, after the event is raised, if we look a the $event automatic variable:\nSo, $event.MessageData now has the value of $foo. @ShayLevy pointed me to an answer he provided on Technet Forums. Check this for an example on how to pass custom objects, etc.\nThis is it for today. Hope you found this useful.\n","date":"2011-06-22T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/passing-variables-or-arguments-to-an-event-action-in-powershell/","series":"WQL via PowerShell","title":"Passing variables or arguments to an event action in PowerShell"},{"categories":["PowerShell","Bangalore IT Pro","Presentations"],"content":"Here are the slides I used for my session “PowerShell in the Enterprise” at TechED India 2011. This was my first ever TechED session and went pretty well. There were 100+ attendees at the session and some good questions too.\nSlideShare Presentation This presentation requires your consent to load content from SlideShare (LinkedIn), which may set tracking cookies. Enable SlideShare Presentation ID: Jk3JoaQUyQDbxu ","date":"2011-03-25T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/teched-india-2011-powershell-in-the-enterprise-slides/","series":null,"title":"Slides: TechED India - PowerShell in the Enterprise"},{"categories":["PowerShell","Bangalore IT Pro","Presentations"],"content":"Early this month, I did an online webcast session for MS Virtual Tech Days. This session was around how administrators can manage remote Windows 7 desktops with Windows PowerShell. Here are the slides from that session.\nSlideShare Presentation This presentation requires your consent to load content from SlideShare (LinkedIn), which may set tracking cookies. Enable SlideShare Presentation ID: yoEBa4SeuaCVc5 ","date":"2011-02-22T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/microsoft-virtual-tech-days-monitoring-managing-remote-windows-7-desktops-with-powershell-slides-scripts/","series":null,"title":"Slides: Virtual Tech Days - Monitoring \u0026 Managing remote Windows 7"},{"categories":["PowerShell","WQL","WMI"],"content":"There are several ways we can create a file monitoring script using PowerShell. There is also a cmdlet in PowerShellPack called Start-FileSystemWatcher to monitor file /folder changes. However, none of these methods survive a exit at the console or wherever the script is running. This is because all these methods create a temporary event consumer. As I’d mentioned in an earlier post, Trevor’s PowerEvents module makes it very easy to create permanent event consumers in PowerShell. In today’s post, we shall look at how we can do that.\nBefore we dig into that, let us first see how we can create a file monitoring script using PowerShell. Many people use CIM_DirectoryContainsFile class and create an event listener. This is how we use do that class in PowerShell.\n1 2 3 4 $query = \u0026#34;Select * from __InstanceCreationEvent WITHIN 5 WHERE TargetInstance ISA \u0026#39;CIM_DirectoryContainsFile\u0026#39; AND TargetInstance.GroupComponent=\u0026#39;Win32_Directory.Name=\u0026#34;\u0026#34;C:\\\\\\\\Scripts\u0026#34;\u0026#34;\u0026#39;\u0026#34; Register-WmiEvent -Query $query -Action { Write-Host \u0026#34;A new file $($event.SourceEventArgs.NewEvent.TargetInstance.PartComponent) got created\u0026#34; } As you see in the above output, what we get as a part of event data is just that string contained in $Event.SourceEventArgs.NewEvent.TargetInstance.PartComponent. Of course, if you are RegEx lover, you’d just parse that and find the name (extension, etc) of the new file that just got created. However, there is an efficient and easy way to do that. And, that is: monitoring the CIM_DataFile class itself. This is how we do it:\n1 2 3 4 $query = \u0026#34;Select * from __InstanceCreationEvent WITHIN 5 WHERE TargetInstance ISA \u0026#39;CIM_DataFile\u0026#39; AND TargetInstance.Drive=\u0026#39;C:\u0026#39; AND TargetInstance.Path=\u0026#39;\\\\Scripts\\\\\u0026#39;\u0026#34; Register-WmiEvent -Query $query -Action { $event.SourceEventArgs.NewEvent.TargetInstance | Select -Expand FileName, Extension, Name | Out-Host } And, this is what we see in the output.\nWe selected only a few properties from the available list of properties. But, this should give you an idea why I prefer using CIM_DataFile as compared to CIM_DirectoryContainsFile when monitoring for file creation. Similarly, we can monitor file deletions and modifications by subscribing to __InstanceDeletionEvent and __InstanceModificationEvent. The usage of these two classes is more or less similar. So, I will skip those aspects in this post.\nHowever, as I mentioned earlier, we are only creating temporary event consumers by using Register-WMIEvent cmdlet. This is not really helpful since we have to keep the console window where we registered the event always open. We can solve this problem by using a permanent WMI event consumer. This is what PowerEvents module does. It helps us create any of the five permanent WMI consumers.\nSo, for todays post, we shall look at creating a log file consumer using PowerEvents module.\nFirst, we need download and import the module using Import-Module cmdlet. Once this is done, we need to create an event filter for the event we want to subscribe. This is done using New-WMIEventFilter cmdlet.\n1 2 $query = \u0026#34;Select * from __InstanceCreationEvent WITHIN 5 WHERE TargetInstance ISA \u0026#39;CIM_DataFile\u0026#39; AND TargetInstance.Drive=\u0026#39;C:\u0026#39; AND TargetInstance.Path=\u0026#39;\\\\Scripts\\\\\u0026#39;\u0026#34; $eventFilter = New-WmiEventFilter -Name \u0026#34;FileMonitor\u0026#34; -Query $query Now, we need to create a event consumer. In this case, a log file consumer.\n1 2 $eventConsumer = New-WmiEventConsumer -ConsumerType LogFile -Name NewFileCreated -FileName C:\\Logs\\FileMonitor.log ` -Text \u0026#34;New File has been created: %TargetInstance.Name%\u0026#34; Once we have both filter and consumer, we can bind them together to create the permanent event consumer.\n1 2 #This is how we create a binding New-WmiFilterToConsumerBinding -Filter $eventFilter -Consumer $eventConsumer This is it. Now, whenever a file gets created in the C:\\Scripts folder, we’ll see a entry in the log file at C:\\Logs\\FileMonitor.log. It’d look like:\nThese log entries will appear even after a system reboot. This is the benefit of WMI permanent consumers. If you want to learn more about WMI query language syntax used in this post, refer to my WQL series.\n","date":"2011-01-19T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/monitoring-file-creation-using-wmi-and-powerevents-module/","series":"WQL via PowerShell","title":"Monitoring file creation using WMI and PowerEvents module"},{"categories":["PowerShell","WQL","WMI"],"content":"A few weeks ago, I wrote about WMI Timer events using Win32_LocalTime and then mentioned how to work around the DayOfWeek issue. In today’s post, I will show you how to use WMI timer events to create complex scheduled tasks.\nAs system administrators, you may have to create scheduled jobs for performing various sysadmin tasks. We generally use Task Scheduler for such jobs. However, using the regular OS task scheduler, there is no easy way to create a scheduled task that occurs — for example — every Thursday of every fourth week of a month in the third quarter of every year.\nAs I mentioned in my earlier posts, this is one area where WMI timer events are quite useful.\n1 2 3 4 5 6 7 8 9 10 $query = \u0026#34;Select * from __InstanceModificationEvent WHERE (TargetInstance ISA \u0026#39;Win32_LocalTime\u0026#39;) AND (TargetInstance.Quarter=3) AND (TargetInstance.WeekInMonth=4) AND (TargetInstance.DayOfWeek=4 OR TargetInstance.DayOfWeek=9) AND (TargetInstance.Hour=12) AND (TargetInstance.Minute=0) AND (TargetInstance.Second=0)\u0026#34; Register-WmiEvent -Query $query -Action { Write-Host \u0026#34;Execute your scheduled task here\u0026#34; } However, the major drawback of Register-WMIEvent is that the event registration is alive only until the PowerShell consle window is open. So, for this task to execute, you must have the console window open at all times. This is because Register-WMIEvent creates only a temporary event consumer. So, how do we create a permanent event consumer?\nWe can use Trevor‘s (@pcgeek86) PowerEvents PowerShell module.\nWhat is PowerEvents? PowerEvents is a Windows PowerShell v2.0 module designed to facilitate the ease of creating, updating, and deleting WMI (Windows Management Instrumentation) permanent event registrations. PowerEvents makes it easy to create WMI event filters (define the events you want to capture) and event consumers (responders to events), and then bind them together to initiate the flow of events. By leveraging permanent event registrations, you can perform advanced monitoring functions on a workstation or server, that would otherwise require implementation of an enterprise monitoring product. Because WMI is incredibly vast in the information it provides, very detailed monitoring can be performed using almost any of the WMI objects that exist on a computer.\nThere are five types of permanent event consumers that are possible (out-of-the-box) in WMI and the PowerEvents module provides cmdlets to create these five event consumers. In today’s post, lets look at the command-line event consumer. This is the apt choice for creating scheduled tasks in combination with WMI timer events.\nTo be able to receive WMI events at all time, we need to create an event filter, create an event consumer, and then bind them together. This process is explained in detail at http://msdn.microsoft.com/en-us/library/aa393014%28v=vs.85%29.aspx. We will see how PowerEvents PowerShell module makes it easy using the new cmdlets.\nNote: At the time of writing this post, the publicly available PowerEvents release (0.2 alpha) did not have a functional command-line consumer. To be able to use the command-line consumer as shown in this post, you need to download the changeset listed under source code tab.\nCreating an event filter An event filter is an instance of the __EventFilter system class that describes an event type and the conditions for delivering a notification. So, in our case it is the same WQL query we used in the example above. PowerEvents module provides a cmdlet to create an event filter — New-WMIEventFilter.\n1 2 3 4 5 6 7 8 9 $query = \u0026#34;Select * from __InstanceModificationEvent WHERE (TargetInstance ISA \u0026#39;Win32_LocalTime\u0026#39;) AND (TargetInstance.Quarter=3) AND (TargetInstance.WeekInMonth=4) AND (TargetInstance.DayOfWeek=4 OR TargetInstance.DayOfWeek=9) AND (TargetInstance.Hour=12) AND (TargetInstance.Minute=0) AND (TargetInstance.Second=0)\u0026#34; $taskFilter = New-WmiEventFilter -Name \u0026#34;WQL for 3rd quarter timer event\u0026#34; -Query $query This is it. You have the event filter created. Make a note that you need to store the event filter instance in a variable. This is required since the published version of PowerEvents has no cmdlet way to get a list of event filters. Also, see how I’d used TargetInstance.DayOfweek=9. In real world, there is no 9th DayOfWeek. This is just a work around we need to put in place to make sure the event gets triggered on the desired day of week — in this case 4 (Thursday). We could use a WMI query to get that list but I will save it for another post.\nCreating an event consumer An event consumer is a recipient of notifications that report an occurrence of an event. An event consumer is either temporary or permanent. The cmdlet for creating an event consumer is New-WMIEventConsumer. In this post, I will show you how to create a command-line consumer. The idea is to invoke a backup script when the event fires. For a list of other consumer types, refer to http://msdn.microsoft.com/en-us/library/aa393649%28v=VS.85%29.aspx. Here is how we create a permanent event consumer using PowerEvents module.\n1 $cmdConsumer = New-WmiEventConsumer -Verbose -Name \u0026#34;bkConsumer1\u0026#34; -ConsumerType CommandLine -CommandLineTemplate \u0026#34;cmd.exe /c `\u0026#34;C:\\Scripts\\backup.bat`\u0026#34;\u0026#34; The *-CommandLineTemplate* takes the path to the backup script. Also, note that the ConsumerType is set to CommandLine in this case. Again, make sure you store the instance of consumer in a variable. We need it later.\nBinding a filter and consumer together Now, as a final step, we need to bind the event filter and the consumer together so that the backup script gets invoked when the timer event gets triggered on the specified date \u0026amp; time. To do this, we will use New-WMIFilterConsumerBinding cmdlet.\n1 New-WmiFilterToConsumerBinding -Filter $taskFilter -Consumer $cmdConsumer -Verbose Remember why I said that we need to store the instances of filter and consumer? It makes it easy to bind them together.\nThis is it. The backup script gets triggered once the timer event gets fired. This is just one example of creating complex scheduling tasks using WMI timer events. And, using PowerEvents makes it easy to create permanent event consumers. Go and explore it yourself.!\n","date":"2010-12-25T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/creating-complex-scheduled-tasks-using-wmi-timer-events-and-powerevents-module/","series":"WQL via PowerShell","title":"Creating complex scheduled tasks using WMI Timer events and PowerEvents Module"},{"categories":["PowerShell","WQL","WMI"],"content":"In this part of the WQL series, we shall look at extrinsic events.\nExtrinsic events represent events that do not directly link to standard WMI model. For example, Windows registry defines extrinsic events for all registry change events. For intrinsic events, having a WMI provider isn’t mandatory. This is mostly because they are defined within the standard WMI model and WMI takes care of these if there is no WMI provider for a given resource in the standard WMI model. However, since extrinsic events are outside of the standard WMI model, having a WMI provider is mandatory.\nWhen creating a WMI query for extrinsic events, we need to use a class derived from __ExtrinsicEvent class. Take a look at this CIM Studio capture.\nAs you see, there is registry provider that provides extrinsic events. So, rest of this post, will use the extrinsic event classes from registry provider to show we can create event notifications for extrinsic events.\nNote: We cannot use __InstanceDeletionEvent, __InstanceCreationEvent, __InstanceModificationevent, or __InstanceOperationEvent for monitoring extrinsic events. This should be obvious from the above screen capture.\nMonitoring registry value change events We use RegistryValueChangeEvent to monitor changes to registry values. Here is how we write a WMI query for registry value change events:\n1 2 $query = \u0026#34;Select * from RegistryValueChangeEvent WHERE Hive=\u0026#39;HKEY_LOCAL_MACHINE\u0026#39; AND KeyPath=\u0026#39;Software\\\\Temp\u0026#39; AND ValueName=\u0026#39;Name\u0026#39;\u0026#34; Register-WmiEvent -Query $query -Action { Write-Host \u0026#34;Value changed\u0026#34; } Yes, that is it. So, when the monitored value changes, you will see the message “value changed” on the screen. However, there is one drawback here. The event notification only tells you that the value has been modified but it won’t return the new value.! Also, deleting a value is also considered modification and you receive a notification. But, agian, the notification won’t tell you that the value was deleted. See the next screen capture.\nAlthough I deleted the value, the resulting event notification does not tell us anything about that value deletion. So, in the -Action script block, we will have to verify the presence of registry value we are monitoring and then display the new value. For example,\n1 2 3 4 5 6 7 8 $query = \u0026#34;Select * from RegistryValueChangeEvent WHERE Hive=\u0026#39;HKEY_LOCAL_MACHINE\u0026#39; AND KeyPath=\u0026#39;Software\\\\Temp\u0026#39; AND ValueName=\u0026#39;Name\u0026#39;\u0026#34; Register-WmiEvent -Query $query -Action { if ((Get-item HKLM:\\SOFTWARE\\Temp).GetValue(\u0026#34;Name\u0026#34;)) { write-host (Get-item HKLM:\\SOFTWARE\\Temp).GetValue(\u0026#34;Name\u0026#34;) } else { Write-host \u0026#34;The registry value was deleted\u0026#34; } } Monitoring registry key change events RegistryKeyChangeEvent can be used to monitor modifications to a registry subkey. Similar to RegistryValueChangeEvent, this event notification also won’t give you any information beyond subkey modification. Here is how we use it:\n1 2 $query = \u0026#34;Select * from RegistryKeyChangeEvent WHERE Hive=\u0026#39;HKEY_LOCAL_MACHINE\u0026#39; AND KeyPath=\u0026#39;Software\\\\Temp\u0026#39;\u0026#34; Register-WmiEvent -Query $query -Action { Write-host \u0026#34;Something changed\u0026#34; } Once again, you need to use some technique similar to whats shown above to retrieve the “real” modification that happened.\nMonitoring registry tree change events RegistryTreeChangeEvent can be used to monitor subtree level modifications. Similar to other two event classes in the registry provider, this event class provides a notification that a change occurred but won’t tell you about what had changed. So, we got to use a method of our own to detect what change generated the event notification.\n1 2 $query = \u0026#34;Select * from RegistryTreeChangeEvent WHERE Hive=\u0026#39;HKEY_LOCAL_MACHINE\u0026#39; AND RootPath=\u0026#39;\u0026#39;\u0026#34; Register-WmiEvent -Query $query -Action { Write-host \u0026#34;Something changed\u0026#34; } There are many vendor provided extrinsic event classes. For example, take a look at how Intel ME WMI provider uses extrinsic events for firmware state notifications: http://software.intel.com/sites/manageability/AMT_Implementation_and_Reference_Guide/default.htm?turl=WordDocuments%2Fwmievents.htm\nThis is it about extrinsic events and WMI event queries. Go and explore these using the examples I’d used through out. We shall look at WMI schema queries in the next part of this series. Stay tuned.!\n","date":"2010-12-24T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/wmi-query-language-wql-event-queries-extrinsic-events/","series":"WQL via PowerShell","title":"WMI Query Language (WQL) – Event Queries: Extrinsic Events"},{"categories":["PowerShell","WQL","WMI"],"content":"In this last and final part of this series, we will look at how to use WQL for querying the WMI schema.\nSchema queries are used to retrieve class definitions (rather than class instances) and schema associations. In simple words, if you need to find out what type of information (this is what schema really means) a specific class holds, you use schema queries.Here is an example of a schema query:\n1 2 $query = \u0026#34;SELECT * FROM meta_class where __this ISA \u0026#39;Win32_Process\u0026#39;\u0026#34; Get-WmiObject -Query $query | fl And, this is what you would see when you execute this:\nIn one of the earlier posts, we looked at retrieving class definitions with ASSOCIATORS OF and REFERENCES OF keywords. So, how are the schema queries different from the data queries using these two keywords?\nWell, the above keywords return class definitions only when there are instances of those classes present. Using a schema query, we can retrieve the class definitions even when there is no instance present.\nTo understand what I just said, take a look at this example that shows how a WMI query was built when using REFERENCES OF.\n1 2 $query = \u0026#34;REFERENCES OF {Win32_Process=$pid} WHERE ClassDefsOnly\u0026#34; Get-WmiObject -Query $query | fl See the {Win32_Process=$pid} part of the query. We have to specify some identifier so that we can get an instance. In the above example, we used $pid variable. $pid is the process ID of PowerShell host. If we don’t specify a PID or some other identifier to get the instance, we end up with an error while executing the query. Now, go back and take a look at how we the schema query. We did not specify any kind of an identifier or property anywhere and we were still able to get the class definitions. That is the difference.\nLet us dig a bit in to the schema query syntax.\nWe are familiar with SELECT keyword. When building schema queries, only “*” is supported. Unlike other queries, you cannot do some thing like SELECT xyz FROM abc. It has to be *SELECT ** always. And, the use of meta_class specifies that we are building a schema query. The only way to narrow down results when using schema queries is to use WHERE clause. Let us now look at a few ways to narrow the query results.\nUsing __this __this is a special property that identifies the target class for the query and using an ISA operator is must. This requests the definitions for the subclasses of the target class. Here is how you use this method:\n1 2 $query = \u0026#34;SELECT * FROM meta_class where __this ISA \u0026#39;Win32_LogicalDisk\u0026#39;\u0026#34; Get-WmiObject -Query $query | fl This when executed, returns the class definitions of Win32_LogicalDisk and all its subclasses.\nUsing __Class Using __Class, we can request for the class definitions of a single class and this is how we do it:\n1 2 $query = \u0026#34;SELECT * FROM meta_class where __class=\u0026#39;Win32_LogicalDisk\u0026#39;\u0026#34; Get-WmiObject -Query $query | fl This is a brief overview of schema queries. We seldom need to use schema queries and IT Pros will probably never have to use this type queries. You can also use *REFERENCES OF* and ASSOCIATORS OF keywords to get schema associations. You can use the SchemaOnly qualifier in the WHERE clause to retrieve the schema associations of a class.\nThis is it. This ends the series on WMI query language.\n","date":"2010-12-24T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/wmi-query-language-wql-schema-queries/","series":"WQL via PowerShell","title":"WMI Query Language (WQL) – Schema Queries"},{"categories":["PowerShell","WQL","WMI"],"content":"Intrinsic events are used to monitor a resource represented by a class in the CIM repository. In other words, the intrinsic events occur in response to a change in the standard WMI data model. WMI creates intrinsic events for objects stored in the WMI repository. A provider generates intrinsic events for dynamic classes, but WMI can create an instance for a dynamic class if no provider is available. WMI uses polling to detect the changes.\nThere are many system classes that WMI uses to report intrinsic events. However, the ones that are most interesting and useful are __InstanceCreationEvent, __InstanceModificationEvent, and __InstanceDeletionEvent. Hence, monitoring resources on a system involves monitoring of these system classes. These classes are derived from the __InstanceOperationEvent class which is derived from the __Event system class under root\\Default namespace. The following capture of WMI CIM Studio shows this hierarchy.\nThe WQL syntax for WMI intrinsic event queries is:\n1 SELECT \\* FROM __InstanceXEvent WITHIN PollingInterval WHERE TargetInstance ISA WMIClassName AND TargetInstance.WMIClassPropertyName = Value This is something similar to what we saw in the earlier post about WQL syntax for event queries. The __InstanceXEvent can be any of the system classes such as __InstanceCreationEvent, __InstanceModificationEvent, __InstanceDeletionEvent, and __InstanceOperationEvent. Now, when do we use each of these event classes?\n__InstanceCreationEvent is used when we want to receive a notification upon creation of an instance. For example, we can use this event class when you want to receive an event notification every time a new process gets created. This can be done by,\n1 2 3 4 #Query for new process events $query = \u0026#34;Select * from __InstanceCreationEvent WITHIN 10 WHERE TargetInstance ISA \u0026#39;Win32_Process\u0026#39;\u0026#34; #Register WMI event Register-WmiEvent -Query $query -Action { Write-Host \u0026#34;New Process Created. Do something useful here\u0026#34; } __InstanceDeletionEvent is used when we want to receive a notification upon deletion of an instance. For example, we can use this class when we want to receive an event notification every time a process is terminated. For example,\n1 2 3 4 #Query for new process events $query = \u0026#34;Select * from __InstanceDeletionEvent WITHIN 5 WHERE TargetInstance ISA \u0026#39;Win32_Process\u0026#39;\u0026#34; #Register WMI event Register-WmiEvent -Query $query -Action { Write-Host \u0026#34;A Process terminated. Do something useful here\u0026#34; } __InstanceModificationEvent is used when we want to monitor changes to an existing instance or a resource. For example, we can use this class when we want to receive an event notification when a the processor utilization on a system goes beyond a specified usage threshold. For example,\n1 2 3 4 #Query for new process events $query = \u0026#34;Select * from __InstanceModificationEvent WITHIN 5 WHERE TargetInstance ISA \u0026#39;Win32_Processor\u0026#39; AND TargetInstance.LoadPercentage \u0026gt; 80\u0026#34; #Register WMI event Register-WmiEvent -Query $query -Action { Write-Host \u0026#34;Processor utilization is more than 80%. Do something useful here\u0026#34; } All of the examples above just displayed a message when the event notification was received. Instead, we can do something useful within the script block. For example, in the __InstanceCreationEvent example, we are just displaying that a new process was created but not the process name that just got created. So, how do we access that information in the script block and tell a user the name of the process that was created?\nSimple, PowerShell creates an automatic variable called $event and stores the last event received in that variable. And, this automatic variable can be accessed in the -Action scriptblock you specify during a WMI event registration. Let us see an example.\nIf you see in the above example, I made an event registration for process creation events and in the -Action script block, assigned the $event variable to a variable in the global scope ($myEvent). This is essential because we cannot access the $event variable outside the -Action script block. Once the registration was done, I opened a notepad. This will fire the __InstanceCreationEvent and $myEvent should have the details around the event. So, I tried looking at all the members of this event. After exploring that a bit, I figured out that $myEvent.SourceEventArgs.NewEvent.TargetInstance.Name has the name of the new process which is notepad.exe. This is precisely what you see in the last line there.\n$Event.SourceEventArgs.NewEvent.TargetInstance will have the instance of the newly created process. I will leave it to you to explore more.\n","date":"2010-12-22T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/wmi-query-language-wql--event-queries-intrinsic-events/","series":"WQL via PowerShell","title":"WMI Query Language (WQL) – Event Queries: Intrinsic Events"},{"categories":["PowerShell","WQL","WMI"],"content":"The WMI query syntax for event queries is a bit different and deserves a discussion. So, before we delve in to the types of event queries, let us first look at the syntax for WQL event queries. As we discussed earlier, we use SELECT statement for event queries too. We can combine this with other keywords such as WITHIN, HAVING, and GROUP to change how we receive these WMI events.\nHere is how a MSDN article shows the syntax for WMI event queries.\nEVENT-WQL = “SELECT” “FROM” / OPTIONAL-WITHIN = [“WITHIN” ] INTERVAL = 1*DIGIT EVENT-WHERE = [“WHERE” ]\nEVENT-EXPR = ( ( “ISA” ) / ) [“GROUP WITHIN” ( [“BY” [ DOT] ] [“HAVING” ]] ) INSTANCE-STATE = “TARGETINSTANCE” / “PREVIOUSINSTANCE”\nIn the above syntax specification, we know the SELECT, FROM, and WHERE keywords. There are also other keywords such as WITHIN, GROUP, BY, and HAVING. Let us look at each one of these keywords now.\nWITHIN WITHIN keyword is used to specify the polling interval or grouping interval (used with GROUP clause) for the events. A polling interval is the interval that WMI uses as the maximum amount of time that can pass before notification of an event must be delivered. The general syntax to specify the polling interval,\n1 SELECT * FROM eventclass WITHIN interval WHERE property = value The polling interval value is specified as number of seconds and is a floating point number. So, we can specify values smaller than one second. However, specifying a polling interval smaller than one second (for example, 0.1) may cause system slow down due to the resource intensive nature of event queries. The recommended values for the polling interval really depend on the event calss. Do not use a small value here unless you really need the event notification be delivered immediately.\nGROUP Using GROUP clause causes WMI to generate a single notification to represent a group of events. When used in a WMI event query, this returns an instance of __AggregateEvent that contains an embedded object of one of the instances received during the grouping interval and number of such events received. These two are represented by representative \u0026amp; NumberOfEvents properties respectively. The grouping interval specifies the time period, after receiving an initial event, during which WMI should collect similar events. The GROUP clause must contain a WITHIN clause to specify the grouping interval and can contain the BY or HAVING keyword, or both. And, the GROUP clause is placed after the WHERE clause if the query contains a WHERE clause. Here is the syntax:\n1 SELECT * FROM EventClass [WHERE property = value] GROUP WITHIN interval The WHERE, BY, and HAVING clauses are optional.\nThis is especially useful when we don’t want to receive an event notification every time the event fires. For example, when monitoring a system, we may not want to receive a notification every time an specific event log gets generated. Instead, we can use GROUP clause to specify a grouping interval and receive only one notification for all the desired event logs generated within the grouping interval. Something similar to this:\n1 2 3 4 #Build a WMI query for receiving an event $query = \u0026#34;Select * from __instanceCreationEvent WHERE TargetInstance ISA \u0026#39;Win32_NTLogEvent\u0026#39; ANDTargetInstance.EventCode=1980 GROUP WITHIN 300\u0026#34; #Register the event Register-WmiEvent -Query $query -Action {Write-Host \u0026#34;Eventlog Arrived\u0026#34; } HAVING In the above imaginary example, we will receive an event notification for all the events received within the grouping interval. But, what if we want to receive the event notification only when ten such event logs are generated within the grouping interval? This is where HAVING plays an important role. Let us look at how we can modify the above sample code to use HAVING keyword.\n1 2 3 4 #Build a WMI query for receiving an event $query = \u0026#34;Select * from __instanceCreationEvent WHERE TargetInstance ISA \u0026#39;Win32_NTLogEvent\u0026#39; ANDTargetInstance.EventCode=1980 GROUP WITHIN 300 HAVING NumberOfEvents \u0026gt; 10\u0026#34; #Register the event Register-WmiEvent -Query $query -Action {Write-Host \u0026#34;Eventlog Arrived\u0026#34; } I mentioned earlier that by using GROUP returns a property called NumberOfEvents that contains the number of events received during the grouping interval. Now, we use that property along with HAVING keyword to filter event notifications. Here is the syntax:\n1 2 SELECT \\* FROM EventClass [WHERE property = value] GROUP WITHIN interval HAVING NumberOfEvents operator constant So, using the HAVING query as shown above, an event notification is delivered only when WMI receives more than 10 events in the grouping interval.\nBY You can use BY keyword along with GROUP clause to group events by one or more properties of the event class. The general syntax for using BY is as follows:\n1 2 SELECT \\* FROM EventClass [WHERE property = value] GROUP WITHIN interval [BY property_list] For example, The following example groups all events received in 300 seconds by the TargetInstance.SourceName property.\n1 2 3 4 5 6 #Build a WMI query for receiving an event $query = \u0026#34;Select * from __instanceCreationEvent WHERE TargetInstance ISA \u0026#39;Win32_NTLogEvent\u0026#39; ` GROUP WITHIN 300 BY TargetInstance.SourceName ` HAVING NumberOfEvents \u0026gt; 10\u0026#34; #Register the event Register-WmiEvent -Query $query -Action {Write-Host \u0026#34;Eventlog Arrived\u0026#34; } In the above example, the events are grouped by the TargetInstance.SourceName property and an event notification is delivered only if the number of events received during the grouping interval exceeds 10.\nSo far, we looked at all the important keywords that can be used while building WMI event queries. But, if you have observed, I kept using the words *eventclass* and *TargetInstance*. What are these?\nEvent Classes As discussed earlier, an event class is a WMI class that event consumers can subscribe to by an event query. The class reports a specific type of occurrence. For example, the Win32_ProcessStopTrace class reports that a specific process has stopped. In the upcoming parts of this series, we will look at several examples of event classes when we discuss intrinsic and extrinsic events. In fact, __InstanceCreationEvent we used in all the above examples is a part of event classes that are used for intrinsic event queries. More on this later.\nTargetInstance TargetInstance references to the instance of the event class. This is precisely the reason why we could use TargetInstance.SourceName or TargetInstance.EventCode in the examples above. Also, take a look at the query again. To refer to an event class instance, we specified TargetInstance ISA ‘Win32_NTLogEvent’. Make a note that we did not use “=” operator or “IS” operator. The only valid comparison operator when referecing TargetInstance is “ISA”.\nThis concludes today’s post on WQL syntax for event queries. We shall look at intrinsic and extrinsic events in the upcoming parts. Stay tuned.!\n","date":"2010-12-22T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/wmi-query-language-wql-event-queries-syntax/","series":"WQL via PowerShell","title":"WMI Query Language (WQL) – Event Queries: Syntax"},{"categories":["PowerShell","WQL","WMI"],"content":"In my earlier post, I showed how Win32_LocalTime WMI class can be used to capture timer events. As mentioned there, WMI events can be quite helpful in creating complex scheduling tasks. For example, you can specify to run a script every Thursday of every fourth week of a month in the third quarter of every year. However, there is a bug in Win32_LocalTime that currently blocks this.\nI created a support incident with MS and reported this bug to them. I got a response that this indeed is a bug and they provided a workaround to solve this temporarily.\nLet us see this with an example. I will use the same scenario I mentioned earlier: every Thursday of every fourth week of a month in the third quarter of every year\n1 2 3 4 5 6 7 8 9 10 $query = \u0026#34;Select * from __InstanceModificationEvent WHERE (TargetInstance ISA \u0026#39;Win32_LocalTime\u0026#39;) AND (TargetInstance.Quarter=3) AND (TargetInstance.WeekInMonth=4) AND (TargetInstance.DayOfWeek=4 OR TargetInstance.DayOfWeek=5) AND (TargetInstance.Hour=12) AND (TargetInstance.Minute=0) AND (TargetInstance.Second=0)\u0026#34; Register-WmiEvent -Query $query -Action { Write-Host \u0026#34;Event Arrived\u0026#34; } Observe how I used DayOfWeek property twice in the query. This is the workaround. This event fires only when DayOfWeek is used this way. However, the side effect of this is: the event fires on both Thursday (4) and Friday (5). We can workaround this in a couple of ways.\n1. You can check for DayOfWeek in the -Action script block as shown here.\n1 2 3 4 5 6 7 8 9 $query = \u0026#34;Select * from __InstanceModificationEvent WHERE (TargetInstance ISA \u0026#39;Win32_LocalTime\u0026#39;) AND (TargetInstance.Quarter=4) AND (TargetInstance.WeekInMonth=1) AND (TargetInstance.Hour=19) AND (TargetInstance.Minute=41) AND (TargetInstance.Second=0)\u0026#34; Register-WmiEvent -Query $query -Action { if ($MyEvent.SourceEventArgs.NewEvent.TargetInstance.DayOfWeek -eq 5) { write-Host \u0026#34;Event Arrived\u0026#34; } } 2. You can provide some dummy DayOfWeek value, anything outside 0-6. For example,\n1 2 3 4 5 6 7 8 9 10 $query = \u0026#34;Select * from __InstanceModificationEvent WHERE (TargetInstance ISA \u0026#39;Win32_LocalTime\u0026#39;) AND (TargetInstance.Quarter=4) AND (TargetInstance.WeekInMonth=1) AND (TargetInstance.DayOfWeek=5 OR TargetInstance.DayOfWeek=9) AND (TargetInstance.Hour=19) AND (TargetInstance.Minute=41) AND (TargetInstance.Second=0)\u0026#34; Register-WmiEvent -Query $query -Action { Write-Host \u0026#34;Event Arrived\u0026#34; } This way event fires on Thursday as desired and the second check for DayOfWeek=9 will never fire. This is it.\n","date":"2010-12-03T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/workarounds-for-dayofweek-timer-events-when-using-win32_localtime/","series":"WQL via PowerShell","title":"Workarounds for DayOfWeek Timer events when using Win32_LocalTime"},{"categories":["PowerShell","WQL","WMI"],"content":"This is not a part of the WQL series I am doing. I happend to take a look at the WMI timer events while providing feedback to an upcoming (cool) PowerEvents module by Trevor (@pcgeek86). BTW, this module will be released on November 30th. So, watch out for the annoncement.\nComing to the subject of this post, in WMI, there are 3 types of events possible. They are Timer events, Intrinsic events, and extrinsic events. My WQL series will soon cover intrinsic and extrensic events. Very few people have written about WMI timer events in the past but this particular post on The SysAdmins blog discusses good amount of details.\nIn today’s post, I am going to write about WMI timer events in PowerShell and how to use Register-WMIEvent cmdlet to create a temporary consumer to consume these events. In general, WMI timer events are generated by WMI infrastructure at a point in time. In the good old days of Windows 2000, we had to capture these by creating an instance of classes derived from the __TimerInstruction class. Also, there are two types of timer events.\nAbsolute Timer Events occur on a specific day at a specific time. Interval Timer Events are events that occur periodically at specified intervals. For example, every 30 minutes. Prior to Windows 2003, we have had to use __AbsoluteTimerInstruction class and __IntervalTimerInstruction class to define these events. However, that isn’t necessary anymore with the introduction of Win32_LocalTime or Win32_UTCTime WMI classes. With these two classes, we can use the standard WMI eventing model to consume timer events.\nLet us take a look at Win32_LocalTime. This class represents an instance of Win32_CurrentTime.\nAs you see in the above screenshot, an instance of this class represents the current date and time. There are a few special properties that tell you which quarter of the year, week in the current month, and etc. Do refer to the MSDN link for this class and read about the properties. Especially, the DayOfWeek property. It always indicates Sunday as zero irrespective of the system culture and langauge settings.\nSince, these this class supports standard WMI eventing model, we can use an intrinsic event query to consume these events. For example,\n1 2 3 4 #Setup WQL query $query = \u0026#34;SELECT * FROM __InstanceModificationEvent WHERE TargetInstance ISA \u0026#39;Win32_LocalTime\u0026#39;\u0026#34; #Register WMI Event Register-WmiEvent -Query $Query -SourceIdentifier \u0026#34;Event-Every-Second\u0026#34; -Action { Write-Host \u0026#34;Event Received\u0026#34; } will result in displaying “Event Received” message at the console every one second. This is because the event fires every one second and that is the smallest time segment supported. Now, this won’t be very useful unless you want to flood your consumer with a bunch of useless messages every one second. Let us look at another example:\n1 2 3 4 5 6 7 #Setup WQL query $query = \u0026#34;SELECT * FROM __InstanceModificationEvent WHERE TargetInstance ISA \u0026#39;Win32_LocalTime\u0026#39; AND TargetInstance.Hour=14 AND TargetInstance.Minute=30 AND TargetInstance.Second=0\u0026#34; #Register WMI Event Register-WmiEvent -Query $Query -SourceIdentifier \u0026#34;Event-Every-Day-14.30PM\u0026#34; -Action { Write-Host \u0026#34;Event Received\u0026#34; } In this example, we use WQL query to register for a WMI event that fires every day at 14.30 hours. Note that the hours value is in 24 hour format. Now, this is useful. I can use the -Action script block to do something better like running a backup script when this event fires. Similarly, you can also create event registrations for events occurring on specific day of every quarter (Quarter) or specific week (WeekInMonth) of every month.\nWMI Timer events can be used to create some very advanced scheduling jobs. Register-WMIEvent creates a temporary consumer but just imagine being able to create a permanent consumer and use that for scheduling several administrative tasks. I will update this post once Trevor (@pcgeek86) releases his PowerEvents module and show you some cool tricks.\nHowever, here is the catch: There is a bug that causes DayOfWeek to stop triggering any timer events. Update: I posted this question on DayOfWeek at http://social.msdn.microsoft.com/Forums/en-US/etw/thread/06b7ae7f-737e-47f2-9ae2-a5b8d4c125a9\n1 2 3 4 5 6 7 8 #Setup WQL query for DayOfWeek $query = \u0026#34;SELECT * FROM __InstanceModificationEvent WHERE TargetInstance ISA \u0026#39;Win32_LocalTime\u0026#39; AND TargetInstance.=3 AND TargetInstance.Hour=12 AND TargetInstance.Minute=16 AND TargetInstance.Second=0\u0026#34; #Register WMI Event Register-WmiEvent -Query $Query -SourceIdentifier \u0026#34;Event-Every-Tuesday-12-16PM\u0026#34; -Action { Write-Host \u0026#34;Start backup\u0026#34; } This is a dissappointing bug and I am going to check if MS has any answer. Their documentation for Win32_LocalTime uses a WQL query that has DayOfWeek property. I’d experimented with the same query in PowerShell, VBScript \u0026amp; C#. It worked no where. So, timer events are quite useful in creating advanced scheduling jobs but the current bug is keeping it away from “real” world use.\nI will update this post with the details I find out or get from Microsoft. Keep watching this space.\n","date":"2010-11-24T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/wmi-timer-events-in-powershell-using-win32_localtime/","series":"WQL via PowerShell","title":"WMI Timer Events in PowerShell using Win32_LocalTime"},{"categories":["PowerShell","WQL","WMI"],"content":"In this post, I will write a bit about basics of WMI events and how Register-WMIEvent cmdlet can be used. To start with, here is an excerpt from Microsoft Scripting guide that introduces WMI events:\nJust as there is a WMI class that represents each type of system resource that can be managed using WMI, there is a WMI class that represents each type of WMI event. When an event that can be monitored by WMI occurs, an instance of the corresponding WMI event class is created. A WMI event occurs when that instance is created.\nWindows PowerShell v2 provides a cmdlet — Register-WMIEvent — to consume WMI events. There are a couple of different ways we can use this cmdlet. You can either use -Class or -Query parameters to create a temporary event consumer. When using -Class parameter, we need to specify a WMI event class. So, what happens if the value provided to the -Class parameter isn’t a WMI event class? Yes, PowerShell complains about it :).\nSo, PowerShell says that Win32_Process isn’t an event class. Now, how do we know what are the WMI event classes? Simple, we can use a WMI query to find out.\n1 2 3 #Get all classes that are WMI event classes #filter class names for Win32 classes Get-WMIObject -Query \u0026#34;Select * from meta_class Where (__This ISA \u0026#39;__Event\u0026#39;) AND (__Class like \u0026#39;win32%\u0026#39;)\u0026#34; This will list all WMI event classes that start with Win32 prefix. You will see many more if you remove the second condition in the WHERE clause but for starters, this is good enough. If you execute the above PowerShell command, you will see a WMI class named Win32_ProcessStartTrace. This class indicates the new process started event. We can use this WMI class to subscribe to all process start events. For example,\n1 2 Register-WmiEvent -Class Win32_ProcessStartTrace -SourceIdentifier \u0026#34;Process Started\u0026#34; ` -Action { Write-Host \u0026#34;$($Event.SourceEventArgs.NewEvent.ProcessName) just started\u0026#34; } Note: You have to open the PowerShell console in elevated mode. Else, you will see an access denied message everytime you try using Register-WMIEvent\nThis command will register an event consumer and display a message with the newly created process name. However, this will result in receiving the messages at the console everytime any process starts and not just the one you are interested in.\nSo, what if you are interested only in one specific process? We could have easily filtered out the unnecessary processes before displaying the process name at the conole. But, why even receive the event when we don’t need it? This is where -Query parameter comes handy. Look at this example\n1 2 3 #Register-WMIEvent using -Query Register-WmiEvent -Query \u0026#34;Select * from Win32_ProcessStartTrace WHERE ProcessName=\u0026#39;notepad.exe\u0026#39;\u0026#34; ` -Action {Write-Host \u0026#34;New notepad process created\u0026#34; } The WQL statement we used should be familiar to you by now. There are many other ways to monitor process creation using WMI events and WQL. What I showed above is just one way of doing it. We shall see the other methods soon.\nWMI Event Types\nThere are two types of WMI events one should understand: Intrinsic Events and Extrinsic Events. It is important to understand what are the two types of events before we dig in to WQL for WMI events. There are also timer events but these type of events are rarely used in any system administration type of scripting. So, let us keep it aside for now and visit it towards the end of this series.\nIntrinsic Events\nIntrinsic events are used to monitor a resource represented by a class in the CIM repository. In other words, the intrinsic events occur in response to a change in the standard WMI data model. WMI creates intrinsic events for objects stored in the WMI repository. A provider generates intrinsic events for dynamic classes, but WMI can create an instance for a dynamic class if no provider is available. WMI uses polling to detect the changes. There are many system classes that WMI uses to report intrinsic events. However, the ones that are most interesting and useful are __InstanceCreationEvent, __InstanceModificationEvent, and __InstanceDeletionEvent. Hence, monitoring resources on a system involves monitoring of these system classes. We shall see some examples when we start discussing intrinsic events in the next post.\nExtrinsic Events\nExtrinsic events represent events that do not directly link to standard WMI model. For example, Windows registry defines extrinsic events for all registry change events. For intrinsic events, having a WMI provider isn’t mandatory. This is mostly because they are defined within the standard WMI model and WMI takes care of these if there is no WMI provider for a given resource in the standard WMI model. However, since extrinsic events are outside of the standard WMI model, having a WMI provider is mandatory. We shall see more examples of this in the later posts.\nThis concludes today’s post. I just introduced you to WMI events, showed some basic examples of using Register-WMIEvent cmdlet, and provided an introduction to different event types. In the next post, I shall talk about WQL syntax for writing event queries and then move on to showing some examples for intrinsic WMI events. Stay tuned.!\n","date":"2010-11-15T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/wmi-query-language-wql--event-queries-introduction/","series":"WQL via PowerShell","title":"WMI Query Language (WQL) – Event Queries: Introduction"},{"categories":["PowerShell","WQL","WMI"],"content":"Per MSDN documentation, the REFERENCES OF statement\nRetrieves all association instances that refer to a particular source instance. The REFERENCES OF statement is similar to the ASSOCIATORS OF statement in its syntax. However, rather than retrieving endpoint instances, it retrieves the intervening association instances.\nThat is very cryptic for beginners like you and me. So, let us look at an example to understand this.\nIf you look at the above diagram (captured from the associations tab of Win32_Process in CIM Studio) and as I showed you in my earlier post, Win32_SessionProcess, in32_NamedJobObjectProcesses, Win32_SystemProcesses are the associator or association classes. Whereas, Win32_Process, Win32_LogonSession, Win32_NamedObject, and Win32_ComputerSystem are the associated classes.\nNow, let us go back to the definition. The REFERENCES OF statement retrieves all association instances that refer to a particular source instance. However, it retrieves only the intervening association instances. Again, I will show you an example to explain this.\nIn the above screen capture, you can see the syntax for using REFERENCES OF is exactly same as that of ASSOCIATORS Of. If you closely observe the output, it is more or less same as the associator classes listed in the CIM Studio output. If you look at the properties of each WMI class listed there, you will find “Antecedent”, “Dependent”, “GroupComponent”, and “PartComponent”. These are called references. They can be identified by type ref.\nNote: There is no Win32_NamedJobObjectProcesses in the output here and there is an additional CIM_ProcessExecutable. There is a missing link I am trying to find. I will update this soon.\nIf we had used “Associators of” instead of “References Of”, we would have seen the endpoint class definitions which are Win32_LogonSession, Win32_ComputerSystem, and Win32_NamedJobObject. You can see that in the output here.\nSimilar to Associators Of keyword, you can use the WHERE clause with “References Of” keyword also. There are predefined keywords that you can use with WHERE clause. They are:\nREFERENCES OF {ObjectPath} WHERE ClassDefsOnly RequiredQualifier = QualifierName ResultClass = ClassName Role = PropertyName\nWe have already seen an example of using ClassDefsOnly. Also, usage of these keywords is similar to what I mentioned an earlier post. This concludes the WQL “Data or Object” queries part of this series. In the next part, we shall look at how to perform event queries and then proceed on to schema queries to end this series. I will also try to put together some “real” world examples for “Associators Of” and “References Of” keywords. Whatever we discussed around these two keywords has been really generic examples and may not have provided lot of insight in to where exactly you can use these.\nI hope you find this useful and I am looking for your feedback on how to improve. Thanks again for reading.\n","date":"2010-11-11T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/wmi-query-language-wql-data-queries-references-of/","series":"WQL via PowerShell","title":"WMI Query Language (WQL) – Data Queries: References Of"},{"categories":["PowerShell","WQL","WMI"],"content":"As we saw in the previous post, Select queries can be used to retrieve instances of WMI class. But select queries are not the only way to query for instances. We can also use Associators Of keyword to the same. However, there is a difference. Select queries always return a collection of instances of a WMI class where as “Associators Of” returns a collection of WMI objects that belong to different WMI classes or associated WMI classes. Before we dig too much in to this, let us first understand what are associated WMI classes.\nTake an example of a network adapter.WMI has several classes that represent network adapter information. Let us look at Win32_NetworkAdapter. This WMI class is associated with Win32_NetworkAdapterConfiguration, Win32_NetworkProtocol, and Win32_SystemDriver.\nIf you look at the above output (from CIM Studio’s association tab for a selected class), you will see that Win32_NetworkAdapterconfiguration is associated to Win32_NetworkAdapter through an association class named Win32_NetworkAdapterSetting. And, the other two classes — Win32_NetworkProtocol \u0026amp; Win32_SystemDriver — are associated through an association class named Win32_ProtocolBinding. Make a note of the terminology I used here: associated Class \u0026amp; association class. We will re-visit this later in this post.\nSo, the basic syntax of this “Associators Of” keyword is:\nASSOCIATORS OF {ObjectPath}\nNote that the braces are part of the syntax. Any valid object path can be used for ObjectPath. Let us look at an example to understand this.\n1 Get-WmiObject -Query \u0026#34;Associators Of {Win32_NetworkAdapter.DeviceID=12}\u0026#34; The above snippet shows the basic usage of Associators Of. Make a note of the syntax inside curly braces. This query — when executed — gets all the instances of all associated classes (see above screenshot). So, this can take a while and the output can be overwhelming. Remember, this query without DeviceID=12 will not return anything. We have to specify a qualifier to get the associated instances. This qualifier can be any property from the source WMI class.\nThe output of above query can be overwhelming as the number of associated classes is very large and each associated class may have more than one instance. You can use WHERE clause to filter this out. However, the usage of WHERE clause is a bit different from how you do that with SELECT queries. There are predefined keywords that you can use with WHERE clause. They are:\nASSOCIATORS OF {ObjectPath} WHERE AssocClass = AssocClassName ClassDefsOnly RequiredAssocQualifier = QualifierName RequiredQualifier = QualifierName ResultClass = ClassName ResultRole = PropertyName Role = PropertyName\nNote: You cannot use logical operators such as AND, OR, and NOT within the WHERE clause while using Associators Of keyword. You can use more than one subclause by just separating them by a space.\nLet us see the examples for some of these now.\nClassDefsOnly\nLet us first see a way to list only the associated class names as shown in the screenshot above. You can use the subclause ClassDefsOnly for this purpose.\n1 Get-WMIObject -Query \u0026#34;Associators Of {Win32_NetworkAdapter.DeviceID=12} WHERE ClassDefsOnly\u0026#34; This will list all the associated class names and methods \u0026amp; properties associated with each of those classes.\nAssocClass\nIf you want to retrieve the instance of associated class though a single association class:\n1 Get-WmiObject -Query \u0026#34;Associators of {Win32_NetworkAdapter.DeviceID=12} WHERE AssocClass=Win32_ProtocolBinding\u0026#34; This will result in the driver \u0026amp; protocol information bound to network adapter with device ID 12.\nResultClass\nThis subclause indicates that you want to retrieve the end points associated only with the specified ResultClass. For example,\n1 Get-WMIObject -Query \u0026#34;Associators Of {Win32_NetworkAdapter.DeviceID=12} WHERE ResultClass=Win32_NetworkAdapterConfiguration\u0026#34; Why the heck do you need associators of?\nTechnically, you don’t need to use this keyword. You can script or parse away to glory to get the same results as what “Associators Of “keyword can do for you. Take an example of Win32_NetworkAdapter class itself. Win32_NetworkAdapter stores the physical adapter details and Win32_NetworkAdapterConfiguration stores the software (IP) configuration pertaining to each adapter instance. These two classes are associated using the Win32_NetworkAdapterSetting association class. To get the IP configuration information without using “Associators of” keyword is not a straight forward task. In layman terms, this is because there is no common key between WIn32_NetworkAdapter and Win32_NetworkAdapterConfiguration. You may think that you can use Win32_NetworkAdapterSetting class to retrieve the information. However, when you run\n1 Get-WMIObject -Class Win32_NetworkAdapaterSetting you will see lot of information related to all network adapters in the system but there is no easy way to filter out that for a specific device. Now, if you use “Associators Of” keyword, you can get the IP information by running\n1 Get-WMIObject -Query \u0026#34;Associators Of {Win32_NetworkAdapter.DeviceID=12} WHERE ResultClass=Win32_NetworkAdapterConfiguration\u0026#34; Or\n1 Get-WMIObject -Query \u0026#34;Associators Of {Win32_NetworkAdapter.DeviceID=12} WHERE AssocClass=Win32_NetworkAdapterSetting\u0026#34; You may now ask me why not just run “gwmi Win32_NetworkAdapaterConfiguration -Filter ‘Index=12′”. Yes, you can. This is probably the simplest way to get IP information without what is shown above. The examples above are intended only to show the usage of “Associators Of” keyword.\nOn the same lines, let us look at another example. For a moment think that Get-Service cmdlet never existed. Now, if you want to get the dependent services of any specific service without using “associators Of”, you would do that by parsing Win32_DependentService instances. However, using the keyword discussed today, you can the do same by\n1 Get-WMIObject -Query \u0026#34;Associators of {Win32_Service.Name=\u0026#39;Winmgmt\u0026#39;} Where ResultRole=Dependent\u0026#34; To get a list of services that must be started before starting a specific service,\n1 Get-WMIObject -Query \u0026#34;Associators of {Win32_Service.Name=\u0026#39;Winmgmt\u0026#39;} Where ResultRole=Antecedent\u0026#34; To get a list of both dependent and required services\n1 Get-WMIObject -Query \u0026#34;Associators of {Win32_Service.Name=\u0026#39;Winmgmt\u0026#39;} Where AssocClass=Win32_DependentService\u0026#34; I shall conclude today’s post here and discuss “References Of” in the next post. I intentionally left a few Sub clauses for you to figure out yourself.\n","date":"2010-09-13T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/wmi-query-language-wql-data-queries-associators-of/","series":"WQL via PowerShell","title":"WMI Query Language (WQL) – Data Queries: Associators Of"},{"categories":["PowerShell","WQL","WMI"],"content":"In this part of the series on WQL, we will look at what are data queries and how some of the WQL keywords \u0026amp; operators can be used to retrieve information from WMI repository. Also, as mentioned earlier, there are many other tools that consume WQL queries to retrieve information from WMI. However, in this series, I shall use only PowerShell to demonstrate WQL.\nWQL data queries are the most simplest form of querying for WMI data. Data queries are used to retrieve class instances and data associations. For example,\n1 Get-WmiObject -Query \u0026#34;Select * FROM Win32_Process WHERE HandleCount\u0026gt;=5500\u0026#34; gives us a list of all processes with a handle count above 5500.\nNote You can use -filter parameter to Get-WMIObject instead of -Query. Whatever you pass as a value to -filter will be used within the WHERE clause of a WQL statement. For example, the above PowerShell example can be re-written as\n1 Get-WmiObject -Class Win32_Process -Filter \u0026#34;HandleCount\u0026gt;=5500\u0026#34; So, here is the difference between using -Query or -Filter. You can pass any valid WQL statement as a value to -Query parameter. However, whatever value you pass as a value to -filter will always be used within the WHERE clause. So, the value to -filter must be a valid WHERE clause value with proper use of WQL operators. For example, using a PowerShell comparison operator such as -eq is not valid within -Filter.\nLet us use this example and discuss a few keywords.\nSELECT In the above WQL query, we used SELECT statement. This statement returns instances of the specified class and any of its subclasses. As a general practice, many people retrieve WMI data by specifying SELECT * FROM . By using *, we retrieve all possible properties of a given WMI class. This type of query can take a while to execute and consume more bandwidth to retrieve the result set. One method to reduce the bandwidth required to retrieve the result set is to replace * with selected set of property names.\n1 Get-WmiObject -Query \u0026#34;SELECT Name FROM Win32_Process WHERE HandleCount\u0026gt;=5500\u0026#34; OR\n1 Get-WmiObject -class Win32_Process -Filter \u0026#34;HandleCount\u0026gt;=5500\u0026#34; -Property Name FROM FROM statement is used to specify the class from which we need to create the instances. Remember that you can perform data queries only from one class at a time. For example, the following query will produce an invalid query error:\n1 Get-WmiObject -Query \u0026#34;Select * from win32_Service, Win32_Process\u0026#34; WHERE As you may be familiar by now, WHERE keyword is used to narrow the scope of retrieved data based on a filter. This keyword can be used in all of the three query types. In general, WHERE clause when used with SELECT statement can take one of the following forms:\nSELECT * FROM class WHERE property operator constant SELECT * FROM class WHERE constant operator property\nIn the above two forms, property denotes a valid property of a WMI instance, operator is any valid WQL operator and constant must be of the correct type for the property. We have already seen an example of the first form of using WHERE. Here is an example for the second form. The following query retrieve all services in stopped state.\n1 Get-WmiObject -Query \u0026#34;SELECT Name FROM Win32_Service WHERE \u0026#39;Stopped\u0026#39;=State\u0026#34; Multiple groups of properties, operators, and constants can be combined in a WHERE clause using logical operators such as AND, OR, and NOT. Here are a few examples to demonstrate these.\n1 Get-WMIObject -Query \u0026#34;Select * from Win32_Service Where State=\u0026#39;Running\u0026#39; AND StartMode=\u0026#39;Manual\u0026#39;\u0026#34; 1 Get-WMIObject -Query \u0026#34;SELECT * FROM Win32_LogicalDisk WHERE Name = \u0026#39;C:\u0026#39; OR Name = \u0026#39;D:\u0026#39;\u0026#34; 1 Get-WMIObject -Query \u0026#34;SELECT * FROM win32_Service WHERE NOT (State=\u0026#39;Stopped\u0026#39;)\u0026#34; The above three examples show using AND, OR, and NOT logical operators with WHERE clause to perform multiple filters. In the third example, we can replace NOT with other WQL operators. For example, the same query can be performed in the following ways:\n1 2 Get-WMIObject -Query \u0026#34;SELECT * FROM win32_Service WHERE State\u0026lt;\u0026gt;\u0026#39;Stopped\u0026#39;\u0026#34; Get-WMIObject -Query \u0026#34;SELECT * FROM win32_Service WHERE State!=\u0026#39;Stopped\u0026#39;\u0026#34; You can also use IS, IS NOT operators within WHERE clause. However, the query will be valid only if the constant is NULL. For example,\n1 Get-WMIObject -query \u0026#34;SELECT * FROM Win32_LogicalDisk WHERE FileSystem IS NULL\u0026#34; is valid and will return the disk drive information with no file system information. However, the following example,\n1 Get-WMIObject -query \u0026#34;SELECT * FROM Win32_LogicalDisk WHERE DriveType IS 5\u0026#34; will result in an invalid query error.\nThere are many other keywords such as REFERENCES OF, ASSOCIATORS OF within the context of data queries. To keep these posts short, I will end today’s post here and discuss a few more keywords in the next post. As usual, please leave your feedback here on what can be improved and what else you want to see.\n","date":"2010-08-03T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/wmi-query-language-wql-data-queries-select-from-and-where/","series":"WQL via PowerShell","title":"WMI Query Language (WQL) – Data Queries: SELECT, FROM, and WHERE"},{"categories":["PowerShell","WQL","WMI"],"content":"In this post, we will look at the a brief description of WQL keywords and operators and see a classification of the keywords based on where (query types) these keywords can be used.\nKeywords Similar to SQL, WQL queries use keywords to retrieve data from the management objects. WQL has 19 keywords to perform these queries against WMI repositories. In the previous post, we discussed about three types of queries: Data, Event, and Schema. Though there are 19 WQL keywords, only a few of them can be used all 3 possible query types. The following table lists all the WQL keywords and lists the query type in which they can be used.\nKeyword Query Type Description Data Schema Event AND X X Combines two Boolean expressions, and returns TRUE when both expressions are TRUE. ASSOCIATORS OF X X Retrieves all instances that are associated with a source instance. Use this statement with schema queries and data queries. __CLASS X X References the class of the object in a query. FROM X X X Specifies the class that contains the properties listed in a SELECT statement. Windows Management Instrumentation (WMI) supports data queries from only one class at a time. GROUP X Causes WMI to generate one notification to represent a group of events. HAVING X Filters the events that are received during the grouping interval that is specified in the WITHIN clause. IS X X Comparison operator used with NOT and NULL. The syntax for this statement is the following: IS [NOT] NULL (where NOT is optional) ISA X X X Operator that applies a query to the subclasses of a specified class KEYSONLY X Used in REFERENCES OF and ASSOCIATORS OF queries to ensure that the resulting instances are only populated with the keys of the instances, which reduces the overhead of the call. LIKE X Operator that determines whether or not a given character string matches a specified pattern. NOT X Comparison operator that use in a WQL SELECT query NULL X Indicates an object does not have an explicitly assigned value. NULL is not equivalent to zero (0) or blank. OR X Combines two conditions. When more than one logical operator is used in a statement, the OR operators are evaluated after the AND operators. REFERENCES OF X X Retrieves all association instances that refer to a specific source instance. Use this statement with schema and data queries. The REFERENCES OF statement is similar to the ASSOCIATORS OF statement. However, it does not retrieve endpoint instances; it retrieves the association instances. SELECT X X X Specifies the properties that are used in a query. TRUE X X Boolean operator that evaluates to -1 (minus one). WHERE X X X Narrows the scope of a data, event, or schema query. WITHIN X Specifies a polling or grouping interval. FALSE X X X Boolean operator that evaluates to 0 (zero). Operators WMI Query Language also uses operators. The following table lists all the opeartors supported in WQL.\nOperator Description = Equal to \u0026lt; Less than \u0026gt; Greater than \u0026lt;= Less than or equal to \u0026gt;= Greater than or equal to != or \u0026lt;\u0026gt; Not equal to A few WQL keywords such as IS, ISA, NOT, and LIKE can also be considered as operators. In these keywords, IS and IS NOT operators are valid in the WHERE clause only if the constant is NULL. We will see a detailed discussion around these keywords \u0026amp; operators in the upcoming posts.\n","date":"2010-08-02T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/wmi-query-language-wql-keywords-and-operators/","series":"WQL via PowerShell","title":"WMI Query Language (WQL) – Keywords and Operators"},{"categories":["PowerShell","WQL","WMI"],"content":"I have been using WMI a lot these days and got to play with WQL as well. In this series of posts, I want to write about how WMI Query Language (WQL) can be used to retrieve management data exposed by WMI. Yes, this has nothing to do with PowerShell but as a PowerShell lover (and a MVP now), I will use PowerShell for all my examples.\nWindows Management Instrumentation (WMI) is the Microsoft implementation of Web-based Enterprise Management (WBEM), which is an industry initiative to develop a standard technology for accessing management information in an enterprise environment. WMI uses the Common Information Model (CIM) industry standard to represent systems, applications, networks, devices, and other managed components. CIM is developed and maintained by the Distributed Management Task Force (DMTF). We can write WMI scripts to automate several tasks on local or remote computer(s).\nPowerShell has a few cmdlets to retrieve the management data exposed by WMI. You can see these cmdlets by running:\n1 2 #Use Get-Command and mention WMI* as the Noun Get-Command -Noun WMI* One of the cmdlets to retrieve WMI information is Get-WMIObject. In it’s basic usage, this cmdlet, gets the instance of a specified WMI class. So, for example, if you need to list out all drives of type 4 (network drives) in a system,\n1 2 3 Get-WmiObject -Class Win32_LogicalDisk | Where-Object { $_.DriveType -eq 4 } In the above method, we retrieve all instances of Win32_LogicalDisk and then pass it to Where-Object to filter out what we need. Depending on how many instances are there, this can take a while. You can use an alternative approach by specifying the -Query parameter instead of -Class.\n1 2 #This example uses -Query parameter and specifies the query using WQL Get-WmiObject -Query \u0026#34;Select * from Win32_LogicalDisk WHERE DriveType=4\u0026#34; The above example uses WMI Query Language to get the same information as the earlier example but a bit more faster. You can, of course, verify that using Measure-Command cmdlet. You can see the clear difference here.\nThe above example is very basic and may not really explain the usefulness of WQL — the speed of execution is just one benefit. When using WQL, there are quite a few advanced querying techniques that can be used to retrieve WMI information in an efficient manner. And, sometimes – such as working with WMI events, WQL becomes a necessity. So, this series of posts will explain each of those scenarios and with some appropriate examples.\nWith that background, let us now look at WMI Query Language.\nThe WMI Query Language (WQL) is a subset of the American National Standards Institute Structured Query Language (ANSI SQL)—with minor semantic changes. Similar to SQL, WQL has a set of keywords and operators. WQL supports three types of queries\nData Queries This type is the most simplet form of querying for WMI data. The earlier example, where we queried for all instances of Win32_LogicalDisk where the driveType is 4, is a data query. Data queries are used to retrieve class instances and data associations. The WQL keywords such as SELECT, ASSOCIATORS OF, REFERENCES OF, and ISA are used in data queries.\nSchema Queries Schema queries are used to retrieve class definitions (rather than class instances) and schema associations. In layman’s terms, these queries are used to get information about WMI and its structure. Schema queries return a result set of class definition objects rather than actual instances of classes. The WQL keywords such as SELECT, ASSOCIATORS OF, REFERENCES OF, and ISA are used in schema queries and of course, in a slightly different way than how data queries use these keywords.\nEvent Queries The event queries are used to create WMI event subscriptions. For example, using these queries, you can create an event subscription to notify whenever a USB drive gets attached to the system. The WQL keywords such as GROUP, HAVING, and WITHIN are used (and are specific to) when creating event queries. The event queries are critical when you want use PowerShell cmdlets such as Register-WMIEvent, etc.\nAnd, finally, a couple of items you should know\nWQL does not support cross-namespace queries or associations. You cannot query for all instances of a specified class residing in all of the namespaces on the target computer. WQL queries are read-only. There are no keywords such as INSERT or UPDATE. Using WQL, we cannot modify the WMI objects. This brings us to the end of this post on WQL introduction. In the subsequent posts, we will look at each of the WQL keywords and each WQL query type. I am also learning as I write this series. So, I welcome your feedback.\n","date":"2010-07-22T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/wmi-query-language-wql-an-introduction/","series":"WQL via PowerShell","title":"WMI Query Language (WQL) – An introduction"},{"categories":["PowerShell","Bangalore IT Pro","Presentations"],"content":"I was at Bangalore ITPro BarCamp today and did a session on “PowerShell for Active Directory”. Here are the slides from that session.\nSlideShare Presentation This presentation requires your consent to load content from SlideShare (LinkedIn), which may set tracking cookies. Enable SlideShare Presentation ID: ETmDJt3jFoSlGQ ","date":"2010-05-15T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/slides-from-bangalore-itpro-barcamp-powershell-for-activedirectory-administration/","series":null,"title":"Slides: Bangalore ITPro - PowerShell for ActiveDirectory Administration"},{"categories":["PowerShell","Bangalore IT Pro","Presentations"],"content":"We had the BITPro / PowerShell Bangalore User Group UG meet today. I presented PowerShell 2.0 remoting. Here are the slides I used at this UG meet.\nSlideShare Presentation This presentation requires your consent to load content from SlideShare (LinkedIn), which may set tracking cookies. Enable SlideShare Presentation ID: wxuMRDES5Q3sCL ","date":"2010-05-15T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/psbug-ug-meet-slides-powershell-2-0-remoting/","series":null,"title":"Slides: PSBUG Meet - PowerShell 2.0 remoting"},{"categories":["PowerShell","Bangalore IT Pro","Presentations"],"content":"I just came back from the PowerShell / Bangalore ITPro UG meet. We had nearly 40 people showing up for this event. I spoke about getting started with PowerShell scripting and Ismail talked about Exchange 2010 migration. Here are the slides I used for my session.\nSlideShare Presentation This presentation requires your consent to load content from SlideShare (LinkedIn), which may set tracking cookies. Enable SlideShare Presentation ID: JDWNWgF5R8BL60 If you were there at the session, please provide your feedback either here or as a tweet to @psbug. See you at the next meet.\n","date":"2010-03-13T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/slides-getting-started-with-powershell-scripting/","series":null,"title":"Slides: Getting Started with PowerShell scripting"},{"categories":["PowerShell","Remoting"],"content":"In this part of the remoting series, we look at how CredSSP can be used for multi-hop authentication in PowerShell remoting. CredSSP and multi-hop support are not features of PowerShell 2.0 or PowerShell remoting, per se. Credential Security Service Provider (CredSSP) is a new security service provider that enables an application to delegate the user’s credentials from the client to the target server. Multi-hop support in Windows Remote Management uses CredSSP for authentication. Since PowerShell 2.0 remoting is built on top of WinRM, we can use CredSSP to perform multi-hop authentication.\nSo, what is multi-hop authentication?\nWell, let us look at an example to understand what is multi-hop authentication. Imagine a group of computers as shown here and you establish a remoting session from computer A (client) to computer B (server) and then from computer B, you try to create a file in a file share on computer C.\nNow, within the remoting session to computer B, we want to execute a command — as below — to create test.txt on computer C.\n1 Invoke-Command -ComputerName Test-PC.SP2010lab.com -credential SP2010LAB\\Administrator -ScriptBlock {[System.IO.File]::Create(\\\\FileServer\\Share\\Test.txt)} This command results in an “Access Denied” error as shown above. This command fails since the remote session tries to access the file share using the machine credentilas instead of the credentials used to invoke the remote session. We could have successfully created the text file if there was a way to pass or delegate credentials from the client so that we can authenticate to the file share. This is what is called multi-hop authentication and PowerShell remoting enables this using CredSSP.\nHow do we delegate credentials?\nThe cmdlets to create a remoting session — Invoke-Command, Enter-PSSession and New-PSSession — have a parameter to specify the authentication method as CredSSP. However, before we use this parameter, we need to enable credSSP on the computers participating in multi-hop authentication. Also, when enabling CredSSP, we need to specify the role — client or server — of a computer. A client is the computer from which the remoting session is initiated and server is the computer from which the multi-hop authentication is triggered. So, from the above example, we need to enable CredSSP authentication on computer A and computer B.\nPowerShell 2.0 has Enable-WSManCredSSP, Disable-WSManCredSSP and Get-WSMANCredSSP cmdlets to manage CredSSP authentication\nLet us now look at how we enable WSManCredSSP and specify client / server roles. First, let us enable CredSSP on computer A.\nNote: You need to run these cmdlets in an elevated prompt.\n1 Enable-WSManCredSSP -Role Client -DelegateComputer \u0026#34;*.SP2010lab.com\u0026#34; As shown here, you can use Enable-WSManCredSSP cmdlet to enable CredSSP authentication and specify the computer role as client. When the computer role is defined as a client, you can also specify the DelegateComputer parameter to specify the server or servers that receive the delegated credentials from the client. The delegateComputer accepts wildcards as shown above. You can also specify “*” to specify all computers in the network.\nWhen Enable-WSManCredSSP cmdlet is used to enable CredSSP on the client by specifying client in the role parameter. The cmdlet then performs the following:\nThe WS-Management setting \u0026lt;localhost|computername\u0026gt;\\Client\\Auth\\CredSSP is set to true. Sets the Windows CredSSP policy AllowFreshCredentials to WSMan/Delegate on the client. Now, we will enable CredSSP on computer B and deginate that as server.\n1 Enable-WSManCredSSP -Role Server The above cmdlet enables CredSSP on computer B and sets the WS-Management setting \u0026lt;localhost|computername\u0026gt;\\Service\\Auth\\CredSSP is to true. Now, we can use Invoke-Command to run the script block as shown at the beginning of this post. However, we will specify the authentication method as CredSSP and pass the credentials.\n1 Invoke-Command -ComputerName test-pc.SP2010lab.com -Credential SP2010Lab\\Administrator -Authentication CredSSP -ScriptBlock {[System.IO.File]::Create(\\\\FileServer\\Share\\Test.txt)} As you see here, we see the output from Create() method which shows the details of the newly created file.\nCaution: CredSSP authentication delegates the user’s credentials from the local computer to a remote computer. This practice increases the security risk of the remote operation. If the remote computer is compromised, when credentials are passed to it, the credentials can be used to control the network session.\nYou can use Disable-WSManCredSSP to disable CredSSP authentication on a client or a server computer.\n1 2 Disable-WSManCredSSP -Role Client Disable-WSManCredSSP -Role Server You can use Get-WSManCredSSP cmdlet to verify if a computer has CredSSP enabled and also the role (client/server).\nThis is it for now. We will look at few more aspects of PowerShell remoting in the next part of this series. Stay tuned..!\n","date":"2010-02-18T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/powershell-2-0-remoting-guide-part-12--using-credssp-for-multi-hop-authentication/","series":"PowerShell Remoting","title":"PowerShell 2.0 remoting guide: Part 12 – Using CredSSP for multi-hop authentication"},{"categories":["PowerShell","Remoting"],"content":"In this part of the remoting series, we look at remoting output. This includes how the output is transferred from remote computer to local, how it is displayed and how we can format this output based on a need. We already discussed various methods to execute commands (part4, part 5 and part 6) on a remote computer. In this post, for the sake of our discussion of remoting output, I will use only Invoke-Command method to execute remote commands. However, I will point out the differences as required.\nNote: Most of this does not apply within an interactive remoting session\nThe concepts of remoting output are explained in a TechNet article at http://technet.microsoft.com/en-us/library/dd347582.aspx. I am going to put some story around this to help you understand the concepts well.\nFirst, let us start with an obvious difference in the output received from a remote session. If you use Invoke-Command to run Get-PSDrive, you see something like this.\nYou can see an additional column in the output that shows the remote computer name with PSComputerName as the column name. This won’t be displayed if you run the same cmdlet on local computer. So, if you don’t want to display this information in the remote output you can use the -HideComputerName parameter.\nIt is also possible that some cmdlets may not display PSComputerName property. For example, Get-Date. In such a scenario you can add PSComputerName to the output of Get-Date as shown here\n1 Invoke-Command -ComputerName SP2010-WFE,SP2010-APP -ScriptBlock {Get-Date} | ft DateTime, PSComputerName -Auto How remote command output is sent over to local computer?\nThe objects that Windows PowerShell cmdlets return cannot be transmitted over the network. So, the live objects are “serialized”. In other words, the live objects are converted into XML representations of the object and its properties. Then, the XML-based serialized object is transmitted across the network to the local session where it gets deserialized in to .NET object. This is how an MSDN article defines serialization in .NET framework.\nWhy would you want to use serialization? The two most important reasons are to persist the state of an object to a storage medium so an exact copy can be recreated at a later stage, and to send the object by value from one application domain to another. For example, serialization is used to save session state in ASP.NET and to copy objects to the clipboard in Windows Forms. It is also used by remoting to pass objects by value from one application domain to another.\nSo, what does this mean to us in PowerShell remoting?\nAs it is defined above, the live objects are converted in to XML based representation. So, once deserialized in the local session, they don’t expose any methods that actually belong to the object. Let us see an example to understand this. First, let us look at Get-Process output in a local session and see what all methods we see.\nHere, you can see a list of methods you can use against a process object. Now, let us take a look at how this looks when we execute the same command in a remote session.\nIf you observe in the above screenshot, TypeName represents a deserialized object and there are no methods that you can use against a process object. A deserialized object represents a snapshot of get-process at the time of command execution in the remote session. This also means that you can’t execute methods such as Kill() against a deserialized process object. Also, no methods to modify the property set will work in the local session.\nWindows PowerShell blog has a nice post on how objects are to and from a remote session. I recommend that you read this post for more information.\nWhat about output formatting?\nMost deserialized objects are automatically formatted for display by entries in the Types.ps1xml or Format.ps1xml files. However, the local computer might not have formatting files for all of the deserialized objects that were generated on a remote computer. When objects are not formatted, all of the properties of each object appear in the console in a streaming list. To get formatting data from another computer, use the Get-FormatData and Export-FormatData cmdlets. Again, let us take an example to understand this.\nTake an example of a SharePoint 2010 farm and you want to access /run SharePoint 2010 cmdlets from a Windows 7 machine using Invoke-Command. First, if we run Get-SPSite on SharePoint 2010 web frontend, you will see\nNow, if we try to run the same in a remote session using Invoke-Command, you will see\nAs you see in the above screenshot, the output from a remote session is quite different from the one you saw in a local session. This is because we don’t have the formatting data available on the Windows 7 computer.\nSo, how do we get the formatting data to local computer?\nWe can use Get-FormatData, Export-FormatData and Update-FormatData cmdlets to get the formatting data from a remote computer to local session. To do this:\n1 2 3 4 $s = New-PSSession -ComputerName SP2010-WFE Invoke-Command -session $s -ScriptBlock {Add-PSSnapin Microsoft.SharePoint.PowerShell} Invoke-Command -Session $s -ScriptBlock {Get-FormatData -TypeName *SharePoint*} | Export-FormatData -Path C:\\scripts\\SharePoint.Format.ps1xml Update-FormatData -PrependPath C:\\scripts\\SharePoint.Format.ps1xml The above code snippet will let you import the formatting data for all SharePoint cmdlets in to the local session. Now, if we run Get-SPSite in the remote session using Invoke-Command, you will see\nNow, with the formatting information in the local session, you can see that Get-SPSite output is formatted similar to the one we saw when we ran the cmdlet in a local session. However, make a note that this applies only to the current session. If you close and re-open the PowerShell console, the formatting data will be lost. You can add the Update-FormatData cmdlet to your PowerShell profile to make the format data across all PowerShell sessions.\nThis is it. We are just a few more posts away from completing this PowerShell remoting series. Do let me know your feedback and let me know if you want to see something more around remoting.\n","date":"2010-02-15T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/powershell-2-0-remoting-guide-part-11--interpreting-formatting-and-displaying-remote-output/","series":"PowerShell Remoting","title":"PowerShell 2.0 remoting guide: Part 11 – Interpreting, formatting and displaying remote output"},{"categories":["PowerShell","Remoting"],"content":"“With great power comes great responsibility”, said uncle Ben. But some people don’t just understand that. That is when you have to rip-off their powers. Similarly, the default PS Session configuration allows full access to PowerShell language, cmdlets, scripts and everything available to PowerShell. Of course, you need to authenticate as a local administrator or should have execute permission to invoke the session. Running a few cmdlets such as Stop-Service or Restart-Computer can be quite dangerous on a production server. This is where a custom session configuration can help provide role based access to remote host using PowerShell remoting.\nWe touched upon creating custom session configuration in part 9 of this PowerShell remoting series. In this part, I will discuss how we can extend the concept of custom session configuration to restrict available commands and PowerShell language in a remote session. I will go striaght in to the startup script used to implement this since we already looked at how to create custom session configuration and assign permissions to a specific user.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 $RequiredCommands = @(\u0026#34;Get-Command\u0026#34;, \u0026#34;Get-FormatData\u0026#34;, \u0026#34;Out-Default\u0026#34;, \u0026#34;Select-Object\u0026#34;, \u0026#34;out-file\u0026#34;, \u0026#34;Measure-Object\u0026#34;, \u0026#34;Exit-PSSession\u0026#34; ) $ExecutionContext.SessionState.Applications.Clear() $ExecutionContext.SessionState.Scripts.Clear() Get-Command -CommandType Cmdlet, alias, function | ?{$RequiredCommands -notcontains $_.Name} | %{$_.Visibility=\u0026#34;Private\u0026#34;} $ExecutionContext.SessionState.LanguageMode=\u0026#34;RestrictedLanguage\u0026#34; As you see here, We have only a few required commands. We don’t want the remote user to execute commands other than this set. BTW, this set is the absolute minimum required even to start remoting session. So, consider this as a standard required commands list. Towards the end, we set the language mode to restricted to make sure the remote user cannot execute infinite loops, etc that could potentially bring the system down. This script, when used as the startup script for a session, will result in something as shown here.\nAs you see above, get-Command lists only the commands we have in the Required Commands list. However, if you have a large list of required commands, the method you have seen in the above code is not scalable. Instead, you can use a denied list of commands that is relatively small. For example, if you don’t want your users to execute Stop-Process or Restart-Computer, your code will look like\n1 2 3 4 5 6 7 8 9 $DeniedCommands = @(\u0026#34;Stop-Process\u0026#34;, \u0026#34;Restart-Computer\u0026#34; ) $ExecutionContext.SessionState.Applications.Clear() $ExecutionContext.SessionState.Scripts.Clear() Get-Command -CommandType Cmdlet, alias, function | ?{$DeniedCommands -contains $_.Name} | %{$_.Visibility=\u0026#34;Private\u0026#34;} $ExecutionContext.SessionState.LanguageMode=\u0026#34;RestrictedLanguage\u0026#34; So, if you use this code for your startup script, you will see something like this:\nI prefer the second method.\nIf you need to extend or modify the behavior of commands in a remote session, you need to create command proxies. You can read more about it @ http://blogs.msdn.com/powershell/archive/2009/01/04/extending-and-or-modifing-commands-with-proxies.aspx.\nWhat I have shown here is just one way of achieving control in the remote sessions. However, based on your organization needs there could be a better way of doing this. These methods include user role based restrictions, etc as discussed at a PDC’09 session. Do refer to that for more information.\nThis is it for now. We will look at the remote session configuration options in the next post. Stay tuned..!\n","date":"2010-02-11T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/powershell-2-0-remoting-guide-part-10--restricting-available-commands-using-custom-session-configuration/","series":"PowerShell Remoting","title":"PowerShell 2.0 remoting guide: Part 10 – Restricting available commands using custom session configuration"},{"categories":["PowerShell","Remoting"],"content":"In part2 of this series on PowerShell remoting we quickly looked at various cmdlets that form part of overall remoting infrastructure. The list there included cmdlets related to PS Session configuration. Now that we have gone through the basics of remoting, it is time for us to dig in to these additional cmdlets and explore what they really do.\nSo, in this part, we will look at all the PS session configurtion cmdlets, discuss how to create custom PS Session configurations and the need for it. Let us dive in to this now.\nWhat is PS session configuration?\nA session configuration can be used to define who can create a Windows PowerShell sesion on the local computer, what level of access — to cmdlets, scripts and PowerShell language — they have on the local computer, etc. When you enable PowerShell remoting using Enable-PSRemoting, you will see a final step performing Microsoft.PowerShell and Microsoft.PowerShell32 (on x64 systems) session configuration registrations. These default session configurations are used when the remote users connecting to local system do not specify a configuration name. By default, only members of administrators group have access to these two session configurations. Hence, only members of administrators group will be able to create remoting sessions by default.\nBased on the above description, PowerShell session configurations can be used to\nCustomize the remoting experience for users delegate administration by creating session configuration with varying levels of access to system In this part, we will look at basics of session configuration and see how we can create custom session configurations. We will discuss delegated administration at depth in a later post.\nWhat cmdlets are available to manage session configurations?\nThe following cmdlets are available to manage session configuration.\nRegister-PSSessionConfiguration Unregister-PSSessionConfiguration Enable-PSSessionConfiguration Disable-PSSessionConfiguration Set-PSSessionConfiguration Get-PSSessionConfiguration\nHow do I create a new session configuration?\nRegister-PSSessionConfiguration cmdlet can be used to create a new session configuration. You can use a C# asembly or a PowerShell script as a startup script for this new session configuration. This startup script can be used to customize the remoting experience. For example, create a script the imports Active Directory module using import-module cmdlet as shown here.\n1 Import-Module ActiveDirectory Save this script as startupscript.ps1 or any name of your choice on the local computer. Now, use the Register-PSSessionConfiguration cmdlet to create a new session configuration. This can be done by running:\n1 Register-PSSessionConfiguration -Name \u0026#34;ActiveDirectory\u0026#34; -StartupScript C:\\scripts\\StartupScript.ps1 You will be prompted to confirm this action and at the end to restart WinRM service on the local computer.\nNote: You must enable script execution on the local computer to be able to use the startup script as a part of session configuration\nHow do I see available session configurations?\nGet-PSSessionConfiguration cmdlet lists all the available session configurations on the local computer.\nAs you see in the above output, Get-PSSessionConfiguration lists all available session configurations on the local computer and who has permission to access the configuration. No permissions have been assigned yet to the new active directory configuration.\nHow do I give permissions to a session configuration?\nYou can use Set-PSSessionConfiguration to allow access to invoke the new session configuration. To do this,\n1 Set-PSSessionConfiguration -Name ActiveDirectory -ShowSecurityDescriptorUI This opens up the dialog to add permissions to invoke this session configuration. As you see in the screenshot here, administrators group has no invoke permission on this session configuration.\nSelect Allow -\u0026gt; Invoke permission here and click OK. You will be prompted to restart the WinRM service. Now, an administrator or a member of administrators group will be able to use this session configuration.\nHow do I invoke a session configuration?\nYou can use New-PSSession, Enter-PSSession and Invoke-Command cmdlets to load a session configuration othen than the default configuration. The ConfigurationName parameter can be used to specify the session configuration.\n1 2 3 $s = New-PSSession -ComputerName SP2010-WFE -ConfigurationName ActiveDirectory Enter-PSSession -ComputerName SP2010-WFE -ConfigurationName ActiveDirectory Invoke-Command -ComputerName SP2010-WFE -ConfigurationName ActiveDirectory -ScriptBlock {Get-Process} Note: To be able to use the -StartupScript, script execution policy must be set to signed or unrestricted on the local computer where the session configuration is registered.\nIn an earlier post — part 6: Implict remoting sessions — we used Invoke-Command to load the active directory module within a persistent sesion and then use that persistent session to import active directory cmdlets in to local session. However, by using a session configuration that import active directory module as a startup script, we will have all the AD cmdlets available as soon as we connect to the remote session.\nHow do I disable a session configuration?\nYou can use Disable-PSSessionConfiguration cmdlet to disable an existing session configuration and prevents users from connecting to the local computer by using this session configuration. You can use -Name parameter to specify what session configuration you want to disable. If you do not specify a configuration name, the default Microsoft.PowerShell session configuration will be disabled.\nThe Disable-PSSessionConfiguration cmdlet adds a “deny all” setting to the security descriptor of one or more registered session configurations. As a result, you can unregister, view, and change the configurations, but you cannot use them in a session. Disable-PSRemoting cmdlet will disable all PS Session configurations available on the local computer.\nEnable-PSSessionConfiguration cmdlet can be used to enable a disabled configuration. You can use -Name parameter to specify what session configuration you need to enable.\nHow do I delete a session configuration?\nYou can use Unregister-PSSessionConfiguration cmdlet to delete a previously defined session configuration. It is quite possible to delete the default session configuration — Microsoft.PowerShell — using this cmdlet. However, this default session configuration gets re-created when you run Enable-PSRemoting cmdlet.\nThis brings us to the end of this part on PS session configurations. In the next part, I will talk about delegated administration using session configurations.\n","date":"2010-02-09T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/powershell-2-0-remoting-guide-part-9--session-configurations-and-creating-custom-configurations/","series":"PowerShell Remoting","title":"PowerShell 2.0 remoting guide: Part 9 – Session configurations and creating custom configurations"},{"categories":["PowerShell","Remoting"],"content":"If you have been following this remoting series and using PS remoting on a test setup or even production, you must have gone through a few initial hiccups. In fact, there could be many such issues if you are working in a mixed environment where you have a few computers in a domain and a few in workgroup. So, in this part of the remoting series I will discuss some of these issues and workarounds to resolve the same. I discussed enabling remoting on workgroup computers in part3 – “Enable remoting” of this series. This post is kind of an extension to that.\nA more detailed guide is available at about_remote_troubleshooting page on TechNet. I will list only the scenarios I have tested.\nRemoting to a computer in workgroup from a computer in domain\nTake an example of two computers, Win7-VM in a domain environment and WinXP-VM in a workgroup. Now, if you want to start a remoting session from Win7-VM using Enter-PSSession using:\n1 Enter-PSSession -ComputerName WinXP-VM -Credential WinXP-VM\\Administrator you will see an error message similar to the one here\nWe can workaround this by adding the remote computer to local computer’s trusted hosts list — in this case, on Win7-VM. You can use one of the following methods.\n1 Set-Item WSMan:\\localhost\\Client\\TrustedHosts -Value * 1 set-item wsman:\\localhost\\Client\\TrustedHosts -value WinXP-VM Note: WSMan:\\LocalHost\\Client will be available only if WinRM service is enabled and running.\nThe first command uses a wildcard to add all computers in the network to the list of trusted hosts and the second command adds only WinXP-VM to that list. You can also supply a comma seperated list.\nRemoting to a computer in domain from a computer in workgroup\nUsing the same example as above — if you want to establish a remoting session from WinXP-VM to Win7-VM, you can change the trusted hosts list on WinXP-VM to add the computers in the domain. Again, it can be done in multiple ways.\n1 2 3 Set-Item WSMan:\\localhost\\Client\\TrustedHosts -Value * set-item wsman:\\localhost\\Client\\TrustedHosts -value Win7-VM set-item wsman:\\localhost\\Client\\TrustedHosts -value *.remotinglab.com The first two commands variations must be familiar by now. The 3rd variation adds all computers in remotinglab.com domain to the trusted hosts on WinXP-VM computer.\nRemoting to a domain computer as a non-administrator from another domain computer\nTo start a remoting a session with a remote computer, the user initiating the remoting session must\nBe a member of the administrators group on the remote computer or Be able to authenticate to the remote computer as an administrator using the Credential parameter Alternatively — on the remote computer, you can give non-administrator users execute permission to default session configuration. This can be done by running the following command at a elevated PowerShell prompt:\n1 2 3 4 Set-PSSessionConfiguration Microsoft.Powershell -ShowSecurityDescriptorUI #In addition to the above, on a x64 computer Set-PSSessionConfiguration Microsoft.Powershell32 -ShowSecurityDescriptorUI When you use the above cmdlet, you will see a dialog box as shown here.\nHere, you need to click add and then select the non-administrator user account. Once added, give the user Execute permission. After you click OK on the main dialog and return to PowerShell, you will be prompted to restart WinRM service. That is it. I recommend this method than the first two. This way a non-administrator user should be able to remote in to a domain computer without providing administrator credentials or adding the user to local administrators group.\nA few things I discussed here today are really meant for an advanced remoting post. However, for the sake of getting started in a mixed environment, I thought this post is must. Feel free to write any questions you may have in an email or tweet to me.\n","date":"2010-02-07T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/powershell-2-0-remoting-guide-part-8--remoting-scenarios-and-troubleshooting/","series":"PowerShell Remoting","title":"PowerShell 2.0 remoting guide: Part 8 – remoting scenarios and troubleshooting"},{"categories":["PowerShell","Remoting"],"content":"In part 6 of the remoting series we looked at how we can use Import-PSSession cmdlet to execute remote commands as if they were local. This is nice but this will last only while the session ($s in the example) is alive. The moment we kill the session — using Rem0ve-PSSession, remoting session will also get killed. In this part of the series, we will look at how we can save a remoting session to disk so that we don’t even have to explicitly create a session.\nThis is achieved using Export-PSSession cmdlet. This cmdlet lets us import commands from a remote session and save the same in a PowerShell module on the local disk. This cmdlet can get cmdlets, functions, aliases, and other command types in to a PowerShell module. The following example shows how we can achieve this.\n1 2 3 $s = New-PSSession -ComputerName SP2010-WFE Invoke-Command -Session $s -ScriptBlock {Import-Module ActiveDirectory} Export-PSSession -Session $s -OutputModule ADRemoteCommands -AllowClobber -Module ActiveDirectory In the above example, the first two lines should be quite familiar by now. The third line is where the magic happens. We tell Export-PSSession cmdlet to export all the commands, aliases, functions, etc available in PS Session $s to a module on hard disk and name it ADRemoteCommands.\nIf the Export-PSSession is successful, you will see output similar to what is shown here\nIn the above output, it is clear that Export-PSSession generates .psm1, .psd1 and format data file for the module automatically. Now, you can load the module at any later point in time to get access to the remote commands.\nHow do we import the saved module?\nIf you observe the output closely, path where the module files are stored is same as $Env:PSModulePath. So, you don’t need to specify the absolute path to the module.\n1 Import-Module ADRemoteCommands This imports all remote commands available in the module to local session. Whenever we execute a remote command, implicit remoting kicks in, establishes the remote session, executes the command in remote session and returns the output. All this is done without you really using any remoting related cmdlets. If establishing a remote session requires a password, you will be prompted for one.\nThis brings us to the end of Part 7. With this, I covered all the remoting basics and we can now move on to more advanced topics like session options and WSMan configuration required for PS Remoting. Stay tuned for that..!\n","date":"2010-02-04T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/powershell-2-0-remoting-guide-part-7--saving-remote-session-to-disk/","series":"PowerShell Remoting","title":"PowerShell 2.0 remoting guide: Part 7 – saving remote session to disk"},{"categories":["PowerShell","Remoting"],"content":"In an earlier post on interactive remoting sessions, we looked at how we can enter a remote session and then execute commands as if they were local. However, if you’d observed it more closely, we were actually sitting in the remote session than local console. The change in PowerShell prompt indicates this fact clearly.\nIn this part of the remoting series, we will look at a feature called implicit remoting which makes it possible to run the commands / scripts on the remote computer while in the local session.\nWe use interactive remoting to overcome a few disadvantages of using Invoke-Command. This method too has its own drawbacks. Within interactive remoting, you explicitly enter/exit a remote session. This also means that you are connected only to one remote computer and you have access only to the cmdlets or modules available on that remote computer. What if you want to access different cmdlets available on different computers?\nFor example, let us say you have two different computers one with Exchange 2010 and other with SharePoint 2010. Now, if you want to access cmdlets available to manage both these technologies from a “single computer” and in the “local session”. Take a note, “single computer” and “local session” is the key to understand the concept of implicit remoting. The important thing to understand is that we need to manage multiple computers / technologies without ever the need to go out of local PowerShell session.\nUsing Invoke-Command is certainly not the choice because it involves setting up a session to the remote computer and then sending a script block to execute in that session. This is quite tedious. Although interactive remoting can eliminate the drawbacks of Invoke-Command, it is specific one remote session. So, if you are connected to the Exchange 2010 remote session, your SharePoint 2010 session is not available. This is where implicit remoting becomes important.\nImplicit remoting can be used to bring remote commands to a local session. In implicit remoting, once you import remote commands in to a local session, you don’t have to worry about the PS session details. You can import any number of remote sessions in to the local session making it possible to access cmdlets from different product technologies in the same local session. PowerShell will take care of that for you in the background.\nNow, we can connect do a different remote session and import cmdlets from that session also.\n1 2 3 $s = New-PSSession -ComputerName SP2010-WFE Invoke-Command -Session $s -ScriptBlock {Add-PSSnapin Microsoft.SharePoint.PowerShell} Import-PSSession -Session $s Now, within the local session, we have access to AD cmdlets from one computer and SharePoint 2010 cmdlets from another machine. This makes it easy to manage both from the same computer and local session without worrying much about creating / destroying sessions.\nNice. So, how do we use implicit remoting?\nWell, we have to first create a persistent PS session using New-PSSession and then use that to import remote commands in to local session. You can do it as shown here\n1 2 $s = New-PSSession -ComputerName SP2010-WFE Import-PSSession -Session $s By default, Import-PSSession imports all commands except for commands that have the same names as commands in the current session. To import all the commands, use the AllowClobber parameter.\nIf you import a command with the same name as a command in the current session, the imported command hides or replaces the original commands. Essentially, imported commands take precedence over the local commands with same name. Irrespective of whether those commands were loaded after importing a session or before. However, aliases are an exception. Original aliases in the local session take precedence over imported aliases.\nTo know more about the command precedence, read about_Command_Precedence.\nHow do we differentiate between local \u0026amp; remote commands or avoid name conflicts while importing?\nImport-PSSession provide -Prefix parameter which adds the specified prefix to the nouns in the names of imported commands. For example,\n1 Import-PSSession -Session $s -Prefix RS will prefix RS to all the cmdlets imported from a remote computer. So, if Get-Command was imported using this method, the local session will have Get-RSCommand and when you use this cmdlet, PowerShell implicitly runs this command inside the remote session.\nAs we discussed earlier in this post, PowerShell manages implicit remoting in the background. So, the behavior of Invoke-Command — creates/destroys a PS session every time we execute a remote command — exists with implicit remoting too. Hence, you will see that executing remote commands over this method a bit slow. To work around this, import-PSSession adds a -asJob parameter to all the commands imported in to the local session.\nFor example,\n1 2 3 4 $s = New-PSSession -ComputerName SP2010-WFE Import-PSSession -Session $s -Prefix RS Get-RSProcess -asJob will run Get-Process on the remote computer as a background job. The original Get-Process has no -asJob parameter.\nHow do we import modules or snap-ins in to local session?\n1 2 3 $s = New-PSSession -ComputerName SP2010-WFE Invoke-Command -Session $s -ScriptBlock {Import-Module ActiveDirectory} Import-PSSession -Session $s -Module ActiveDirectory In the above example, we first create a PS session, import active directory module using Invoke-Command and then import the session in to the local session. This makes all the active directory cmdlets available in the local session.\nThis is the end of part 6 of the remoting series. In the next post, I will talk about how we can save the imported session to hard disk.\n","date":"2010-02-03T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/powershell-2-0-remoting-guide-part-6-implicit-remoting-sessions/","series":"PowerShell Remoting","title":"PowerShell 2.0 remoting guide: Part 6 – implicit remoting sessions"},{"categories":["PowerShell","Remoting"],"content":"In this part of the remoting series, I will talk about using Enter-PSSession and Exit-PSSession cmdlets to perform interactive remoting.\nWhy do you need interactive remoting?\nTo understand the advantages of interactive remoting in PowerShell 2.0, let us first look at some gotchas with Invoke-Command. Take an example of a remote system where SharePoint 2010 is installed. SharePoint 2010 provides native PowerShell cmdlets and these cmdlets can be accesses only if you load Microsoft.SharePoint.PowerShell PS snap-in. So, to do this using Invoke-Command\n1 2 3 4 5 6 7 $s = New-PSSession -ComputerName SP2010-WFE #load the PS Snap-in to enable SharePoint PS cmdlets Invoke-Command -Session $s -ScriptBlock {Add-PSSnapin Microsoft.SharePoint.PowerShell} #$s has the PowerShell cmdlets now Invoke-Command -Session $s -ScriptBlock {Get-SPWeb http://sp2010-wfe:999} If you look at the above code, we will have to use a persistent session so that we can use SharePoint cmdlets in subsequent Invoke-Command calls.\nAnother caveat will be the unavailability of remote computer cmdlets in the local PowerShell session — in this case, the SharePoint 2010 cmdlets. This — essentially – means that we cannot use Get-Help or Get-Command cmdlets against the SharePoint 2010 cmdlets in the local session unless we pass that as a script block to Invoke-Command.\nOne more disadvantage of using Invoke-Command is unavailability of command completion. Unless the cmdlet you are using inside the scriptblock is available locally, you cannot use tab completion. This can be a pain for many, including me.\nThis where interactive remoting comes in to play.\nHow do you start interactive remoting?\nEnter-PSSession enables interactive sessions with remote computer. You can use this cmdlet the same way you’d use Invoke-Command.\n1 Enter-PSSession -ComputerName SP2010-WFE As shown in the above screenshot, PowerShell prompt changes to reflect that you are in the remote session. Now, taking the above example of SharePoint 2010 cmdlets, you can load the PS Snap-in as if you were loading it locally without using Invoke-Command\n1 Add-PSSnapin Microsoft.SharePoint.PowerShell Once the snap-in loaded, you will have access to all the SharePoint 2010 cmdlets as if they are available on the local computer. You can verify that by using Get-Help against one of the SharePoint 2010 cmdlets.\n1 Get-Help Get-SPWeb -Full Enter-PSSession when used with -ComputerName parameter, creates a PSSession in the background and uses that throughout the life of a remote session.\nHow do I exit a interactive session?\nYou can use Exit-PSSession to come out of an interactive PS Session. This will close any temporary session created in the background. So, it means that any variables or data you created during the interactive session won’t be available once you exit the session.\nHow about using persistent sessions in interative remoting?\nOh yes, that is a great idea. In fact, it will be advantageous to use persistent sessions. By using a persistent session, you can enter and exit the interactive session as many times as you like. All the data and variables you created in the remote session will persist until you remove the session. You can do it the same way you used persistent sessions with Invoke-Command.\n1 2 $s = New-PSSession -ComputerName SP2010-WFE Enter-PSSession -Session $s Can I enter an existing PSSession?\nYes. You can use Get-PSSession cmdlet to see a list of all available/opened PS Sessions and then use Enter-PSSession as shown above to start interactive remoting. As you see here, I will pipe Get-PSSession output to Format-List cmdlet to get all session details.\nThere are four ways to enter an existing PS Session for interactive remoting. I have highlighted the available options in the above screenshot. You can use which ever way is convenient.\n1 2 3 4 5 6 Enter-PSSession -id 1 Enter-PSSession -InstanceId 55a417ed-f903-4265-a4dc-c892c2500e0d Enter-PSSession -Name Session1 $s = Get-PSSession -Id 1 Enter-PSSession -Session $s All of the above options start interactive session using the persistent session “session1”. It is just more than one way to do the same thing.\nAlright. This brings us to the end of this part on Interactive remoting sessions. In this next part, I will talk about something called implicit remoting. Stay tuned..!\n","date":"2010-02-02T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/powershell-2-0-remoting-guide-part-5-interactive-remoting-sessions/","series":"PowerShell Remoting","title":"PowerShell 2.0 remoting guide: Part 5 – interactive remoting sessions"},{"categories":["PowerShell","Remoting"],"content":"In this part of PowerShell remoting series, I will discuss how to run commands or scripts on remote computer(s). Within remoting, there are couple of ways to run commands or scripts on a remote machine. This includes Invoke-Command cmdlet and interactive remoting sessions. These two methods deserve a separate post for each and hence I will discuss the Invoke-Command method in today\u0026rsquo;s post.\nOnce you have enabled remoting on all your computers, you can use Invoke-Command cmdlet to run commands and scripts on local computer or on remote computer(s). There are many possible variations of this cmdlet. I will cover most of them here.\nInvoke-Command to run commands on local or remote computer You can invoke a command on local or remote computer(s) using the below method:\n1 Invoke-Command -ComputerName SP2010-WFE -ScriptBlock { Get-Process } The ScriptBlock parameter can be used to specify a list of commands you want to run on the remote computer. ComputerName parameter is not required for running commands on the local machine. If you want to run the same command on multiple remote computers, you can supply the computer names as a comma separated list to ComputerName parameter or use a text file as shown in the example here\n1 Invoke-Command -ComputerName SP2010-WFE,SP2010-DB -ScriptBlock{ Get-Process } or\r```\rInvoke-Command -ComputerName (get-content c:\\scripts\\servers.txt) -ScriptBlock {Get-Process}\r```\rAll command names and variables in the ScriptBlock are evaluated on the remote computer. So, if you do something like -ScriptBlock {Get-Process -Name $procName}, PowerShell expects the remote computer session to have $procName defined. You can however pass variables on the local computer to a remote session when using Invoke-Command. This brings us to the next point in our discussion.\nPassing local variables as arguments to remote computer Taking the above example, we can pass the Name of the process you are looking for as a variable to the script block. ArgumentList parameter helps you achieve this. You can do this as shown here.\n1 2 $procName = \u0026#34;powerShell\u0026#34; Invoke-Command -ComputerName (get-content c:\\scripts\\servers.txt) -ScriptBlock {param ($Name) Get-Process -Name $Name} -ArgumentList $procName The above example may be a simple one but it shows how to use -ArgumentList parameter to pass local variables to the remote session.\nInvoke-Command to execute scripts on remote computer(s) Using ScriptBlock parameter can be quite tedious when you have to execute a bunch of PowerShell commands. This can be confusing when you have loops and conditional statements inside the scriptblock. Invoke-Command provides FilePath parameter to address this. You can use this parameter as shown below\n1 Invoke-Command -ComputerName SP2010-WFE -FilePath C:\\scripts\\Test.PS1 Make a note that the script you provide as FilePath must exist on the local machine or at a place accessible to the local machine.\nUsing -Session parameter for better performance and sharing data between commands Whenever you run Invoke-Command with -ComputerName parameter, a temporary session gets established to execute the remote command. So, establishing a session every time you use this cmdlet can be time consuming. So, to avoid that we can use a persistent connection to the remote computer and that is what -Session uses. You can create a persistent connection to a remote computer by using New-PSSession cmdlet as shown here\n1 $s = New-PSSession -ComputerName SP2010-WFE Now, $s contains the session details for the persistent connection. We can use $s to invoke a command on the remote computer and the syntax for that will be\n1 Invoke-Commad -Session $s -ScriptBlock {get-Process} $s contains all the variables you create / modify when you execute commands on the remote computer. So, subsequent command execution with $s as the session will have access to all of the variables created / updated on the remote computer. For example,\n1 2 3 $s = new-pssession -computername SP2010-WFE Invoke-Command -Session $s -ScriptBlock {$fileCount = (Get-ChildItem C:\\ -Recurse).Count} invoke-command -session $s -scriptblock {$fileCount} We could access $fileCount variable only because we used a persistent session to run the command. This would not have been possible if used -ComputerName to invoke the remote command.\nRunning remote command as a background job The example shown above — which gets the total file count on C:\\ of a remote machine — can be quite time consuming based on how big is C:\\ on the remote computer. In such case, you will have to wait for the remote command to complete execution. To avoid this, you can use -AsJob parameter to run the command as a background job on the remote computer.\n1 Invoke-Command -ComputerName SP2010-WFE -ScriptBlock {(Get-ChildItem C:\\ -Recurse).Count} -asJob Once you run this, you will see the job details listed as shown here:\nNow, you can use Get-Job and receive job cmdlets to see the output from the background job as shown below.\n1 Get-Job -id 1 | Receive-Job A complete discussion on Background jobs deserves a series of posts. I will plan to do that next. If you don’t want to wait and learn about it right away, you can read about it @ http://technet.microsoft.com/en-us/library/dd315273.aspx\nSpecifying credentials required for remoting As we have seen the enable remoting post, you can use PowerShell remoting between computers in a workgroup environment too. All of the examples I showed above assume that you have access to remote computer as an administrator. This method works quite well in a domain environment where the logged on user has administrator credentials to access any computer in the domain. However, this will not work in a workgroup setup. Within a workgroup you need to pass the credentials along with Invoke-Command. To do that,\n1 2 $cred = Get-Credential Invoke-Command -ComputerName SP2010-WFE -ScriptBlock { Get-Process} -Credential $cred In the example above, Get-Credential prompts for the credentials to access remote computer and uses the same while calling Invoke-Command cmdlet.\nThis is the end of this article on Invoke-Command. Invoke-Command has many other parameters which are not discussed here. They are more advanced and used in specific scenarios. I will discuss those use cases as we proceed further in this series of posts on PowerShell 2.0 remoting.\n","date":"2010-02-01T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/powershell-2-0-remoting-guide-part-4--execute-commands-or-scripts-on-a-remote-computer-using-invoke-command/","series":"PowerShell Remoting","title":"PowerShell 2.0 remoting guide: Part 4 – Execute commands or scripts on a remote computer using Invoke-Command"},{"categories":["PowerShell","Remoting"],"content":"In this part of the series of articles on PowerShell 2.0 remoting, we will look at how to enable remoting in different scenarios. This post assumes that you are running a supported operating system and you have installed all necesary pre-requisite software.\nSo, how do you enable remoting?\nRemoting in PowerShell 2.0 can be enabled by just running the following cmdlet\nEnable-PSRemoting Note: You have to run this at a elevated PowerShell prompt. Also, all your active networks should be set to “Home” or “Work” network location. Setting firewall exceptions for remoting will fail if the network location is set to “Public”.\nYes. That is it. You will be asked to respond to a couple of questions — based on OS architecture — as you see in the screenshot here.\nAs you see above, Enable-PSRemoting internally uses Set-WSManQuickConfig and a few other cmdlets. The second prompt around Microsoft.PowerShell32 will appear only on x64 OS. However, you should always use the more comprehensive Enable-PSRemoting cmdlet. If you don’t want to see all these prompts and but want to enable remoting in a more slient manner, you can use the -force parameter along with the cmdlet. Other parameters to Enable-PSRemoting cmdlet include -confirm and -whatif. You can run this cmdlet on all machines where you want to be able to receive commands from other machines in the network.\nHow can you verify if remoting is enabled or not? You can use the Enter-PSSession cmdlet to test if remoting is enabled on the local machine or not.\n1 Enter-PSSession -ComputerName localhost If remoting is enabled and functional, you will see the prompt changing to something like this:\nPowerShell remoting in a workgroup environment You will not be able to connect to a computer in workgroup just by running Enable-PSRemoting cmdlet. This is essentially because the security levels on a workgroup joined computer are more stringent than on a domain joined computer.\nOn Windows XP systems, you need to make sure the local security policy to enable classic mode authentication for network logons. This cabe be done by opening “Local Security Policy” from Control Panel -\u0026gt; Administrative Tools. Over there, navigate to Local Policies -\u0026gt; Security Options and double click on “Network Access: Sharing and Security Model for local accounts” and set it to classic.\nOnce the above change is made, you can enable remoting using Enable-PSremoting and then run:\n1 set-item wsman:localhost\\client\\trustedhosts -value * On Vista and later operating systems in workgroup model, you need to run the above cmdlet only. After these steps are performed, you should be able to send commands to a remote computer in a workgroup enviroment.\nTo enable remoting for multiple computers in an enterprise or domain environment, you can use group policy. For more information on this, refer to the “HOW TO ENABLE REMOTING IN AN ENTERPRISE” section at http://technet.microsoft.com/en-us/library/dd347642.aspx\nIn the next set of posts, we will see how to execute commands on remote machines using various cmdlets available within PowerShell remoting.\n","date":"2010-01-31T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/powershell-2-0-remoting-guide-part-3--enable-remoting/","series":"PowerShell Remoting","title":"PowerShell 2.0 remoting guide: Part 3 – Enable remoting"},{"categories":["PowerShell","Remoting"],"content":"In part 1 of this series I gave a quick introduction to PowerShell 2.0 remoting. Before we look at how to enable or configure a computer for remoting, let us take a quick look at PowerShell 2.0 remoting cmdlets. Here is a complete list of cmdlets with a brief overview.\nThis list will also include cmdlets that are not directly used within remoting but help configure various aspects of remoting. The knowledge of these cmdlets such as WSMan, etc in this list is not mandatory for basic usage of PowerShell remoting. In this post, I will only discuss what each of these cmdlets are capable of and list any gotchas. A detailed usage of these cmdlets will be discussed later in the series.\nEnable-PSRemoting\nThe Enable-PSRemoting cmdlet configures the computer to receive Windows PowerShell remote commands that are sent by using the WS-Management technology. This cmdlet will be the first one to run if you want to use PowerShell 2.0 remoting features and needs to be run just once. This cmdlet internally calls Set-WSManQuickConfig to configure WinRM service, enable firewall exceptions for WS Management and finally enables all registered PowerShell configurations.\nBTW, you need to enable PowerShell remoting only if you want the computer receive commands from a remote machine. To only send commands to a remote machine, you don’t need to enable PowerShell remoting.\nDisable-PSRemoting\nThe Disable-PSRemoting cmdlet disables all PowerShell session configurations on the local computer to prevent the computer from receiving any remote commands. You will have to manually stop the WinRM service if you don’t want the service to be running after you disable PowerShell remoting.\nInvoke-Command The Invoke-Command cmdlet runs commands on a local or remote computer and returns all output from the commands, including errors. With a single Invoke-Command command, you can run commands on multiple computers. This cmdlet — in it’s default form — opens a session for running a command against a remote computer and closes it once the execution is complete. This method — to some extent — is slow and can be worked around by specifying pre-defined session information.\nNew-PSSession Invoke-Command cmdlet supports specifying an existing session to enhance the speed of overall command execution. By specifying an existing session, we eliminate the need for creating/destroying the sessions on the fly. New-PSSession cmdlet can be used to create a persistent connection to a remote computer. By creating a persistent session, we will be able to share data, such as a function or the value of a variable between different commands executing within the PSSession.\nEnter-PSSession\nIn part 1, I briefly touched upon interactive remote sessions feature. The Enter-PSSession cmdlet starts an interactive session with a single remote computer. During the session, the commands that you type run on the remote computer, just as though you were typing directly on the remote computer. You can have only one interactive session at a time. You can specify the PSSession you created using New-PSSession as a parameter to this cmdlet.\nExit-PSSesstion\nExit-PSSession exits an interactive PS Session created using Enter-PSSession cmdlet.\nGet-PSSession The Get-PSSession cmdlet gets the Windows PowerShell sessions (PSSessions) that were created in the current session. This cmdlet gets all the PSSessions returns all the PSSessions in to a variable when no parameters are specified. You can then use the session information with other cmdlets such as Invoke-Command, Enter-PSSession, Remove-PSSession, etc\nRemove-PSSession\nThe Remove-PSSession cmdlet closes PS session(s). It stops any commands that are running in the PSSessions, ends the PSSession, and releases the resources that the PSSession was using. If the PSSession is connected to a remote computer, Remove-PSSession also closes the connection between the local and remote computers.\nImport-PSSession Import-PSSession cmdlet uses the implicit remoting feature of PowerShell 2.0. Implicit remoting enables you to import commands from a local/remote computer in to an existing PS session and run those commands as if they were local to the session.\nExport-PSSession The Export-PSSession cmdlet gets cmdlets, functions, aliases, and other command types from another PSSession on a local or remote computer and saves them to local disk as a Windows PowerShell module. We can now use the Import-Module cmdlet to add the commands from the saved module to a PS Session.\nRegister-PSSessionConfiguration Any PS session created using Invoke-Command or New-PSSession or any other PowerShell remoting cmdlet for that matter uses the default PS Session configuration as specified in the $PSSessionConfigurationName variable. PS Session configuration determines which commands are available in the session, and it can include settings that protect the computer, such as those that limit the amount of data that the session can receive remotely in a single object or command. So, you can use the Register-PSSessionConfiguration cmdlet creates and registers a new session configuration on the local computer.\nUnregister-PSSessionConfiguration\nThe Unregister-PSSessionConfiguration cmdlet deletes registered session configurations from the computer. It is possible to delete the default PSSession configurations (Microsoft.PowerShell or Microsoft.PowerShell32) using this cmdlet. In such a case, you can use Enable-PSRemoting cmdlet to re-create and register the default PS Session configurations.\nDisable-PSSessionConfiguration\nDisable-PSSessionConfiguration disables a registered PS Session configuration. Remember, this only disables the configuration but not un-register or delete the information from local computer. These disabled session configurations cannot be used to establish a remoting session.\nEnable-PSSessionConfiguration The Enable-PSSessionConfiguration cmdlet re-enables registered session configurations that have been disabled by using the Disable-PSSessionConfiguration cmdlet.\nGet-PSSessionConfiguration The Get-PSSessionConfiguration cmdlet gets the session configurations that have been registered on the local computer.\nSet-PSSessionConfiguration The Set-PSSessionConfiguration cmdlet changes the properties of the registered session configurations on the local computer.\nTest-WSMan\nPowerShell remoting requires WinRM service to be running on the remote machines. You can use Test-WSMan cmdlet to quickly check if you can establish a remoting session with other computers. If WinRM is not enabled on remote machine, you can safely assume that PowerShell remoting is not enabled. However, you can assume that PowerShell remoting is enabled just by verifying that WinRM service is running. Remember, this checks only for WinRM service and remoting requires many other components to function.\nEnable-WSManCredSSP PowerShell remoting supports CredSSP authentication and the same can be enabled by using Enable-WSManCredSSP cmdlet. The Enable-WSManCredSPP cmdlet enables CredSSP authentication on a client or on a server computer. When CredSSP authentication is used, the user’s credentials are passed to a remote computer to be authenticated. This type of authentication is designed for commands that create a remote session from within another remote session. For example, you use this type of authentication if you want to run a background job on a remote computer.\nDisable-WSManCredSSP The Disable-WSManCredSPP cmdlet disables CredSSP authentication on a client or on a server computer.\nThere are other WSMan cmdlets introduced in PowerShell 2.0 such as Connect-WSMan, Disconnect-WSMan, Get-WSManInstance, New-WSManInstance, New-WSManSessionOption, Remove-WSManInstance and Set-WSManInstance. These cmdlets are not really meant for PowerShell remoting but we will discuss them as required.\nThis brings us to the end of part 2 and in the next set of posts we will look at how to enable/configure computer(s) for remoting and do some cool things. Do leave your feedback on this article. Let me know if I missed something or need to correct something.\n","date":"2010-01-27T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/powershell-2-0-remoting-guide-part-2--overview-of-remoting-cmdlets/","series":"PowerShell Remoting","title":"PowerShell 2.0 remoting guide: Part 2 – Overview of remoting cmdlets"},{"categories":["PowerShell","Remoting"],"content":"I am starting a series of articles on remoting feature of PowerShell 2.0. This is one of the best features of PowerShell 2.0 and my favorite feature for sure. The number of very cool things one can achieve using this feature is just un-imaginable. I have started digging deep in to this feature as I start writing a network file browser powerpack as a part of hands-on. I hope it is worth sharing what I learn by writing about it here. So, this is the first in that series of posts. In this post, we will look at absolute basics to start using PowerShell remoting.\nWhat is PowerShell remoting? This is a new feature in PowerShell 2.0 that enables remote management of computers from a central location. Remoting uses WS-Management to invoke scripts and cmdlets on remote machine(s). This feature also enables what is known as “Universal Code Execution Model” in Windows PowerShell v2. UCEM means that whatever runs locally should run anywhere. PowerShell remoting lets you import remote commands in to a local session — a feature known as implicit remoting and also enables you to save or export these imported commands to local disk as a module for later use. There are bunch of other features such as interactive sessions, etc. We will look in to all these features but one thing at a time.\nPowerShell remoting allows for multiple ways of connecting. These ways include interactive (1:1), fan-out (1:many), and fan-in (many:1 by using the IIS hosting model, Ex: MobileShell ). We will look at each of these methods in this series of articles on remoting.\nRemoting requirements To enable PowerShell remoting, all computers participating in remote management should have the following software.\nWindows PowerShell 2.0\r.NET framework 2.0 SP1 or later\rWindows Remote Management (WinRM) 2.0\rAll of the above are installed by default on Windows 7 and Windows Server 2008 R2. However, earlier versions of Windows will require you to download the updates from Microsoft website and install them yourself.\rPowerShell 2.0 and WinRM 2.0 are included as a part of Windows Management Framework download and hence is available for Windows XP, Windows Server 2003, Windows Vista and Windows Server 2008. WinRM 2.0 and PowerShell 2.0 can be installed on the following supported operating systems:\nWindows Server 2008 with Service Pack 1\rWindows Server 2008 with Service Pack 2\rWindows Server 2003 with Service Pack 2\rWindows Vista with Service Pack 2\rWindows Vista with Service Pack 1\rWindows XP with Service Pack 3\rApart from the OS versions listed above, PowerShell 2.0 is supported on Windows Embedded POSReady 2009 and Windows Embedded for Point of Service 1.1 as well. Thanks to @alexaindar for the pointer on this. I did not know that such OS existed apart from Windows XP Embedded which I saw a few years ago 🙂\rTo be able run scripts and commands on remote computers, the user performing remote script execution must be a member of the administrators group on the remote machine or should be able to provide administrator credentials at the time of remote execution. Also, on client versions of Windows such as Windows Vista and Windows 7, network location must be set either to Home or Work. WS-Management may not function properly if the network location is set to Public.\nHere is the setup I have to experiment with PowerShell remoting. I will use these virtual machines and the Windows Server 2008 R2 OS to write my file browser PowerPack using PowerShell remoting. This will help me get a good coverage of all supported operating system families.\nJonathan Medd is currently writing a PowerShell cmdlet series in which he is talking about remoting cmdlets also. What I am going to write here is not a repeat of that. In the next post, we will look at how to configure the available systems to enable at an overview of PowerShell remoting cmdlets. I will also talk about various methods to enable PowerShell remoting. So, stay tuned..!\n","date":"2010-01-22T00:00:00Z","excerpt":null,"permalink":"https://ravichaganti.com/blog/powershell-2-0-remoting-guide-part-1-the-basics/","series":"PowerShell Remoting","title":"PowerShell 2.0 remoting guide: Part 1 – The basics"}]