# Ravikanth Chaganti

> Ravikanth Chaganti on AI infrastructure, Agentic AI, Cloud Native infrastructure, and automation.

## Blog Posts
- [Google ADK - Agent Skills](/blog/google-adk-agent-skills/index.md): Dive into a powerful new feature in Google ADK: Agent Skills. We will explore what Agent Skills are, why they matter, and how to implement a practical use case.
- [Microsoft Agent Framework - Agent Skills](/blog/microsoft-agent-framework-agent-skills/index.md): Latest release of Microsoft Agent Framework offers support for Agent Skills. We will explore implementing a practical use case for Skills with MAF.
- [Implementing llms.txt and Markdown Output in Hugo](/blog/implementing-llms.txt-and-markdown-output-in-hugo/index.md): Learn how to extend Hugo to generate llms.txt and clean Markdown versions of your content, making your site easily consumable by AI agents and LLMs.
- [GitHub Agentic Workflows](/blog/github-agentic-workflows/index.md): GitHub Agentic Workflows let you write automation in natural language. Instead of stitching together YAML steps, shell scripts, and third-party Actions, you describe what you want an AI agent to accomplish and give it the tools to do it.
- [Documentation stack for AI agents](/blog/documentation-stack-for-ai-agents/index.md): Learn what you need to make your knowledge consumable by AI — from static files to live APIs.
- [Agent skills vs Model Context Protocol - [How] do you choose?](/blog/agent-skills-vs-model-context-protocol-how-do-you-choose/index.md): Agent Skills and MCP are a hot topic. Understanding these standards genuinely overlap, where they serve fundamentally different purposes, and where each falls short, is essential for anyone building agent-powered systems.
- [Cross-Framework Agent Communication: Microsoft Agent Framework meets Google ADK via A2A](/blog/cross-framework-agent-communication-microsoft-agent-framework-meets-google-adk-via-a2a/index.md): Microsoft Agent Protocol is an open framework for building enterprise-grade agents. It offers support for using the A2A protocol for cross-agent communication.
- [Building multi-agent systems with Google ADK and the A2A protocol](/blog/building-multi-agent-systems-with-google-adk-and-the-a2a-protocol/index.md): Google Agent Development Kit (ADK) is a flexible and modular framework for developing and deploying AI agents. It offers native support to interact with other agents via the agent-to-agent protocol.
- [Building a Multi-Agent LinkedIn Newsletter System with Google ADK](/blog/building-a-multi-agent-linkedin-newsletter-system-with-google-adk/index.md): Automating content creation with a team of AI agents that research, write, edit, and publish
- [Announcing a self-paced learning platform and preview of LearnCTL](/blog/announcing-a-self-paced-learning-platform-and-preview-of-learnctl/index.md): An open and free learning platform for the technical community.
- [Google ADK - OpenAPI tools, agents-as-tools, authentication, and long-running operations](/blog/google-adk-openapi-tools-agents-as-tools-authentication-and-long-running-operations/index.md): Your agent needs to talk to the outside world. It needs to call REST APIs, delegate complex work to specialized agents, authenticate with protected services, and kick off operations that take minutes instead of milliseconds. Google ADK provides four powerful patterns to make all of this possible.
- [Google ADK - MCP tools](/blog/google-adk-mcp-tools/index.md): An agent that can only use the tools you hard-code into it has a ceiling. The Model Context Protocol (MCP) shatters that ceiling by giving your ADK agents a standardized way to discover and use tools hosted anywhere — local processes, remote servers, or cloud services.
- [Google ADK - Runner and execution architecture](/blog/google-adk-runner-and-execution-architecture/index.md): You have agents, tools, callbacks, and sessions. But what actually runs them? The Runner is the central orchestrator that powers every agent interaction in Google ADK, driving the event loop that connects all these pieces together.
- [Google ADK - Callbacks](/blog/google-adk-callbacks/index.md): Callbacks are user-defined functions that hook into an agent's execution pipeline at predefined checkpoints. They let you observe, customize, and control agent behavior without modifying the ADK framework code.
- [Google ADK - Sessions, state, and memory](/blog/google-adk-sessions-state-and-memory/index.md): LLMs are stateless. Every API call to an LLM is independent. The model does not inherently remember what was said before. Yet meaningful conversations are inherently multi-turn, contextual, and stateful.
- [Google ADK - types of agents](/blog/google-adk-types-of-agents/index.md): Google Agent Development Kit (ADK) is a flexible and modular framework for developing and deploying AI agents. It offers different types agents.
- [Introduction to Google Agent Development Kit](/blog/introduction-to-google-agent-development-kit/index.md): Google Agent Development Kit (ADK) is a flexible and modular framework for developing and deploying AI agents.
- [Magentic workflows in Microsoft Agent Framework](/blog/magentic-workflows-in-microsoft-agent-framework/index.md): If you need a workflow that can adapt its routing decisions based on what agents discover during execution, look no further. Magentic is for you!
- [Group chat workflows in Microsoft Agent Framework](/blog/group-chat-workflows-in-microsoft-agent-framework/index.md): Building dynamic multi-agent discussions where AI agents collaborate, debate, and converge on solutions with GroupChatBuilder
- [Docker cagent - An introduction](/blog/docker-cagent-an-introduction/index.md): Docker cagent is an open-source, multi-agent AI runtime that lets you build, orchestrate, and share teams of specialized AI agents — all defined declaratively in YAML.
- [Handoff workflows in Microsoft Agent Framework](/blog/handoff-workflows-in-microsoft-agent-framework/index.md): Learn how to orchestrate AI agents in a concurrent workflow using Microsoft Agent Framework's HandoffBuilder
- [Concurrent workflows in Microsoft Agent Framework](/blog/concurrent-workflows-in-microsoft-agent-framework/index.md): Learn how to orchestrate AI agents in a concurrent workflow using Microsoft Agent Framework's ConcurrentBuilder
- [Sequential workflows in Microsoft Agent Framework](/blog/sequential-workflows-in-microsoft-agent-framework/index.md): Learn how to orchestrate AI agents in a step-by-step pipeline using Microsoft Agent Framework's SequentialBuilder
- [Building persistent AI Agents with Microsoft Agent Framework and Microsoft Foundry](/blog/building-persistent-ai-agents-with-microsoft-agent-framework-and-microsoft-foundry/index.md): The Microsoft Agent Framework represents a significant improvement in developer experience over the raw Foundry SDK. While the native SDK gives you complete control, the Agent Framework provides less boilerplate, type-safe tools, consistent patterns, and resource safety.
- [Building AI Agents with Microsoft Agent Framework](/blog/building-ai-agents-with-microsoft-agent-framework/index.md): The world of AI agents is evolving rapidly, and Microsoft's Agent Framework provides a powerful, unified foundation for building intelligent agents that can reason, take actions, and interact with users naturally. In this blog post, I'll walk you through a series of practical examples that demonstrate how to create and use Azure AI agents using MAF and Python.
- [Getting Started with Foundry Agents](/blog/getting-started-with-foundry-agents/index.md): Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and other aspects such as observability, guardrails, and enterprise-ready governance for creating AI agents.
- [Introduction to Microsoft Agent Framework](/blog/introduction-to-microsoft-agent-framework/index.md): Microsoft Agent Framework is an open-source development kit for building AI agents and multi-agent workflows.
- [Local model serving - Using Foundry Local](/blog/local-model-serving-using-foundry-local/index.md): There are several options available for running Large Language Model (LLM) inference locally. Foundry Local by Microsoft is a new entrant.
- [Local model serving - Using Docker model runner](/blog/local-model-serving-using-docker-model-runner/index.md): Docker Model Runner — a faster, simpler way to run and test AI models locally, right from your existing workflow. Whether you’re experimenting with the latest LLMs or deploying to production, Model Runner brings the performance and control you need, without the friction.
- [Local model serving - Using LM Studio](/blog/local-model-serving-using-lm-studio/index.md): There are several options available for running Large Language Model (LLM) inference locally. LM Studio is one such option. It is more comprehensive and offers some great features.
- [Local model serving - Using Ollama](/blog/local-model-serving-using-ollama/index.md): There are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.
- [Changes in Autogen release 0.5.1](/blog/changes-in-autogen-release-0_5_1/index.md): Know what went into AutoGen 0.5.1 -- the open framework for creating multi-agent systems.
- [Kube Resource Orchestrator (KRO) - Deploying the sample voting application](/blog/kube-resource-orchestrator-kro-deploying-the-sample-voting-application/index.md): Extending what you learned in the previous article on the basics of kro, this article demonstrates a few more features of kro with the help of a sample voting application.
- [Kube Resource Orchestrator (KRO) - The Basics](/blog/kube-resource-orchestrator-kro-the-basics/index.md): Kube Resource Orchestrator (KRO) introduces a new Kubernetes-native and cloud-agnostic way to group Kubernetes resources.
- [Building a Model Context Protocol server for Azure](/blog/building-a-model-context-protocol-server-for-azure/index.md): Extend knowledge of creating MCP servers to achieve more practical applications
- [Understanding Agents in Microsoft AutoGen framework](/blog/understanding-agents-in-autogen-framework/index.md): In the previous parts of this series on the Microsoft AutoGen framework, we looked at how to get started with the AutoGen framework. We examined the process of building a simple agent using the prebuilt AssistantAgent in AutoGen and explored the development of a multi-agent team.
However, what is an agent in the context of AutoGen? How do agents communicate with each other? I needed answers to these questions before proceeding to the next step in using AutoGen to build larger, multi-agent applications. So, I started exploring the Core package upon which the autogen-chat is built. This article is a result of that exploration.

- [Teams of agents in AutoGen](/blog/teams-of-agents-in-autogen/index.md): In the previous article in this series on Microsoft AutoGen, we looked at how to get started with Microsoft AutoGen and create a simple agent. We built a weather agent capable of retrieving real-time weather data. Through this demonstration, we gained an understanding of the basics of implementing an agent using the AutoGen framework. In this article, we will examine the process of building a team of agents or a multi-agent team to achieve a slightly more complex goal. We will start this exploration by building a single-agent team. Yes, you read that correctly. AutoGen allows you to create a single-agent team. This is useful in running an agent in a loop until a goal is achieved.

- [Model Context Protocol by Anthropic for connecting AI models to data](/blog/anthropic-model-context-protocol/index.md): A couple of months ago, Anthropic introduced and open-sourced the Model Context Protocol (MCP). MCP is the new standard for connecting AI models to external data sources and APIs more easily and consistently. With the advances in AI, models are becoming increasingly powerful in reasoning and quality. However, as text-completion machines, these models still lack access to real-time data. AI providers have worked around this using Retrieval Augmented Generation (RAG) and tool calling. Every data source requires custom implementation, and every provider has a way of integrating tools with AI models. MCP addresses these silos by providing a universal, open standard for connecting AI systems with data sources.

- [Getting started with AutoGen framework for building AI agents and applications](/blog/getting-started-with-autogen-framework-for-building-ai-agents-and-applications/index.md): I have been closely following developments in the Agentic AI space. AutoGen from Microsoft Research is emerging as a powerful yet easy-to-use framework for building AI agents and applications. With their re-architected release (0.4.0), they have re-imagined building advanced AI applications. This release introduces an asynchronous, event-driven architecture.

- [Predicted outputs in Azure OpenAI](/blog/predicted-outputs-azure-openai/index.md): Sometimes, you may want the LLM to perform only minimal changes to what is provided as a prompt. For example, you have a couple of paragraphs of text that you want the LLM to modify to ensure no spelling mistakes. You do not want the LLM to change the overall content, but make sure the misspellings are corrected. This usually helps in reducing the LLM response latency. This is useful in scenarios where you already know a large portion of the expected response and is well-suited for code completion and error detection scenarios. In this part of the series of articles on Azure OpenAI, we will use predicted outputs with Azure OpenAI to build AI applications.

- [Parallel tool calling in Azure OpenAI](/blog/parallel-tool-calling-azure-openai/index.md): We have learned to perform single- and multi-tool calling with the Azure OpenAI API for chat completions. This part of the series on Azure OpenAI will describe the parallel tool calling feature and how to implement it. Parallel tool calling allows you to perform multiple calls together. This enables parallel execution, result retrieval, and fewer calls to the LLM. Parallelizing tool calls improves overall performance.
In a previous example on retrieving weather at a given location, we examined how to iterate over the LLM response for tool calls and append it to the conversation history before making the next LLM API call.

- [Structured output in Azure OpenAI](/blog/structured-output-azure-openai/index.md): In this series, we have examined the basics of Azure Open AI, using the chat completions API, streaming responses, and finally, single and multi-tool calling. In today&rsquo;s article, we will examine how to return structured output from the LLM response. We will first examine structured output without function calling and then update the earlier multi-function calling example to output JSON instead of text.
Structured outputs tell an LLM to follow the schema represented by the response_format parameter of a request to the LLM. We can use Pydantic to build the schema.

- [Implementing multiple tool/function calling when using Azure OpenAI](/blog/azure-openai-function-calling-with-multiple-tools/index.md): In the last article of this series, we learned about function/tool calling. Based on the prompt, the LLM indicates that we must call the get_weather tool. The LLM finally returns the answer to our prompt using the tool response. However, let us try to add a few more variables to our prompt. The updated prompt will be &ldquo;What&rsquo;s the weather like in Bengaluru next week?&rdquo;.
1 2 3 $ python .\05_function_calling.py get_current_time called with location: Bengaluru The current temperature in Bengaluru is approximately 28.2°C. However, for next week&#39;s weather prediction, you&#39;d need a forecast service as I currently provide only current weather information. LLM uses the get_weather tool to determine the current weather but fails to determine next week&rsquo;s weather. This is because we have not provided any tool for the LLM to determine what next week means. Determining the meaning of next week requires the knowledge of the current date and time. This article will demonstrate how to add multiple tool-calling capabilities to our program. With the updated script, you can receive the weather information for a specific date.

- [Implementing tool/function calling when using Azure OpenAI](/blog/azure-openai-function-calling/index.md): In the last article of this series, we learned how to use the chat completion API. Towards the end, we learned that the LLMs have a knowledge cut-off date and no real-time access to information. However, this can be bridged using the tool/function calling feature of Azure OpenAI service. In this article, we shall learn how to implement tool calling.
Azure OpenAI&rsquo;s function calling capability lets you connect your language models to external tools and APIs, enabling them to perform a wider range of tasks and access real-time information. This opens up a world of possibilities, allowing your models to interact with the real world in ways never imagined.

- [Using chat completion API in Azure OpenAI](/blog/using-chat-completion-api-azure-openai/index.md): So far in this series, we have looked at the Azure OpenAI completion API, which generates a response for a given prompt. This is a legacy API, and using the chat completion API is recommended. We can build conversational chatbots and similar applications with the chat completion API. This article will examine how to use the Azure OpenAI chat completion API. In the earlier articles, we used the client.completions.create() function to generate a response. We need to use the client.chat.completions.create() in the openai library to build a conversation with the LLM.

- [Streaming model responses when using Azure OpenAI](/blog/streaming-completions-azure-openai/index.md): Responses are streamed to the user interface as they are generated using ChatGPT and similar tools. This eliminates the need for the user to wait until the complete response is generated. In today&rsquo;s article, we shall look at streaming LLM-generated responses when using Azure OpenAI API in Python. In the earlier part of this series, we learned about the client.completions.create() function used to send a prompt to the LLM and retrieve one or more responses. This function supports a parameter called stream when set to True, asks LLM to stream the response as it gets generated. The way to handle this response is a bit different from a standard completion response.

- [Getting started with Azure OpenAI](/blog/getting-started-with-azure-openai/index.md): Generative AI and OpenAI should not be alien anymore. Several startups are already riding this new wave and creating stunning applications that solve several important use cases. I use GenAI regularly to learn and become more efficient in coding. GitHub Copilot has been a good friend. I experimented with creating Large Language Model (LLM) applications using different providers (OpenAI and Google Gemini) and in different programming languages. OpenAI provides client libraries that can be used with any provider that offers an OpenAI-compatible API. For example, we can use the OpenAI Python library to work with OpenAI and Azure OpenAI services.

- [Devcontainers for cloud-native application development](/blog/devcontainers-for-cloud-native-application-development/index.md): Development containers are a great way to develop modern applications. Cloud-native applications usually implement more than one service to provide the application functionality. Using dev containers for microservices-based application development requires more than one container. This is where using Docker Compose with dev containers is useful. This article explores creating a multi-container development environment using VS Code dev containers and Docker Compose.
- [Using Bicep to provision Azure Red Hat OpenShift cluster](/blog/using-bicep-to-provision-azure-red-hat-openshift-cluster/index.md): Bicep simplifies provisioning Azure Red Hat OpenShift clusters. This article explains how!
- [Azure SDK for Go - Authentication methods - Chained Credentials](/blog/azure-sdk-for-go-authentication-methods-chained-credentials/index.md): So far in this series, you learned how to use different types of credentials that the Azure SDK for Go offers. You used specific credential types to authenticate with Azure in all the examples. You use a set of credential types for the development environment and a different set for production use cases. However, what if you want to use the code unmodified between development and production? What if you must test the code that uses managed identity credentials locally where the type of credentials you must use are different? This is where credential chaining comes to the rescue.

- [Azure SDK for Go - Authentication methods - Managed Identity Credential](/blog/azure-sdk-for-go-authentication-methods-managed-identity-credential/index.md): When you have applications or services that run in the Azure cloud infrastructure and require access to Azure services, the best way to authenticate to Azure is to use the managed identity. Using managed identity, you can eliminate the need to manage artifacts such as secrets, certificates, and credentials. For example, if you need to access Azure Key Vault to retrieve an API key for authentication purposes. To do this, you must first authenticate to the Key Vault service. With managed identity, you get automatically authenticated.

- [Azure SDK for Go - Authentication methods - Environmental credential](/blog/azure-sdk-for-go-authentication-methods-environmental-credential/index.md): In the last part of this series, you learned how to use different credential types in a local development environment. Another method you can use within local development and in a hosted/deployed service is providing credentials through environment variables. This is done using the NewEnvironmentCredential() method in the azidentity package.
1 2 3 4 5 // github.com/rchaganti/azure-go/02-auth101/envCredential.go cred, err := azidentity.NewEnvironmentCredential(nil) if err != nil { log.Fatal(err) } This credential type supports multiple types of authentication in the following order.

- [Azure SDK for Go - Authentication methods for local development environment](/blog/azure-sdk-for-go-authentication-methods-for-local-dev-environment/index.md): The earlier article introduced you to the Azure SDK for Go. In the example towards the end of that article, you may have noticed how the NewAzureCLICredential method of the azidentity package was used to access locally available Azure CLI credentials. This is perfect for local development and is not recommended for production environments. The Azure SDK supports different types of credentials depending on where your Go code is running. In this article in the Azure SDK for Go series, you will learn about each of these credential types and when you use each of these credentials.

- [Getting Started with Azure SDK for Go](/blog/2023-07-13-getting-started-with-azure-sdk-for-go/index.md): Microsoft Azure provides vast services from basic computing to AI/ML. Azure Resource Manager (ARM) is our entry point to access and manage these services and provides a consistent management layer (APIs) that enables you to work with resources in Azure.

- [Determining time zone differences in PowerShell for effective meeting planning](/blog/2023-03-20-determining-time-zone-differences-in-powershell-for-effective-meeting-planning/index.md): I work on a team with members across multiple time zones. It is important to schedule meetings so that team members do not have to stay late or wake up early. Being a command line person, I quickly wrote this PowerShell script to help me determine the time differences.
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 param ( [Parameter(Mandatory = $true)] [String[]] $Timezone, [Parameter()] [Datetime] $Target ) if (!$Target) { # Get the local time as the target $Target = Get-Date } $tzObject = [System.Collections.Arraylist]::new() $tzObject.Add( [PSCustomObject]@{ &#39;Timezone&#39; = &#39;Local&#39; &#39;Time&#39; = $Target &#39;DifferenceInHours&#39; = 0 } ) | Out-Null foreach ($tz in $Timezone) { # Get the timezone difference $localTz = [System.TimeZoneInfo]::Local $remoteTz = [System.TimeZoneInfo]::FindSystemTimeZoneById($tz) $tzDifference = [float]($remoteTz.BaseUtcOffset.TotalHours - $localTz.BaseUtcOffset.TotalHours) $remoteTime = [System.TimeZoneInfo]::ConvertTime($Target, $localTz, $remoteTz) $tzObject.Add( [PSCustomObject]@{ &#39;Timezone&#39; = $tz &#39;Time&#39; = $remoteTime &#39;DifferenceInHours&#39; = $tzDifference } ) | Out-Null } return $tzObject This one is simple to use.

- [WebAssembly (wasm) learning playground in VS Code](/blog/2023-01-12-webassembly-learning-playground-in-vs-code/index.md): WebAssembly, a.k.a Wasm, defines a portable binary-code format for a stack-based virtual machine. Wasm enables developers to write code that runs in web browsers with near-native performance using languages such as C, C++, Rust, Go, and many other modern languages. Wasm is not a replacement for JavaScript but is designed to complement and run alongside JavaScript.
I am still learning about Wasm and looking at opportunities to use Wasm in designing modern and cloud-native applications. My focus has been using the Go programming language for Wasm as well. So, when I started learning Wasm, I quickly looked for ways to create a development environment that would help me in my journey toward mastering Wasm. For this, I started looking at VS Code development containers. This resulted in a set of devcontainer features that I can use within a development container definition.

- [Applied Go - Creating a CLI application](/blog/2022-12-16-applied-go-creating-a-cli-application/index.md):  Practice makes permanent - Bobby Robson.
When learning a new programming language or any technology for that matter, we need to practice what we learn, and that is when it becomes permanent. I am starting a new series of posts to share what I have tried or built, so that what I have learned— through Go programming books, video courses, and community content—stays with me permanently.
The first one in this series is about using Cobra package to build a CLI application.

- [Keyless git commit signing using Sigstore gitsign in a VS Code devcontainer](/blog/2022-12-13-keyless-git-commit-signing-using-sigstore-gitsign-in-a-vscode-devcontainer/index.md): Once a Sigstore fan, always a fan!
Sigstore has become a good part of my research around SBOM and supply-chain security. I wrote about a cosign VS Code devcontainer feature earlier and then stumbled upon gitsign. In the past, I tried using GPG keys to sign my git commits but could not really sustain that as I kept changing machines and using different development environments. When I first looked at cosign, my initial thought was using it also for git commit signing but as I started reading Sigstore documentation, I discovered gitsign!

- [Sigstore Cosign VS Code development container feature](/blog/2022-12-12-sigstore-cosign-dev-container-feature/index.md): For those following me here or social media will know that I am a big fan of Visual Studio development containers. I wrote a bit about Visual Studio Code development containers. I have been using devcontainers for different development environments extensively. I have a sample repo on GitHub that has all devcontainer definitions that I have been using for different projects. A latest addition to this list is the Sigstore cosign devcontainer feature.

- [Building container images - using no tools](/blog/2022-11-28-building-container-images-using-no-tools/index.md): In a couple of earlier articles, we looked at Linux constructs used in building container images and the OCI image specification which standardizes the container image format. It is now time to understand how we can create container images. Knowing this is important to optimize the image size and building secure container images. And, of course, this knowledge also helps us appreciate what tools like Docker CLI or Buildah among many others help us achieve. There are many tools to build container images. We shall look at each of these methods in-depth in later parts of this series of articles on container images. For today, we will look into building container images using no tools! Actually, just using built-in OS tools.

- [Fully automated Kubernetes cluster deployment on Azure in under 8 minutes](/blog/2022-11-26-fully-automated-kubernetes-cluster-deployment-on-azure-in-under-8-minutes/index.md): In an earlier article, I wrote about provisioning a virtual Kubernetes cluster with kubeadm](https://ravichaganti.com/blog/2022-11-13-installing-and-configuring-kubernetes-cluster-using-kubeadm-on-ubuntu/). I use this method on a laptop with limited resources. This is good to a large extent but not good enough when I want to scale my experiments and learning. This is where I began planning my Kubernetes lab on Azure. I needed something I could spin quickly with no manual intervention. Given my interest in the Bicep language, I naturally wrote a Bicep template to perform this deployment.

- [Fully Automated Kubernetes Cluster Deployment on Azure in Under 8 Minutes](/blog/fully-automated-kubernetes-cluster-deployment-on-azure-in-under-8-minutes/index.md): Kubeadm is the go to tool for Kubernetes administrators. Understand how to use this tool combined with Bicep to provision a virtualized K8s cluster.
- [Installing and Configuring Kubernetes cluster using kubeadm on Ubuntu](/blog/2022-11-13-installing-and-configuring-kubernetes-cluster-using-kubeadm-on-ubuntu/index.md): This article is more like a note to myself, but this may help if you are looking at configuring a Kubernetes cluster on Ubuntu.
There are many ways to install and configure Kubernetes cluster for learning and development purposes. You can use Docker Desktop or Rancher Desktop or https://podman-desktop.io/ or minikube or microk8s to create a single node cluster for your development work quickly. I am a big fan of Docker Desktop and use that for any quick experiments. However, I need a multi-node cluster with additional services for anything more than simple deployment. I use Ubuntu virtual machines and configure a Kubernetes cluster using kubeadm.

- [Installing and Configuring Kubernetes Cluster using Kubeadm on Ubuntu](/blog/installing-and-configuring-kubernetes-cluster-using-kubeadm-on-ubuntu/index.md): Kubeadm is a handy tool to configure Kubernetes clusters. This article explains how to create a K8s cluster using Ubuntu VMs.
- [Understanding and using Azure Container Instances service - Multi-container Applications using Bicep](/blog/2022-11-03-understanding-and-using-azure-container-instances-service-multi-container-apps-using-bicep/index.md): In the previous article, we looked at getting started with Azure Container Instances service, learned some basic concepts of ACI, and saw an example of provisioning a hello-world application. In this article, we shall extend this knowledge by provisioning a multi-container application to ACI using Bicep templates. You can also use a YAML method of provisioning multi-container applications to ACI. We shall look at this later.
As we learned earlier, a container group resembles a Kubernetes pod. The containers within the group share resources such as network and storage. The resource usage of containers can be controlled at the container group level. As a general rule of thumb, we should group containers with similar lifecycle and resource requirements into a container group. The multi-container groups are available for Linux containers only at the moment. Based on the region, there will be additional restrictions in terms of resource allocation.

- [Understanding and using Azure Container Instances service - The Basics](/blog/2022-11-02-understanding-and-using-azure-container-instances-service-the-basics/index.md): In an earlier article, we looked at different containerization options in Microsoft Azure. In today&rsquo;s article, we shall start with Azure Container Instances (ACI) and learn the fundamentals. In a later article, we will learn how to perform multi-container applications to ACI.
Azure Container Instances Azure Container Instances (ACI) service offers a quick and easy way to run containers in the Microsoft Azure cloud. It is as simple as running a container locally using the Docker engine. ACI does not provide full container orchestration. ACI is great for applications that require faster startup times and hassle-free management. ACI runs inside Azure VMs and therefore provides isolation enabled by virtualization.

- [Containerization in Microsoft Azure](/blog/2022-11-01-containerization-in-azure/index.md): Containerization is everywhere &ndash; from on-premises data centers to the edge and in the cloud. We have been using containers right from development to production. I wrote about VS Code development containers and how I have standardized my development environment using devcontainers. There are several options for companies to run containerized applications in production. we can run individual application containers on a container host using engines like Docker. We can use tools like Docker Compose or Swarm for slightly complex multi-container, multi-host environments. However, it becomes complex to manage such environments. This needs us to implement a physical or virtualized server environment and handle all operational overhead associated with running our data center.

- [Publish container images to Azure Container Registry](/blog/2022-10-29-publish-container-images-to-azure-container-registry/index.md): In the previous articles in this series, we looked at how to build container images and understand the OCI image specification that governs how the images are built and dealt with. Once the images are built, we may want to push them to a central repository for sharing with a larger community or a private container registry such as the Azure Container Registry (ACR).
In today&rsquo;s article, we shall learn how to create an Azure Container Registry, push an image, and consume it back in the local environment. Let&rsquo;s start!

- [Understanding container images - OCI image specification](/blog/2022-10-28-understanding-container-images-oci-image-specification/index.md): In the previous article, we looked at how OverlayFS is used to work with container images. We learned how the image layers are associated with each other and how the copy-on-write layer gets created when a containers gets created from an image. Container registries such as Docker Hub and Quay offer container images for several popular applications and you can publish your images as well. There are different tools available to build and push images to container registries. Whatever container engine you use &ndash; Docker or Podman &ndash; you can simply pull the image [built using any tool] from the registry and use it. How is this interoperability made possible? The answer is the work Open Container Initiative (OCI) is doing.

- [Understanding container images - The fundamentals](/blog/2022-10-18-understanding-container-images-the-fundamentals/index.md): Containers have been around for a while. There is no need for an introduction to containers anymore but If you are still looking for some background, you can read a series of articles I have written on this blog. Docker simplified working with containers by introducing a way to package your applications and application dependencies as container images. It also provided the tooling that made running container instances from these images easy. This meant the user space tools abstracted all complex details and removed the need to know the inner workings of containers. Like every other technology, knowing the internals of how something is built becomes important when troubleshooting issues or optimizing what the technology does. And, of course, knowing / learning internals is fun too!

- [CUE language VS Code development container feature](/blog/2022-10-10-cue-lang-dev-container-feature/index.md): In an earlier article, I wrote a bit about Visual Studio Code development containers. I have been using devcontainers for different development environments extensively. This is helping me keep the environment consistent and helps me rebuild an environment from scratch almost instantly. I use dev containers for all my Go language, Python, Rust, and now CUE language learning and development work. You can find all my dev container definition in a sample repository I created on GitHub.

- [[Book] Azure Bicep - Zero to Hero is complete](/blog/2022-07-09-azure-bicep-zero-to-hero-s-complete/index.md): Finally! I published the final chapter in my book Azure Bicep - Zero to Hero today. I started this project more than a year ago with an intention that I will complete it before end of year 2021. However, a few personal life events made me shift focus to my family and that pushed the book timelines to the right. In the last few weeks, I spent good amount of my personal time to explore a few new areas within Bicep and formed the content for the last few chapters.

- [Local CI/CD for your Bicep templates](/blog/2022-07-09-local-ci-cd-for-your-bicep-templates/index.md): One of the very important aspects of Infrastructure as Code (IaC) is automation. And, it is not just about the configuration tasks that we must automate but it is also the entire validation and deployment / delivery workflow itself. We call this a continuous integration (CI) and continuous delivery / deployment (CD) pipeline. The core idea of a CI/CD pipeline is to accelerate testing and uncover any bugs and fix those bugs before the code reaches production. There are many ways and tools to implement a CI/CD pipeline. We can use GitHub actions or Jenkins or AppVeyor to name just a few. But, as a starting point, I prefer building a local CI/CD pipeline using git hooks. In this article, I will show you how I built a simple test and deploy pipeline for my Azure Bicep experiments.

- [Working with Azure Bicep Public modules registry](/blog/working-with-azure-bicep-public-modules-registry/index.md): Starting with Bicep version 0.5.6, Microsoft added support for consuming from a public module repository hosted on Microsoft Container Registry (MCR). To consume a module from Microsoft’s pubic registry, you need to use the following syntax.
1 module &lt;module-id&gt; &#39;br/public:&lt;module-group&gt;/&lt;module-name&gt;:&lt;version&gt;&#39; = { ... } br/public indicates that you want to consume the module hosted on MCR. Here is an example of creating a Azure virtual network using the virtual-network module. The version tag is mandatory.

- [Azure Bicep as a feature in VS Code development containers](/blog/bicep-feature-in-vscode-devcontainer/index.md): Visual Studio Code development containers are magic! With the help of VS Code Remote - Containers extension and Docker Desktop, you can run a full-featured development environment within a container.

- [Azure Policy Guest Configuration - Assignments](/blog/azure-policy-guest-configuration-assignments/index.md): In the last part of this series around Azure Policy Guest Configuration, you got an introduction to what is Guest Configuration. In this part, you will learn about assigning Guest Configuration policies and initiatives.
Azure Policy Guest Configuration (GC) is an extension to Azure Policy and therefore use the same JSON definition structure. For example, here is the (redacted) GC definition for the policy Audit Windows machines that do not have the specified Windows PowerShell execution policy.

- [[Book] - Annoucing Azure Bicep - Zero to Hero](/blog/book-announcing-azure-bicep-zero-to-hero/index.md):  Many are struggling to make ends meet ever since the pandemic started. Therefore, I decided that all proceeds from this book will go to charity.
Project Bicep is undoubtedly the best thing that happened in the Azure world, and I have been following the development and using Bicep quite often. I published a series of articles on this blog around Bicep. However, there is always more. This series gave me an idea about a complete book on Bicep. And, today, I am announcing this new book project - Azure Bicep - Zero to Hero. This book is still a work in progress and 30% complete. I will be spending a few more weekends completing the rest of the chapters. You can read it as I complete and provide your feedback. Since this is a self-published book, all updates to the book will be free forever.

- [Bicep v0.4 and the linter](/blog/bicep-v0-4-and-linter/index.md):  Bicep v0.4.1 was released yesterday. This release includes a bunch of bug fixes, more snippets, and most importantly, a linter!
Linters bring in a variety of capabilities to a developer toolkit. These tools can help you perform static code analysis, check compliance against a style guide, find syntax errors, and flag potential optimizations in the code.
For a developer, linting support in an IDE is a must. With this Bicep release, Microsoft added support for linting of Bicep files in VS Code and at the command line. To realize the benefits of linting, you must upgrade Bicep CLI and the VS Code extension to v4.0.1 or later. The current set of linter rules are minimal and taken from arm-ttk test cases. Both VS Code extension and Bicep CLI check for all available rules by default and all rules are set at warning level. Based on the level of a rule, you will see errors or warnings or informational messages within the editor.

- [Bicep Container Image](/blog/bicep-container-image/index.md):  While using the Azure CLI container image for one of my side projects, I started looking for an image that contains Bicep CLI as well. I found Mikolaj Mackowiak&rsquo;s Bicep-cli image. This was what I really needed and it includes both Azure CLI and Bicep CLI. However, just as a fun side project, I wanted to publish my own image as well.

- [Bicep Visualizer](/blog/bicep-visualizer/index.md):  I wrote a series of articles to introduce Bicep language fundamentals. I really enjoyed working on these articles and learnt quite a lot in that process.
The Bicep team released version 0.3.539 of Bicep command line as well as the VS Code extension a few hours ago and it came with a lot of goodness &ndash; bug fixes as well as new shiny things! One of the first things that caught my eye was the Bicep visualizer.

- [Bicep Language - Beyond Basics - Scoped Deployment](/blog/bicep-basics-beyond-basics-scoped-deployment/index.md):  When using ARM JSON templates, you can deploy to any of the four supported scopes &ndash; management groups, tenants, subscriptions, and resource groups. What you can deploy using ARM JSON templates differs between these different target scopes. For example, you can create resource groups at the subscription level only. You identify the target scope for an ARM JSON template using the $schema element in the template. Each scope has a specific schema.

- [Bicep Language - Beyond Basics - Conditional Deployment](/blog/bicep-basics-beyond-basics-conditional-deployment/index.md):  When you go to the Azure portal and deploy a service, you may have seen a prompt where you are asked to select an existing resource group or create a new resource group. And, a few more service deployment scenarios might ask you to select between selecting an existing or creating a new storage account.

- [Bicep Language - Beyond Basics - Modules](/blog/bicep-basics-beyond-basics-modules/index.md):  Reusability and repeatability are two basics requirements when you want to implement Infrastructure as Code (IaC) practices. You looked at how parameters can be added to Bicep files to make those programs reusable. This is the first step towards modularizing your programs. With the ARM JSON templates, you might have used linked templates that are meant to provide similar functionality. Bicep has support for modularity and helps simplify complex configurations into smaller reusable modules.

- [Slides and Demo Scripts: Global Azure Bootcamp 2021 - Azure Bicep](/blog/slides-and-demo-scripts-introduction-to-azure-bicep-gab-2021/index.md): Today I had the opportunity to present live (virtually) at the Global Azure Bootcamp 2021. My session was around Azure Bicep. I really enjoyed preparing for and delivering this session.
If you follow me on Twitter or this blog, you know I&rsquo;ve started a new series of articles on Azure Bicep. These articles are essentially notes from my Bicep learning sessions. There are a few more pending in the next few days, but overall, I really enjoyed learning about Bicep. This series and my session at GAB 2021 helped me share my learning with the community.

- [Bicep Language - Beyond Basics - Output](/blog/bicep-basics-beyond-basics-output/index.md):  You use outputs in ARM JSON templates to return value(s) from the deployment. Returning output from a template is not mandatory, but it is useful in scenarios such as deploying templates through a CI/CD pipeline or creating templates as reusable modules. To support this, Bicep language has an outputs element.
Output Syntax for adding an output element to Bicep is:

- [Bicep Language - Beyond Basics - Iterations](/blog/bicep-basics-beyond-basics-iterations/index.md):  When working ARM templates, you may have come across a situation where you want to provision multiple instances of a resource with similar configuration. For example, multiple storage accounts or virtual machines or multiple data disks attached to a virtual machine. In the JSON template language, the copy element is used for this purpose. Bicep language has a few different ways you can achieve this at different levels like resources, resource properties, and outputs. Variable iterations are not available yet and mostly coming as a part of 0.4 release which is due by May 25th 2021.

- [Bicep Language - Beyond Basics - Expressions](/blog/bicep-basics-beyond-basics-expressions/index.md):  Expressions are a critical part of any programming language. For a Domain-Specific Language (DSL) such as Bicep that transpiles to ARM template JSON, expressions add a great value in reducing the complexity in authoring templates. All built-in functions available within ARM JSON template language are available within Bicep as well. Using these functions and the supported language constructs, you can create some powerful expressions in Bicep that significantly reduces the complexity compared to JSON templates. In this article, you will learn about writing a few different ways of writing expressions.

- [Bicep Language - Beyond Basics - Variables](/blog/bicep-basics-beyond-basics-variables/index.md):  This part of the series will take you beyond Bicep language basics by showing you how to add variables to your Bicep files. If you have worked on ARM JSON templates, you know that variables exist there as well. The primary intention of using variables in either ARM JSON templates or Bicep language is to bring in a balance between free-form vs fixed configurations.

- [Bicep Language - Beyond Basics - Parameters](/blog/bicep-basics-beyond-basics-parameters/index.md):  In the previous part, you learned how you can get started with Bicep language to write your first Bicep file. As you learned, resource keyword along with resource declaration and resource properties are the minimum required in a Bicep file. In today&rsquo;s article, you will see what is beyond those basics.
Parameters One of the goals of Infrastructure as Code (IaC) practice is to create reusable and repeatable automation for deploying your infrastructure. To make this happen, you need to parameterize your infrastructure configuration definitions. In this case, your Bicep files.

- [Getting Started With Azure Bicep](/blog/getting-started-with-azure-bicep/index.md):  In the first part of this new series of articles on Azure Bicep, you learnt about what is Azure Bicep and why it is needed. In this part, you will learn about how to get started with Azure Bicep.
Installing Bicep To start working with Bicep you will need, at a minimum, the Bicep CLI. There are multiple ways to get Bicep CLI on to your system. On Windows systems, you can use any of the following methods.

- [Introduction to Azure Bicep](/blog/introduction-to-azure-bicep/index.md):  As I started preparing slides and demos for my session on Azure Bicep at the Global Azure Bootcamp 2021, I made a bunch of notes. As a part of this new series of articles on Azure Bicep, I will start sharing those notes here.
Azure Bicep is a new Domain-Specific Language (DSL) for declaratively deploying Azure resources. Bicep is not a general purpose programming language but a transparent abstraction for Azure Resource Manager (ARM) Templates. This ensures that the properties that are valid in ARM templates are valid in Bicep as well. Azure Bicep acts as a transpiler for generating ARM templates from Bicep files. But, what is wrong with ARM templates?

- [Retrieve and save Project Bicep examples from GitHub](/blog/retrieve-and-save-project-bicep-examples-from-github/index.md):  Project Bicep aims to simplify how you author Azure Resource Manager (ARM) templates. Bicep is a new declarative language and a transpiler. This transparent abstraction takes a bicep file and transpiles it into an ARM template JSON. The Bicep CLI provides the necessary capablities to compile bicep files to ARM templates and decompile ARM templates into bicep files.
Bicep language itself is very easy to understand and start using. However, as you start learning it may be helpful to see a few examples and draw some inspiration as you build your won bicep files. Project Bicep respository on GitHub has a good number of examples for varied level of complexity. While looking at these, I thought it may be a good idea to create scripts that can retrieve and save selected example locally.

- [Speaking at Global Azure Bootcamp India 2021](/blog/speaking-at-global-azure-bootcamp-india-2021/index.md):  Global Azure Bootcamp is happening this year from April 15th to 17th as an online event. And, I will be speaking this year. I submitted a couple of sessions and the Azure Bicep session got accepted.
So, what is Azure Bicep? Azure Bicep is the Domain-Specific Language (DSL) and a transparent abstraction built on top of Azure Resource Manager (ARM). The intention behind this project is to simplify authoring ARM templates. Bicep language transpiles into an ARM template JSON which you can deploy using already well-known methods and your existing CI/CD pipelines.

- [PowerShell script to download and install Azure CLI](/blog/powershell-script-to-download-and-install-azure-cli/index.md): Whenever I have to build a new development machine or VM, I usually go about installing a few development tools and command line tools that I use quite often. One such tool is the Azure CLI.
There are many alternatives for installing Azure CLI as a part of build process but what I use is this PowerShell script.
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 [CmdletBinding()] param ( ) $InformationPreference = &#39;Continue&#39; # Verify that the script is running as administrator if ([bool](([System.Security.Principal.WindowsIdentity]::GetCurrent()).groups -match &#39;S-1-5-32-544&#39;)) { # Download Azure CLI MSI package $uri = &#39;https://aka.ms/installazurecliwindows&#39; $request = Invoke-WebRequest -UseBasicParsing -Uri $uri -MaximumRedirection 0 -ErrorAction Ignore if(($request.StatusCode -ge 300) -and ($request.StatusCode -lt 400)) { $location = $request.Headers.Location $fileName = Split-Path -Path $location -Leaf $cliVersion = $fileName.Split(&#39;-&#39;)[2].trim(&#39;.msi&#39;) $downloadPath = &#34;${env:Temp}\$fileName&#34; Write-Information -Message (&#34;Downloading Azure CLI version {0} to {1}&#34; -f $cliVersion, $downloadPath) Invoke-WebRequest -UseBasicParsing -Uri $location -OutFile $downloadPath -ErrorAction SilentlyContinue -Verbose:$false if (Test-Path -Path $downloadPath) { Write-Information -Message (&#34;Installing Azure CLI version {0} from {1}&#34; -f $cliVersion, $downloadPath) #Start-Process -FilePath msiexec.exe -ArgumentList &#34;/i $downloadPath /qb /passive&#34; -Wait -ErrorAction Stop -Verbose:$false Write-Information -Message (&#34;Remove Azure CLI installer file from {0}&#34; -f $downloadPath) Remove-Item -Path $downloadPath -Force } } else { throw &#39;Cloud not retrieve the redirected URL&#39; } } else { throw &#39;This must run as administrator at an elevated PowerShell prompt.&#39; } This script downloads the most recent public build and installs it.

- [PowerShell and Secret Management - Updates](/blog/powershell-and-secrets-management-updates/index.md): At the recent Ignite event, Microsoft announced the release candidate 2 update of PowerShell SecretManagement module and the SecretStore extension vault modules. Both modules have been bumped up to version 0.9.1.
In the updated release, the secret management module has added support for secret metadata. The extension vault module must implement support for the metadata for this to work. To this extent, the SecretStore module has been updated to support metadata as well.

- [PowerShell and Secret Management - Using Secret Store Extension Vault](/blog/powershell-and-secret-management-using-secret-store-extension-vault/index.md): Update: This article has been updated to show the latest version of the secret store module.
In the last part, you have seen an introduction to secret management in PowerShell and towards the end you learned that they are vault extensions that provide the real functionality for storing and managing secrets. In this part of the series, you will learn more about the SecretStore vault extension for the SecretManagement PowerShell module.
The SecretStore extension vault is a wrapper around the .NET Crypto API. This vault stores the secrets to the local machine based on the current user account context. You can install this from the PowerShell gallery.

- [PowerShell and Secret Management - Introduction](/blog/powershell-and-secret-management-introduction/index.md): Update: This article has been updated to show the latest version of the secret management module.
Within the infrastructure automation, you always stumble upon a need to store and retrieve credentials. For example, imagine deploying a database instance and you want the database administrator username and password to be same across based on some IT standard. For this, you either have to hardcode the values in a script or prompt the user running the automation for these values. Storing secrets such as passwords, API keys, etc is a big NO. You should never do that. And, prompting a user will hamper the automation flow and will require manual intervention. This is where secret vaults play a role. You can store all secrets in a vault (local or remote) and then on-demand you can retrieve these values from the vault and consume in your automation. This method is most preferred when delegating automation tasks, running scripts in CI / CD pipelines, and in general anything that requires unattended automation.

- [Azure Policy Guest Configuration - Introduction](/blog/azure-policy-guest-configuration-introduction/index.md): As your Azure adoption increases and as you on-board various departments and groups within your organization to start using Azure services, it is important that you put in place some level of governance. As cloud architect, you would want control over who can create what type resources, where can they create those resources and how many. You would also want to standardize on the deployments so that the configuration of resources is in compliance with your organization standards. Among many other aspects like this you would want the ability to track your cloud cost more granularly for every service deployed in the cloud and for every business unit or group within your organization. To address this and to implement efficient governance Azure offers features and services such as Policies, Management Groups, Resource Graph, and Blueprints. Here is an excellent depiction of Azure governance architecture from the partner blog.

- [Get set Go - Methods in Go language](/blog/get-set-go-methods-in-go-language/index.md):  In the last two parts of this series, you looked at functions and anonymous functions. Both provide a way to create reusable blocks of code and each having its own purpose and use within Go programming. As learned earlier, Go is not a pure object oriented programming language. But, it does support some aspects of OOP. Methods in Go are one way to implement that &ldquo;object-orientedness&rdquo;. This is based on the concept that a type in Go language can have methods associated with it.

- [Get set Go - Anonymous Functions in Go language](/blog/get-set-go-anonymous-functions-in-go-language/index.md):  In the last part of the series, you learned about functions in Go language. Functions are first-class citizens in Golang. What this means is that you can not only use function declarations as just reusable code blocks but you can also assign functions to variables, use functions as parameters on other functions, and even return functions from other functions. This is achieved using function literals which are also known as anonymous functions.

- [Get set Go - Functions in Go language](/blog/get-set-go-functions-in-go-language/index.md):  Alright. In this series so far, you learned about some basics of programming in Go language, about data types, conditions, and loops. In all the examples in the earlier parts of the series, there was just the main function which is the entry point into a Go program. All execution of a Go program starts at the main function. The idea of a function in any language is to basically group together a set of instructions and these instructions can now be used repeatedly as needed. As you learned, the main function is a bit special though. Functions promote reusability and readability of your code. In this part of the series, you shall learn about functions in Go language.

- [Get set Go - Labels in Go language](/blog/get-set-go-labels-in-go-language/index.md):  In the two previous parts of the series, you learned how to use if, switch, and for loop in Go language. You also learned about using break and continue statements in Go language to alter the control flow. What you have seen in the examples with these statements was an unlabeled way to break or continue the iteration. Go language supports labels that let you transfer control to the place in the (same function) code where the label is defined. There are different ways to use labeled statements.

- [Get set Go - Loops in Go language](/blog/get-set-go-loops-in-go-language/index.md):  With what you learned about basic data types and structs in this series so far, you are now ready to look at the looping construct in Go language. Go language is simple and to that extent Go has just one looping construct &ndash; For loop.
For loop A few programming languages that you may have used in the past may have more than one looping construct &ndash; do - while, do - until, for, and foreach and so on. You use a specific construct based on what you need to achieve and type of conditions that need to be tested. However, Go language has just one looping construct &ndash; the for loop. Although there is just one looping construct, go supports multiple variations of this construct for different use cases.

- [Get set Go - Condition statements in Go language](/blog/get-set-go-condition-statements-in-go-language/index.md):  With what you learned about basic data types and structs in this series so far, you are now ready to look at the branching constructs in Go language. You may have used branching constructs such as if..else if..else and switch statements in other languages. Go language too offers these constructs.
If .. else if .. else If construct in any programming language is a branching construct and moves the execution from one place in the code to another based on a condition. Go language is no exception.

- [Get set Go - Structs in Go language](/blog/get-set-go-structs-in-go-language/index.md):  So far in this series, you learned about different built-in data types such as integers, strings, floats, arrays, slices, maps, and pointers. All of these allow only one type of data. All elements in the array have to of the same time. All keys and values have to be of the same type. What if you have need to combine multiple types and create a custom type for your programs? This is where structs play a role in Go language.

- [Get set Go - Maps in Go language](/blog/get-set-go-maps-in-go-language/index.md):  In the previous part of this series, you learned about pointers in Go language. In this part, you will learn about maps in Go language. Maps are another built-in data type in Go and maps store key-value pairs. This is like dictionaries in other programming languages.
Maps First, look at the below example.
1 2 3 4 5 6 7 8 package main import &#34;fmt&#34; func main() { var m1 map[string]int fmt.Println(m1) } In the above example, variable m1 is of map data type and declares an empty map. When you run this, all you will see is map[]. The zero value of map keys will be nil and therefore this will be called a nil map. Maps store key-value pairs. So, if we generalize the syntax from the above example, it will be:

- [Get set Go - Pointers in Go language](/blog/get-set-go-pointers-in-go-language/index.md):  In the previous part of this series, you learned about slices in Go language. Slices, unlike arrays, allow flexibility and certainly are the most used compared to arrays in Go language. Towards the end of the article on slices, you learned that slices are essentially references to an underlying array and any changes made to the slice will reflect in the underlying array as well. In this article, you shall learn about another reference type called pointers in Go language.

- [Get set Go - Slices in Go language](/blog/get-set-go-slices-in-go-language/index.md):  In the previous part of this series, you learned about arrays in Go language. As you have learned, arrays have a fixed size and therefore you cannot resize (grow or shrink) arrays. This limitation can be overcome using slices in Go language. In this part, you will learn about slices and how to use this data type in your Go programs.

- [Azure Community Conference - 2020](/blog/azure-community-conference-2020/index.md):  Azure Community Conference is a 3 day multi-track virtual event scheduled to go live on November 24th 2020. This event is being organized by the Bangalore Azure User Group. The target audience of this virtual event are cloud and web developers working with open source and cloud native technologies on the Microsoft stack. This event will be a free two conference days that includes a free workshop as well. Registration for this event is open.

- [Get set Go - Arrays in Go language](/blog/get-set-go-arrays-in-go-language/index.md):  In the earlier part of this series, you looked at types in Go language and learned about the basic data types &ndash; int, float, and strings. You also learned about other categories of data types such as aggregate types, reference types, and interface types. In the next subsequent parts, you will dive into each of these different types.
Aggregate Types Aggregate types in Go language include arrays and structs. These data types are formed by combining basic data types. Learning structs will require knowledge of a few more things about Go language and Go types. You will learn about that soon. In today&rsquo;s part, you will learn about arrays.

- [Get set Go - Types in Go language](/blog/get-set-go-types-in-go-language/index.md): After looking at variables in the previous part of this series, you have a fair understanding of using different types of variable declarations and using the variable values in your program. Each of these variables you created and used have an associated type. The type of a variable dictates what that variable is allowed store. So, what are different data types in Go language? You will learn that today!
Data types in Golang Go language has several built-in data types and these types can be classified into three different categories &ndash; basic data types, aggregate data types, reference types, and interface types.

- [Get set Go - Variables in Go language](/blog/get-set-go-variables-in-go-language/index.md):  In the last part of this series, you looked at writing your first program and understood the program structure and understood the keywords such as package, import, and func. It was a good start. In this part, you will learn how to declare and use variables in Go programs.
Naming convention in Go Before you start looking at variables in Golang, you must first understand some rules for naming different program entities &ndash; variables, types, statements, packages, and constants &ndash; in Go.

- [Azure Resource Manager - Using secrets in ARM templates](/blog/azure-resource-manager-using-secrets-in-arm-templates/index.md):  In this series so far, you have seen how to get started with ARM templates, parameterize them by adding parameters, and optimize them using variables, expressions, and user-defined functions. It is now time to move on to more advanced topics and begin building the template for the remaining components of the architecture shown above. In the architecture shown above, you will implement an ARM template that provisions virtual machines, requiring the administrator and domain-join credentials as inputs. Also, the configuration scripts used to configure the guest OS may need to access an internal storage account blob, which requires the storage connection strings and access keys. Storing these secrets in plain text is not recommended. Also, as an architect, you may want to standardize on passwords for local administrator accounts and avoid sharing domain-join credentials with any user when provisioning an ARM template. This needs a more centralized credential and secret store. Azure Vault provides this capability. Today, you will learn how to handle secrets such as passwords, access keys, certificates, and more in an ARM template.

- [Get set Go - First program](/blog/get-set-go-first-program/index.md):  With the background provided in the introduction part of this series, you must now be equipped with the necessary tools for the job &ndash; learning Go! So, in this part, you will write your first program in Go language. You will not only learn about the very minimal structure needed for a Go program but you will also learn about organizing your Go programs and why it matters. Get set Go!

- [Get set Go - Introduction to Go language](/blog/get-set-go-introduction-to-go-lang/index.md):  Go programming language isn&rsquo;t new. It existed for more than 13 years now. The idea of Go language was first conceived in 2007 at Google by Ken Thompson, Robert Griesemer, and Rob Pike. Go was developed in response to some of the challenges the development teams at Google were facing internally. Some of these issues included ridiculously longer build times, baggage that legacy languages imposed on modern infrastructure / systems development, and uncontrolled dependencies among many other pain points. Go was designed to make it easy and efficient to program modern multicore systems, web service backends, and command line tools. The design considerations such as rigorous dependency management and efficiency at scale make Go one of the best modern programming languages.

- [Azure Resource Manager - Using functions in ARM templates](/blog/azure-resource-manager-using-functions-in-arm-templates/index.md):  Expressions in ARM template language are a way to reduce complexity in the templates. Using parameters and variables in ARM templates always requires the use of expressions. Parameter and variable values are combined with standard (built-in) template functions (STF) to create complex expressions to implement the business requirements and implement known configurations. Updated ARM template syntax introduced the ability to implement user-defined functions for complicated expressions and use repeatedly in your templates. In this part, you will learn more about standard template functions and learn how to implement user-defined functions.

- [Azure Resource Manager - Using expressions and variables in ARM templates](/blog/azure-resource-manager-in-30-days-using-expressions-and-variables-in-arm-templates/index.md):  So far in this series, you learned how to use VS Code and ARM Tools extension to get started with the ARM template development, how to perform a subscription scoped deployment to create a resource group and then looked at resource group scoped deployment to create a storage account and a virtual network resources. You learned about parameterizing the ARM template so that it becomes reusable. Equipped with that knowledge, you can now start digging into expressions and variables in the ARM templates.

- [Azure Resource Manager - Adding parameters to ARM templates](/blog/azure-resource-manager-adding-parameters-to-arm-templates/index.md):  In the last part of this series, you learned the template syntax and authored an ARM template that provisions the resource group needed for the above architecture. This deployment was performed at the subscription level using Azure CLI. In this part, you will create another template for performing additional resource &ndash; storage account and virtual network &ndash; provisioning at the resource group level. You will, then, parameterize this template to understand how to add parameters to ARM templates.

- [Azure Resource Manager - ARM template basics](/blog/azure-resource-manager-arm-template-basics/index.md): In the last two parts of the series, you have seen a brief introduction to Azure Resource Manager and the services that complement ARM to extend its functionality or support the services offered by ARM. You also looked at an overview of ARM templates and the tools available to design, author, and deploy these templates.
Starting this part of the series for the next few days, you will start learning about authoring ARM templates right from the basics and to the advanced template syntax and techniques with a goal to build an ARM template that helps provision the following architecture.

- [GitHub Hub CLI - An extension to git command-line](/blog/github-hub-cli-an-extension-to-git-command-line/index.md): As I started writing more in markdown and using GitHub pages, my work with git and GitHub increased quite a bit. At times, I find myself fighting with git conflicts and merges. I am not an expert in git command-line and in a complex scenario, I figure my way out looking for some help.
You may have already seen in my articles that I use GitHub CLI a lot to work with my GitHub repositories, pull requests, and issues. I recently started using another CLI tool from GitHub called hub. Hub is an extension to git command-line and makes working with GitHub from the command-line a breeze.

- [Azure Resource Manager - Tools for ARM template design, authoring, and deployment](/blog/azure-resource-manager-tools-for-arm-template-design-authoring-and-deployment/index.md): The first part of this series of articles on Azure Resource Manager (ARM) provided a quick overview of ARM. You looked a quick introduction to ARM templates as well and looked at an overview of different template deployment methods. In this part of the series, we will look at different methods to author ARM templates and an in-depth look at preferred deployment methods. You can choose any of the methods mentioned here but this series of articles will use one preferred method each for authoring and deploying templates. You will read more about those methods and the reasons behind that choice as well.

- [Azure Resource Manager - Introduction](/blog/azure-resource-manager-introduction/index.md): A while ago, I had written a series of articles introducing and diving deep into authoring Azure Resource Manager (ARM) templates. A few things have changed in authoring ARM templates and new features got added in the recent past. I thought it is probably a better time to revisit and do it all over again. At the same time, I thought why just limit to just authoring ARM templates. Over the years, I made notes around ARM and learned quite a bit. Starting today, I will share those notes with you here.

- [PowerShell commands to generate Git.io shorturls](/blog/powershell-commands-to-generate-git-io-shorturls/index.md):  Git.io &ndash; service by GitHub &ndash; can be used generate the short URLs for any GitHub.com based URL. You can navigate to Git.io and paste a GitHub URL into the input box and generate a short URL. Note that this works only with GitHub.com URLs.
I have been using gists a lot lately and the URL to gists is usually very long and includes guids. For these gist URLs, I have been using the git.io service and wrapped that in a quick PowerShell module to help me generate these URLs. Today, I published it on GitHub and PowerShell Gallery for you to use it as well.

- [Adding visitor counter to statically generated web pages](/blog/adding-visitor-counter-to-statically-generated-web-pages/index.md): One of the to-do list items I had for this static blog site was to figure out a way to add page views counter to every article. This is especially important for PowerShell Magazine articles as we move that to a static site as well. I tried looking for a few solutions but found mostly things that would require playing with JavaScript.
While looking at different GitHub profile pages this morning, I came across a visitor counter on Tyler&rsquo;s page. This piqued my interest and looked at how he was generating it. It was made possible through a service hosted on glitch.me. There is a GitHub repo that contains the source for this. At first, it seemed like a thing only for GitHub readme markdown but looking at the docs, I realized that I can embed that in simple webpages too.

- [My custom Hugo shortcode collection](/blog/my-custom-hugo-shortcode-collection/index.md): In the past I wrote here about the Az Deploy button shortcode and the channel9 video embed shortcode for Hugo generated static pages. I have been creating some simple shortcodes for my own use on this blog as I start writing on different technologies again. A couple of more shortcodes I added recently are for the ARM template visualize button and launching Azure Cloud Shell.
Today I decided to publish these shortcodes as a collection on GitHub. Instead of writing about each and every shortcode &ndash; unless it has significant value to others as well &ndash; this article will get updated every time I publish a new shortcode.

- [GitHub workflow for streamlined publishing of static pages](/blog/github-workflow-for-streamlined-publishing-of-static-pages/index.md): Ever since I moved to this site/blog to static pages using Hugo and GitHub pages, I have been finding ways to optimize the publishing experience. If you ever used Wordpress, the plugins that are available to customize the site look and feel and functionality will spoil you. They make your life easy by extending base functionality and you can achieve pretty much anything you want as long as there is a plugin for that and you will always find one. Moving from such an environment to a static page generator makes you feel that you have to do all the heavy lifting.

- [Microsoft MVP for another year](/blog/micosoft-mvp-for-another-year/index.md): July 1st of every year is a hit-refresh or F5 day for most of us in the Microsoft MVP community. This year is no different.
I will be a Cloud &amp; Data Center Management (CDM) MVP for another year. This is my 12th award and still excited the same as my first year in the MVP award program. The last MVP year (2019-2020) was good with PowerShell Conference Asia moving to India and being super successful with over 230 attendees. For the upcoming MVP year (2020-2021), I have a good number of things lined up on this blog already. You will start seeing those contributions starting this month. I am in the process (very slow) of moving PowerShell Magazine as well to a static site so that it becomes easy to maintain and author more articles. Stay tuned for these updates.

- [How I use GitHub Gists to store and display code snippets in blog posts](/blog/how-i-use-gists-to-store-and-display-code-snippets-in-blog-posts/index.md): After I moved to a static site hosted on GitHub Pages, I was looking at different options to share the code snippets in a better way. With the Hugo generated static pages, there are a few options for code sharing in an article.
You can use the simple pre-formatted text in markdown. This is usually done by enclosing code snippet in ```. This appears (in Hugo Coder theme I am using) as a big black block and has no way to choose the right syntax highlighting based on what programming language the snippet is written in. There are no line numbers or no highlighting of lines in a code snippet and so on.

- [Embedding Channel9 Videos in Hugo Static Pages](/blog/embedding-channel9-videos-in-hugo-static-pages/index.md): I have been trying a few things with my new blogging platform here and in the preparation for a new series of articles, I wanted the ability to embed Channel9 videos. This is Hugo platform! So, it was not a big deal. I just had to create another shortcode like the one I created for Deploy to Azure button.
Here is the code for the shortcode.
GitHub Gist This code snippet requires your consent to load content from GitHub, which may set cookies. 
- [Hugo shortcode for Deploy to Azure Button in a Static Page](/blog/adding-azdeploy-button-in-static-pages-using-hugo/index.md): As I moved to this new implementation of the blog using Hugo and GitHub pages, I decided to move older article series around authoring Azure Resource Manager templates. This series was one of the most visited ones on this blog. This series provided an incremental way to learn how to author ARM templates and deploy these ARM templates. In each of these articles, I added a &ldquo;Deploy to Azure Button&rdquo; that you could just click and open the template directly in Azure Portal ready for deployment. The code behind this button usually looks like this.

- [Moved Blog to a Static Site Using Hugo and GitHub Pages](/blog/moved-to-static-site-using-hugo-and-github-pages/index.md):  It has been a while since I wrote something here. I was using a Digital Ocean droplet to run this blog using self-hosted version of Wordpress. Overall, it was good experience but there are issues with Wordpress installs. While it allows greater levels of customization, when all you need is just sharing your ideas, it becomes rather heavy. Also, the recent vulnerabilities made me think about it again. While I was having this thought process, I came across a few sites hosted on GitHub pages using Jekyll and simple static pages. I really wanted to move but moving from Wordpress to static site isn&rsquo;t an easy option. There are Wordpress plugins available to export to markdown format and then host the same on GitHub pages. This is done using Hugo. While, on paper, it looks very easy job, it is actually not. The exported markdown files won&rsquo;t be compatible with your selected Hugo theme. As I was wondering about this whole process, Chen came to the rescue and gave me a head start. After reviewing what he sent and doing some more research, I decided to build the site from scratch.

- [Slides: Global Azure Bootcamp - Azure CLI 2.0](/blog/slides-and-demo-scripts-from-azure-cli-2-0-tips-and-tricks-session-at-globalazurebootcamp/index.md): I had the opportunity over the weekend to speak at the Global Azure Bootcamp organized by local Microsoft user groups. This event was held at ABB office in Bangalore and I must say that they have done a splendid job. It was a very well organized event. I had only one session and it was about Azure CLI 2.0 Tips and Tricks. I mentioned earlier that I am using CLI mostly these days to work with Azure resources and in this session I shared some tips and tricks around making it easier to work with Azure resource management using Azure CLI 2.0.

- [Slides: BITPro UG Meet - Tests Driven PowerShell Learning using PSKoans](/blog/session-slides-tests-driven-powershell-learning-using-pskoans/index.md): At today’s Bangalore IT Pro and PowerShell Bangalore User Group (PSBUG) event, I had the opportunity to showcase the PSKoans module by Joel Sallow. This module has a set of Pester tests that you have to fix as you proceed in your efforts towards mastering PowerShell.
I have found this method engaging and decided that I must show this to the community.

- [Slides: Experts Live India – Azure at the command line](/blog/session-slides-experts-live-india-azure-at-the-command-line/index.md): I had the opportunity to speak at the inaugural edition of Experts Live India 2019 over the weekend. It was a great experience. The organizers did a great job even when there were teething facilities issues that plagued the event right from the start.
My first session was on Azure at the command line. In this session, I walked through both Az CLI 2.0 and Az PowerShell modules and explained the differences to help the audience choose the right tool for their job. The slides do not contain a huge deal of information as I preferred showing the audience a good demo to understand the differences.

- [Slides: Experts Live India – PowerShell Data as File System](/blog/session-slides-experts-live-india-powershell-data-as-file-system/index.md): I had the opportunity to speak at the inaugural edition of Experts Live India 2019 over the weekend. It was a great experience. The organizers did a great job even when there were teething facilities issues that plagued the event right from the start.
I had planned only for one session at the event but as some international speakers could not make it to the event, I was asked to take up one more session and I chose to show the audience SHiPS module. Hierarchical data can be read as a file system in PowerShell using SHiPS open source module. In this session, I walked the audience through what are providers and what is the advantage of implementing PowerShel providers using the SHiPS module.

- [Slides: Azure MVP Bootcamp – Microsoft Azure Cloud and DevOps](/blog/slides-from-azure-mvp-bootcamp-microsoft-azure-cloud-and-devops/index.md): I had the privilege to join the Miracle team at Visakhapatnam today for an Azure MVP bootcamp. I spoke about Microsoft Azure Cloud and DevOps. This event, I must say, it a very well organized and orchestrated. I have been to conferences and events in India and outside India and never seen this level of care of speakers and attendees anywhere.
I started my day with a good morning walk along the beach and just loved it. I wish I get to do this often.

- [Slides: PowerShell Conference Asia 2016](/blog/powershell-conference-asia-2016-slides-and-demo-scripts/index.md): Yet another successful conference this year. All kudos to Matt, Milton, Sebastian, and Ben for making this year’s PowerShell Conference Asia such a super hit.
I had a great time speaking about Developing Infrastructure Code for CI &amp; CD and Using PowerShell DSC with AWS Cloud. I have had the opportunity to do an impromptu session with Deepak &amp; Matt on the release pipeline. Overall, I am really happy with the way all my sessions went through and the response from attendees.

- [Slides: PowerShell Conference EU 2016](/blog/session-slides-and-demo-scripts-from-powershell-conference-eu-2016/index.md): I returned home from PowerShell Conference EU this morning. Wow, what an amazing event was that! Seriously, right from the opening event to each and every session, this was truly the global PowerShell conference with attendees from more than twelve countries and speakers from four continents.
I had two sessions at this conference and these were very well received. I uploaded the slides and demo scripts to a PSConfEU Github repository. Feel free to use them as needed. For those who prefer, Slideshare instead, here are the slide decks. The sessions recordings should be online in a couple of weeks. I will updated this post.

- [Custom Template Deployment Improvements in Azure Portal](/blog/custom-template-deployment-improvements-in-azure-portal/index.md): When testing ARM templates, I typically use the custom template deployment option in the Azure Portal. This UI option just got better! This deployment option now supports lists the parameters, variables, and resources used in template in a nice treeview navigation.
To start a new deployment, you can click on New -&gt; Template Deployment.
This opens up the Custom Deployment blade. Click on Edit Template.

- [Building Azure Resource Manager Templates – Forcing WMF 4.0 when using DSC Extension](/blog/building-azure-resource-manager-templates-forcing-wmf-4-0-when-using-dsc-extension/index.md): This subject won&rsquo;t really need a post of its own but I will do it anyway. In the previous part of this ARM series, I showed how we can use the domain join extension instead of DSC to join a VM to an existing AD domain. This resulted is huge savings in time taken to deploy the entire scenario. When I was talking to my good friend, fellow PowerShell MVP and all-things-Azure guru, Ben Gelens, he was quick to point that the delays when using DSC extension here could be due to the install of WMF 5.0. Indeed!

- [Building Azure Resource Manager Templates – Using Domain Join Extension](/blog/building-azure-resource-manager-templates-using-domain-join-extension/index.md): The scenario that we used to understand and build ARM templates contained a domain controller VM along with one or more VMs that joined the domain service hosted by the DC VM.
To make sure the VMs join the domain, we used PowerShell DSC configuration. One of the biggest quirks, at least what I faced, with DSC extension with ARM templates is that it takes little longer to complete. For example, the complete scenario deployment took almost 48 minutes to deploy. I am not making up that number. Here is the proof.

- [Building Azure Resource Manager Templates – Putting it all together](/blog/building-azure-resource-manager-templates-putting-it-all-together/index.md): In this series so far, we looked at building ARM templates by example. The focus was not really on the resource types or how to use resource definitions. Instead, our focus was on learning the basics of template language. To that extent, we have a scenario that we want to deploy and we are incrementally building the template for it.
While building an ARM template for this, we looked at how to use parameters and variables. We looked at using copy object to create multiple instances of a resource type without really writing the resource definition multiple times. We went on to find out how we can define dependencies between different resource types so they are orchestrated in the right order. We looked at how we can decompose the template into purpose-specific external templates and how to link them together. While learning these concepts, we created a template that almost built the scenario we started with.

- [Building Azure Resource Manager Templates – Using Linked Templates](/blog/building-azure-resource-manager-templates-using-linked-templates/index.md): One of the ARM template authoring best practices is to decompose the JSON template, if applicable, into multiple target-specific templates. Think of this as creating re-usable code. You can leverage the re-usable parts of your code within multiple aspects of your application or the deployment.
For linking different external templates within the main template, we need to define the Microsoft.Resources/deployments resource instance. Before we proceed let us look at the scenario for which we are building an ARM template.

- [Building Azure Resource Manager Templates – Defining Resource Dependencies](/blog/building-azure-resource-manager-templates-defining-resource-dependencies/index.md): We will continue learning about building ARM templates by looking at how we can define dependencies between resources. To recap, here is the scenario we are working on.
In the earlier parts of this series, we created the storage account, virtual network, a public IP, a load balancer, and added all inbound NAT rules required for the virtual machine RDP access. If you notice, we have components that depend on others. For example, the inbound NAT rules depend on the load balancer. Similarly, VMs depend on network interfaces which in turn depend on the virtual network. In the absence of dependencies, ARM will attempt to deploy these resources in parallel which may result in errors. So, within the resource template, we need to define these dependencies so that ARM can make decisions about the deployment sequence. There are multiple ways of doing this.

- [Building Azure Resource Manager Templates – Using Copy Object](/blog/building-azure-resource-manager-templates-using-copy-object/index.md): If you are following this series, by now you know how to use parameters and variables in ARM template language. We used that knowledge to create a template that creates relevant things to like storage account and virtual network. Let us revisit the scenario.
In this part, we will extend the template to add publicIP, load balancer, and RDP endpoints for the virtual machines. Before we do that, let us review what we need. To achieve what we want in this, we will use copy object in the template.

- [Building Azure Resource Manager Templates – Using Variables](/blog/building-azure-resource-manager-templates-using-variables/index.md): If you have been following this series on ARM templates, in the last part, we started with a sample scenario that we are using to build an ARM template. Here it is again.
In the last part, we completed creation of storage account required for backing the OS and data disks for virtual machines in the deployment. The next step in our scenario is to create the virtual network for the VM connectivity. A virtual network and any subnets required must exist before the creation of virtual machines. So, in today’s article, we will see how we can use variables in the ARM template language while incrementally building an ARM template for our scenario and add virtual network. Towards the end, we will review some best practices guidance with regards to using parameters and variables in developing ARM templates.

- [Building Azure Resource Manager Templates – Using Parameters](/blog/building-azure-resource-manager-templates-using-parameters/index.md): In the earlier parts of this series, we briefly looked at different methods of deploying ARM templates and then basics of ARM template language. Starting today’s article, we will see our leaning in action. To get going with our leaning, we will start building the ARM template for the following scenario.
We will incrementally build this scenario by learning different aspects of ARM template language. We won’t be covering all resource types in Azure but we will cover every aspect of the ARM template language and functions and expressions used in the template language. We will also review best practices in building these ARM templates as we proceed forward in this series.

- [Slides: Future Unleashed – Authoring ARM Templates](/blog/slides-from-future-unleashed-2015-authoring-azure-resource-manager-templates/index.md):  I had a great time at Future Unleashed 2015 talking about Authoring Azure Resource Manager Templates. This was a one-day technical event where there were 11 tracks covering different aspects from Cloud to Data to Analytics.
The breakout session setup was a little weird. There were no walls for the breakout session room (or whatever you call it). It was just a big expo hall with 11 tracks spread across like exhibitor booths. All the attendees had to wear a headset to hear the speaker.

- [Building Azure Resource Manager Templates – The Basics](/blog/building-azure-resource-manager-templates-the-basics/index.md): As a part of this series, we will go through the language semantics of writing ARM templates. We will explore each and every aspect of the ARM template language and go through multiple examples to understand the concepts better. We won’t focus much on the resource types and resource definitions until we complete the discussion around the language semantics and artifacts.
Let us get started.
Tools for the job JSON templates for ARM can be edited in notepad too. But, there are better tools that can help us with this.

- [Building Azure Resource Manager Templates – An Introduction](/blog/building-azure-resource-manager-templates-an-introduction/index.md): Azure Resource Manager isn’t a new thing! It was announced during Build 2014. ARM is certainly the preferred way, with more and more services getting ARM support, to deploy Azure services. Microsoft also announced that the upcoming Azure Stack release will feature Azure Resource Manager and the template deployment. This means that learning how to use ARM and write you own templates is an essential skill not just for public cloud but also for the private and hybrid cloud administrators.

- [Containers – What are they? And, their history! – Part 2](/blog/containers-what-are-they-and-their-history-part-2/index.md): In the earlier article, I explained the hurdles in the traditional and virtualized ways of implementing workloads. Also, we looked how the DevOps challenges are demanding changes the continuous delivery and integration processes. Towards the end, I’d mentioned that Containers are an answer.
In today’s article, we will see what are containers and some history behind containerization. Let us get started. In the subsequent parts, we will dig into each building block used within container technologies and understand how to use them.

- [Slides: Reboot Camp Bangalore - Cloud Computing for IT Professionals](/blog/slides-from-reboot-camp-bangalore-keynote-cloud-computing-for-it-professionals/index.md): I had the opportunity to do the keynote at the Microsoft Reboot Camp Bangalore. This is a community driven event with support from Microsoft. The theme for the event was cloud computing and Microsoft Cloud offerings.
In my keynote, I talked about what cloud computing means to the IT professionals and how they should get ready for the future.
Here are the slides from my keynote.
SlideShare Presentation This presentation requires your consent to load content from SlideShare (LinkedIn), which may set tracking cookies. 
- [Containers – What are they? Why do you need one? – Part 1](/blog/containers-what-are-they-why-do-you-need-one-part-1/index.md): Containers? No, not the shipping containers but the much-hyped container technology in the computer industry. I am sure even if you have not used any of the existing container technologies, you must have at least read or heard about them.
I am not the first one to write about containers and this is certainly not an in-depth overview of container technologies. I will eventually get there and show you how you can deploy different container technologies. This series of articles is about my own thoughts and a documentation for my own reference. In the process, I wish to help my readers as well.

- [Slides: Community Day 2014 – Introduction to Microsoft Azure Compute](/blog/session-slides-community-day-2014-introduction-to-microsoft-azure-compute/index.md): Microsoft Azure offers several services each categorized into one of the four major categories – Compute, Data, App, and Network Services. This session takes you through an overview of the Microsoft Azure Compute Services.
SlideShare Presentation This presentation requires your consent to load content from SlideShare (LinkedIn), which may set tracking cookies. 
- [Slides: Reboot IT, Bangalore - Hyper-V Replica](/blog/reboot-it-bangalore-2014-session-slides-hyper-v-replica/index.md): Here are the session slides for my second talk on Hyper-V. In this session, I had introduced Hyper-V Replica and demonstrated how to manage Hyper-V replica in Windows Server 2012 and Server 2012 R2.
SlideShare Presentation This presentation requires your consent to load content from SlideShare (LinkedIn), which may set tracking cookies. 
- [Slides: Reboot IT, Bangalore - Whats new in Server 2012 R2 Hyper-V](/blog/reboot-it-bangalore-2014-session-slides-whats-new-in-server-2012-r2-hyper-v/index.md): Here are the slides I used for my first session at the Reboot IT conference in Bangalore. We had over 130 attendees and the entire event was very well received.
SlideShare Presentation This presentation requires your consent to load content from SlideShare (LinkedIn), which may set tracking cookies. 
- [Slides: PowerShell Saturday - Windows PowerShell 4.0](/blog/powershell-4-0-slides-and-session-recording-from-powershell-saturday-singapore/index.md): I just completed my online session on PowerShell 4.0, First Look for the Singapore PowerShell User Group. It went on pretty well. Here are the slides I used for this session.
SlideShare Presentation This presentation requires your consent to load content from SlideShare (LinkedIn), which may set tracking cookies. 
- [Slides: Virtual Tech Days - Windows PowerShell 3.0 – A first look](/blog/vtd-session-windows-powershell-3-0--a-first-look--session-recording/index.md):  SlideShare Presentation This presentation requires your consent to load content from SlideShare (LinkedIn), which may set tracking cookies. 
- [Slides: VTC Online - Desired State Configuration](/blog/slides-and-demo-scripts-from-my-desired-state-configuration-session-at-vtc-online-india/index.md): The Microsoft MVP community in India delivered a Virtual Tech Conference (VTC) online yesterday and I had the opportunity to talk about one my favorite topics in PowerShell 4.0 – Desired State Configuration. I used this opportunity to build a custom DSC resource for managing hosts file using DSC. It was a good learning experience for me. And, I see a lot more possibilities with this. Here are the slides I used for this session:

- [Monitoring Volume Change Events in PowerShell using WMI](/blog/monitoring-volume-change-events-in-powershell-using-wmi/index.md): While I was preparing a few demo scripts for a Bangalore IT Pro UG meet session, I tumbled upon on WMI event class Win32_VolumeChangeEvent. This one is interesting. It is derived from Win32_DeviceChangeEvent class and gives us the ability to monitor local drive events directly.For example, you can get a notification when a local drive or mount point gets removed or added. The following table shows a list of event types we can monitor.

- [Slides: BITPro UG Meet - PowerShell and WMI Eventing](/blog/bitpro-ug-meet-powershell-and-wmi-eventing-slides-demo-scripts/index.md): I did a session at @BangaloreITPro this afternoon and the topic was “PowerShell WMI Eventing for IT Pros”. This session’s focus was to give an introduction to WMI events and how to use them in system administration. Here are the slides and demo scripts I used.
SlideShare Presentation This presentation requires your consent to load content from SlideShare (LinkedIn), which may set tracking cookies. 
- [Attaching scripts or tasks to Windows event log entries using PowerShell and WMI](/blog/attaching-scripts-or-tasks-to-windows-event-log-entries-using-powershell-and-wmi/index.md): During a few load test iterations on a SharePoint farm, I started seeing some SQL exceptions in the application log of SharePoint servers. If you are familiar with SharePoint platform, you may have seen these events such as event ID 3355. This event complains that the SharePoint server cannot connect to SQL server. This need not really mean that the DB server is offline. So, to find out the real reason behind these event logs, I needed to start some trace activities whenever event ID 3355 gets logged.

- [ebook: Layman's Guide to PowerShell 2.0 remoting](/blog/laymans-guide-to-powershell-2-0-remoting/index.md): This eBook has been updated since the initial release.
You can download this ebook for free at https://www.ravichaganti.com/ebooks/AlaymansguidetoPowerShell2remotingv2.pdf.
Here are the additions in the update.
Chapter 2 Enable remoting for only a specific network adapter Remoting in an Enterprise
Chapter 3 Run script files on remote computers
Chapter 9 Added a note on Domain controller credential delegation
​	Updated a note on Windows XP/2003 support for CredSSP
Appendix A Added some more FAQ

- [ebook: WMI query language (WQL) via PowerShell](/blog/ebook-wmi-query-language-wql-via-powershell/index.md): This eBook has been updated since the initial release.
You can download this ebook at no cost at https://www.ravichaganti.com/ebooks/WMIQueryLanguageviaPowerShellv20.pdf.
Here are the additions in the update.
Chapter 2 Enable remoting for only a specific network adapter Remoting in an Enterprise
Chapter 3 Run script files on remote computers
Chapter 9 Added a note on Domain controller credential delegation
​	Updated a note on Windows XP/2003 support for CredSSP
Appendix A Added some more FAQ

- [Passing variables or arguments to an event action in PowerShell](/blog/passing-variables-or-arguments-to-an-event-action-in-powershell/index.md): Update: Based on feedback from @ShayLevy, we don’t have to send the background job object to Event action as a variable. It is already available as $event.Sender or just $sender. You can see that in the image below. I just picked up a wrong example to explain the -MessageData parameter. Although, the article originally showed a background job as an example, it was just to show how any object can be passed to event action. For example, if all you want to access is a background job object which you are monitoring using Register-ObjectEvent, you can access the job name using $event.Sender.Name or $sender.Name, job Id using $event.Sender.Id or $sender.Id, and so on.

- [Slides: TechED India - PowerShell in the Enterprise](/blog/teched-india-2011-powershell-in-the-enterprise-slides/index.md): Here are the slides I used for my session “PowerShell in the Enterprise” at TechED India 2011. This was my first ever TechED session and went pretty well. There were 100+ attendees at the session and some good questions too.
SlideShare Presentation This presentation requires your consent to load content from SlideShare (LinkedIn), which may set tracking cookies. 
- [Slides: Virtual Tech Days - Monitoring & Managing remote Windows 7](/blog/microsoft-virtual-tech-days-monitoring-managing-remote-windows-7-desktops-with-powershell-slides-scripts/index.md): Early this month, I did an online webcast session for MS Virtual Tech Days. This session was around how administrators can manage remote Windows 7 desktops with Windows PowerShell. Here are the slides from that session.
SlideShare Presentation This presentation requires your consent to load content from SlideShare (LinkedIn), which may set tracking cookies. 
- [Monitoring file creation using WMI and PowerEvents module](/blog/monitoring-file-creation-using-wmi-and-powerevents-module/index.md): There are several ways we can create a file monitoring script using PowerShell. There is also a cmdlet in PowerShellPack called Start-FileSystemWatcher to monitor file /folder changes. However, none of these methods survive a exit at the console or wherever the script is running. This is because all these methods create a temporary event consumer. As I’d mentioned in an earlier post, Trevor’s PowerEvents module makes it very easy to create permanent event consumers in PowerShell. In today’s post, we shall look at how we can do that.

- [Creating complex scheduled tasks using WMI Timer events and PowerEvents Module](/blog/creating-complex-scheduled-tasks-using-wmi-timer-events-and-powerevents-module/index.md): A few weeks ago, I wrote about WMI Timer events using Win32_LocalTime and then mentioned how to work around the DayOfWeek issue. In today’s post, I will show you how to use WMI timer events to create complex scheduled tasks.
As system administrators, you may have to create scheduled jobs for performing various sysadmin tasks. We generally use Task Scheduler for such jobs. However, using the regular OS task scheduler, there is no easy way to create a scheduled task that occurs — for example — every Thursday of every fourth week of a month in the third quarter of every year.

- [WMI Query Language (WQL) – Event Queries: Extrinsic Events](/blog/wmi-query-language-wql-event-queries-extrinsic-events/index.md): In this part of the WQL series, we shall look at extrinsic events.
Extrinsic events represent events that do not directly link to standard WMI model. For example, Windows registry defines extrinsic events for all registry change events. For intrinsic events, having a WMI provider isn’t mandatory. This is mostly because they are defined within the standard WMI model and WMI takes care of these if there is no WMI provider for a given resource in the standard WMI model. However, since extrinsic events are outside of the standard WMI model, having a WMI provider is mandatory.

- [WMI Query Language (WQL) – Schema Queries](/blog/wmi-query-language-wql-schema-queries/index.md): In this last and final part of this series, we will look at how to use WQL for querying the WMI schema.
Schema queries are used to retrieve class definitions (rather than class instances) and schema associations. In simple words, if you need to find out what type of information (this is what schema really means) a specific class holds, you use schema queries.Here is an example of a schema query:

- [WMI Query Language (WQL) – Event Queries: Intrinsic Events](/blog/wmi-query-language-wql--event-queries-intrinsic-events/index.md): Intrinsic events are used to monitor a resource represented by a class in the CIM repository. In other words, the intrinsic events occur in response to a change in the standard WMI data model. WMI creates intrinsic events for objects stored in the WMI repository. A provider generates intrinsic events for dynamic classes, but WMI can create an instance for a dynamic class if no provider is available. WMI uses polling to detect the changes.

- [WMI Query Language (WQL) – Event Queries: Syntax](/blog/wmi-query-language-wql-event-queries-syntax/index.md): The WMI query syntax for event queries is a bit different and deserves a discussion. So, before we delve in to the types of event queries, let us first look at the syntax for WQL event queries. As we discussed earlier, we use SELECT statement for event queries too. We can combine this with other keywords such as WITHIN, HAVING, and GROUP to change how we receive these WMI events.
Here is how a MSDN article shows the syntax for WMI event queries.

- [Workarounds for DayOfWeek Timer events when using Win32_LocalTime](/blog/workarounds-for-dayofweek-timer-events-when-using-win32_localtime/index.md): In my earlier post, I showed how Win32_LocalTime WMI class can be used to capture timer events. As mentioned there, WMI events can be quite helpful in creating complex scheduling tasks. For example, you can specify to run a script every Thursday of every fourth week of a month in the third quarter of every year. However, there is a bug in Win32_LocalTime that currently blocks this.
I created a support incident with MS and reported this bug to them. I got a response that this indeed is a bug and they provided a workaround to solve this temporarily.

- [WMI Timer Events in PowerShell using Win32_LocalTime](/blog/wmi-timer-events-in-powershell-using-win32_localtime/index.md): This is not a part of the WQL series I am doing. I happend to take a look at the WMI timer events while providing feedback to an upcoming (cool) PowerEvents module by Trevor (@pcgeek86). BTW, this module will be released on November 30th. So, watch out for the annoncement.
Coming to the subject of this post, in WMI, there are 3 types of events possible. They are Timer events, Intrinsic events, and extrinsic events. My WQL series will soon cover intrinsic and extrensic events. Very few people have written about WMI timer events in the past but this particular post on The SysAdmins blog discusses good amount of details.

- [WMI Query Language (WQL) – Event Queries: Introduction](/blog/wmi-query-language-wql--event-queries-introduction/index.md): In this post, I will write a bit about basics of WMI events and how Register-WMIEvent cmdlet can be used. To start with, here is an excerpt from Microsoft Scripting guide that introduces WMI events:
Just as there is a WMI class that represents each type of system resource that can be managed using WMI, there is a WMI class that represents each type of WMI event. When an event that can be monitored by WMI occurs, an instance of the corresponding WMI event class is created. A WMI event occurs when that instance is created.

- [WMI Query Language (WQL) – Data Queries: References Of](/blog/wmi-query-language-wql-data-queries-references-of/index.md): Per MSDN documentation, the REFERENCES OF statement
Retrieves all association instances that refer to a particular source instance. The REFERENCES OF statement is similar to the ASSOCIATORS OF statement in its syntax. However, rather than retrieving endpoint instances, it retrieves the intervening association instances.
That is very cryptic for beginners like you and me. So, let us look at an example to understand this.
If you look at the above diagram (captured from the associations tab of Win32_Process in CIM Studio) and as I showed you in my earlier post, Win32_SessionProcess, in32_NamedJobObjectProcesses, Win32_SystemProcesses are the associator or association classes. Whereas, Win32_Process, Win32_LogonSession, Win32_NamedObject, and Win32_ComputerSystem are the associated classes.

- [WMI Query Language (WQL) – Data Queries: Associators Of](/blog/wmi-query-language-wql-data-queries-associators-of/index.md): As we saw in the previous post, Select queries can be used to retrieve instances of WMI class. But select queries are not the only way to query for instances. We can also use Associators Of keyword to the same. However, there is a difference. Select queries always return a collection of instances of a WMI class where as “Associators Of” returns a collection of WMI objects that belong to different WMI classes or associated WMI classes. Before we dig too much in to this, let us first understand what are associated WMI classes.

- [WMI Query Language (WQL) – Data Queries: SELECT, FROM, and WHERE](/blog/wmi-query-language-wql-data-queries-select-from-and-where/index.md): In this part of the series on WQL, we will look at what are data queries and how some of the WQL keywords &amp; operators can be used to retrieve information from WMI repository. Also, as mentioned earlier, there are many other tools that consume WQL queries to retrieve information from WMI. However, in this series, I shall use only PowerShell to demonstrate WQL.
WQL data queries are the most simplest form of querying for WMI data. Data queries are used to retrieve class instances and data associations. For example,

- [WMI Query Language (WQL) – Keywords and Operators](/blog/wmi-query-language-wql-keywords-and-operators/index.md): In this post, we will look at the a brief description of WQL keywords and operators and see a classification of the keywords based on where (query types) these keywords can be used.
Keywords Similar to SQL, WQL queries use keywords to retrieve data from the management objects. WQL has 19 keywords to perform these queries against WMI repositories. In the previous post, we discussed about three types of queries: Data, Event, and Schema. Though there are 19 WQL keywords, only a few of them can be used all 3 possible query types. The following table lists all the WQL keywords and lists the query type in which they can be used.

- [WMI Query Language (WQL) – An introduction](/blog/wmi-query-language-wql-an-introduction/index.md): I have been using WMI a lot these days and got to play with WQL as well. In this series of posts, I want to write about how WMI Query Language (WQL) can be used to retrieve management data exposed by WMI. Yes, this has nothing to do with PowerShell but as a PowerShell lover (and a MVP now), I will use PowerShell for all my examples.
Windows Management Instrumentation (WMI) is the Microsoft implementation of Web-based Enterprise Management (WBEM), which is an industry initiative to develop a standard technology for accessing management information in an enterprise environment. WMI uses the Common Information Model (CIM) industry standard to represent systems, applications, networks, devices, and other managed components. CIM is developed and maintained by the Distributed Management Task Force (DMTF). We can write WMI scripts to automate several tasks on local or remote computer(s).

- [Slides: Bangalore ITPro - PowerShell for ActiveDirectory Administration](/blog/slides-from-bangalore-itpro-barcamp-powershell-for-activedirectory-administration/index.md): I was at Bangalore ITPro BarCamp today and did a session on “PowerShell for Active Directory”. Here are the slides from that session.
SlideShare Presentation This presentation requires your consent to load content from SlideShare (LinkedIn), which may set tracking cookies. 
- [Slides: PSBUG Meet - PowerShell 2.0 remoting](/blog/psbug-ug-meet-slides-powershell-2-0-remoting/index.md): We had the BITPro / PowerShell Bangalore User Group UG meet today. I presented PowerShell 2.0 remoting. Here are the slides I used at this UG meet.
SlideShare Presentation This presentation requires your consent to load content from SlideShare (LinkedIn), which may set tracking cookies. 
- [Slides: Getting Started with PowerShell scripting](/blog/slides-getting-started-with-powershell-scripting/index.md): I just came back from the PowerShell / Bangalore ITPro UG meet. We had nearly 40 people showing up for this event. I spoke about getting started with PowerShell scripting and Ismail talked about Exchange 2010 migration. Here are the slides I used for my session.
SlideShare Presentation This presentation requires your consent to load content from SlideShare (LinkedIn), which may set tracking cookies. 
- [PowerShell 2.0 remoting guide: Part 12 – Using CredSSP for multi-hop authentication](/blog/powershell-2-0-remoting-guide-part-12--using-credssp-for-multi-hop-authentication/index.md): In this part of the remoting series, we look at how CredSSP can be used for multi-hop authentication in PowerShell remoting. CredSSP and multi-hop support are not features of PowerShell 2.0 or PowerShell remoting, per se. Credential Security Service Provider (CredSSP) is a new security service provider that enables an application to delegate the user’s credentials from the client to the target server. Multi-hop support in Windows Remote Management uses CredSSP for authentication. Since PowerShell 2.0 remoting is built on top of WinRM, we can use CredSSP to perform multi-hop authentication.

- [PowerShell 2.0 remoting guide: Part 11 – Interpreting, formatting and displaying remote output](/blog/powershell-2-0-remoting-guide-part-11--interpreting-formatting-and-displaying-remote-output/index.md): In this part of the remoting series, we look at remoting output. This includes how the output is transferred from remote computer to local, how it is displayed and how we can format this output based on a need. We already discussed various methods to execute commands (part4, part 5 and part 6) on a remote computer. In this post, for the sake of our discussion of remoting output, I will use only Invoke-Command method to execute remote commands. However, I will point out the differences as required.

- [PowerShell 2.0 remoting guide: Part 10 – Restricting available commands using custom session configuration](/blog/powershell-2-0-remoting-guide-part-10--restricting-available-commands-using-custom-session-configuration/index.md): “With great power comes great responsibility”, said uncle Ben. But some people don’t just understand that. That is when you have to rip-off their powers. Similarly, the default PS Session configuration allows full access to PowerShell language, cmdlets, scripts and everything available to PowerShell. Of course, you need to authenticate as a local administrator or should have execute permission to invoke the session. Running a few cmdlets such as Stop-Service or Restart-Computer can be quite dangerous on a production server. This is where a custom session configuration can help provide role based access to remote host using PowerShell remoting.

- [PowerShell 2.0 remoting guide: Part 9 – Session configurations and creating custom configurations](/blog/powershell-2-0-remoting-guide-part-9--session-configurations-and-creating-custom-configurations/index.md): In part2 of this series on PowerShell remoting we quickly looked at various cmdlets that form part of overall remoting infrastructure. The list there included cmdlets related to PS Session configuration. Now that we have gone through the basics of remoting, it is time for us to dig in to these additional cmdlets and explore what they really do.
So, in this part, we will look at all the PS session configurtion cmdlets, discuss how to create custom PS Session configurations and the need for it. Let us dive in to this now.

- [PowerShell 2.0 remoting guide: Part 8 – remoting scenarios and troubleshooting](/blog/powershell-2-0-remoting-guide-part-8--remoting-scenarios-and-troubleshooting/index.md): If you have been following this remoting series and using PS remoting on a test setup or even production, you must have gone through a few initial hiccups. In fact, there could be many such issues if you are working in a mixed environment where you have a few computers in a domain and a few in workgroup. So, in this part of the remoting series I will discuss some of these issues and workarounds to resolve the same. I discussed enabling remoting on workgroup computers in part3 – “Enable remoting” of this series. This post is kind of an extension to that.

- [PowerShell 2.0 remoting guide: Part 7 – saving remote session to disk](/blog/powershell-2-0-remoting-guide-part-7--saving-remote-session-to-disk/index.md): In part 6 of the remoting series we looked at how we can use Import-PSSession cmdlet to execute remote commands as if they were local. This is nice but this will last only while the session ($s in the example) is alive. The moment we kill the session — using Rem0ve-PSSession, remoting session will also get killed. In this part of the series, we will look at how we can save a remoting session to disk so that we don’t even have to explicitly create a session.

- [PowerShell 2.0 remoting guide: Part 6 – implicit remoting sessions](/blog/powershell-2-0-remoting-guide-part-6-implicit-remoting-sessions/index.md): In an earlier post on interactive remoting sessions, we looked at how we can enter a remote session and then execute commands as if they were local. However, if you’d observed it more closely, we were actually sitting in the remote session than local console. The change in PowerShell prompt indicates this fact clearly.
In this part of the remoting series, we will look at a feature called implicit remoting which makes it possible to run the commands / scripts on the remote computer while in the local session.

- [PowerShell 2.0 remoting guide: Part 5 – interactive remoting sessions](/blog/powershell-2-0-remoting-guide-part-5-interactive-remoting-sessions/index.md): In this part of the remoting series, I will talk about using Enter-PSSession and Exit-PSSession cmdlets to perform interactive remoting.
Why do you need interactive remoting?
To understand the advantages of interactive remoting in PowerShell 2.0, let us first look at some gotchas with Invoke-Command. Take an example of a remote system where SharePoint 2010 is installed. SharePoint 2010 provides native PowerShell cmdlets and these cmdlets can be accesses only if you load Microsoft.SharePoint.PowerShell PS snap-in. So, to do this using Invoke-Command

- [PowerShell 2.0 remoting guide: Part 4 – Execute commands or scripts on a remote computer using Invoke-Command](/blog/powershell-2-0-remoting-guide-part-4--execute-commands-or-scripts-on-a-remote-computer-using-invoke-command/index.md): In this part of PowerShell remoting series, I will discuss how to run commands or scripts on remote computer(s). Within remoting, there are couple of ways to run commands or scripts on a remote machine. This includes Invoke-Command cmdlet and interactive remoting sessions. These two methods deserve a separate post for each and hence I will discuss the Invoke-Command method in today&rsquo;s post.
Once you have enabled remoting on all your computers, you can use Invoke-Command cmdlet to run commands and scripts on local computer or on remote computer(s). There are many possible variations of this cmdlet. I will cover most of them here.

- [PowerShell 2.0 remoting guide: Part 3 – Enable remoting](/blog/powershell-2-0-remoting-guide-part-3--enable-remoting/index.md): In this part of the series of articles on PowerShell 2.0 remoting, we will look at how to enable remoting in different scenarios. This post assumes that you are running a supported operating system and you have installed all necesary pre-requisite software.
So, how do you enable remoting?
Remoting in PowerShell 2.0 can be enabled by just running the following cmdlet
Enable-PSRemoting Note: You have to run this at a elevated PowerShell prompt. Also, all your active networks should be set to “Home” or “Work” network location. Setting firewall exceptions for remoting will fail if the network location is set to “Public”.

- [PowerShell 2.0 remoting guide: Part 2 – Overview of remoting cmdlets](/blog/powershell-2-0-remoting-guide-part-2--overview-of-remoting-cmdlets/index.md): In part 1 of this series I gave a quick introduction to PowerShell 2.0 remoting. Before we look at how to enable or configure a computer for remoting, let us take a quick look at PowerShell 2.0 remoting cmdlets. Here is a complete list of cmdlets with a brief overview.
This list will also include cmdlets that are not directly used within remoting but help configure various aspects of remoting. The knowledge of these cmdlets such as WSMan, etc in this list is not mandatory for basic usage of PowerShell remoting. In this post, I will only discuss what each of these cmdlets are capable of and list any gotchas. A detailed usage of these cmdlets will be discussed later in the series.

- [PowerShell 2.0 remoting guide: Part 1 – The basics](/blog/powershell-2-0-remoting-guide-part-1-the-basics/index.md): I am starting a series of articles on remoting feature of PowerShell 2.0. This is one of the best features of PowerShell 2.0 and my favorite feature for sure. The number of very cool things one can achieve using this feature is just un-imaginable. I have started digging deep in to this feature as I start writing a network file browser powerpack as a part of hands-on. I hope it is worth sharing what I learn by writing about it here. So, this is the first in that series of posts. In this post, we will look at absolute basics to start using PowerShell remoting.

