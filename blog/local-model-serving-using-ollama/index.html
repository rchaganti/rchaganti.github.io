<!DOCTYPE html>
<html lang="en-us">
<head>
  
  <script>
    (function() {
      const stored = localStorage.getItem('theme-preference');
      const prefersDark = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;
      const theme = stored || (prefersDark ? 'dark' : 'light');

      if (theme === 'dark') {
        document.documentElement.classList.add('dark');
      }
    })();
  </script>

  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  
  <title>Local model serving - Using Ollama | Ravikanth Chaganti</title>

  
  <meta name="description" content="As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.">
  <meta name="author" content="Ravikanth Chaganti">

  
  
    <meta name="robots" content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1">
  

  
  <meta property="og:title" content="Local model serving - Using Ollama">
  <meta property="og:description" content="As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.">
  <meta property="og:type" content="article">
  <meta property="og:url" content="https://ravichaganti.com/blog/local-model-serving-using-ollama/">
  <meta property="og:site_name" content="Ravikanth Chaganti">
  <meta property="og:locale" content="en_US">
  
  <meta property="og:image" content="https://ravichaganti.com/images/localllm.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="Local model serving - Using Ollama">

  
  
    <meta property="article:published_time" content="2025-07-14T00:00:00Z">
    <meta property="article:modified_time" content="2025-07-14T00:00:00Z">
    <meta property="article:author" content="Ravikanth Chaganti">
    <meta property="article:section" content="inferencing">
    
  

  
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="Local model serving - Using Ollama">
  <meta name="twitter:description" content="As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.">
  <meta name="twitter:image" content="https://ravichaganti.com/images/localllm.png">
  <meta name="twitter:site" content="@ravikanth">

  
  <link rel="canonical" href="https://ravichaganti.com/blog/local-model-serving-using-ollama/">

  
  <link rel="alternate" type="application/rss+xml" href="https://ravichaganti.com/index.xml">

  
  <link rel="icon" type="image/svg+xml" href="/favicon.svg">
  <link rel="alternate icon" type="image/x-icon" href="/favicon.ico">

  
  <link rel="stylesheet" href="/css/main.css">

  
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@400;500;600;700&family=JetBrains+Mono:wght@400;500;600&display=swap" rel="stylesheet">

  
  






<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Local model serving - Using Ollama",
  "description": "As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or ‚Ä¶",
  "articleBody": "\"As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\\nIn today\\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 PS C:\\\\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \\u0026#34;ollama [command] --help\\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\\nFor the model names, refer to the model registry.\\n1 2 3 4 5 6 7 8 9 10 11 PS C:\\\\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\\n1 2 3 4 5 6 7 PS C:\\\\\\u0026gt; ollama run llama3.2:3b \\u0026gt;\\u0026gt;\\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \\u0026gt;\\u0026gt;\\u0026gt; /bye PS C:\\\\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\\n1 2 3 PS C:\\\\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\\n1 2 3 4 5 6 PS C:\\\\Users\\\\ravik\\u0026gt; ollama run llama3.2:3b --keepalive 10m \\u0026gt;\\u0026gt;\\u0026gt; /bye PS C:\\\\Users\\\\ravik\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 PS C:\\\\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \\u0026#34;5m\\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \\u0026#34;5m\\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\\n1 2 PS C:\\\\\\u0026gt; $env:OLLAMA_HOST=\\u0026#34;127.0.0.1:8080\\u0026#34; PS C:\\\\\\u0026gt; ollama serve ‚ÑπÔ∏è The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails. Ollama GUI Ollama features a minimal GUI interface as well.\\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\\n1 2 3 4 5 6 7 8 9 10 11 from ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\\u0026#39;llama3.2:3b\\u0026#39;, messages=[ { \\u0026#39;role\\u0026#39;: \\u0026#39;user\\u0026#39;, \\u0026#39;content\\u0026#39;: \\u0026#39;In one sentence, what is a Llama?\\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\\n1 2 (.venv) PS C:\\\\\\u0026gt; python .\\\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\\n‚ÑπÔ∏è Last updated: 29th September 2025 \"",
  "url": "https:\/\/ravichaganti.com\/blog\/local-model-serving-using-ollama\/",
  "datePublished": "2025-07-14T00:00:00Z",
  "dateModified": "2025-07-14T00:00:00Z",
  "author": {
    "@type": "Person",
    "name": "Ravikanth Chaganti"
    ,"sameAs": ["https://twitter.com/ravikanth"]
  },
  "publisher": {
    "@type": "Person",
    "name": "Ravikanth Chaganti",
    "sameAs": ["https://twitter.com/ravikanth"]
  },
  
  "image": {
    "@type": "ImageObject",
    "url": "https:\/\/ravichaganti.com\/images\/localllm.png",
    "width": 1200,
    "height": 630
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https:\/\/ravichaganti.com\/blog\/local-model-serving-using-ollama\/"
  },
  
  "articleSection": "\"inferencing\"",
  
  
  
  "timeRequired": "PT6M",
  
  "inLanguage": "en-US",
  "isAccessibleForFree": true
}
</script>




<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position": 1,
      "name": "Home",
      "item": "https:\/\/ravichaganti.com\/"
    }
    ,
    {
      "@type": "ListItem",
      "position": 2,
      "name": "Blog",
      "item": "https:\/\/ravichaganti.com\/blog/"
    }
    
    ,
    {
      "@type": "ListItem",
      "position": 3,
      "name": "inferencing",
      "item": "https:\/\/ravichaganti.com\/categories/inferencing/"
    },
    {
      "@type": "ListItem",
      "position": 4,
      "name": "Local model serving - Using Ollama",
      "item": "https:\/\/ravichaganti.com\/blog\/local-model-serving-using-ollama\/"
    }
    
    
  ]
}
</script>


</head>

<body class="flex flex-col min-h-screen bg-gray-50 dark:bg-dark-bg">
  <header class="sticky top-0 z-50 bg-white dark:bg-dark-bg-secondary shadow-md relative">
  <nav class="container mx-auto px-4 py-4">
    <div class="flex items-center justify-between">
      
      <div class="flex items-center">
        <a href="https://ravichaganti.com/" class="flex items-center gap-3 text-gray-900 dark:text-dark-text hover:text-primary transition-colors no-underline group">
          
          <svg width="40" height="40" viewBox="0 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg" class="flex-shrink-0">
            <circle cx="20" cy="20" r="18" stroke="currentColor" stroke-width="2" class="group-hover:stroke-primary transition-colors"/>
            <text x="20" y="26" font-family="sans-serif" font-size="16" font-weight="bold" text-anchor="middle" fill="currentColor">RC</text>
          </svg>
          
          <div class="flex flex-col leading-tight">
            <span class="text-xs sm:text-sm font-semibold">Cloud Native</span>
            <span class="text-xs sm:text-sm font-semibold">Agentic AI</span>
            <span class="text-xs sm:text-sm font-semibold">Infrastructure</span>
          </div>
        </a>
      </div>

      
      <div class="hidden md:flex items-center space-x-8">
        
          <a href="/blog/" class="text-gray-700 dark:text-dark-text-secondary hover:text-primary font-medium transition-all no-underline hover:underline hover:decoration-blue-500 hover:decoration-2 hover:underline-offset-4">
            Blog
          </a>
        
          <a href="/books/" class="text-gray-700 dark:text-dark-text-secondary hover:text-primary font-medium transition-all no-underline hover:underline hover:decoration-blue-500 hover:decoration-2 hover:underline-offset-4">
            Books
          </a>
        
          <a href="/slides/" class="text-gray-700 dark:text-dark-text-secondary hover:text-primary font-medium transition-all no-underline hover:underline hover:decoration-blue-500 hover:decoration-2 hover:underline-offset-4">
            Slides
          </a>
        
          <a href="/videos/" class="text-gray-700 dark:text-dark-text-secondary hover:text-primary font-medium transition-all no-underline hover:underline hover:decoration-blue-500 hover:decoration-2 hover:underline-offset-4">
            Videos
          </a>
        
          <a href="/about/" class="text-gray-700 dark:text-dark-text-secondary hover:text-primary font-medium transition-all no-underline hover:underline hover:decoration-blue-500 hover:decoration-2 hover:underline-offset-4">
            About
          </a>
        
      </div>

      
      <div class="hidden md:flex items-center space-x-4" id="social-search-container">
        
        <div id="social-icons" class="flex items-center space-x-3">
          

<a href="https://twitter.com/ravikanth" target="_blank" rel="noopener noreferrer" class="p-2 hover:bg-gray-100 dark:hover:bg-dark-bg-tertiary rounded-full transition-colors" aria-label="Twitter">
  <svg class="w-5 h-5 text-gray-700 dark:text-dark-text hover:text-primary dark:hover:text-blue-400" fill="currentColor" viewBox="0 0 24 24">
    <path d="M23.953 4.57a10 10 0 01-2.825.775 4.958 4.958 0 002.163-2.723c-.951.555-2.005.959-3.127 1.184a4.92 4.92 0 00-8.384 4.482C7.69 8.095 4.067 6.13 1.64 3.162a4.822 4.822 0 00-.666 2.475c0 1.71.87 3.213 2.188 4.096a4.904 4.904 0 01-2.228-.616v.06a4.923 4.923 0 003.946 4.827 4.996 4.996 0 01-2.212.085 4.936 4.936 0 004.604 3.417 9.867 9.867 0 01-6.102 2.105c-.39 0-.779-.023-1.17-.067a13.995 13.995 0 007.557 2.209c9.053 0 13.998-7.496 13.998-13.985 0-.21 0-.42-.015-.63A9.935 9.935 0 0024 4.59z"/>
  </svg>
</a>



<a href="https://github.com/rchaganti" target="_blank" rel="noopener noreferrer" class="p-2 hover:bg-gray-100 dark:hover:bg-dark-bg-tertiary rounded-full transition-colors" aria-label="GitHub">
  <svg class="w-5 h-5 text-gray-700 dark:text-dark-text hover:text-primary dark:hover:text-blue-400" fill="currentColor" viewBox="0 0 24 24">
    <path fill-rule="evenodd" d="M12 2C6.477 2 2 6.484 2 12.017c0 4.425 2.865 8.18 6.839 9.504.5.092.682-.217.682-.483 0-.237-.008-.868-.013-1.703-2.782.605-3.369-1.343-3.369-1.343-.454-1.158-1.11-1.466-1.11-1.466-.908-.62.069-.608.069-.608 1.003.07 1.531 1.032 1.531 1.032.892 1.53 2.341 1.088 2.91.832.092-.647.35-1.088.636-1.338-2.22-.253-4.555-1.113-4.555-4.951 0-1.093.39-1.988 1.029-2.688-.103-.253-.446-1.272.098-2.65 0 0 .84-.27 2.75 1.026A9.564 9.564 0 0112 6.844c.85.004 1.705.115 2.504.337 1.909-1.296 2.747-1.027 2.747-1.027.546 1.379.202 2.398.1 2.651.64.7 1.028 1.595 1.028 2.688 0 3.848-2.339 4.695-4.566 4.943.359.309.678.92.678 1.855 0 1.338-.012 2.419-.012 2.747 0 .268.18.58.688.482A10.019 10.019 0 0022 12.017C22 6.484 17.522 2 12 2z" clip-rule="evenodd"/>
  </svg>
</a>



<a href="https://linkedin.com/in/rchaganti" target="_blank" rel="noopener noreferrer" class="p-2 hover:bg-gray-100 dark:hover:bg-dark-bg-tertiary rounded-full transition-colors" aria-label="LinkedIn">
  <svg class="w-5 h-5 text-gray-700 dark:text-dark-text hover:text-primary dark:hover:text-blue-400" fill="currentColor" viewBox="0 0 24 24">
    <path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144 0-2.063-.926-2.063-2.065 0-1.138.92-2.063 2.063-2.063 1.14 0 2.064.925 2.064 2.063 0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"/>
  </svg>
</a>




        </div>

        
        <button
          id="theme-toggle"
          class="p-2 hover:bg-gray-100 dark:hover:bg-dark-bg-tertiary rounded-full transition-colors"
          aria-label="Toggle dark mode"
          title="Toggle dark mode"
        >
          
          <svg class="theme-icon-sun w-5 h-5 text-gray-700 dark:text-dark-text hidden" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 3v1m0 16v1m9-9h-1M4 12H3m15.364 6.364l-.707-.707M6.343 6.343l-.707-.707m12.728 0l-.707.707M6.343 17.657l-.707.707M16 12a4 4 0 11-8 0 4 4 0 018 0z"></path>
          </svg>

          
          <svg class="theme-icon-moon w-5 h-5 text-gray-700 dark:text-dark-text" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M20.354 15.354A9 9 0 018.646 3.646 9.003 9.003 0 0012 21a9.003 9.003 0 008.354-5.646z"></path>
          </svg>
        </button>

        
        <button id="search-toggle" class="p-2 hover:bg-gray-100 dark:hover:bg-dark-bg-tertiary rounded-full transition-colors" aria-label="Search">
          <svg class="w-5 h-5 text-gray-700 dark:text-dark-text" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M21 21l-6-6m2-5a7 7 0 11-14 0 7 7 0 0114 0z"></path>
          </svg>
        </button>

        
        <div id="search-box" class="hidden">
          
<div class="relative">
  <input
    type="text"
    id="search-input"
    placeholder="Search articles..."
    class="w-64 px-4 py-2 text-sm border border-gray-300 dark:border-dark-border bg-white dark:bg-dark-bg-secondary text-gray-900 dark:text-dark-text rounded-lg focus:outline-none focus:ring-2 focus:ring-primary"
    aria-label="Search"
    autocomplete="off"
  >
  <div id="search-results" class="absolute top-full mt-2 w-96 bg-white dark:bg-dark-bg-secondary border border-gray-200 dark:border-dark-border rounded-lg shadow-xl hidden max-h-96 overflow-y-auto z-50">
    
  </div>
</div>


<script src="https://cdn.jsdelivr.net/npm/fuse.js@7.0.0"></script>


<script>
(function() {
  let headerFuse;
  let headerSearchData = [];

  
  fetch('/index.json')
    .then(response => response.json())
    .then(data => {
      headerSearchData = data;

      
      const options = {
        keys: [
          { name: 'title', weight: 3 },
          { name: 'excerpt', weight: 2 },
          { name: 'content', weight: 1 },
          { name: 'categories', weight: 1.5 },
          { name: 'series', weight: 1.5 }
        ],
        threshold: 0.4,
        includeScore: true,
        ignoreLocation: true,
        minMatchCharLength: 2
      };

      headerFuse = new Fuse(headerSearchData, options);
    })
    .catch(error => console.error('Error loading search index:', error));

  
  const searchInput = document.getElementById('search-input');
  const searchResults = document.getElementById('search-results');

  if (searchInput && searchResults) {
    searchInput.addEventListener('input', function(e) {
      const query = e.target.value.trim();

      if (query.length < 2) {
        searchResults.classList.add('hidden');
        return;
      }

      
      const results = headerFuse.search(query);

      
      if (results.length === 0) {
        searchResults.innerHTML = '<div class="p-4 text-center text-gray-500 text-sm">No results found</div>';
        searchResults.classList.remove('hidden');
        return;
      }

      searchResults.innerHTML = results.slice(0, 5).map(result => {
        const item = result.item;
        const date = new Date(item.date).toLocaleDateString('en-US', { month: 'short', day: 'numeric', year: 'numeric' });

        return `
          <a href="${item.permalink}" class="block p-3 hover:bg-gray-50 border-b border-gray-100 last:border-b-0 no-underline">
            <div class="font-medium text-sm text-gray-900 mb-1">${item.title}</div>
            <div class="flex items-center gap-2 mb-1">
              <span class="text-xs text-gray-500">${date}</span>
              ${item.categories ? item.categories.slice(0, 2).map(cat => `
                <span class="inline-block px-1.5 py-0.5 text-xs rounded" style="background-color: rgba(30, 64, 175, 0.1); color: #1e40af;">
                  ${cat}
                </span>
              `).join('') : ''}
            </div>
            ${item.excerpt ? `<p class="text-xs text-gray-600 line-clamp-2">${item.excerpt}</p>` : ''}
          </a>
        `;
      }).join('');

      searchResults.classList.remove('hidden');
    });

    
    document.addEventListener('click', function(e) {
      if (!searchInput.contains(e.target) && !searchResults.contains(e.target)) {
        searchResults.classList.add('hidden');
      }
    });

    
    searchInput.addEventListener('keydown', function(e) {
      if (e.key === 'Escape') {
        searchResults.classList.add('hidden');
        searchInput.blur();
      }
    });
  }
})();
</script>

        </div>
      </div>

      
      <button id="mobile-menu-toggle" class="md:hidden p-2 hover:bg-gray-100 rounded" aria-label="Menu">
        <svg class="w-6 h-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16M4 18h16"></path>
        </svg>
      </button>
    </div>

    
    <div id="mobile-menu" class="hidden md:hidden mt-4 pb-4">
      
        <a href="/blog/" class="block py-3 px-2 text-base text-gray-700 dark:text-dark-text-secondary hover:text-primary hover:bg-gray-100 dark:hover:bg-dark-bg-tertiary transition-colors no-underline rounded-md min-h-[44px] flex items-center">
          Blog
        </a>
      
        <a href="/books/" class="block py-3 px-2 text-base text-gray-700 dark:text-dark-text-secondary hover:text-primary hover:bg-gray-100 dark:hover:bg-dark-bg-tertiary transition-colors no-underline rounded-md min-h-[44px] flex items-center">
          Books
        </a>
      
        <a href="/slides/" class="block py-3 px-2 text-base text-gray-700 dark:text-dark-text-secondary hover:text-primary hover:bg-gray-100 dark:hover:bg-dark-bg-tertiary transition-colors no-underline rounded-md min-h-[44px] flex items-center">
          Slides
        </a>
      
        <a href="/videos/" class="block py-3 px-2 text-base text-gray-700 dark:text-dark-text-secondary hover:text-primary hover:bg-gray-100 dark:hover:bg-dark-bg-tertiary transition-colors no-underline rounded-md min-h-[44px] flex items-center">
          Videos
        </a>
      
        <a href="/about/" class="block py-3 px-2 text-base text-gray-700 dark:text-dark-text-secondary hover:text-primary hover:bg-gray-100 dark:hover:bg-dark-bg-tertiary transition-colors no-underline rounded-md min-h-[44px] flex items-center">
          About
        </a>
      
    </div>
  </nav>

  
  
    <div id="reading-progress" class="absolute bottom-0 left-0 w-full h-1 bg-primary transition-all duration-200 z-10" style="width: 0%; max-width: 100%;"></div>
  
</header>


<script>
  document.getElementById('mobile-menu-toggle')?.addEventListener('click', function() {
    const menu = document.getElementById('mobile-menu');
    menu?.classList.toggle('hidden');
  });

  document.getElementById('search-toggle')?.addEventListener('click', function() {
    const socialIcons = document.getElementById('social-icons');
    const searchBox = document.getElementById('search-box');
    socialIcons?.classList.toggle('hidden');
    searchBox?.classList.toggle('hidden');
  });
</script>


  <main class="flex-grow">
    
<article class="container mx-auto px-4 sm:px-6 md:px-4 py-8 sm:py-10 md:py-12">
  <div class="max-w-4xl mx-auto">
    
    <header class="mb-8">
      
      <nav class="text-xs sm:text-sm mb-4">
        <ol class="flex items-center space-x-2 text-gray-600 dark:text-dark-text-secondary">
          <li><a href="https://ravichaganti.com/" class="hover:text-primary dark:hover:text-blue-400">Home</a></li>
          <li><span class="mx-2">/</span></li>
          <li><a href="/blog/" class="hover:text-primary dark:hover:text-blue-400">Blog</a></li>
          
            <li><span class="mx-2">/</span></li>
            <li><a href="/categories/inferencing/" class="hover:text-primary dark:hover:text-blue-400">inferencing</a></li>
          
        </ol>
      </nav>

      
      <div class="flex flex-wrap items-center gap-2 sm:gap-3 md:gap-4 text-sm sm:text-base text-gray-600 dark:text-dark-text-secondary mb-6">
        
        <div class="flex items-center">
          <svg class="w-4 h-4 sm:w-5 sm:h-5 mr-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 7V3m8 4V3m-9 8h10M5 21h14a2 2 0 002-2V7a2 2 0 00-2-2H5a2 2 0 00-2 2v12a2 2 0 002 2z"></path>
          </svg>
          <time datetime="2025-07-14">July 14, 2025</time>
        </div>

        
        
          <span>‚Ä¢</span>
          <div class="flex items-center">
            <svg class="w-4 h-4 sm:w-5 sm:h-5 mr-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 8v4l3 3m6-3a9 9 0 11-18 0 9 9 0 0118 0z"></path>
            </svg>
            <span>6 min read</span>
          </div>
        

        
        
      </div>

      
      <div class="flex flex-wrap gap-2 mb-6">
        
          
            <a href="/categories/inferencing/" class="inline-block px-3 py-1 text-sm font-medium bg-primary/10 dark:bg-primary/20 text-primary dark:text-blue-400 rounded-full hover:bg-primary/20 dark:hover:bg-primary/30 transition-colors no-underline">
              inferencing
            </a>
          
            <a href="/categories/local-serving/" class="inline-block px-3 py-1 text-sm font-medium bg-primary/10 dark:bg-primary/20 text-primary dark:text-blue-400 rounded-full hover:bg-primary/20 dark:hover:bg-primary/30 transition-colors no-underline">
              local serving
            </a>
          
        
        
          
            <a href="/series/local-llm-serving/" class="inline-block px-3 py-1 text-sm font-medium bg-secondary/10 dark:bg-secondary/20 text-secondary dark:text-purple-400 rounded-full hover:bg-secondary/20 dark:hover:bg-secondary/30 transition-colors no-underline">
              üìö Series: Local LLM Serving
            </a>
          
        
      </div>

      
      <div class="mb-6 sm:mb-8 rounded-lg overflow-hidden shadow-lg">
        
        <img src="/images/localllm.png" alt="Local model serving - Using Ollama" class="w-full h-auto max-h-64 sm:max-h-96 md:max-h-none object-cover">
      </div>

      
      
        <div class="text-base sm:text-lg md:text-xl text-gray-700 dark:text-dark-text-secondary italic border-l-4 border-primary dark:border-blue-400 pl-4 sm:pl-6 py-2 mb-6 sm:mb-8 bg-gray-50 dark:bg-dark-bg-tertiary rounded-r">
          There are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.
        </div>
      
    </header>

    
    


  
  
  
    <aside class="bg-gradient-to-br from-secondary/5 to-primary/5 dark:from-secondary/10 dark:to-primary/10 rounded-lg shadow-md dark:shadow-2xl dark:shadow-black/20 p-4 mb-6 border border-secondary/20 dark:border-secondary/30">
      
      <button
        class="flex items-center justify-between w-full text-left font-semibold text-base text-gray-900 dark:text-dark-text hover:text-secondary dark:hover:text-purple-400 transition-colors"
        aria-expanded="false"
        aria-controls="series-content"
      >
        <span class="flex items-center">
          <svg class="w-4 h-4 mr-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 6.253v13m0-13C10.832 5.477 9.246 5 7.5 5S4.168 5.477 3 6.253v13C4.168 18.477 5.754 18 7.5 18s3.332.477 4.5 1.253m0-13C13.168 5.477 14.754 5 16.5 5c1.747 0 3.332.477 4.5 1.253v13C19.832 18.477 18.247 18 16.5 18c-1.746 0-3.332.477-4.5 1.253"></path>
          </svg>
          Series: Local LLM Serving
        </span>
        <svg
          id="series-chevron"
          class="w-4 h-4 transform transition-transform -rotate-90"
          fill="none"
          stroke="currentColor"
          viewBox="0 0 24 24"
        >
          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"></path>
        </svg>
      </button>

      
      <p class="text-xs text-gray-600 dark:text-dark-text-secondary mt-2 mb-2 collapsed" id="series-description">
        Part of a 4-part series
      </p>

      
      <nav id="series-content" class="series-content collapsed mt-1">
        <ol class="space-y-3">
          
            <li class="flex items-start">
              <span class="flex-shrink-0 w-6 h-6 rounded-full bg-secondary/20 dark:bg-secondary/30 text-secondary dark:text-purple-400 flex items-center justify-center text-xs font-bold mr-3 mt-0.5">
                1
              </span>
              <div class="flex-grow">
                
                  <span class="font-bold text-secondary dark:text-purple-400">
                    Local model serving - Using Ollama
                    <span class="text-xs ml-2 px-2 py-0.5 bg-secondary dark:bg-purple-600 text-white rounded">You are here</span>
                  </span>
                
                <p class="text-xs text-gray-500 dark:text-dark-text-muted mt-1">Jul 14, 2025</p>
              </div>
            </li>
          
            <li class="flex items-start">
              <span class="flex-shrink-0 w-6 h-6 rounded-full bg-secondary/20 dark:bg-secondary/30 text-secondary dark:text-purple-400 flex items-center justify-center text-xs font-bold mr-3 mt-0.5">
                2
              </span>
              <div class="flex-grow">
                
                  <a href="https://ravichaganti.com/blog/local-model-serving-using-lm-studio/" class="text-gray-700 dark:text-dark-text-secondary hover:text-secondary dark:hover:text-purple-400 transition-colors no-underline hover:underline">
                    Local model serving - Using LM Studio
                  </a>
                
                <p class="text-xs text-gray-500 dark:text-dark-text-muted mt-1">Jul 15, 2025</p>
              </div>
            </li>
          
            <li class="flex items-start">
              <span class="flex-shrink-0 w-6 h-6 rounded-full bg-secondary/20 dark:bg-secondary/30 text-secondary dark:text-purple-400 flex items-center justify-center text-xs font-bold mr-3 mt-0.5">
                3
              </span>
              <div class="flex-grow">
                
                  <a href="https://ravichaganti.com/blog/local-model-serving-using-docker-model-runner/" class="text-gray-700 dark:text-dark-text-secondary hover:text-secondary dark:hover:text-purple-400 transition-colors no-underline hover:underline">
                    Local model serving - Using Docker model runner
                  </a>
                
                <p class="text-xs text-gray-500 dark:text-dark-text-muted mt-1">Jul 16, 2025</p>
              </div>
            </li>
          
            <li class="flex items-start">
              <span class="flex-shrink-0 w-6 h-6 rounded-full bg-secondary/20 dark:bg-secondary/30 text-secondary dark:text-purple-400 flex items-center justify-center text-xs font-bold mr-3 mt-0.5">
                4
              </span>
              <div class="flex-grow">
                
                  <a href="https://ravichaganti.com/blog/local-model-serving-using-foundry-local/" class="text-gray-700 dark:text-dark-text-secondary hover:text-secondary dark:hover:text-purple-400 transition-colors no-underline hover:underline">
                    Local model serving - Using Foundry Local
                  </a>
                
                <p class="text-xs text-gray-500 dark:text-dark-text-muted mt-1">Jul 17, 2025</p>
              </div>
            </li>
          
        </ol>

        
        <div class="mt-6 pt-6 border-t border-secondary/20 dark:border-secondary/30 grid grid-cols-2 gap-4">
          
          
            
              
            
          
            
          
            
          
            
          

          
          
            <div></div>
          

          
          
            
            <a href="https://ravichaganti.com/blog/local-model-serving-using-lm-studio/" class="flex items-center justify-end text-sm text-gray-700 dark:text-dark-text-secondary hover:text-secondary dark:hover:text-purple-400 transition-colors no-underline group">
              <span class="truncate">Local model serving - Using LM Studio</span>
              <svg class="w-4 h-4 ml-1 group-hover:translate-x-1 transition-transform" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7"></path>
              </svg>
            </a>
          
        </div>
      </nav>
    </aside>
  



    
    



    
    <div class="prose prose-sm sm:prose-base md:prose-lg max-w-none mb-8 sm:mb-10 md:mb-12">
      <p>As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.</p>
<p>There are several options available for running Large Language Model (LLM) inference locally. <a href="https://ollama.com/">Ollama</a> is one such option and my favorite among all. Ollama offers access to a <a href="https://ollama.com/search">wide range of models</a> and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.</p>
<p>In today&rsquo;s article, we will learn about Ollama and explore its capabilities.</p>
<h2 id="get-started">Get started</h2>
<p><a href="https://ollama.com/">Ollama</a> provides the easiest way to run LLMs on your local machine. <a href="https://ollama.com/download">Ollama is available</a> on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">PS C:<span class="se">\&gt;</span> ollama
</span></span><span class="line"><span class="cl">Usage:
</span></span><span class="line"><span class="cl">  ollama <span class="o">[</span>flags<span class="o">]</span>
</span></span><span class="line"><span class="cl">  ollama <span class="o">[</span>command<span class="o">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Available Commands:
</span></span><span class="line"><span class="cl">  serve       Start ollama
</span></span><span class="line"><span class="cl">  create      Create a model
</span></span><span class="line"><span class="cl">  show        Show information <span class="k">for</span> a model
</span></span><span class="line"><span class="cl">  run         Run a model
</span></span><span class="line"><span class="cl">  stop        Stop a running model
</span></span><span class="line"><span class="cl">  pull        Pull a model from a registry
</span></span><span class="line"><span class="cl">  push        Push a model to a registry
</span></span><span class="line"><span class="cl">  signin      Sign in to ollama.com
</span></span><span class="line"><span class="cl">  signout     Sign out from ollama.com
</span></span><span class="line"><span class="cl">  list        List models
</span></span><span class="line"><span class="cl">  ps          List running models
</span></span><span class="line"><span class="cl">  cp          Copy a model
</span></span><span class="line"><span class="cl">  rm          Remove a model
</span></span><span class="line"><span class="cl">  <span class="nb">help</span>        Help about any <span class="nb">command</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Flags:
</span></span><span class="line"><span class="cl">  -h, --help      <span class="nb">help</span> <span class="k">for</span> ollama
</span></span><span class="line"><span class="cl">  -v, --version   Show version information
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Use <span class="s2">&#34;ollama [command] --help&#34;</span> <span class="k">for</span> more information about a command.
</span></span></code></pre></td></tr></table>
</div>
</div><p>To download a model, you can use <code>ollama pull</code> or <code>ollama run</code>. The <code>run</code> command will load the model if it is already available in the local cache. If not, it will download the model weights.</p>
<blockquote>
<p>For the model names, refer to the <a href="https://ollama.com/search">model registry</a>.</p>
</blockquote>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">PS C:<span class="se">\&gt;</span> ollama pull llama3.2:3b
</span></span><span class="line"><span class="cl">pulling manifest
</span></span><span class="line"><span class="cl">pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB
</span></span><span class="line"><span class="cl">pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB
</span></span><span class="line"><span class="cl">pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB
</span></span><span class="line"><span class="cl">pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB
</span></span><span class="line"><span class="cl">pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   <span class="m">96</span> B
</span></span><span class="line"><span class="cl">pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  <span class="m">561</span> B
</span></span><span class="line"><span class="cl">verifying sha256 digest
</span></span><span class="line"><span class="cl">writing manifest
</span></span><span class="line"><span class="cl">success
</span></span></code></pre></td></tr></table>
</div>
</div><p>The downloaded model can be served using the <code>run</code> command.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">PS C:<span class="se">\&gt;</span> ollama run llama3.2:3b
</span></span><span class="line"><span class="cl">&gt;&gt;&gt; In one sentence, what is a Llama?
</span></span><span class="line"><span class="cl">A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known <span class="k">for</span> its
</span></span><span class="line"><span class="cl">distinctive long neck, soft fur, and gentle temperament.
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">&gt;&gt;&gt; /bye
</span></span><span class="line"><span class="cl">PS C:<span class="se">\&gt;</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>You can exit the chat interface by specifying <code>/bye</code> as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">PS C:<span class="se">\&gt;</span> ollama ps
</span></span><span class="line"><span class="cl">NAME           ID              SIZE      PROCESSOR    CONTEXT    UNTIL       
</span></span><span class="line"><span class="cl">llama3.2:3b    a80c4f17acd5    2.8 GB    100% CPU     <span class="m">4096</span>       <span class="m">2</span> minutes from now
</span></span></code></pre></td></tr></table>
</div>
</div><p>As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the <code>--keepalive</code> optional parameter.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">PS C:<span class="se">\U</span>sers<span class="se">\r</span>avik&gt; ollama run llama3.2:3b --keepalive 10m
</span></span><span class="line"><span class="cl">&gt;&gt;&gt; /bye
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">PS C:<span class="se">\U</span>sers<span class="se">\r</span>avik&gt; ollama ps
</span></span><span class="line"><span class="cl">NAME           ID              SIZE      PROCESSOR    CONTEXT    UNTIL
</span></span><span class="line"><span class="cl">llama3.2:3b    a80c4f17acd5    2.8 GB    100% CPU     <span class="m">4096</span>       <span class="m">9</span> minutes from now
</span></span></code></pre></td></tr></table>
</div>
</div><p>Besides the <code>ollama run</code> and <code>ollama pull</code> commands, you can also a serve a model using the <code>ollama serve</code> command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">PS C:<span class="se">\&gt;</span> ollama serve -h
</span></span><span class="line"><span class="cl">Start ollama
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Usage:
</span></span><span class="line"><span class="cl">  ollama serve <span class="o">[</span>flags<span class="o">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Aliases:
</span></span><span class="line"><span class="cl">  serve, start
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Flags:
</span></span><span class="line"><span class="cl">  -h, --help   <span class="nb">help</span> <span class="k">for</span> serve
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Environment Variables:
</span></span><span class="line"><span class="cl">      OLLAMA_DEBUG               Show additional debug information <span class="o">(</span>e.g. <span class="nv">OLLAMA_DEBUG</span><span class="o">=</span>1<span class="o">)</span>
</span></span><span class="line"><span class="cl">      OLLAMA_HOST                IP Address <span class="k">for</span> the ollama server <span class="o">(</span>default 127.0.0.1:11434<span class="o">)</span>
</span></span><span class="line"><span class="cl">      OLLAMA_CONTEXT_LENGTH      Context length to use unless otherwise specified <span class="o">(</span>default: 4096<span class="o">)</span>
</span></span><span class="line"><span class="cl">      OLLAMA_KEEP_ALIVE          The duration that models stay loaded in memory <span class="o">(</span>default <span class="s2">&#34;5m&#34;</span><span class="o">)</span>
</span></span><span class="line"><span class="cl">      OLLAMA_MAX_LOADED_MODELS   Maximum number of loaded models per GPU
</span></span><span class="line"><span class="cl">      OLLAMA_MAX_QUEUE           Maximum number of queued requests
</span></span><span class="line"><span class="cl">      OLLAMA_MODELS              The path to the models directory
</span></span><span class="line"><span class="cl">      OLLAMA_NUM_PARALLEL        Maximum number of parallel requests
</span></span><span class="line"><span class="cl">      OLLAMA_NOPRUNE             Do not prune model blobs on startup
</span></span><span class="line"><span class="cl">      OLLAMA_ORIGINS             A comma separated list of allowed origins
</span></span><span class="line"><span class="cl">      OLLAMA_SCHED_SPREAD        Always schedule model across all GPUs
</span></span><span class="line"><span class="cl">      OLLAMA_FLASH_ATTENTION     Enabled flash attention
</span></span><span class="line"><span class="cl">      OLLAMA_KV_CACHE_TYPE       Quantization <span class="nb">type</span> <span class="k">for</span> the K/V cache <span class="o">(</span>default: f16<span class="o">)</span>
</span></span><span class="line"><span class="cl">      OLLAMA_LLM_LIBRARY         Set LLM library to bypass autodetection
</span></span><span class="line"><span class="cl">      OLLAMA_GPU_OVERHEAD        Reserve a portion of VRAM per GPU <span class="o">(</span>bytes<span class="o">)</span>
</span></span><span class="line"><span class="cl">      OLLAMA_LOAD_TIMEOUT        How long to allow model loads to stall before giving up <span class="o">(</span>default <span class="s2">&#34;5m&#34;</span><span class="o">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>For example, if you want to run at API endpoint at port 8080, you can run the following command.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">PS C:<span class="se">\&gt;</span> <span class="nv">$env</span>:OLLAMA_HOST<span class="o">=</span><span class="s2">&#34;127.0.0.1:8080&#34;</span>
</span></span><span class="line"><span class="cl">PS C:<span class="se">\&gt;</span> ollama serve
</span></span></code></pre></td></tr></table>
</div>
</div><style>
.notice-shortcode.notice-info {
  background-color: #eff6ff;
  border-color: #bfdbfe;
  color: #1e3a8a;
}
.dark .notice-shortcode.notice-info {
  background-color: rgba(30, 58, 138, 0.2);
  border-color: #1e40af;
  color: #f1f5f9;
}
.notice-shortcode.notice-warning {
  background-color: #fefce8;
  border-color: #fde047;
  color: #854d0e;
}
.dark .notice-shortcode.notice-warning {
  background-color: rgba(133, 77, 14, 0.2);
  border-color: #ca8a04;
  color: #fef9c3;
}
.notice-shortcode.notice-tip {
  background-color: #f0fdf4;
  border-color: #86efac;
  color: #14532d;
}
.dark .notice-shortcode.notice-tip {
  background-color: rgba(20, 83, 45, 0.2);
  border-color: #16a34a;
  color: #dcfce7;
}
.notice-shortcode.notice-danger {
  background-color: #fef2f2;
  border-color: #fecaca;
  color: #7f1d1d;
}
.dark .notice-shortcode.notice-danger {
  background-color: rgba(127, 29, 29, 0.2);
  border-color: #dc2626;
  color: #fecaca;
}
.notice-shortcode.notice-success {
  background-color: #ecfdf5;
  border-color: #6ee7b7;
  color: #064e3b;
}
.dark .notice-shortcode.notice-success {
  background-color: rgba(6, 78, 59, 0.2);
  border-color: #059669;
  color: #d1fae5;
}
</style>

<div class="notice-shortcode notice-info my-6 p-4 border-l-4 rounded-r-lg">
  <div class="flex items-start gap-3">
    <span class="text-2xl flex-shrink-0" style="line-height: 1;">‚ÑπÔ∏è</span>
    <div class="flex-1 notice-content">
      The Ollama GUI runs in the background and it runs at port 11434. If you run <code>ollama serve</code> without setting the OLLAMA_HOST environment variable, the command fails.
    </div>
  </div>
</div>

<h2 id="ollama-gui">Ollama GUI</h2>
<p>Ollama features a minimal GUI interface as well.</p>
<p><figure class="figure " style="max-width: 100%; margin: 1.5rem auto;">
  <a href="/images/ollama-1.png" class="glightbox" data-gallery="post-images" data-glightbox="description: ">
    <img src="/images/ollama-1.png" alt="" loading="lazy" style="width: 100%; height: auto; border-radius: 0.5rem; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);">
  </a></figure>
 
 <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/glightbox@3.3.0/dist/css/glightbox.min.css">
 
 
 <script src="https://cdn.jsdelivr.net/npm/glightbox@3.3.0/dist/js/glightbox.min.js"></script>
 
 
 <script>
   document.addEventListener('DOMContentLoaded', function() {
     const lightbox = GLightbox({
       selector: '.glightbox',
       touchNavigation: true,
       loop: true,
       autoplayVideos: true,
       zoomable: true,
       draggable: true,
       closeButton: true,
       moreLength: 0
     });
   });
 </script></p>
<p>I like this GUI and use it often to quickly chat with a few models running locally or in the cloud!</p>
<h2 id="ollama-api">Ollama API</h2>
<p>You can programmatically interact with the model using the <a href="https://docs.ollama.com/api">Ollama API</a>. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the <a href="https://github.com/ollama/ollama-python">Ollama Python SDK</a>.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">ollama</span> <span class="kn">import</span> <span class="n">chat</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">ollama</span> <span class="kn">import</span> <span class="n">ChatResponse</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">response</span><span class="p">:</span> <span class="n">ChatResponse</span> <span class="o">=</span> <span class="n">chat</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s1">&#39;llama3.2:3b&#39;</span><span class="p">,</span> <span class="n">messages</span><span class="o">=</span><span class="p">[</span>
</span></span><span class="line"><span class="cl">  <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="s1">&#39;user&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="s1">&#39;In one sentence, what is a Llama?&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="p">},</span>
</span></span><span class="line"><span class="cl"><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>When you run this, you will see the response from the model.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl"><span class="o">(</span>.venv<span class="o">)</span> PS C:<span class="se">\&gt;</span> python .<span class="se">\0</span>1-get-started.py
</span></span><span class="line"><span class="cl">A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known <span class="k">for</span> its distinctive appearance, soft wool, and gentle temperament.
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="cloud-models">Cloud models</h2>
<p>With the <a href="https://ollama.com/blog/cloud-models">recent update</a>, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.</p>
<ul>
<li>qwen3-coder:480b-cloud</li>
<li>gpt-oss:120b-cloud</li>
<li>gpt-oss:20b-cloud</li>
<li>deepseek-v3.1:671b-cloud</li>
</ul>
<p>These models can be accessed at the command line as well as the cloud.</p>
<figure class="figure " style="max-width: 100%; margin: 1.5rem auto;">
  <a href="/images/ollama-2.png" class="glightbox" data-gallery="post-images" data-glightbox="description: ">
    <img src="/images/ollama-2.png" alt="" loading="lazy" style="width: 100%; height: auto; border-radius: 0.5rem; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);">
  </a></figure>

<p>Overall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.</p>
<style>
.notice-shortcode.notice-info {
  background-color: #eff6ff;
  border-color: #bfdbfe;
  color: #1e3a8a;
}
.dark .notice-shortcode.notice-info {
  background-color: rgba(30, 58, 138, 0.2);
  border-color: #1e40af;
  color: #f1f5f9;
}
.notice-shortcode.notice-warning {
  background-color: #fefce8;
  border-color: #fde047;
  color: #854d0e;
}
.dark .notice-shortcode.notice-warning {
  background-color: rgba(133, 77, 14, 0.2);
  border-color: #ca8a04;
  color: #fef9c3;
}
.notice-shortcode.notice-tip {
  background-color: #f0fdf4;
  border-color: #86efac;
  color: #14532d;
}
.dark .notice-shortcode.notice-tip {
  background-color: rgba(20, 83, 45, 0.2);
  border-color: #16a34a;
  color: #dcfce7;
}
.notice-shortcode.notice-danger {
  background-color: #fef2f2;
  border-color: #fecaca;
  color: #7f1d1d;
}
.dark .notice-shortcode.notice-danger {
  background-color: rgba(127, 29, 29, 0.2);
  border-color: #dc2626;
  color: #fecaca;
}
.notice-shortcode.notice-success {
  background-color: #ecfdf5;
  border-color: #6ee7b7;
  color: #064e3b;
}
.dark .notice-shortcode.notice-success {
  background-color: rgba(6, 78, 59, 0.2);
  border-color: #059669;
  color: #d1fae5;
}
</style>

<div class="notice-shortcode notice-info my-6 p-4 border-l-4 rounded-r-lg">
  <div class="flex items-start gap-3">
    <span class="text-2xl flex-shrink-0" style="line-height: 1;">‚ÑπÔ∏è</span>
    <div class="flex-1 notice-content">
      Last updated: 29th September 2025
    </div>
  </div>
</div>


    </div>

    
    

    
    

<div class="mt-12 pt-8 border-t border-gray-200 dark:border-dark-border">
  <h3 class="text-lg font-bold mb-4 text-center text-gray-900 dark:text-dark-text">Share this article</h3>
  <div class="flex flex-wrap justify-center items-center gap-3 sm:gap-4">
    
    <button
      onclick="shareNative()"
      class="sm:hidden w-12 h-12 rounded-full bg-primary hover:bg-primary/90 dark:bg-blue-600 dark:hover:bg-blue-700 text-white flex items-center justify-center transition-colors shadow-md hover:shadow-lg"
      aria-label="Share"
      title="Share"
      id="native-share-btn"
    >
      <svg class="w-5 h-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
        <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8.684 13.342C8.886 12.938 9 12.482 9 12c0-.482-.114-.938-.316-1.342m0 2.684a3 3 0 110-2.684m0 2.684l6.632 3.316m-6.632-6l6.632-3.316m0 0a3 3 0 105.367-2.684 3 3 0 00-5.367 2.684zm0 9.316a3 3 0 105.368 2.684 3 3 0 00-5.368-2.684z"></path>
      </svg>
    </button>

    
    <a
      href="https://twitter.com/intent/tweet?url=https%3A%2F%2Fravichaganti.com%2Fblog%2Flocal-model-serving-using-ollama%2F&text=Local&#43;model&#43;serving&#43;-&#43;Using&#43;Ollama"
      target="_blank"
      rel="noopener noreferrer"
      class="w-12 h-12 rounded-full bg-[#1DA1F2] hover:bg-[#1a8cd8] text-white flex items-center justify-center transition-all no-underline shadow-md hover:shadow-lg hover:scale-105"
      aria-label="Share on Twitter"
      title="Share on Twitter"
    >
      <svg class="w-5 h-5" fill="currentColor" viewBox="0 0 24 24">
        <path d="M23.953 4.57a10 10 0 01-2.825.775 4.958 4.958 0 002.163-2.723c-.951.555-2.005.959-3.127 1.184a4.92 4.92 0 00-8.384 4.482C7.69 8.095 4.067 6.13 1.64 3.162a4.822 4.822 0 00-.666 2.475c0 1.71.87 3.213 2.188 4.096a4.904 4.904 0 01-2.228-.616v.06a4.923 4.923 0 003.946 4.827 4.996 4.996 0 01-2.212.085 4.936 4.936 0 004.604 3.417 9.867 9.867 0 01-6.102 2.105c-.39 0-.779-.023-1.17-.067a13.995 13.995 0 007.557 2.209c9.053 0 13.998-7.496 13.998-13.985 0-.21 0-.42-.015-.63A9.935 9.935 0 0024 4.59z"/>
      </svg>
    </a>

    
    <a
      href="https://www.linkedin.com/sharing/share-offsite/?url=https%3A%2F%2Fravichaganti.com%2Fblog%2Flocal-model-serving-using-ollama%2F"
      target="_blank"
      rel="noopener noreferrer"
      class="w-12 h-12 rounded-full bg-[#0077B5] hover:bg-[#006399] text-white flex items-center justify-center transition-all no-underline shadow-md hover:shadow-lg hover:scale-105"
      aria-label="Share on LinkedIn"
      title="Share on LinkedIn"
    >
      <svg class="w-5 h-5" fill="currentColor" viewBox="0 0 24 24">
        <path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144 0-2.063-.926-2.063-2.065 0-1.138.92-2.063 2.063-2.063 1.14 0 2.064.925 2.064 2.063 0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"/>
      </svg>
    </a>

    
    <a
      href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fravichaganti.com%2Fblog%2Flocal-model-serving-using-ollama%2F"
      target="_blank"
      rel="noopener noreferrer"
      class="w-12 h-12 rounded-full bg-[#1877F2] hover:bg-[#1564d3] text-white flex items-center justify-center transition-all no-underline shadow-md hover:shadow-lg hover:scale-105"
      aria-label="Share on Facebook"
      title="Share on Facebook"
    >
      <svg class="w-5 h-5" fill="currentColor" viewBox="0 0 24 24">
        <path d="M24 12.073c0-6.627-5.373-12-12-12s-12 5.373-12 12c0 5.99 4.388 10.954 10.125 11.854v-8.385H7.078v-3.47h3.047V9.43c0-3.007 1.792-4.669 4.533-4.669 1.312 0 2.686.235 2.686.235v2.953H15.83c-1.491 0-1.956.925-1.956 1.874v2.25h3.328l-.532 3.47h-2.796v8.385C19.612 23.027 24 18.062 24 12.073z"/>
      </svg>
    </a>

    
    <button
      onclick="copyToClipboard('https:\/\/ravichaganti.com\/blog\/local-model-serving-using-ollama\/')"
      class="w-12 h-12 rounded-full bg-accent hover:bg-amber-600 dark:bg-amber-500 dark:hover:bg-amber-600 text-white flex items-center justify-center transition-all shadow-md hover:shadow-lg hover:scale-105"
      aria-label="Copy link"
      title="Copy link"
      id="copy-link-btn"
    >
      <svg class="w-5 h-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
        <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 16H6a2 2 0 01-2-2V6a2 2 0 012-2h8a2 2 0 012 2v2m-6 12h8a2 2 0 002-2v-8a2 2 0 00-2-2h-8a2 2 0 00-2 2v8a2 2 0 002 2z"></path>
      </svg>
    </button>
  </div>
</div>

<script>

function shareNative() {
  if (navigator.share) {
    navigator.share({
      title: 'Local model serving - Using Ollama',
      text: 'There are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.',
      url: 'https:\/\/ravichaganti.com\/blog\/local-model-serving-using-ollama\/'
    }).catch(function(error) {
      console.log('Error sharing:', error);
    });
  }
}


function copyToClipboard(text) {
  navigator.clipboard.writeText(text).then(function() {
    const btn = document.getElementById('copy-link-btn');
    const originalHTML = btn.innerHTML;
    const originalTitle = btn.title;

    
    btn.innerHTML = '<svg class="w-5 h-5" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 13l4 4L19 7"></path></svg>';
    btn.title = 'Copied!';

    
    setTimeout(function() {
      btn.innerHTML = originalHTML;
      btn.title = originalTitle;
    }, 2000);
  }).catch(function(error) {
    console.error('Failed to copy:', error);
    alert('Failed to copy link. Please copy manually: ' + text);
  });
}
</script>



    
    
<nav class="mt-12 pt-8 border-t border-gray-200 dark:border-dark-border" aria-label="Post navigation">
  
  
  
  

  
  
  
    
    
    

    
    
    

    
    
    
      
        
      
    
      
    
      
    
      
    

    
    
      
      
        
        
      
    
  

  
  <div class="grid grid-cols-1 md:grid-cols-2 gap-4">
    
    <div class="flex">
      
        
        <div class="flex-1 card bg-gray-50 dark:bg-dark-bg-tertiary opacity-50 cursor-not-allowed">
          <div class="flex items-start gap-3">
            <div class="flex-shrink-0 text-gray-400 dark:text-dark-text-muted mt-1">
              <svg class="w-6 h-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M15 19l-7-7 7-7"></path>
              </svg>
            </div>
            <div class="flex-1">
              <p class="text-xs text-gray-400 dark:text-dark-text-muted mb-1 uppercase tracking-wide">No Previous Article</p>
              <p class="text-sm text-gray-400 dark:text-dark-text-muted">This is the first post</p>
            </div>
          </div>
        </div>
      
    </div>

    
    <div class="flex">
      
        <a href="https://ravichaganti.com/blog/local-model-serving-using-lm-studio/"
           class="flex-1 group card hover:shadow-xl transition-all hover:scale-[1.02] no-underline"
           aria-label="Next in Local LLM Serving: Local model serving - Using LM Studio">
          <div class="flex items-start gap-3">
            
            <div class="flex-1 min-w-0 text-right md:text-left md:order-1">
              <p class="text-xs text-gray-500 dark:text-dark-text-muted mb-1 uppercase tracking-wide">Next in Local LLM Serving</p>
              <h3 class="text-base font-bold text-gray-900 dark:text-dark-text mb-2 group-hover:text-primary dark:group-hover:text-blue-400 transition-colors">
                Local model serving - Using LM Studio
              </h3>
              
                <p class="text-sm text-gray-600 dark:text-dark-text-secondary line-clamp-2">There are several options available for running Large Language Model (LLM) inference locally. LM Studio is one such option. It is more comprehensive and offers some great features.</p>
              
            </div>

            
            <div class="flex-shrink-0 text-primary dark:text-blue-400 group-hover:text-secondary dark:group-hover:text-purple-400 transition-colors mt-1 md:order-2">
              <svg class="w-6 h-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7"></path>
              </svg>
            </div>
          </div>
        </a>
      
    </div>
  </div>
</nav>


    
    

    
    

    
    
  </div>

  
  <script src="/js/collapse.js"></script>
  <script src="/js/code-copy.js"></script>
</article>

  </main>

  <footer class="bg-gray-900 dark:bg-dark-bg text-gray-300 dark:text-dark-text-secondary mt-16">
  <div class="container mx-auto px-4 py-8">
    <div class="text-center text-sm">
      <p>&copy; 2026 Ravikanth Chaganti. All rights reserved.</p>
    </div>
  </div>
</footer>


  
  <button id="back-to-top"
          class="fixed bottom-8 right-8 w-12 h-12 bg-primary text-white rounded-full shadow-lg hover:bg-blue-700 transition-all opacity-0 pointer-events-none z-40"
          aria-label="Back to top"
          title="Back to top">
    <svg class="w-6 h-6 mx-auto" fill="none" stroke="currentColor" viewBox="0 0 24 24">
      <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 10l7-7m0 0l7 7m-7-7v18"></path>
    </svg>
  </button>

  
  

  
  <script>
    
    
    
  </script>

  
  
    <script src="/js/reading-progress.js"></script>
  

  
  <script src="/js/back-to-top.js"></script>

  
  <script src="/js/theme-toggle.js"></script>
</body>
</html>
