<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Inferencing on Ravikanth Chaganti</title>
    <link>https://ravichaganti.com/categories/inferencing/</link>
    <description>Recent content in Inferencing on Ravikanth Chaganti</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 17 Jul 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://ravichaganti.com/categories/inferencing/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Local model serving - Using Foundry Local</title>
      <link>https://ravichaganti.com/blog/local-model-serving-using-foundry-local/</link>
      <pubDate>Thu, 17 Jul 2025 00:00:00 +0000</pubDate>
      <guid>https://ravichaganti.com/blog/local-model-serving-using-foundry-local/</guid>
      <description>&lt;p&gt;In this series of articles on local model serving, we learned about using &lt;a href=&#34;https://ai-engineer.in/post/local-model-serving-using-ollama/&#34;&gt;Ollama&lt;/a&gt;, &lt;a href=&#34;https://ai-engineer.in/post/local-model-serving-using-docker-model-runner/&#34;&gt;Docker Model Runner&lt;/a&gt;, and &lt;a href=&#34;https://ai-engineer.in/post/local-model-serving-using-lmstudio/&#34;&gt;LMStudio&lt;/a&gt;. At Build 2025, &lt;a href=&#34;https://devblogs.microsoft.com/foundry/foundry-local-a-new-era-of-edge-ai/&#34;&gt;Microsoft announced Foundry Local&lt;/a&gt;. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a &lt;a href=&#34;https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/reference/reference-cli&#34;&gt;command-line interface&lt;/a&gt;, and offers an OpenAI API-compatible &lt;a href=&#34;https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/reference/reference-rest&#34;&gt;RESTful API&lt;/a&gt;. It also supports Python, C#, Rust, and JavaScript &lt;a href=&#34;https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/reference/reference-sdk&#34;&gt;SDKs for local AI model management&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Local model serving - Using Docker model runner</title>
      <link>https://ravichaganti.com/blog/local-model-serving-using-docker-model-runner/</link>
      <pubDate>Wed, 16 Jul 2025 00:00:00 +0000</pubDate>
      <guid>https://ravichaganti.com/blog/local-model-serving-using-docker-model-runner/</guid>
      <description>&lt;p&gt;In the local model serving landscape, we have already looked at &lt;a href=&#34;https://ravichaganti.com/post/local-model-serving-using-ollama/&#34;&gt;Ollama&lt;/a&gt; and &lt;a href=&#34;https://ravichaganti.com/post/local-model-serving-using-lmstudio/&#34;&gt;LM Studio&lt;/a&gt;. The other option I explored was &lt;a href=&#34;https://www.docker.com/blog/introducing-docker-model-runner/&#34;&gt;Docker Model Runner&lt;/a&gt; (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Serve models on OpenAI-compatible APIs&lt;/li&gt;&#xA;&lt;li&gt;Pull and push models to and from Docker Hub.&lt;/li&gt;&#xA;&lt;li&gt;Manage local models&lt;/li&gt;&#xA;&lt;li&gt;Run and interact with models both from the command line and the Docker Desktop GUI.&lt;/li&gt;&#xA;&lt;li&gt;Package model &lt;a href=&#34;https://huggingface.co/docs/hub/en/gguf&#34;&gt;GGUF&lt;/a&gt; files as OCI artifacts and publish them to any container registry&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the &lt;code&gt;docker-model&lt;/code&gt; CLI plugin.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Local model serving - Using LM Studio</title>
      <link>https://ravichaganti.com/blog/local-model-serving-using-lm-studio/</link>
      <pubDate>Tue, 15 Jul 2025 00:00:00 +0000</pubDate>
      <guid>https://ravichaganti.com/blog/local-model-serving-using-lm-studio/</guid>
      <description>&lt;p&gt;In an earlier article, we looked at &lt;a href=&#34;https://ravichaganti.com/post/local-model-serving-using-ollama/&#34;&gt;Ollama for serving models locally&lt;/a&gt;. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today&amp;rsquo;s article, we will go over some of these features.&lt;/p&gt;&#xA;&lt;p&gt;You can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Local model serving - Using Ollama</title>
      <link>https://ravichaganti.com/blog/local-model-serving-using-ollama/</link>
      <pubDate>Mon, 14 Jul 2025 00:00:00 +0000</pubDate>
      <guid>https://ravichaganti.com/blog/local-model-serving-using-ollama/</guid>
      <description>&lt;p&gt;As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
